\begin{chapter}{\'Algebra de matrices}\label{chap-algebra-de-matrices}
            
            En  este capítulo estudiaremos las operaciones que pueden en las  matrices y veremos propiedades algebraicas de las matrices,  en particular probaremos que dado $n\in \mathbb N$,  podemos definir una suma y un producto en el conjunto de matrices $n \times n$ con la propiedad de que estas operaciones satisfacen muchos de los axiomas que satisface  $\mathbb Z$ (sección \ref{seccion-operaciones-con-matrices}).

            Las operaciones entre matrices permiten explicar desde un punto algebraico la solución de sistemas de ecuaciones lineales (sección \ref{seccion-matrices-elementales}).

            \begin{section}{Algunos tipos de matrices}\label{seccion-algunos-tipos-de-matrices}
                
                \textbf{Matriz cuadrada.} Es aquella que tiene igual número de filas que de columnas, es decir si es una matriz $n \times n$ para algún $n \in\mathbb N$. En ese caso, se dice que la matriz es de orden $n$. Por ejemplo, la matriz
                \begin{equation*}
                A= \begin{bmatrix}
                1&3&0\\-1&4&7\\-2&0&1
                \end{bmatrix}
                \end{equation*}
                es cuadrada de orden $3$.
                
                Denotaremos el conjunto de todas las matrices cuadradas\index{matriz!cuadrada} de orden $n$ con entradas en $\K$ por $M_n(\K)$ o simplemente $M_n$ si $\K$  está sobreentendido. Así, en el     ejemplo anterior $A \in M_3$.
                
                Los elementos de la \textit{diagonal principal} de una matriz\index{matriz!diagonal principal}\index{diagonal principal de una matriz} cuadrada son aquellos que están situados
                en la diagonal que va desde la esquina superior izquierda hasta la inferior derecha. En otras
                palabras, la diagonal principal de una matriz $A=[a_{ij}]$ está formada por los elementos
                $a_{11},a_{22},\ldots,a_{nn}$.  En el ejemplo anterior la diagonal principal está compuesta por los elementos: $a_{11} = 1$, $a_{22} = 4$ , $a_{33} = 1$.

                \textbf{Matriz diagonal y matriz escalar.} Una matriz cuadrada, $A=[a_{ij}]$ de orden $n$, es \textit{diagonal}\index{matriz!diagonal} si $a_{ij}  =0$ , para $i \not= j$ . Es decir, si todos los elementos situados fuera de la diagonal principal son cero. Por ejemplo, la siguiente matriz es diagonal:
                \begin{equation}
                \begin{bmatrix}
                2&0&0&0\\0&-1&0&0\\0&0&5&0\\0&0&0&3
                \end{bmatrix}.
                \end{equation}
                
                Un matriz $n \times n$  es \textit{escalar}\index{matriz!escalar} si  es diagonal y todos los elementos de la diagonal son iguales, por ejemplo, en el caso $4 \times 4$ las matrices escalares son 
                
                \begin{equation}
                \begin{bmatrix}
                c&0&0&0\\0&c&0&0\\0&0&c&0\\0&0&0&c
                \end{bmatrix},
                \end{equation}
                con $c \in \K$.
                
                \textbf{Matriz unidad o identidad.} Esta matriz ya la hemos definido anteriormente. Recordemos que es una matriz diagonal cuya diagonal principal está compuesta de $1$'s. 
                
                Más adelante veremos que la matriz identidad, respecto a la multiplicación de matrices cuadradas, juega un papel similar al número $1$ respecto a la multiplicación de números reales o enteros (elemento neutro del producto).
                
                \textbf{Matriz nula}. La \textit{matriz nula}\index{matriz!nula} de orden $m\times n$, denotada $0_{m \times n}$ o simplemente $0$ si $m$ y $n$ están sobreentendidos,  es la  matriz $m \times n$ cuyas entradas son todas nulas ($=0$). 
                
                Por ejemplo, la matriz nula $2 \times 3$ es
                \begin{equation*}
                \begin{bmatrix} 0&0&0\\ 0&0&0\end{bmatrix}.
                \end{equation*}
                Veremos luego que la matriz nula juega un papel similar al número $0$ en el álgebra de matrices cuadradas (elemento neutro de la suma).  
                
                \textbf{Matriz triangular.} Una matriz cuadrada es \textit{triangular superior} o \textit{escalón}\index{matriz!triangular superior}\index{matriz!escalón} si todos los elementos situados por debajo de la diagonal principal son cero. Por ejemplo, la siguiente matriz es triangular superior:
                \begin{equation}
                \begin{bmatrix}
                2&-1&3&1\\0&-1&0&2\\0&0&5&1\\0&0&0&3
                \end{bmatrix}.
                \end{equation}
                Análogamente, una matriz cuadrada es \textit{triangular inferior}\index{matriz!triangular inferior} si todos los elementos situados por encima de la diagonal principal son cero. Un matriz triangular (superior o inferior) se dice \textit{estricta}\index{matriz!triangular inferior estricta}\index{matriz!triangular superior estricta} si la diagonal principal es $0$.
                
                En forma más precisa, sea $A =[a_{ij}]\in M_n(\K)$,  entonces
                \begin{itemize}
                    \item $A$ es \textit{triangular superior} (\textit{triangular superior estricta}) si $a_{ij} =0$ para $i < j$ (respectivamente $i \le j$),
                    \item $A$ es \textit{triangular inferior} (\textit{triangular inferior estricta}) si $a_{ij} =0$ para $i > j$ (respectivamente $i \ge j$).
                \end{itemize}
                
                Por  ejemplo, cualquier matriz diagonal es triangular superior y también triangular inferior. 
                
                No es difícil comprobar que si $R$ es una matriz cuadrada $n \times n$  que es una MERF,  entonces $R$  es triangular superior. 
            \end{section}
            
            \begin{section}{Operaciones con matrices}\label{seccion-operaciones-con-matrices}
                \begin{definicion} 
                    Sean $A=[a_{ij}]$, $B=[b_{ij}]$ matrices  $m \times n$. La matriz $C= [a_{ij} + b_{ij}]$ de orden $m \times n$,  es decir la matriz cuyo valor en la posición $ij$ es  $a_{ij} + b_{ij}$, es llamada \textit{la suma de las matrices $A$ y $B$}\index{suma de matrices} y se denota $A+B$.  
                \end{definicion}
                
                 En otras palabras, la suma de dos matrices es la matriz que resulta de sumar ``coordenada a coordenada'' ambas matrices. 
                
                Veamos un ejemplo, consideremos las siguientes matrices:
                \begin{equation*}
                A = \begin{bmatrix} -1&3&5 \\ 2&0&-1  \end{bmatrix}, \qquad
                B = \begin{bmatrix} 5&2&-3 \\ 0&-1&1  \end{bmatrix}, \qquad
                M = \begin{bmatrix} 2&1&-3 \\ 3&0&-1 \\ 0&8&5 \end{bmatrix}.
                \end{equation*}
                Las matrices $A$ y $B$ son de orden $2 \times 3$, mientras la matriz $M$ es cuadrada de orden 3. Por tanto, no podemos calcular la suma de $A$ y $M$ y tampoco la suma de $B$ y $M$, en cambio, sí podemos sumar $A$ y $B$ ya que tienen el mismo orden. Esto es,
                \begin{align*}
                A + B &= \begin{bmatrix} -1&3&5 \\ 2&0&-1  \end{bmatrix}+ \begin{bmatrix} 5&2&-3 \\ 0&-1&1  \end{bmatrix} \\
                &=  \begin{bmatrix} -1+5&3+2&5-3 \\ 2+0&0-1&-1+1  \end{bmatrix} \\
                &=  \begin{bmatrix} 4&5&2 \\ 2&-1&0  \end{bmatrix}
                \end{align*}
                
                
                Dadas $A$, $B$ y $C$ matrices $m \times n$, podemos deducir fácilmente las siguientes propiedades de la suma de matrices de matrices:
                \begin{itemize}
                    \item Conmutativa: $A + B = B + A$,
                    \item Asociativa: $A + (B + C) = (A + B) + C$,
                    \item Elemento neutro (la matriz nula): $A + 0 = 0 + A = A$,
                    \item Elemento opuesto: existe una matriz $-A$ de orden $m \times n$ tal que  $A + (-A) = (-A) + A = 0$.
                \end{itemize}
                Debemos explicitar la matriz opuesta: si $A = [a_{ij}]$,  entonces $-A = [-a_{ij}]$. Usualmente denotaremos $A + (-B)$ como $A-B$ y $(-A)+B$ como $-A+B$. 
                
                La demostración de las propiedades anteriores se deduce de que las mismas propiedades valen coordenada a coordenada y se dejan a cargo del lector.         
            
                \begin{definicion}
                    \index{producto de matrices} \index{multiplicación de matrices} Sean $A=[a_{ij}]$ matriz $m \times n$ y $B=[b_{ij}]$ matriz $n \times p$, entonces $C=[c_{ij}]$ matriz $m \times p$  es el \textit{producto} de $A$ y $B$, si 
                \begin{equation}\label{mtrx-mult}
                c_{ij} = a_{i1}b_{1j}+a_{i2}b_{2j}+\cdots+a_{in}b_{nj}= \sum_{k=1}^{n}a_{ik}b_{kj}.
                \end{equation}
                Es decir, los elementos que ocupan la posición $ij$ en la matriz producto, se obtienen sumando     los productos que resultan de multiplicar los elementos de la fila $i$ en la primera matriz por los
                elementos de la columna $j$ de la segunda matriz. Al producto de $A$ por $B$ lo denotamos $AB$. 
                \end{definicion}
            
                Es muy importante recalcar que por la definición, se puede multiplicar una matriz $m \times n$ por una matriz $r \times p$, sólo si $n=r$ y en ese caso, la multiplicación resulta ser una matriz $m \times p$. 

                Podemos visualizar la multiplicación así:
                \begin{align*}
                    \left[
                    \begin{array}{cccc}
                        a_{11} & a_{12} & \cdots & a_{1n}\\ 
                        a_{21} & a_{22} & \cdots & a_{2n}\\
                        \vdots & \vdots & & \vdots\\
                        \textcolor{red}{a_{i1}} & \textcolor{red}{a_{i2}} & \cdots & \textcolor{red}{a_{in}}\\  
                        \vdots & \vdots & & \vdots\\
                        a_{m1} & a_{m2} & \cdots & a_{mn}
                    \end{array}
                    \right] 
                    \cdot
                    \left[
                    \begin{array}{ccc}
                        \cdots & \textcolor{blue}{b_{1j}} & \cdots\\ 
                        \cdots & \textcolor{blue}{b_{2j}} & \cdots\\
                        & \textcolor{blue}{\vdots} & \\
                        \cdots & \textcolor{blue}{b_{nj}} & \cdots 
                    \end{array}
                    \right]
                    =
                    \left[
                    \begin{array}{ccc}
                        &\vdots&\\
                        \cdots&\sum_{k=1}^n\textcolor{red}{a_{ik}}\cdot
                        \textcolor{blue}{b_{kj}}&\cdots\\
                        &\vdots&
                    \end{array}
                    \right]
                \end{align*} 

                \begin{obs}\label{mtrx-filasxcols}
                     Sean $A=[a_{ij}]$ matriz $m \times n$ y $B=[b_{ij}]$ matriz $n \times p$, entonces si 
                     multiplicamos la matriz que se forma con la fila $i$ de $A$ por la matriz que determina la columna $j$ de $B$, obtenemos el coeficiente $ij$ de $AB$.
                    Esquemáticamente
                    \begin{equation*}
                    \begin{bmatrix} a_{i1}& a_{i2}& \cdots &a_{in}\end{bmatrix}
                    \begin{bmatrix} b_{1j}\\ b_{2j}\\ \vdots \\b_{nj}\end{bmatrix} =  \sum_{k=1}^{n}a_{ik}b_{kj} = c_{ij}.
                    \end{equation*}
                    Por lo tanto diremos a veces, que el coeficiente $ij$ de la matriz $AB$ es la fila $i$ de $A$ por la columna $j$ de $B$. 
                    
                    El lector recordará el producto escalar definido en  el capítulo \ref{chap-vectores} y notará que el coeficiente $ij$ de $AB$ es el producto escalar de la fila $i$ de A por la columna $j$ de B, ambos pensados como vectores. 
                \end{obs}    
                
                \begin{ejemplo*}
                    Si 
                    \begin{equation*}
                        A = \begin{bmatrix}1&0\\-3&1\end{bmatrix}, \qquad B = \begin{bmatrix}5&-1&2\\15&4&8\end{bmatrix},
                    \end{equation*}
                    como $A$ es $2 \times 2$ y $B$ es $2 \times 3$, la matriz $AB$ será $2 \times 3$ y  aplicando la regla \eqref{mtrx-mult}, obtenemos:
                    \begin{equation*}
                        AB = \begin{bmatrix}1\times  5 + 0\times 15&1\times (-1) + 0\times 4&1\times 2 + 0\times 8
                            \\-3\times 5 + 1\times 15&-3\times (-1) + 1\times 4&-3\times 2 + 1\times 8
                        \end{bmatrix} =
                        \begin{bmatrix} 5 &-1 &2 
                            \\ 0 &7 &2
                        \end{bmatrix}.
                    \end{equation*}
                    Observemos que, debido a nuestra definición, no es posible multiplicar $B$ por $A$, pues no está definido multiplicar una matriz $2 \times 3$ por una $2 \times 2$.
                \end{ejemplo*}
                     
                
                Como veremos en el siguiente ejemplo, en  el caso de matrices cuadradas se pueden calcular ambos productos, aunque muchas veces se obtienen resultados diferentes. Consideremos las siguientes matrices:
                \begin{equation*}
                A = \begin{bmatrix}2&1\\-3&1\end{bmatrix}, \qquad 
                B = \begin{bmatrix}1&3\\-1&1\end{bmatrix}
                \end{equation*}
                Entonces, por un lado,
                \begin{equation*}
                AB = \begin{bmatrix}2&1\\-3&1\end{bmatrix}
                \begin{bmatrix}1&3\\-1&1\end{bmatrix} =
                \begin{bmatrix}1&7\\-4&-8\end{bmatrix}, 
                \end{equation*}
                y por otro lado,
                \begin{equation*}
                BA = \begin{bmatrix}1&3\\-1&1\end{bmatrix} \begin{bmatrix}2&1\\-3&1\end{bmatrix}
                =\begin{bmatrix}-7&4\\-5&0\end{bmatrix}.
                \end{equation*}
                
                Según se pudo comprobar a través del ejemplo anterior, la multiplicación de matrices
                no cumple la propiedad conmutativa. Veamos algunas propiedades que sí cumple esta operación:
                \begin{itemize}
                    \item Asociativa: 
                    \begin{equation*}
                    A (B C) = (A B) C, \quad\forall\, A \in M_{m \times n}, \;B \in M_{n \times p}, \;C  \in M_{p \times q},
                    \end{equation*}
                    
                    \item Elemento neutro: si  $A$ es matriz $m \times n$,  entonces
                    \begin{equation*}
                    A\Id_{n} = A = \Id_{m}A,
                    \end{equation*}
                    \item Distributiva:
                    \begin{equation*}
                    A(B + C) = AB + AC,\qquad \forall\, A \in M_{m \times n}, \;B, C \in M_{n \times p},
                    \end{equation*}
                    y
                    \begin{equation*}
                    (A+ B)C = AC + BC,\qquad \forall\, A, B \in M_{m \times n}, \; C \in M_{n \times p}.
                    \end{equation*}
                \end{itemize}    
                
                Como en el caso de la suma,  la demostración las propiedades anteriores se deja a cargo del lector. 
                
                En virtud de estas propiedades y de las anteriores de la suma de matrices, resulta que el conjunto $(M_n ,+,.)$  de las matrices cuadradas de orden $n$, respecto a las dos leyes de composición interna, ``+'' y ``·'', tiene estructura de anillo unitario no conmutativo. En Wikipedia se puede encontrar un artículo al respecto:
                \begin{center}
                    \href{ https://es.wikipedia.org/wiki/Anillo\_(matemática)}{ https://es.wikipedia.org/wiki/Anillo\_(matemática)}.     
                \end{center}
                
                Cuando las matrices son cuadradas podemos multiplicarlas por si mismas y  definimos,  de forma análoga a lo que ocurre en los productos de números, la potencia de una matriz: sea $A$ matriz $n \times n$, y sea $m \in \mathbb N$ entonces
                \begin{equation*}
                A^0 = \Id_{n}, \quad A^m = A^{m-1}A,        
                \end{equation*}
                es decir $A^m$ es multiplicar $A$ consigo mismo $m$-veces.  
                
                \begin{observacion}\label{obs-mult-matrices-diagonales}
                    Un  caso especial de multiplicación es la multiplicación por matrices diagonales. Sea $n \in \mathbb N$ y 
                \begin{equation*}
                    \operatorname{diag}(d_1,d_2,\ldots,d_n) := \begin{bmatrix}
                    d_1 & 0 & \cdots &0 \\
                    0 & d_2 & \cdots &0 \\
                    \vdots & \vdots & \ddots &\vdots \\
                    0 & 0 & \cdots & d_n 
                    \end{bmatrix}
                \end{equation*}
                matriz $n \times n$ diagonal con valor $d_i$  en la posición $ii$,  entonces si $A$ es matriz $n \times p$,  con la multiplicación a izquierda de la matriz diagonal por $A$  se obtiene la matriz que en la fila $i$ tiene a la fila $i$ de $A$ multiplicada por $d_i$.  Es decir,
                \begin{equation*}
                \begin{bmatrix}
                d_1 & 0 & \cdots &0 \\
                0 & d_2 & \cdots &0 \\
                \vdots & \vdots & \ddots &\vdots \\
                0 & 0 & \cdots & d_n 
                \end{bmatrix}
                \begin{bmatrix}
                a_{11} & a_{12} & \cdots &a_{1p} \\
                a_{21} & a_{22} & \cdots &a_{2p}\\
                \vdots & \vdots & \ddots &\vdots \\
                a_{n1} & a_{n2} & \cdots & a_{np} 
                \end{bmatrix}
                =
                \begin{bmatrix}
                d_1a_{11} & d_1a_{12} & \cdots &d_1a_{1p} \\
                d_2a_{21} & d_2a_{22} & \cdots &d_2a_{2p}\\
                \vdots & \vdots & \ddots &\vdots \\
                d_n a_{n1} & d_n a_{n2} & \cdots & d_n a_{np} 
                \end{bmatrix}.
                \end{equation*}
                Esto es claro, pues si denotamos $D = \operatorname{diag}(d_1,d_2,\ldots,d_n)$, el coeficiente $ij$  de $DA$ es la fila $i$ de $D$ por la columna $j$ de $A$, es decir
                $$
                [DA]_{ij} = 0.a_{1j}+\cdots + 0.a_{i-1,j}+d_i.a_{ij}+0.a_{i+1,j}+\cdots + 0.a_{nj} = d_ia_{ij}. 
                $$
                Observar que en el caso de que $D$ sea una matriz escalar (es decir $d_1=d_2=\cdots=d_n$), $DA$ es multiplicar por el mismo número todos los coeficientes de $A$. En particular, en este caso, si $A$  es $n \times n$,  $DA = AD$.
                 
                
                
                 Si $B$ es $m \times n$, el lector podrá comprobar que 
                 \begin{equation*}
                     B\,\operatorname{diag}(d_1,d_2,\ldots,d_n) = \begin{bmatrix}
                         d_1C_1& d_2C_2& \cdots &d_nC_n
                     \end{bmatrix},
                 \end{equation*} 
                 donde $C_1,C_2,\ldots,C_n$ son las columnas de $B$.
                 
                 Finalmente,  de lo visto más arriba respecto a la multiplicación por una matriz diagonal obtenemos:
                 \begin{equation*}
                    \begin{bmatrix}
                        d_1 & 0 & \cdots &0 \\
                        0 & d_2 & \cdots &0 \\
                        \vdots & \vdots & \ddots &\vdots \\
                        0 & 0 & \cdots & d_n 
                        \end{bmatrix}^k =
                        \begin{bmatrix}
                            d_1^k & 0 & \cdots &0 \\
                            0 & d_2^k & \cdots &0 \\
                            \vdots & \vdots & \ddots &\vdots \\
                            0 & 0 & \cdots & d_n^k 
                            \end{bmatrix} ,
                 \end{equation*}
                 para $k \in \mathbb N$.
                
                \end{observacion}

                Otras observaciones importantes: 
                
                \begin{itemize}
                    \item multiplicar cualquier matriz por la matriz nula resulta la matriz nula,
                    \item existen divisores de cero: en general, $AB = 0$ no implica que $A = 0$ o $B = 0$  o,  lo que es lo mismo, el producto de matrices no nulas puede resultar en una matriz nula. Por
                    ejemplo,
                    \begin{equation*}
                    \begin{bmatrix}1&0\\2&0\end{bmatrix} \begin{bmatrix}0&0\\8&1\end{bmatrix} = \begin{bmatrix}0&0\\0&0\end{bmatrix}.
                    \end{equation*}
                    \item En general no se cumple la propiedad cancelativa: si $A\not=0$ y  $AB = AC$ no necesariamente se cumple que $B = C$. Por
                    ejemplo,
                    \begin{equation*}
                    \begin{bmatrix}2&0\\4&0\end{bmatrix}=
                    \begin{bmatrix}1&0\\2&0\end{bmatrix} \begin{bmatrix}2&0\\8&1\end{bmatrix} =
                    \begin{bmatrix}1&0\\2&0\end{bmatrix} \begin{bmatrix}2&0\\5&3\end{bmatrix}
                    \end{equation*}
                    \item No se cumple la fórmula del binomio:     sean $A, B$ matrices $n \times n$, entonces        
                    \begin{align*}
                    (A+B)^2 &= (A+B)(A+B) \\&= A(A+B) + B(A+B) \\&= AA + AB + BA + BB \\&= A^2 + AB + BA + B^2,
                    \end{align*}
                    y  esta última expresión puede no ser  igual a $A^2 + 2AB + B^2$ ya que el producto de matrices no es conmutativo (en general). 
                \end{itemize}
        
        
        \begin{subsection}{Multiplicaci\'on de una matriz por un escalar} Otra operación importante es la multiplicación de una matriz por un elemento de $\K$: sea $A=[a_{ij}]$ matriz $m \times n$ y $c \in \K$,  entonces el producto de $c$ por $A$ es la matriz
            $$
            cA=[ca_{ij}].
            $$ 
        Por ejemplo, 
        $$
        2\begin{bmatrix*}[r]
        -1& 0& 3 & 4\\
        5& 1& -2 & 1\\
        3& 2& 1 & -3
        \end{bmatrix*} =
        \begin{bmatrix*}[r]
        -2& 0& 6& 8\\
        10& 2& -4 & 2\\
        6& 4& 2 & -6
        \end{bmatrix*}.
        $$
        
        Observar que multiplicar por $c$ una matriz $m \times n$,  es lo mismo que multiplicar por la matriz escalar $m \times m$ con los coeficientes de la diagonal iguales a $c$,  es decir
        \begin{align*}
            cA &= \begin{bmatrix*}[r]
            c     &     0& \cdots & 0\\
            0     &     c& \cdots & 0\\
            \vdots&\vdots&  \ddots      &\vdots\\
            0     &    0  & \cdots & c
            \end{bmatrix*}
             \begin{bmatrix}
            a_{11}&a_{12}& \cdots & & a_{1n}\\
            a_{21}&a_{22}& \cdots & &a_{2n}\\
            \vdots&\vdots&  & &\vdots\\
            a_{m1}&a_{m2}& \cdots & & a_{mn}
            \end{bmatrix}  \\
            &=
             \begin{bmatrix}
            ca_{11}&ca_{12}& \cdots & & ca_{1n}\\
            ca_{21}&ca_{22}& \cdots & &ca_{2n}\\
            \vdots&\vdots&  & &\vdots\\
            ca_{m1}&ca_{m2}& \cdots & & ca_{mn}
            \end{bmatrix}
        \end{align*}
        
        Debido a esta observación y a las propiedades del producto de matrices, se cumple lo siguiente:
            \begin{align*}
             c(A B) = (cA) B,\qquad &\forall\, c \in \K,  A \in M_{m \times n}, \;B \in M_{n \times p}, \\
             (cd)A = c(dA),  \qquad &\forall\, c,d \in \K,  A \in M_{m \times n},, \\
            1.A = A ,\qquad&\forall\, c \in \K, A \in M_{m \times n}\\
            c(A + B) = cA + cB,\qquad& \forall\, c \in \K, A,B \in M_{m \times n}, \\
            (c+ d)A = cA + dA,\qquad& \forall\, c,d \in \K, \; A \in M_{m \times n}.
            \end{align*}
            
            Si $A$ es $n \times n$,  entonces $DA = AD$ cuando $D$  es una matriz escalar. Por lo tanto
            \begin{align*}
            c(A B) = (cA) B = A(cB),\qquad &\forall\, c \in \K,  A \in M_{n \times n}, \;B \in M_{n \times n}.
            \end{align*} 

        \end{subsection}
            
        \subsection*{$\S$ Ejercicios}
        \begin{enumex}
            \item Calcule las siguientes operaciones de matrices.
                \begin{enumex}
                    \begin{minipage}{0.45\textwidth}
                    \item $\begin{bmatrix} 2&1&4\\5&-6&7 \end{bmatrix}+
                    \begin{bmatrix} 1&1&1\\0&2&-2 \end{bmatrix}$,
                    \end{minipage}
                    \begin{minipage}{0.4\textwidth}
                    \item $3 \begin{bmatrix}
                        4&3&2\\-4&8&1
                    \end{bmatrix}$,
                    \end{minipage}

                    \begin{minipage}{0.45\textwidth}
                    \item $\begin{bmatrix}
                        2&1\\0&3
                    \end{bmatrix} + 3 \begin{bmatrix}
                        2&1\\-1&0
                    \end{bmatrix}$,
                    \end{minipage}
                    \begin{minipage}{0.4\textwidth}
                    \item $4\begin{bmatrix}
                        2&-1\\1&2
                    \end{bmatrix} + 3  \begin{bmatrix} 1&1\\0&2 \end{bmatrix}$. 
                    \end{minipage}
                \end{enumex}

            \item Calcule las siguientes operaciones de matrices o diga ``no está definida''.
                \begin{enumex}
                    \begin{minipage}{0.45\textwidth}
                    \item $4\cdot \begin{bmatrix}
                        2&-1\\1&2
                    \end{bmatrix}  \begin{bmatrix} 1&1\\0&2 \end{bmatrix}$,
                \end{minipage}
                \begin{minipage}{0.4\textwidth}
                    \item $\begin{bmatrix}
                        2&1\\0&3
                    \end{bmatrix}  \left(3 \begin{bmatrix}
                        2&1\\-1&0
                    \end{bmatrix}\right)$,
                \end{minipage}

                \begin{minipage}{0.45\textwidth}
                    \item $\begin{bmatrix} 2&1&4\\5&-6&7 \end{bmatrix}
                    \begin{bmatrix} 1&1&1\\0&2&-2\\-3&2&1 \end{bmatrix}$,
                \end{minipage}
                \begin{minipage}{0.4\textwidth}
                    \item $3 \begin{bmatrix}
                        4&3&2\\-4&8&1
                    \end{bmatrix}\begin{bmatrix}
                        2&1\\-1&0
                    \end{bmatrix}$. 
                \end{minipage}
                \end{enumex}

            \item Sean
            $$
            A= \begin{bmatrix} 1&-1\\ 3&-2 \end{bmatrix},\qquad
            B= \begin{bmatrix} 5&2\\ 4&4 \end{bmatrix},\qquad
            C= \begin{bmatrix} -2&3\\ -4&1 \end{bmatrix}.
            $$
            Calcular 
                \begin{enumex}
                    \begin{minipage}{0.2\textwidth}
                    \item $AB$, \end{minipage}
                    \begin{minipage}{0.2\textwidth}
                    \item $(AB)C$,\end{minipage}
                    \begin{minipage}{0.2\textwidth}
                    \item $BC$,\end{minipage}
                    \begin{minipage}{0.2\textwidth}
                    \item $A(BC)$.\end{minipage}
                \end{enumex}

            \item De lel tamaño del producto $AB$ o diga ``no está definido'' para:
            \begin{enumex}
                \item $A$ una matriz $2 \times 2$ y $B$ una matriz $2 \times 4$,
                \item $A$ una matriz $3 \times 3$ y $B$ una matriz $3 \times 3$,
                \item $A$ una matriz $3 \times 10$ y $B$ una matriz $10 \times 2$,
                \item $A$ una matriz $3 \times 2$ y $B$ una matriz $3 \times 2$.
            \end{enumex}

            \item\label{matriz-de-bloques} (Matrices  de bloques)  Si $k_1, k_2 \in \N$ y $A_{ij} \in \K^{k_i \times k_j}$, para $i,j=1,2$, entonces podemos combinar esas matrices en la matriz cuadrada
            $$
            A = \begin{bmatrix} A_{11}&A_{12}\\A_{21}&A_{22}  \end{bmatrix} \in \K^{(k_1 + k_2) \times (k_1 + k_2)}. 
            $$ 
            Diremos entonces que \textit{$A$ es una matriz de bloques  $k_1,k_2$.}
            
            Probar las siguientes fórmula para matrices de bloques:
            \begin{enumex}
                \item \begin{equation*}
                    \begin{bmatrix} A_{11}&A_{12}\\A_{21}&A_{22}  \end{bmatrix} + 
                    \begin{bmatrix} B_{11}&B_{12}\\B_{21}&B_{22}  \end{bmatrix} =
                    \begin{bmatrix} A_{11} + B_{11} & A_{12} + B_{12} \\ 
                        A_{21} + B_{21} & A_{22} + B_{22} \end{bmatrix}. 
                \end{equation*}
                \item \begin{equation*}
                    \begin{bmatrix} A_{11}&A_{12}\\A_{21}&A_{22}  \end{bmatrix}
                    \begin{bmatrix} B_{11}&B_{12}\\B_{21}&B_{22}  \end{bmatrix} =
                    \begin{bmatrix} A_{11}B_{11}+A_{12}B_{21} & A_{11}B_{12}+A_{12}B_{22} \\ 
                        A_{21}B_{11}+A_{22}B_{21} & A_{21}B_{12}+A_{22}B_{22} \end{bmatrix}. 
                \end{equation*}
                \item Si $c \in \K$,
                \begin{equation*}
                   c \begin{bmatrix} A_{11}&A_{12}\\A_{21}&A_{22}  \end{bmatrix} =
                     \begin{bmatrix} cA_{11}&cA_{12}\\cA_{21}&cA_{22}  \end{bmatrix}
                \end{equation*}
            \end{enumex}
            
        \end{enumex}
            
        \end{section}
        
        \begin{section}{Matrices elementales}\label{seccion-matrices-elementales}
            Veremos ahora la relación entre el álgebra de matrices y la solución de sistemas de ecuaciones lineales. 
            
            Primero recordemos que dado un sistema de $m$  ecuaciones lineales con $n$ incógnitas
            \begin{equation}\label{sist-eq-gen-3}
            \begin{matrix} 
            a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &y_1\\
            \vdots&  &\vdots& &&  &\vdots \\
            a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&y_m
            \end{matrix}
            \end{equation}
            donde $y_1, \ldots,y_m$ y $a_{i,j}$ ($1 \le i \le m$, $1 \le j \le n$) son números en $\K$. Si denotamos
            \begin{equation*}
            A = \begin{bmatrix}
            a_{11}& a_{12}& \cdots &a_{1n} \\
            \vdots&\vdots  &  &\vdots \\
            a_{m1} &a_{m2}&\cdots &a_{mn}\end{bmatrix},\qquad
            X = \begin{bmatrix}
            x_1 \\ \vdots \\ x_n 
            \end{bmatrix},
            \qquad 
            Y = \begin{bmatrix}
            y_1 \\ \vdots \\ y_n
            \end{bmatrix},
            \end{equation*}
            entonces 
            \begin{align*}
            AX &= \begin{bmatrix}
            a_{11}& a_{12}& \cdots &a_{1n} \\
            \vdots&\vdots  &  &\vdots \\
            a_{m1} &a_{m2}&\cdots &a_{mn}\end{bmatrix} \begin{bmatrix} 
            x_1 \\ \vdots \\ x_n 
            \end{bmatrix} \\
            &=
            \begin{bmatrix}
            a_{11}x_1+ a_{12}x_2+ \cdots +a_{1n}x_n \\
            \vdots \\
            a_{m1}x_1 +a_{m2}x_2+\cdots +a_{mn}x_n\end{bmatrix} =
            \begin{bmatrix}
            y_1 \\ \vdots \\ y_n
            \end{bmatrix} = Y
            \end{align*}
            (producto de matrices). Es decir, la notación antes utilizada es consistente con el, ahora definido, producto de matrices.  

            Es decir $v = (v_1, v_2, \ldots, v_n)$ es solución del sistema \eqref{sist-eq-gen-3} si y sólo si $A\begin{bmatrix} 
                v_1 \\ \vdots \\ v_n 
                \end{bmatrix} = Y$, donde $A\begin{bmatrix} 
                    v_1 \\ \vdots \\ v_n 
                    \end{bmatrix}$ es un producto de matrices. Muchas veces, por comodidad, denotaremos al vector columna $\begin{bmatrix} v_1 \\ \vdots \\ v_n  \end{bmatrix} \in \mathbb K^{n\times 1}$ como  $v = (v_1, v_2, \ldots, v_n) \in \mathbb K^n$.
            
            En lo que resta de la sección veremos que las operaciones elementales pueden ser vistas como multiplicaciones por matrices y por lo tanto la matriz que se obtiene por el métodos de eliminación de Gauss puede ser obtenida por multiplicación a izquierda de matrices que llamaremos elementales y la matriz original. 

            \begin{definicion} Una matriz $m \times m$  se dice  \textit{elemental}\index{matriz!elemental} si fue obtenida por medio de una única operación elemental a partir de la matriz identidad $\Id_m$. Sea $E$ una matriz elemental tal que $E= e(\Id)$ con $e$ una operación elemental. Diremos que \textit{$E$ es de tipo E1} si $e$ es de tipo \ref{elem-1}, \textit{de tipo E2} si $e$ es de tipo \ref{elem-2} y \textit{de tipo E3} si $e$ es de tipo \ref{elem-3}.
            \end{definicion}

            \begin{ejemplo*} Veamos cuales son las matrices elementales  $2 \times 2$:
                \begin{enumerate}
                    \item Si $c \not=0$, multiplicar por  $c$ la primera fila y multiplicar $c$ por la segunda fila son, respectivamente,
                    \begin{equation*}
                    \begin{bmatrix} c& 0\\ 0&1\end{bmatrix}\;\text{ y }\; \begin{bmatrix} 1& 0\\ 0&c\end{bmatrix},
                    \end{equation*}
                    \item si  $c \in \K$, sumar a la fila $2$ la fila $1$ multiplicada por $c$ o sumar a la fila $1$ la fila $2$ multiplicada por $c$ son, respectivamente,
                    \begin{equation*}
                    \begin{bmatrix} 1& 0\\ c&1\end{bmatrix}\;\text{ y }\; \begin{bmatrix} 1& c\\ 0&1\end{bmatrix}.
                    \end{equation*}
                    \item Finalmente, intercambiando la fila $1$ por la fila $2$ obtenemos la matriz
                    \begin{equation*}
                    \begin{bmatrix} 0& 1\\ 1&0\end{bmatrix}.
                    \end{equation*}
                \end{enumerate}
            \end{ejemplo*}

            En el caso de matrices $m \times m$ tampoco es difícil encontrar las matrices elementales:
            \begin{enumerate}
                \item Si $c \not=0$, multiplicar por  $c$ la fila $k$ de la matriz identidad, resulta en la matriz elemental que tiene todos $1$'s en la diagonal, excepto en la posición $k,k$ donde vale $c$,  es decir si $e(\Id_m) = [a_{ij}]$,  entonces
                \begin{equation}\label{elem-tipo-1}
                a_{ij} = \left\{ 
                \begin{matrix*}[l]
                1 &\text{si $i=j$ e $i\ne k$,}\\
                c &\text{si $i=j=k$,} \\
                0 \quad&\text{si $i \ne j$.}
                \end{matrix*}\right.
                \end{equation}
                Gráficamente,
                \begin{align*}
                &\begin{matrix}
                {}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{k}{\downarrow}&{}^{}&{}^{}&{}^{}
                \end{matrix} \\
                \begin{matrix}
                {}^{}\\
                {}^{}\\
                \overset{k}{\to}\\
                {}^{}\\
                {}^{}
                \end{matrix}
                &\begin{bmatrix}
                1 & 0 &  &\cdots & 0  \\
                \vdots  & \ddots  & & & \vdots \\
                0 & \cdots &c &\cdots &0 \\
                \vdots  &   & &\ddots & \vdots \\
                0  & \cdots  & &\cdots & 1
                \end{bmatrix}
                \end{align*}
                \item si  $c \in \K$, sumar a la fila $r$  la fila $s$ multiplicada por $c$, resulta en la matriz elemental que tiene todos $1$'s en la diagonal, y todos los demás coeficientes son $0$,  excepto en la fila  $r$ y columna $s$ donde  vale $c$,  es decir si $e(\Id_m) = [a_{ij}]$,  entonces
                \begin{equation}\label{elem-tipo-2}
                a_{ij} = \left\{ 
                \begin{matrix*}[l]
                1 &\text{si $i=j$}\\
                c &\text{si $i=r$, $j=s$,} \\
                0 \quad&\text{otro caso.}
                \end{matrix*}\right.
                \end{equation}
                Gráficamente, 
                \begin{align*}
                &\begin{matrix}
                {}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{r}{\downarrow}&{}^{}&{}^{}&{}^{}{}^{}\overset{s}{\downarrow}&{}^{}
                \end{matrix} \\
                \begin{matrix}
                {}^{}\\{}^{}\\
                \overset{r}{\to}\\
                {}^{}\\
                {}^{}\\
                {}^{}
                \end{matrix}
                &\begin{bmatrix}
                1 & 0 &  &\cdots &&& 0  \\
                \vdots  & \ddots  & & &&& \vdots \\
                0 & \cdots &1 &\cdots&c&\cdots &0 \\
                \vdots  &   & &\ddots &&& \vdots \\
                \vdots  &   & & &\ddots&& \vdots \\
                0  & \cdots  & &\cdots &&& 1
                \end{bmatrix}
                \end{align*}
                
                
                \item Finalmente, intercambiar la fila $r$ por la fila $s$ resulta ser
                \begin{equation}\label{elem-tipo-3}
                a_{ij} = \left\{ 
                \begin{matrix*}[l]
                1 &\text{si ($i=j$, $i \ne r$, $i \ne s$) o ($i=r$, $j=s$) o ($i=s$, $j=r$) }\\
                0 \quad&\text{otro caso.}
                \end{matrix*}\right.
                \end{equation}
                    Gráficamente, 
                \begin{align*}
                &\begin{matrix}
                {}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{r}{\downarrow}&{}^{}&{}^{}&{}^{}{}^{}\overset{s}{\downarrow}&{}^{}
                \end{matrix} \\
                \begin{matrix}
                {}^{}\\{}^{}\\
                \overset{r}{\to}\\
                {}^{}\\
                \overset{s}{\to}\\{}^{}\\
                {}^{}
                \end{matrix}
                &\begin{bmatrix}
                1 & \cdots &  &\cdots &&\cdots& 0  \\
                \vdots  & \ddots  & & &&& \vdots \\
                0 & \cdots &0 &\cdots&1&\cdots &0 \\
                \vdots  &   & &\ddots &&& \vdots \\
                0  & \cdots  &1 &\cdots &0& \cdots& 0 \\
                \vdots  &   & & &&\ddots& \vdots \\
                0  & \cdots  & &\cdots &&\cdots& 1
                \end{bmatrix}
                \end{align*}
            \end{enumerate}
            
            Veamos ahora que, dada una matriz $A$,   hacer una operación elemental en $A$ es igual a multiplicar $A$ a izquierda por una matriz elemental. Más precisamente:
            
            \begin{teorema}\label{th-mrtx-elem}
                Sea $e$ una operación elemental por fila y sea $E$ la matriz elemental $E=e(\Id)$. Entonces $e(A) = EA$.
            \end{teorema}
            \begin{proof}
                Hagamos la prueba para matrices $2 \times 2$. La prueba en general es similar, pero requiere de un complicado manejo de índices.
                \begin{enumelem}
                    \item Sea  $c \in \K$, y  sea $e$ la operación elemental de a la fila $2$ le sumarle  la fila $1$ multiplicada por $c$. Entonces. $E:=e(\Id_2)$ resulta en la matriz elemental:
                    \begin{equation*}
                    E= \begin{bmatrix} c& 0\\ 0&1\end{bmatrix}.
                    \end{equation*}
                    Ahora bien,
                    \begin{align*}
                    E A&=\begin{bmatrix} c& 0\\ 0&1\end{bmatrix}
                    \begin{bmatrix} a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix} \\
                    &\qquad= 
                    \begin{bmatrix} 
                    c\,.\,a_{11} + 0 \,.\,a_{21}&c\,.\,a_{12}+0\,.\,a_{22}\\
                    0\,.\,a_{11} + 1 \,.\,a_{21}&0\,.\,a_{12}+1\,.\,a_{22}\end{bmatrix} 
                    =
                    \begin{bmatrix} 
                    c\,.\,a_{11}&c\,.\,a_{12}\\
                    a_{21}&a_{22}\end{bmatrix} = e(A).
                    \end{align*}
                    De forma análoga se demuestra en el caso que la operación elemental sea  multiplicar la segunda fila por $c$.
                    
                    \item Sea  $c \in \K$, y  sea $e$ la operación elemental de a la fila $2$ le sumarle  la fila $1$ multiplicada por $c$. Entonces. $E:=e(\Id_2)$ resulta en la matriz elemental: 
                    \begin{equation*}
                    E=\begin{bmatrix} 1& 0\\ c&1\end{bmatrix}.
                    \end{equation*}
                    Luego 
                    \begin{equation*}
                    EA= \begin{bmatrix} 1& 0\\ c&1\end{bmatrix}    \begin{bmatrix} a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix} = 
                    \begin{bmatrix} 
                    a_{11} &a_{12}\\
                    c\,.\,a_{11} + a_{21}&c\,.\,a_{12}+a_{22}\end{bmatrix} = e(A).
                    \end{equation*}
                    La demostración es análoga si la operación elemental es sumar a la fila $1$ la fila $2$ multiplicada por $c$.
                    
                    \item  Finalmente, sea $e$ la operación elemental que intercambia la fila $1$ por la fila $2$. Entonces,  $E:=e(\Id_2)$ es la matriz 
                    \begin{equation*}
                    E=\begin{bmatrix} 0& 1\\ 1&0\end{bmatrix}.
                    \end{equation*}
                    Luego
                    \begin{equation*}
                    EA= \begin{bmatrix} 0& 1\\ 1&0\end{bmatrix}    \begin{bmatrix} a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix} = 
                    \begin{bmatrix}
                    a_{21} &a_{22}\\
                    a_{11} &a_{12}\end{bmatrix} = e(A).
                    \end{equation*}
                \end{enumelem}
                
    
            \end{proof}
            
            
            
            
            \begin{corolario}\label{coro-mrtx-elem}
                Sean $A$ y $B$ matrices $m \times n$. Entonces $B$ equivalente por filas a $A$ si y sólo si $B=PA$ donde $P$ es  producto de matrices elementales. Más aún, si $B = e_k(e_{k-1}(\cdots(e_1(A))\cdots))$ con $e_1,e_2,\ldots,e_k$ operaciones elementales de fila y $E_i=e_i(\Id)$ para $i=1,\ldots,k$,  entonces $B =  E_kE_{k-1}\cdots E_1A$.
            \end{corolario}
            \begin{proof} 
                \
                
                ($\Rightarrow$) Si $B$ equivalente por filas a $A$  existen operaciones elementales $e_1,\ldots,e_k$ tal que $B = e_k(e_{k-1}(\cdots(e_1(A))\cdots))$, más formalmente
                \begin{equation*}
                \text{si } A_1 = e_1(A)\text{ y } A_i = e_i(A_{i-1})\text{ para }i=2,\ldots,k\text{, entonces } e_k(A_{k-1})= B.
                \end{equation*}
                Sea $E_i = e_i(\Id_m)$, entonces, por el teorema anterior $A_1= E_1A$ y $A_i = E_iA_{i-1}$ ($i=2,\ldots,k$). Por  lo tanto $B=E_kA_{k-1}$, en otras palabras $B =  E_kE_{k-1}\cdots E_1A$, luego $P = E_kE_{k-1}\cdots E_1$.
                
                ($\Leftarrow$) Si $B= PA$, con  $P = E_kE_{k-1}\cdots E_1$ donde $E_i=e_i(\Id_m)$ es una matriz elemental,  entonces
                $$
                B = PA = E_kE_{k-1}\cdots E_1A \stackrel{\text{Teor. \ref{th-mrtx-elem}}}{=\joinrel=}  e_k(e_{k-1}(\cdots(e_1(A))\cdots)). 
                $$
                Por lo tanto, $B$ es equivalente por filas a $A$.
            \end{proof} 
            
            \subsection*{$\S$ Ejercicios}
            \begin{enumex}
                \item Sea 
                $$
                A = \begin{bmatrix}
                    1&2&1 \\2&3&1 \\7&11&4
                \end{bmatrix}.
                $$
                Multiplicar por matrices elementales la matriz $A$ hasta obtener la matriz identidad.

                \item Expresar 
                $$
                \begin{bmatrix}
                    1&0 \\ -3&3
                \end{bmatrix}
                $$
                como producto de dos matrices elementales. 

                \item Expresar 
                $$
                \begin{bmatrix}
                    1&2&0 \\ 2&-1&0 \\3&1&2
                \end{bmatrix}
                $$
                como producto de matrices elementales.

                \item\label{matriz-de-permutacion} Una \textit{matriz de permutación} es una matriz cuadrada donde cada fila y  cada columna tiene  un $1$ y todas las demás entradas son $0$. 
                \begin{enumex}
                    \item Calcular   
                    $$
                    \begin{bmatrix} 0&1&0 \\ 1&0&0\\ 0&0&1 \end{bmatrix} 
                    \begin{bmatrix} x_1 \\ x_2\\ x_3 \end{bmatrix}, \qquad 
                    \begin{bmatrix} 0&0&1&0 \\ 1&0&0&0\\ 0&0&0&1 \\0&1&0&0 \end{bmatrix} 
                    \begin{bmatrix} x_1 \\ x_2\\ x_3 \\x_4 \end{bmatrix}.
                    $$
                    (observar que,  justamente, las matrices de permutación permutan las coordenadas de un vector).
                    \item Escribir todas la matrices de permutación $3\times 3$. Mostrar que si $A$ es una matriz $3 \times 3$ y $P$ una matriz de permutación $3 \times 3$,  entonces $PA$ es una matriz que tiene las filas de $A$ permutadas.  
                    \item Probar que toda matriz de permutación es producto de matrices elementales de tipo E3. 
                \end{enumex}
                  
            \end{enumex}

        \end{section}
        
        \begin{section}{Matrices invertibles}\label{seccion-matrices-invertibles}
            \begin{definicion} Sea $A$ una  matriz $n \times n$ con coeficientes en $\K$. Una matriz $B \in M_{n\times n}(\K)$  es \textit{inversa  de $A$}\index{matriz!inversa} si $BA=AB=\Id_n$. En  ese caso,  diremos que  $A$ es \textit{invertible}.\index{matriz!invertible}
            \end{definicion}
            
            \begin{ejemplo*}
                La matriz $\begin{bmatrix} 2&-1\\0&1\end{bmatrix}$ tiene inversa 
                $\begin{bmatrix} \frac12&\frac12\\0&1\end{bmatrix}$ pues es fácil comprobar que 
                \begin{equation*}
                \begin{bmatrix} 2&-1\\0&1\end{bmatrix}
                \begin{bmatrix} \frac12&\frac12\\0&1\end{bmatrix} =
                \begin{bmatrix} 1&0\\0&1\end{bmatrix}\quad\text{ y } \quad
                \begin{bmatrix} \frac12&\frac12\\0&1\end{bmatrix} 
                \begin{bmatrix} 2&-1\\0&1\end{bmatrix}=
                \begin{bmatrix} 1&0\\0&1\end{bmatrix}.
                \end{equation*}
            \end{ejemplo*}
 
            \begin{proposicion}
                Sea $A  \in M_{n\times n}(\K)$, 
                \begin{enumerate}
                    \item\label{inv_mtrx_1}    sean $B, C \in M_{n \times n}(\K)$ tales que $BA =\Id_n$ y $AC = \Id_n$, entonces $B=C$;
                    \item\label{inv_mtrx_2}  si $A$ invertible la inversa es única.
                \end{enumerate}
            \end{proposicion}
            \begin{proof} \ref{inv_mtrx_1}
                \begin{equation*}
                B = B\Id_n = B(AC) = (BA)C = \Id_nC = C.
                \end{equation*}
                
                \ref{inv_mtrx_2} Sean $B$ y $C$ inversas de $A$, es decir $BA=AB=\Id_n$ y  $CA=AC=\Id_n$. En particular, $BA=\Id_n$ y $AC= \Id_n$, luego, por \ref{inv_mtrx_1}, $B=C$.  
            \end{proof}
            
            
            
            \begin{definicion}
                Sea $A\in M_{n\times n}(\K)$ invertible. A la única matriz inversa de $A$ la llamamos \textit{la matriz inversa de $A$} y la denotamos $A^{-1}$.
            \end{definicion}
            
            Veremos más adelante que si una matriz $n \times n$ admite una inversa a izquierda,  es decir si existe $B$ tal que $BA=\Id_n$, entonces la matriz es invertible. Lo mismo vale si $A$  admite inversa a derecha.
            
            \begin{ejemplo*}
                Sea $A$ la matriz 
                \begin{equation*}
                \begin{bmatrix} 2&1&-2\\ 1&1&-2\\ -1&0&1
                \end{bmatrix}.
                \end{equation*}
                Entonces,  $A$ es invertible y su inversa es
                \begin{equation*}
                A^{-1} = \begin{bmatrix} 1&-1&0\\ 1&0&2\\ 1&-1&1
                \end{bmatrix}.
                \end{equation*}
                Esto se resuelve comprobando que $AA^{-1}=\Id_3$ (por lo dicho más arriba es innecesario comprobar que $A^{-1}A=\Id_3$).
            \end{ejemplo*} 
            
            \begin{observacion*}
                No toda matriz tiene inversa, por ejemplo  la  matriz nula (cuyos coeficientes son todos iguales a $0$) no tiene inversa pues $0\,.\, A= 0 \not= \Id$.  También existen matrices no nulas no invertibles,  por ejemplo la matriz 
                \begin{equation*}
                A = \begin{bmatrix} 2&1\\ 0&0\end{bmatrix}
                \end{equation*}
                no tiene inversa.
                Si  multiplicamos a $A$ por una cualquier matriz  $B =[b_{ij}]$ obtenemos
                \begin{equation*}
                AB = \begin{bmatrix} 2&1\\ 0&0\end{bmatrix}
                \begin{bmatrix} b_{11}&b_{12}\\ b_{21}&b_{22}\end{bmatrix} =
                \begin{bmatrix} 2b_{11}+b_{21}&2b_{12}+b_{22}\\0 &0\end{bmatrix}.
                \end{equation*}
                Luego $AB$, al tener una fila idénticamente nula, no puede ser nunca la identidad. 
            \end{observacion*}
            
            
            
            \begin{teorema}\label{th-prod-inv-impl-inv}
                Sean $A$ y $B$ matrices $n \times n$ con coeficientes en $\K$. Entonces
                \begin{enumerate}
                    \item \label{inv-itm1} si $A$ invertible,  entonces $A^{-1}$  es invertible y su inversa es $A$,  es decir $(A^{-1})^{-1}=A$;
                    \item \label{inv-itm2} si $A$ y $B$ son invertibles, entonces $AB$ es invertible y $(AB)^{-1} = B^{-1}A^{-1}$.
                \end{enumerate}
            \end{teorema}
                \begin{proof}
                    \ref{inv-itm1} La inversa a izquierda de $A^{-1}$ es $A$, pues $AA^{-1}=\Id_n$. Análogamente, la inversa a derecha de $A^{-1}$ es $A$, pues $A^{-1}A = \Id_n$. Concluyendo: $A$  es la inversa de $A^{-1}$.
                    
                    \ref{inv-itm2} Simplemente debemos comprobar que $B^{-1}A^{-1}$ es inversa a izquierda y derecha de $AB$:
                    \begin{equation*}
                    (B^{-1}A^{-1})AB = B^{-1}(A^{-1}A)B = B^{-1}\Id_nB =B^{-1}B = \Id_n,
                    \end{equation*}
                    y,  análogamente, comprobemos que es inversa a derecha, 
                    \begin{equation*}
                    AB(B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = A\Id_nA^{-1} =AA^{-1} = \Id_n.
                    \end{equation*} 
                \end{proof}
            
            
            \begin{observacion*}
                Si $A_1,\ldots,A_k$  son invertibles,  entonces $A_1\ldots A_k$ es invertible y su inversa es  $$(A_1\ldots A_k)^{-1} = A_k^{-1}\ldots A_1^{-1} .$$
                El resultado es una generalización del punto  \ref{inv-itm2} del teorema anterior y su demostración se hace por inducción en $k$ (usando  \ref{inv-itm2}  del teorema anterior). Se deja como ejercicio al lector. 
            \end{observacion*}
            
            \begin{observacion*}
                La suma de matrices invertibles no necesariamente es invertible, por ejemplo $A+ (-A)= 0$ que no es invertible. 
            \end{observacion*}    
            
            
            \begin{teorema}\label{th-elmental-impl-invertible}
                Una matriz elemental es invertible.
            \end{teorema}
            \begin{proof}
                Sea $E$ la matriz elemental que se obtiene a partir de $\Id_n$ por la operación elemental $e$. Se $e_1$ la operación elemental inversa (teorema \ref{op-elem}) y $E_1 = e_1(\Id_n)$. Entonces 
                \begin{align*}
                EE_1 &= e(e_1(\Id_n)) = \Id_n \\
                E_1E &= e_1(e(\Id_n)) = \Id_n.
                \end{align*}
                Luego  $E_1 = E^{-1}$. 
            \end{proof}    
            
            
            \begin{ejemplo*}
                Es fácil encontrar explícitamente la matriz inversa de una matríz elemental, por ejemplo, en el caso $2 \times 2$ tenemos:
                \begin{enumerate}
                    \item Si $c \not=0$,
                    \begin{equation*}
                    \begin{bmatrix} c& 0\\ 0&1\end{bmatrix}^{-1}=\begin{bmatrix} 1/c& 0\\ 0&1\end{bmatrix}
                    \;\text{ y }\; \begin{bmatrix} 1& 0\\ 0&c\end{bmatrix}^{-1}=\begin{bmatrix} 1& 0\\ 0&1/c\end{bmatrix},
                    \end{equation*}
                    \item si  $c \in \K$, ,
                    \begin{equation*}
                    \begin{bmatrix} 1& 0\\ c&1\end{bmatrix}^{-1}=\begin{bmatrix} 1& 0\\ -c&1\end{bmatrix}
                    \;\text{ y }\; \begin{bmatrix} 1& c\\ 0&1\end{bmatrix}^{-1}=\begin{bmatrix} 1& -c\\ 0&1\end{bmatrix}.
                    \end{equation*}
                    \item Finalmente, 
                    \begin{equation*}
                    \begin{bmatrix} 0& 1\\ 1&0\end{bmatrix} ^{-1}=     \begin{bmatrix} 0& 1\\ 1&0\end{bmatrix}.
                    \end{equation*}
                \end{enumerate}
            En  el caso general tenemos:
            
            (1) 
            \begin{align*}
            &\begin{matrix}
            {}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{k}{\downarrow}&{}^{}&{}^{}&{}^{}
            \end{matrix} \\
            \text{La inversa de \quad}
            \begin{matrix}
            {}^{}\\
            {}^{}\\
            \overset{k}{\to}\\
            {}^{}\\
            {}^{}
            \end{matrix}
            &\begin{bmatrix}
            1 & 0 &  &\cdots & 0  \\
            \vdots  & \ddots  & & & \vdots \\
            0 & \cdots &c &\cdots &0 \\
            \vdots  &   & &\ddots & \vdots \\
            0  & \cdots  & &\cdots & 1
            \end{bmatrix}
            \text{\quad es \quad}
            \begin{bmatrix}
            1 & 0 &  &\cdots & 0  \\
            \vdots  & \ddots  & & & \vdots \\
            0 & \cdots &1/c &\cdots &0 \\
            \vdots  &   & &\ddots & \vdots \\
            0  & \cdots  & &\cdots & 1
            \end{bmatrix}
            \end{align*}
            
            (2)
            \begin{align*}
            &\begin{matrix}
            {}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{r}{\downarrow}&{}^{}&{}^{}&{}^{}{}^{}\overset{s}{\downarrow}&{}^{}
            \end{matrix} \\
            \text{La inversa de \quad}
            \begin{matrix}
            {}^{}\\{}^{}\\
            \overset{r}{\to}\\
            {}^{}\\
            {}^{}\\
            {}^{}
            \end{matrix}
            &\begin{bmatrix}
            1 & 0 &  &\cdots &&& 0  \\
            \vdots  & \ddots  & & &&& \vdots \\
            0 & \cdots &1 &\cdots&c&\cdots &0 \\
            \vdots  &   & &\ddots &&& \vdots \\
            \vdots  &   & & &\ddots&& \vdots \\
            0  & \cdots  & &\cdots &&& 1
            \end{bmatrix}\qquad\qquad\quad
        \end{align*}
        \begin{equation*}
            \text{es \qquad}
            \begin{bmatrix}
            1 & 0 &  &\cdots &&& 0  \\
            \vdots  & \ddots  & & &&& \vdots \\
            0 & \cdots &1 &\cdots&-c&\cdots &0 \\
            \vdots  &   & &\ddots &&& \vdots \\
            \vdots  &   & & &\ddots&& \vdots \\
            0  & \cdots  & &\cdots &&& 1
            \end{bmatrix}. 
        \end{equation*}
            
            
            
            (3)
            
            \begin{align*}
            &\begin{matrix}
            {}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{r}{\downarrow}&{}^{}&{}^{}&{}^{}{}^{}\overset{s}{\downarrow}&{}^{}
            \end{matrix} \\
            \text{La inversa de \quad}\begin{matrix}
            {}^{}\\{}^{}\\
            \overset{r}{\to}\\
            {}^{}\\
            \overset{s}{\to}\\{}^{}\\
            {}^{}
            \end{matrix}
            &\begin{bmatrix}
            1 & \cdots &  &\cdots &&\cdots& 0  \\
            \vdots  & \ddots  & & &&& \vdots \\
            0 & \cdots &0 &\cdots&1&\cdots &0 \\
            \vdots  &   & &\ddots &&& \vdots \\
            0  & \cdots  &1 &\cdots &0& \cdots& 0 \\
            \vdots  &   & & &&\ddots& \vdots \\
            0  & \cdots  & &\cdots &&\cdots& 1
            \end{bmatrix}
            \text{\quad es la misma matriz.\quad}
            \end{align*}
        
                        
        
    
\end{ejemplo*}
        
        
            
            
            \begin{teorema}\label{mtrx-inv-equiv} Sea $A$ matriz $n \times n$ con coeficientes en $\K$. Las siguientes afirmaciones son equivalentes
                \begin{enumerate}
                    \item\label{teo-inv-1} $A$ es invertible,
                    \item\label{teo-inv-2} $A$  es equivalente por filas a $\Id_n$, 
                    \item\label{teo-inv-3} $A$ es producto de matrices elementales.
                \end{enumerate}
            \end{teorema}
            \begin{proof}
                \

                
                \ref{teo-inv-1} $\Rightarrow$ \ref{teo-inv-2}\; Sea $R$ la matriz escalón reducida por fila equivalente por filas a $A$. Entonces,  existen $E_1,\ldots,E_k$ matrices elementales tal que $E_1,\ldots,E_kA = R$. Como las matrices elementales son invertibles, el producto de matrices elementales es invertible, luego  $E_1,\ldots,E_k$ es invertible y por lo tanto $R=E_1,\ldots,E_kA$ es invertible. 
                
                Recordemos que las matrices escalón reducidas por fila si tienen filas nulas, ellas se encuentran al final.  Ahora bien,  si la última fila de $R$ es nula entonces,  $RB$ tiene la última fila nula también y por lo tanto no puede ser igual a la identidad, es decir, en ese caso $R$ no es invertible, lo cual produce un absurdo. Concluyendo: la última fila (la fila $n$) de $R$ no es nula y como es MERF, $R$ no tiene filas nulas. Por lo tanto $R=\Id_n$ (lema \ref{lem-mtrx-merf-id}) y,  entonces, $A$ es equivalente por filas a $\Id_n$. 
                
                \ref{teo-inv-2} $\Rightarrow$ \ref{teo-inv-3}\; Como $A$  es equivalente por filas a $\Id_n$, al ser la equivalencia por filas una relación de equivalencia,  tenemos que $\Id_n$ es equivalente por filas a $A$, es decir  existen $E_1,\ldots,E_k$ matrices elementales, tales que $E_1E_2,\ldots,E_k\Id_n = A$. Por lo tanto, $A =E_1E_2,\ldots,E_k$ producto de matrices elementales.
                
                \ref{teo-inv-3} $\Rightarrow$ \ref{teo-inv-1} \; Sea $A = E_1E_2,\ldots,E_k$ donde $E_i$  es una matriz elemental ($i=1,\ldots,k$). Como cada $E_i$ es invertible,  el producto de ellos es invertible,  por lo tanto $A$ es invertible.
            \end{proof}    
            
            \begin{corolario}
                Sean $A$ y $B$ matrices $m \times n$. Entonces,   $B$ es equivalente por filas a $A$ si y sólo si existe matriz invertible $P$ de orden $m \times m$ tal que $B =PA$ . 
            \end{corolario}
            \begin{proof}
                
                \
                
                ($\Rightarrow$) $B$ es equivalente por filas a $A$,  luego existe $P$ matriz producto de matrices elementales tal que $B =PA$. Como cada matriz elemental es invertible (teorema \ref{th-elmental-impl-invertible}) y el producto de matrices invertibles es invertible (teorema  \ref{th-prod-inv-impl-inv} \ref{inv-itm2}), se deduce que $P$ es invertible. 
                
                ($\Leftarrow$) Sea  $P$  matriz invertible tal que $B =PA$. Como $P$ es invertible, por el teorema anterior, $P$ es producto de matrices elementales, luego $B =PA$ es equivalente por filas a $A$.
            \end{proof}    
            
            \begin{corolario}\label{mtrx-inv-gauss}
                Sea $A$ matriz $n \times n$. Sean $e_1,\ldots,e_k$ las operaciones elementales por filas que reducen a $A$  a una MERF y esta MERF es la identidad,  es decir $e_1(e_{2}(\cdots(e_k(A))\cdots)) =\Id_n$. Entonces, $A$ invertible y  las mismas operaciones elementales aplicadas a $\Id_n$ nos llevan a $A^{-1}$,  es decir $e_1(e_{2}(\cdots(e_k(\Id_n))\cdots)) =A^{-1}$.
            \end{corolario}
            \begin{proof} Por el teorema anterior, al ser $A$ equivalente por filas a la identidad, $A$ es invertible.  
                Sean las matrices elementales  $E_i = e_i(\Id_n)$ para $i=1,\ldots,k$,  entonces (ver corolario \ref{coro-mrtx-elem}) $E_1E_2\ldots E_kA = \Id_n$, por lo tanto, multiplicando por $A^{-1}$ a derecha en ambos miembros,    
                \begin{align*}
                E_1E_2\ldots E_kA A^{-1}&= \Id_nA^{-1} \quad \Leftrightarrow \\
                E_1E_2\ldots E_k\Id_n&= A^{-1} \quad \Leftrightarrow \\
                e_1(e_{2}(\cdots(e_k(\Id_n))\cdots)) &=A^{-1}.
                \end{align*}
            \end{proof}
            
            Este último corolario nos provee un método sencillo para calcular la inversa de una matriz $A$ (invertible). Primero,  encontramos $R = \Id_n$ la MERF  equivalente por filas a $A$, luego, aplicando la mismas operaciones elementales a $\Id_n$, obtenemos la inversa de $A$. Para facilitar el cálculo es  conveniente comenzar con $A$ e $\Id_n$ e ir aplicando paralelamente las operaciones elementales por fila. Veamos un ejemplo.

            \begin{ejemplo*}
                Calculemos la inversa (si tiene) de 
                \begin{equation*}
                A=\begin{bmatrix}2&-1\\1&3 \end{bmatrix}.
                \end{equation*}
            \end{ejemplo*}
            \begin{proof}[Solución] Por lo que ya hemos demostrado, si $A$ tiene inversa es reducible por filas a la identidad y las operaciones que llevan a $A$ a la identidad, llevan también la identidad  a $A^{-1}$. Luego  trataremos de reducir por filas a $A$ y todas las operaciones elementales las haremos en paralelo partiendo de la matriz identidad:
                %$$
                %\left[\begin{array}{@{}*{4}{c}|c@{}}
                %1 & 0 & 3 & -1 & 0 \\
                %0 & 1 & 1 & -1 & 0 \\
                %0 & 0 & 0 & 0 & 0 \\
                %\end{array}\right]
                %$$
                \begin{align*}
                [A|\Id] &= \left[\begin{array}{cc|cc}2&-1 &  1&0\\1&3& 0&1\end{array}\right] 
                \stackrel{F_1\leftrightarrow F_2}{\longrightarrow} 
                \left[\begin{array}{cc|cc}1&3& 0&1\\2&-1 &  1&0 \end{array}\right]
                \stackrel{F_2-2 F_1}{\longrightarrow}\\
                &\left[\begin{array}{cc|cc}1&3& 0&1\\0&-7 &  1&-2 \end{array}\right]
                \stackrel{F_2/(-7)}{\longrightarrow} 
                \left[\begin{array}{cc|cc}1&3& 0&1\\0&1 &  -\frac17&\frac27\end{array}\right]
                \stackrel{F_1-3 F_2}{\longrightarrow}
                \left[\begin{array}{cc|cc}1&0&  \frac37&\frac17\\0&1 &  -\frac17&\frac27 \end{array}\right].
                \end{align*}
                Luego, como $A$ se reduce por filas a la identidad, $A$ es invertible y su inversa es  
                \begin{equation*}
                A^{-1}=\begin{bmatrix}\frac37&\frac17\\-\frac17&\frac27 \end{bmatrix}.
                \end{equation*}
                El lector desconfiado  podrá comprobar, haciendo el producto de matrices, que $AA^{-1} = A^{-1}A=\Id_2$.
            \end{proof}
            
            
            \begin{teorema}\label{mtrx-inv-equiv2} 
                Sea $A$ matriz $n \times n$ con coeficientes en $\K$. Entonces,  las siguientes afirmaciones son equivalentes. 
                \begin{enumerate}[label=\textit{\roman*)}, ref=\textit{\roman*)}]
                    \item\label{equiv-inv-1} $A$ es invertible.
                    \item\label{equiv-inv-2} El sistema $AX=Y$ tiene una única solución para toda matriz $Y$ de orden $n \times 1$. 
                    \item\label{equiv-inv-3} El sistema homogéneo $AX=0$ tiene una única solución trivial.
                \end{enumerate}
            \end{teorema}
            \begin{proof}

                \

                \ref{equiv-inv-1} $\Rightarrow$  \ref{equiv-inv-2} Sea $X_0$ solución del sistema $AX=Y$, luego
                \begin{equation*}
                AX_0=Y  \quad \Rightarrow \quad  A^{-1}AX_0 = A^{-1}Y  \quad \Rightarrow \quad  X_0 = A^{-1}Y.
                \end{equation*}
                Es decir, $X_0$ es único (siempre igual  a $A^{-1}Y$).  
                
                \ref{equiv-inv-2} $\Rightarrow$  \ref{equiv-inv-3} Es trivial, tomando $Y =0$.
                
                \ref{equiv-inv-3} $\Rightarrow$  \ref{equiv-inv-1} Sea $R$ la matriz escalón reducida por filas equivalente a $A$, es decir $R=PA$ con $P$ invertible y $R$ es MERF. Si $R$ tiene una fila nula, entonces por corolario  \ref{inf-sol},  el sistema $AX =0$ tiene más de una solución, lo cual es absurdo.  Por lo tanto, $R$ no tiene filas nulas. Como es una matriz cuadrada y es MERF, tenemos que $R=\Id_n$. Luego $A$ es equivalente por filas a $\Id_n$ y por teorema \ref{mtrx-inv-equiv} se deduce que $A$ es invertible.             
                
            \end{proof}
            
            \begin{corolario}
                Sea $A$ una matriz $n \times n$ con coeficientes  en $\K$. Si $A$ tiene inversa a izquierda,  es decir si existe $B$ matriz $n \times n$ tal que $BA=\Id_n$,   entonces $A$ es invertible.  Lo mismo vale si $A$ tiene inversa a derecha. 
            \end{corolario}    
            \begin{proof}
                Supongamos que  $A$ tiene inversa a izquierda y  que $B$ sea la inversa a izquierda,  es decir $BA=\Id_n$. El sistema $AX=0$ tiene una única solución, pues $AX_0=0 \Rightarrow BAX_0=B0 \Rightarrow X_0=0$. Luego, $A$  es invertible (y su inversa es $B$). 
                
                Supongamos que  $A$ tiene inversa a derecha y  que $C$ sea la inversa a derecha, es decir $AC=\Id$. Por lo demostrado más arriba, $C$ es invertible y su inversa es $A$, es decir $AC=\Id$ y $CA=\Id$, luego $A$  es invertible. 
            \end{proof}    
            
            
            
            Terminaremos la  sección calculando algunas matrices inversas usando el corolario  \ref{mtrx-inv-gauss}. 
            
            \begin{ejemplo*}
                Calcular la inversa (si tiene) de la matriz $A=\begin{bmatrix}
                1&-1&2\\ 3&2&4\\ 0&1&-2
                \end{bmatrix}$. 
            \end{ejemplo*}
            \begin{proof}[Solución]
                %\left[\begin{array}{cc|cc}2&-1 &  1&0\\1&3& 0&1\end{array}\right] 
                \begin{align*} 
                &\left[\begin{array}{rrr|rrr}    1&-1&2&1&0&0\\ 3&2&4&0&1&0\\ 0&1&-2&0&0&1 \end{array}\right]
                \stackrel{F_2-3 F_1}{\longrightarrow}
                \left[\begin{array}{rrr|rrr}    1&-1&2
                &1&0&0\\ 0&5&-2&-3&1&0\\ 0&1&-2&0&0&1 \end{array}\right]
                \stackrel{F_2\leftrightarrow F_3}{\longrightarrow} \\
                &\longrightarrow 
                \left[\begin{array}{rrr|rrr}    1&-1&2&1&0&0\\ 0&1&-2&0&0&1 \\ 0&5&-2&-3&1&0 \end{array}\right]
                \stackrel{F_1 + F_2}{\longrightarrow}
                \left[\begin{array}{rrr|rrr}    1&0&0&1&0&1\\ 0&1&-2&0&0&1 \\ 0&5&-2&-3&1&0 \end{array}\right]
                \stackrel{F_3-5F_2}{\longrightarrow} \\
                &\longrightarrow
                \left[\begin{array}{rrr|rrr}    1&0&0&1&0&1\\ 0&1&-2&0&0&1 \\ 0&0&8&-3&1&-5 \end{array}\right]
                \stackrel{F_3/8}{\longrightarrow}
                \left[\begin{array}{rrr|rrr}    1&0&0&1&0&1\\ 0&1&-2&0&0&1 \\ 0&0&1&-\frac38&\frac18&-\frac58 \end{array}\right]
                \stackrel{F_2+2F_3}{\longrightarrow} \\
                &\longrightarrow
                \left[\begin{array}{rrr|rrr}    1&0&0&1&0&1\\ 0&1&0&-\frac34&\frac14&-\frac14 \\ 0&0&1&-\frac38&\frac18&-\frac58 \end{array}\right].
                \end{align*}
                
                Por lo tanto 
                \begin{equation*}
                A^{-1} =     \begin{bmatrix*}[r]    1&0&1\\ -\frac34&\frac14&-\frac14 \\ -\frac38&\frac18&-\frac58 \end{bmatrix*}
                \end{equation*}
            \end{proof}
            
            \begin{ejemplo*}\label{inv-2x2-0}
                Dados $a,b,c,d \in \mathbb R$, determinar cuando la matriz $A = \begin{bmatrix*} a&b\\c&d\end{bmatrix*}$  es invertible y en ese caso,  cual es su inversa. 
            \end{ejemplo*}
            \begin{proof}[Solución] Para poder aplicar el método de Gauss, debemos ir haciendo casos. 
                
                \noindent 1) Supongamos que  $a\not=0$, entonces 
                \begin{equation*}
                \begin{bmatrix}a&b\\c&d\end{bmatrix} \stackrel{F_1/a}{\longrightarrow}
                \begin{bmatrix}1&\dfrac{b}{a}\\[6pt]c&d\end{bmatrix} \stackrel{F_2 -cF_1}{\longrightarrow}
                \begin{bmatrix}1&\dfrac{b}{a}\\[6pt]0&d- c\dfrac{b}{a}\end{bmatrix} =
                \begin{bmatrix}1&\dfrac{b}{a}\\[6pt]0&\dfrac{ad-bc}{a}\end{bmatrix}
                \end{equation*}   
                Si $ad-bc=0$,  entonces la matriz se encuentra reducida por filas y la última fila es $0$, luego en ese caso no es invertible.  Si $ad-bc\not=0$, entonces
                \begin{equation*}
                \begin{bmatrix}1&\dfrac{b}{a}\\[8pt]0&\dfrac{ad-bc}{a}\end{bmatrix} \stackrel{a/(ad-bc)\,F_2}{\longrightarrow}
                \begin{bmatrix}1&\dfrac{b}{a}\\[6pt]0&1\end{bmatrix}
                \stackrel{F1-b/a\,F_2}{\longrightarrow}
                \begin{bmatrix}1&0\\0&1\end{bmatrix}.
                \end{equation*} 
                Luego, en el caso $a\not=0$, $ad-bc\not=0$ hemos reducido por filas la matriz $A$  a la identidad y por lo tanto $A$  es invertible. Además, podemos encontrar $A^{-1}$ aplicando a $\Id$ las mismas operaciones elementales que reducían $A$ a la identidad:
                \begin{align*}
                \begin{bmatrix}1&0\\0&1\end{bmatrix} 
                &\stackrel{F_1/a}{\longrightarrow}
                \begin{bmatrix}\dfrac1a&0\\[8pt]0&1\end{bmatrix} 
                \stackrel{F_2 -cF_1}{\longrightarrow}
                \begin{bmatrix*}[r]\dfrac1a&0\\[8pt]-\dfrac{c}{a}&1\end{bmatrix*}
                \stackrel{a/(ad-bc)\,F_2}{\longrightarrow}
                \begin{bmatrix*}[c]\dfrac1a&0\\[8pt]-\dfrac{c}{ad-bc}&\dfrac{a}{ad-bc}\end{bmatrix*}
                \\& \stackrel{F1-b/a\,F_2}{\longrightarrow}        
                \begin{bmatrix*}[c]\dfrac1a+\dfrac{bc}{a(ad-bc)}&-\dfrac{b}{ad-bc}\\[8pt]-\dfrac{c}{ad-bc}&\dfrac{a}{ad-bc}\end{bmatrix*} 
                =
                \begin{bmatrix*}[c]\dfrac{d}{ad-bc}&-\dfrac{b}{ad-bc}\\[8pt]-\dfrac{c}{ad-bc}&\dfrac{a}{ad-bc}\end{bmatrix*} .
                \end{align*}  
                Concluyendo, en el caso $a\not=0$, $ad-bc\not=0$, $A$  es invertible y 
                \begin{equation}\label{inv-2x2}
                A^{-1} = \dfrac{1}{ad-bc}
                \begin{bmatrix*}[c]d&-b\\-c&a\end{bmatrix*}.
                \end{equation}
                
                \noindent 2) Estudiemos el caso $a=0$. Primero observemos que si $c=0$ o $b=0$ , entonces la matriz no es invertible, pues en ambos casos nos quedan matrices que no pueden ser reducidas por fila a la identidad. Luego la matriz puede ser invertible si $bc\not=0$ y en este caso la reducción por filas es:
                \begin{equation*}
                \begin{bmatrix}0&b\\c&d\end{bmatrix} \stackrel{F_1\leftrightarrow F_2}{\longrightarrow}
                \begin{bmatrix}c&d\\0&b\end{bmatrix} \stackrel{F_1/c}{\longrightarrow}
                \begin{bmatrix}1&\dfrac{d}{c}\\[6pt]0&b\end{bmatrix} \stackrel{F_2/b}{\longrightarrow}
                \begin{bmatrix}1&\dfrac{d}{c}\\[6pt]0&1\end{bmatrix}
                \stackrel{F_1 - d/c F_2}{\longrightarrow}
                \begin{bmatrix}1&0\\0&1\end{bmatrix}.
                \end{equation*}   
                Luego $A$  es invertible y aplicando estas mismas operaciones elementales a la identidad  obtenemos la inversa:
                \begin{equation*}
                \begin{bmatrix}1&0\\0&1\end{bmatrix} \stackrel{F_1\leftrightarrow F_2}{\longrightarrow}
                \begin{bmatrix}0&1\\1&0\end{bmatrix} \stackrel{F_1/c}{\longrightarrow}
                \begin{bmatrix}0&\dfrac{1}{c}\\[6pt]1&0\end{bmatrix} \stackrel{F_2/b}{\longrightarrow}
                \begin{bmatrix}0&\dfrac{1}{c}\\[6pt]\dfrac{1}{b}&0\end{bmatrix}
                \stackrel{F_1 - d/c F_2}{\longrightarrow}
                \begin{bmatrix}-\dfrac{d}{bc}&\dfrac{1}{c}\\[6pt]\dfrac{1}{b}&0\end{bmatrix}.
                \end{equation*}  
                Luego, en el caso  que $a=0$, entonces $A$ invertible si  $b c\not=0$ y su inversa es
                \begin{equation*}
                A^{-1} = \begin{bmatrix}-\dfrac{d}{bc}&\dfrac{1}{c}\\[6pt]\dfrac{1}{b}&0\end{bmatrix} = 
                \dfrac{1}{-bc}
                \begin{bmatrix*}[c]d&-b\\-c&0\end{bmatrix*}.
                \end{equation*}
                Es decir, la expresión de la inversa es igual a \eqref{inv-2x2} (considerando que  $a=0$).
                
                Reuniendo los dos casos:  $A$ es invertible si $a\not=0$ y  $ad-bc\not=0$ o si $a=0$ y $bc\not=0$, pero esto es lógicamente equivalente a pedir solamente  $ad-bc\not=0$, es decir
                \begin{equation*}
                (a\not=0 \wedge ad-bc\not=0) \vee (a=0 \wedge bc\not=0)\; \Leftrightarrow\; ad-bc\not=0
                \end{equation*}
                (ejercicio).
                
                Resumiendo, $\begin{bmatrix*} a&b\\c&d\end{bmatrix*}$ es invertible  $\Leftrightarrow ad-bc\not=0$  y en ese caso,  su inversa viene dada por 
                \begin{equation}
                \begin{bmatrix*} a&b\\c&d\end{bmatrix*}^{-1} =  \dfrac{1}{ad-bc}
                \begin{bmatrix*}[c]d&-b\\-c&a\end{bmatrix*}
                \end{equation} 

                Veremos en la próxima sección que el uso de determinantes permitirá establecer la generalización de este resultado para matrices $n \times n$ con $n\ge 1$.
                
            \end{proof}
            
            \subsection*{$\S$ Ejercicios}

            \begin{enumex}
                \item Encontrar la inversa de las siguientes matrices.
                    \begin{enumex}
                        \begin{minipage}{0.4\textwidth}
                        \item $\begin{bmatrix}
                            -3&-2\\3&3
                        \end{bmatrix}$\,,
                    \end{minipage}
                    \begin{minipage}{0.4\textwidth}
                        \item $\begin{bmatrix}
                            1&0&1\\ 0&1&1\\ 1&1&1
                        \end{bmatrix}$\,,
                    \end{minipage}

                    \begin{minipage}{0.4\textwidth}
                        \item $\begin{bmatrix}
                            1&3&1 \\ 3&2&5\\ 2&2&2
                        \end{bmatrix}$\,,
                    \end{minipage}
                    \begin{minipage}{0.4\textwidth}
                        \item $\begin{bmatrix}
                            3&2&1&2\\ 7&5&2&5\\ 0&0&9&4\\ 0&0&11&5
                        \end{bmatrix}$\,. 
                    \end{minipage}
                    \end{enumex}

                \item\label{matrices-semejantes} Sean $A$, $B$ dos matrices cuadradas del mismo tamaño. Decimos que $A$ es \textit{semejante} a $B$ si existe una matriz invertible $P$ tal que $B = P^{-1} A P$. Suponga que $A$ es semejante a $B$, probar:
                \begin{enumex}
                    \item $B$ es semejante a $A$.
                    \item Sea $C$ otra matriz cuadrada del mismo tamaño que $A$. Si  $B$ es semejante a $C$, entonces $A$  es semejante a $C$.
                    \item $A$ es invertible si y solo si $B$ es invertible.
                    \item Suponga que $A^n = 0$. Probar que $B^n=0$.
                \end{enumex}
                
                \item Sea $\begin{bmatrix}
                    a & b \\ 0 &c
                \end{bmatrix}$ con $a$ y $c$ no nulos. Probar que esta matriz es invertible y que su inversa es 
                $$
                \begin{bmatrix}
                    a^{-1} &  a^{-1} b  c^{-1} \\
                    0 &  c^{-1}
                \end{bmatrix}. 
                $$
                \item Sea la matriz de bloques $k,r \in \N$
                $$
                 \begin{bmatrix}
                    A & B \\ 0 & C
                \end{bmatrix}.
                $$
                Es decir $A \in \K^{k \times k}$, $B \in \K^{k \times r}$ y $C \in \K^{r \times r}$ (ver el capítulo \ref{capitulo-algebra-de-matrices} ejercicio \ref{matriz-de-bloques}). Si $A$ y $C$ son invertibles probar que la matriz de bloques es invertible y su inversa es 
                $$
                \begin{bmatrix}
                    A^{-1} &  A^{-1} B  C^{-1} \\
                    0 &  C^{-1}
                \end{bmatrix}. 
                $$
            \end{enumex}
        \end{section}
        
        
    \begin{section}{Determinante}\label{seccion-determinante}
    El determinante puede ser pensado como una función que a cada matriz cuadrada $n \times n$ con coeficientes en $\K$,  le asocia un elemento de $\K$. En  esta sección veremos como se define esta función y algunas propiedades de la misma. Algunas  demostraciones se omitirán, pues se pondrá énfasis en los usos del determinante y no tanto en sus propiedades teóricas. Las demostraciones faltantes se pueden ver en el Apéndice \ref{apend.Determinante}.
    
    El determinante, permite, entre otras cosas, 
    \begin{itemize}
        \item determinar si  una matriz cuadrada es invertible,
        \item dar una fórmula cerrada para la inversa de una matriz invertible.
    \end{itemize}
    Como consecuencia de lo anterior, el determinante permite determinar si un sistema de $n$  ecuaciones lineales con $n$ incógnitas admite una única solución o no, y en el caso de que exista una única solución, dar una fórmula cerrada de esa solución. 
    
    Una forma de definir determinante es con una fórmula cerrada que usa el \textit{grupo de permutaciones.}\index{grupo de permutaciones} Esta forma de definir determinante está fuera del alcance de este curso. La forma  que usaremos nosotros para definir determinante es mediante una definición recursiva: para calcular el determinante de una matriz $n \times n$, usaremos el cálculo  del determinante para matrices $n-1 \times n-1$,  que a su vez se calcula usando el determinante de matrices $n-2 \times n-2$ y así sucesivamente hasta llegar al caso base, que es el caso  de matrices $1 \times 1$.

    \vskip .3cm

    \begin{definicion} Sea $A \in M_n(\K)$. Sean  $i, j$ tal que  $1 \le i,j \le n$. Entonces
        $A(i|j)$ es la matriz $n-1 \times n-1$ que se obtiene eliminando la fila $i$ y la columna $j$ de $A$.
    \end{definicion}
    
    \begin{ejemplo*}
        Sea $A= \begin{bmatrix}1&-1&3\\4&2&-5 \\ 0&7&3\end{bmatrix}$,  entonces
        \begin{equation*}
        A(1|1)= \begin{bmatrix} 2&-5 \\ 7&3\end{bmatrix}, \qquad
        A(2|3)= \begin{bmatrix} 1&-1\\0&7\end{bmatrix}, \qquad
        A(3|1)= \begin{bmatrix} -1&3\\2&-5 \end{bmatrix}.
        \end{equation*}
    \end{ejemplo*}        

    \vskip .3cm

    \begin{definicion}
        Sea $n \in \mathbb N$ y $A =[a_{ij}] \in M_n(\K)$ , entonces el \textit{determinante de $A$}\index{determinante}, denotado $\det(A)$ se define como:
        \begin{enumerate}
            \item[(1)] si $n=1$,  $\det([a]) =a$;
            \item[($n$)] si $n >1$, 
            \begin{align*}
            \det(A) &=  a_{11}\det A(1|1) - a_{21}\det A(2|1) + \cdots + (-1)^{1+n}  a_{n1}\det A(n|1) \\
            &= \sum_{i=1}^{n} (-1)^{1+i}  a_{i1}\det A(i|1).
            \end{align*}
        \end{enumerate}
        Si  $1 \le i,j \le n$, al número $\det A(i|j)$ se lo llama el \textit{menor $i,j$ de $A$}\index{menores de una matriz}\index{matriz!menores} y a $C^A_{ij}:= (-1)^{i+j} \det A(i|j)$ se lo denomina  el \textit{cofactor $i,j$ de $A$.}\index{cofactores de una matriz}\index{matriz!cofactores} Si la matriz $A$ está sobreentendida se denota, a veces, $C_{ij} := C^A_{ij}$.
    \end{definicion}
    
    Observemos, que con las definiciones introducidas  tenemos
    \begin{equation}\label{def-determinante}
    \det(A) = \sum_{i=1}^{n}  a_{i1}C^A_{i1}.
    \end{equation}
    A este cálculo  se lo denomina \textit{calculo del determinante por desarrollo por la primera columna}, debido  a que usamos los coeficientes de la primera columna,  multiplicados por los cofactores correspondientes. A veces, para simplificar,  denotaremos
    $$
    |A| := \det A.
    $$
    
    \begin{observacion*}[\textsc{Determinantes $\mathbf{2 \times 2}$}] Calculemos el determinante de las matrices $2 \times 2$. Sea 
        $$A=\begin{bmatrix}a&b\\c&d\end{bmatrix},$$  entonces
        $$
        \det A = a \det [d] - c \det [b] = ad-bc.
        $$
        
       Cuando estudiamos la matrices invertibles $2\times 2$ (ejemplo de p. \pageref{inv-2x2-0}), vimos que $A$ es invertible si y solo si $ ad-bc \not=0$,  es decir 
        \begin{equation}
        \text{\textit{$A$ es invertible si y solo si $ \det A \not=0$.}}
        \end{equation}
        Este resultado se generaliza para matrices $n \times n$. Más aún, la fórmula \eqref{inv-2x2},  que aquí reescribimos como
        \begin{equation*}
            A^{-1} = \dfrac{1}{\det(A)}
            \begin{bmatrix*}[c]C_{11}&C_{21}\\C_{12}&C_{22}\end{bmatrix*},
        \end{equation*} se generaliza también para matrices cuadradas de cualquier dimensión (ver el corolario \ref{cor-inv-x-det}).
    \end{observacion*}
    
    
    \begin{observacion*}[{\textsc{Determinantes $\mathbf{3 \times 3}$}}]  Calculemos el determinante de las matrices $3 \times 3$. Sea 
        $$A=\begin{bmatrix}a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33}\end{bmatrix},$$  entonces
        \begin{multline*}
        \det A = a_{11}\left|\begin{matrix}a_{22}&a_{23}\\a_{32}&a_{33}\end{matrix}\right|
        - a_{21}\left|\begin{matrix}a_{12}&a_{13}\\a_{32}&a_{33}\end{matrix}\right|
        + a_{31}\left|\begin{matrix}a_{12}&a_{13}\\a_{22}&a_{23}\end{matrix}\right|\\
        = a_{11}(a_{22}a_{33}- a_{23}a_{32})
        - a_{21}(a_{12}a_{33}-a_{13}a_{32}) 
        + a_{31}(a_{12}a_{23} - a_{13}a_{22}) \\
        =a_{11}a_{22}a_{33}- a_{11}a_{23}a_{32} 
        - a_{12}a_{21}a_{33}+ a_{13}a_{21}a_{32}+ \\
       a_{12}a_{23}a_{31} - a_{13}a_{22}a_{31}.    
        \end{multline*}

        
        Observar que  el determinante de una matriz $3 \times 3$ es una sumatoria de seis términos cada uno de los cuales es de la forma $\pm a_{1\,i_1}a_{2\,i_2}a_{3\,i_3}$ e $i_1i_2i_3$ puede ser cualquier permutación de $123$. La fórmula 
        \begin{multline}\label{det3x3}
        \det A =a_{11}a_{22}a_{33}- a_{11}a_{23}a_{32} 
        - a_{12}a_{21}a_{33}+ \\ a_{13}a_{21}a_{32}+ a_{12}a_{23}a_{31}
        - a_{13}a_{22}a_{31},
        \end{multline} 
        no es fácil de recordar, pero existe un procedimiento sencillo que nos permite obtenerla y es el siguiente: 
        \begin{enumerate}
            \item a la matriz original le agregamos las dos primeras filas al final, 
            \item ``sumamos''  cada producto de las diagonales descendentes y ``restamos'' cada producto de las diagonales ascendentes.
        \end{enumerate}
            
        %prueba y otra 
        \begin{equation}
        \begin{tikzpicture}[baseline=(A.center)]
        \tikzset{node style ge/.style={circle}}
        \tikzset{BarreStyle/.style =   {opacity=.4,line width=0.5 mm,line cap=round,color=#1}}
        \tikzset{SignePlus/.style =   {above left,,opacity=1}}
        \tikzset{SigneMoins/.style =   {below left,,opacity=1}}
        % les matrices
        \matrix (A) [matrix of math nodes, nodes = {node style ge},,column sep=0 mm] 
        { a_{11} & a_{12} & a_{13}  \\
            a_{21} & a_{22} & a_{23}  \\
            a_{31} & a_{32} & a_{33}  \\
            a_{11} & a_{12} & a_{13} \\
            a_{21} & a_{22} & a_{13}\\
        };
        
        \draw [BarreStyle=blue] (A-1-1.north west) node[SignePlus=blue] {$+$} to (A-3-3.south east) ;
        \draw [BarreStyle=blue] (A-2-1.north west) node[SignePlus=blue] {$+$} to (A-4-3.south east) ;
        \draw [BarreStyle=blue] (A-3-1.north west) node[SignePlus=blue] {$+$} to (A-5-3.south east) ;
        \draw [BarreStyle=red]  (A-3-1.south west) node[SigneMoins=red] {$-$} to (A-1-3.north east);
        \draw [BarreStyle=red]  (A-4-1.south west) node[SigneMoins=red] {$-$} to (A-2-3.north east);
        \draw [BarreStyle=red]  (A-5-1.south west) node[SigneMoins=red] {$-$} to (A-3-3.north east);
        \end{tikzpicture}
        \end{equation}
        
        Es decir,
        \begin{enumerate}
            \item[(a)]  se suman  $a_{11}a_{22}a_{33}$, $a_{21}a_{32}a_{13}$, $a_{31}a_{12}a_{23}$,  y
            \item[(b)]  se restan  $a_{31}a_{22}a_{13}$,  $a_{11}a_{32}a_{23}$,  $a_{21}a_{12}a_{33}$. 
        \end{enumerate}
    \end{observacion*}
    
    \begin{ejemplo*}
        Calcular el determinante de 
        $$ A = \begin{bmatrix}1&-2&2\\3&-1&1\\2&5&4\end{bmatrix}.$$
        
        La forma más sencilla es ampliando la matriz y calculando:
        \begin{equation*}
        \begin{tikzpicture}[baseline=(A.center)]
        \tikzset{node style ge/.style={circle}}
        \tikzset{BarreStyle/.style =   {opacity=.4,line width=0.5 mm,line cap=round,color=#1}}
        \tikzset{SignePlus/.style =   {above left,,opacity=1}}
        \tikzset{SigneMoins/.style =   {below left,,opacity=1}}
        % les matrices
        \matrix (A) [matrix of math nodes, nodes = {node style ge},,column sep=0 mm] 
        { 1&-2&2  \\
            3&-1&1  \\
            2&5&4  \\
            1&-2&2 \\
            3&-1&1\\
        };
        
        \draw [BarreStyle=blue] (A-1-1.north west) node[SignePlus=blue] {$+$} to (A-3-3.south east) ;
        \draw [BarreStyle=blue] (A-2-1.north west) node[SignePlus=blue] {$+$} to (A-4-3.south east) ;
        \draw [BarreStyle=blue] (A-3-1.north west) node[SignePlus=blue] {$+$} to (A-5-3.south east) ;
        \draw [BarreStyle=red]  (A-3-1.south west) node[SigneMoins=red] {$-$} to (A-1-3.north east);
        \draw [BarreStyle=red]  (A-4-1.south west) node[SigneMoins=red] {$-$} to (A-2-3.north east);
        \draw [BarreStyle=red]  (A-5-1.south west) node[SigneMoins=red] {$-$} to (A-3-3.north east);
        \end{tikzpicture}.
        \end{equation*}
        Luego 
        \begin{equation*}
        \begin{matrix*}[l]
        \det A &= 1\times (-1) \times 4 \quad\;\,\,+ 3\times5 \times2\quad\;+ 2\times (-2) \times 1\\
        &\quad- 2\times (-1) \times 2\quad - 1 \times 5 \times 1 \quad - 3 \times (-2) \times 4\\
        &= -4+ 30 -4 +4 -5 +24 \\
        &= 35.
        \end{matrix*}
        \end{equation*}
    \end{ejemplo*}
    
    \begin{observacion*}
        La regla para calcular el determinante de matrices $3 \times 3$ \textbf{\large no} se aplica a matrices $n \times n$ con $n \ne 3$.
    \end{observacion*}


    \begin{observacion*}
        Observemos que para calcular el determinante usando la definición,  en el primer paso recursivo hacemos una sumatoria de $n$ términos,  donde cada uno  es $\pm a_{i1}$ por un un determinante de orden $n-1$, lo cual implicará,  en cada término calcular una sumatoria con $n-1$ términos,  donde cada uno es $\pm a_{i2}$ por un un determinante de orden $n-2$. Es decir después del segundo paso tenemos $n(n-1)$  sumandos y cada uno es de la forma $\pm a_{i1}a_{k2}$ por un determinante de orden $n-2$. Siguiendo con este razonamiento, concluimos que para calcular el determinante debemos hacer una sumatoria de $n!$ términos (y cada uno de ellos es $\pm$ un producto  de  $n$ $a_{ij}$'s).    Teniendo esto en cuenta concluimos que para calcular el determinante por definición  hacen falta, al menos, hacer $n!$  operaciones. Para $n$ grandes (por ejemplo $n > 200$) esto es y será imposible para cualquier computadora. Como veremos en el corolario  \ref{cor-det-merf} hay maneras mucho más eficientes de calcular el determinante. 
    \end{observacion*}
    
   \begin{proposicion}\label{det-triang-sup}
    Sea $A \in M_n(\K)$ matriz triangular  superior cuyos elementos en la diagonal son $d_1,\ldots,d_n$. Entonces $\det A = d_1.d_2.\ldots d_n$.
    \end{proposicion}
    \begin{proof} Podemos demostrar el resultado por inducción sobre $n$: es claro que si $n=1$,  es decir si $A = [d_1]$, el determinante vale $d_1$. Por otro lado, si $n>1$,  observemos que $A(1|1)$ es también triangular superior con valores $d_2,\ldots,d_n$  en la diagonal principal. Entonces,  usamos la definición de la fórmula \eqref{def-determinante} y observamos que el desarrollo por la primera  columna solo tiene un término, pues esta columna solo tiene un coeficiente no nulo, el $d_1$ en la primera posición. Por lo tanto, 
        \begin{equation*}
        \det(A) = d_1 \det(A(1|1)) \stackrel{\text{(HI)}}{=} d_1.(d_2.\ldots.d_n).
        \end{equation*}
    \end{proof}
    
    \begin{corolario}
        $det \Id_n = 1$.
    \end{corolario}
    \begin{proof}
        Se deduce del hecho que $\Id_n$  es triangular superior y todo coeficiente de la diagonal principal vale 1.
    \end{proof}
    
    \begin{corolario}\label{cor-det-merf}
        Si $R$ es una MERF, entonces 
        \begin{align*}
        \det R = \left\{ \begin{matrix*}[l]
        1 \;&\text{si $R$ no tiene filas nulas,}\\
        0&\text{si $R$ tiene filas nulas.}
        \end{matrix*}\right.  
        \end{align*}
    \end{corolario}
    \begin{proof}
        Si $R$ no tiene filas nulas es igual a $\Id_n$ (lema \ref{lem-mtrx-merf-id}), luego $\det R = 1$. En general, $R$ es una matriz triangular superior y si tiene alguna fila nula $r$, entonces el coeficiente en la diagonal de la fila $r$ es igual a $0$ y por lo tanto     $\det R = 0$.
    \end{proof}

    \begin{ejemplo*} Veamos,  en el caso de una matriz $A= [a_{ij}]$ de orden  $2 \times 2$ que ocurre con el determinante cuando hacemos una operación elemental. 
            \begin{enumerate}
            \item Si $c \not=0$, sean $e$ y $e'$ las  operaciones elementales multiplicar por  $c$ la primera fila y multiplicar $c$ por la segunda fila, respectivamente. Entonces,
            \begin{equation*}
            e(A) = \begin{bmatrix} ca_{11}& ca_{12}\\ a_{21}&a_{22}\end{bmatrix}\;\text{ y }\; e'(A) = \begin{bmatrix} a_{11}& a_{12}\\ ca_{21}&ca_{22}\end{bmatrix},
            \end{equation*}
            luego 
            \begin{equation*}  
                \det e = \det \begin{bmatrix} ca_{11}& ca_{12}\\ a_{21}&a_{22}\end{bmatrix} = ca_{11}a_{22} - ca_{12}a_{21}\
            \end{equation*}
            y
            \begin{equation*}
              \det e' =  \det \begin{bmatrix} a_{11}& a_{12}\\ ca_{21}&ca_{22}\end{bmatrix} = ca_{11}a_{22} - ca_{12}a_{21}. 
            \end{equation*}  
            Por lo tanto, $\det e(A) =\det e'(A) = c \det A$. 
            \item Sea  $c \in \K$, si sumamos a la fila $2$ la fila $1$ multiplicada por $c$ o sumamos a la fila $1$  la fila $2$ multiplicada por $c$ obtenemos, respectivamente,
            \begin{equation*}
            e(A) = \begin{bmatrix} a_{11}& a_{12}\\ a_{21}+ ca_{11}&a_{22}+ ca_{12}\end{bmatrix}\;\text{ y }\; 
            e'(A) = \begin{bmatrix} a_{11} + ca_{21}& a_{12} + ca_{22}\\ a_{21}&a_{22}\end{bmatrix}.
            \end{equation*}
            Por lo tanto, 
            \begin{align*}
                \det \begin{bmatrix} a_{11}& a_{12}\\ a_{21}+ ca_{11}&a_{22}+ ca_{12}\end{bmatrix} &=
                a_{11}(a_{22}+ ca_{12})- a_{12}( a_{21}+ ca_{11}) \\
                &= a_{11}a_{22}+ ca_{11}a_{12}- a_{12} a_{21}- ca_{12} a_{11}\\
                &= a_{11}a_{22}- a_{12} a_{21} \\
                &= \det A.
            \end{align*}
            Luego,  $\det e(A) = \det A$. Análogamente,   $\det e'(A) = \det A$. 
            \item Finalmente, intercambiando la fila $1$ por la fila $2$ obtenemos la matriz
            \begin{equation*}
            e(A)=\begin{bmatrix} a_{21}& a_{22}\\ a_{11}&a_{12}\end{bmatrix},
            \end{equation*}
            por lo tanto 
            \begin{equation*}
            \det e(A)= \det \begin{bmatrix} a_{21}& a_{22}\\ a_{11}&a_{12}\end{bmatrix} = a_{21}a_{12} - a_{22}a_{11} = -\det A.
            \end{equation*}
        \end{enumerate}
    \end{ejemplo*}
    
    Todos los resultado del ejemplo anterior se pueden generalizar. 
    
    \begin{teorema} \label{det-prop-fundamentales}
        Sea $A  \in M_n(\K)$ y sean $1 \le r,s \le n$.
        \begin{enumerate}
            \item\label{det-prop-fundamentales-1} Sea $c \in \K$ y $B$ la matriz que se obtiene de $A$ multiplicando la fila $r$ por $c$, es decir $A  \stackrel{cF_r}{\longrightarrow} B$, entonces $\det B = c \det A$.
            \item \label{det-prop-fundamentales-2} Sea $c \in \K$, $r \ne s$ y $B$ la matriz que se obtiene de $A$ sumando a la fila $r$ la fila $s$ multiplicada por $c$, es decir  $A  \stackrel{F_r + cF_s}{\longrightarrow} B$, entonces $\det B = \det A$.
            \item\label{det-prop-fundamentales-3} Sea $r \ne s$ y sea $B$ la matriz que se obtiene de $A$ permutando la fila $r$ con la fila $s$, es decir  $A  \stackrel{F_r \leftrightarrow F_s}{\longrightarrow}B$, entonces $\det B = -\det A$.
            
        \end{enumerate}
    \end{teorema}
    \begin{proof}
        Ver  los teoremas \ref{th-E1}, \ref{th-E3}, \ref{th-E2} y  sus demostraciones.
    \end{proof}

    Este resultado nos permite calcular el determinante de matrices elementales.
    
    \begin{corolario}\label{det-mtrx-elem} Sea $n \in \mathbb N$ y $c \in \K$. Sean $1 \le r,s \le n$,  con $r \ne s$.
        \begin{enumerate}
            \item Si $c \not=0$, la matriz elemental que se obtiene de multiplicar por  $c$ la fila $r$ de $\Id_n$, tiene determinante igual a $c$.
            \item Sea $r \ne s$. La matriz elemental que se obtiene de sumar a la fila $r$ de $\Id_n$  la fila $s$ multiplicada por $c$, tiene determinante $1$.
            \item Finalmente, si $r \ne s$, la matriz elemental que se obtiene de intercambiar la fila $r$ por la fila $s$ de $\Id_n$ tiene determinante $-1$.
        \end{enumerate}    
    \end{corolario} 
    \begin{proof}
        Se deduce fácilmente del teorema anterior y del hecho de que $det \Id_n =1$.            
    \end{proof}
     
    \begin{corolario}\label{det-filas-iguales} Sea $A  \in M_n(\K)$.
        \begin{enumerate}
            \item\label{cor-det-1} Si $A$ tiene dos filas iguales,  entonces $\det A=0$.
            \item\label{cor-det-2} Si $A$ tiene una fila nula, entonces $\det A =0$.
        \end{enumerate}
    \end{corolario}
    \begin{proof}
        \ref{cor-det-1} Sea $A$ matriz donde $F_r = F_s$ con $r\ne s$. Luego, intercambiando la fila $r$ por la fila $s$ obtenemos la misma matriz. Es decir $A  \stackrel{F_r \leftrightarrow F_s}{\longrightarrow} A$. Por el teorema \ref{det-prop-fundamentales} \ref{det-prop-fundamentales-3}, tenemos entonces que $\det A = - \det A$, por lo tanto  $\det A =0$. 
        
        \ref{cor-det-2} Sea $F_r$ una fila nula de $A$, por lo tanto multiplicar por $2$ esa fila no cambia la matriz. Es decir $A  \stackrel{2F_r}{\longrightarrow} A$. Por el teorema \ref{det-prop-fundamentales} \ref{det-prop-fundamentales-1}, tenemos entonces que $\det A = 2\det A$, por lo tanto  $\det A =0$.
    \end{proof}
    
    
    
    \begin{teorema} \label{mtrx-inv-equiv3} Sean $A,B \in M_n(\K)$, entonces
        \begin{enumerate}
            \item $A$ invertible si y solo si  $\det(A)\ne0$.
            \item $\det (A B) = \det(A)\det(B)$. 
        \end{enumerate}
    \end{teorema}
\begin{proof}
    Ver el teoremas \ref{th-det-matriz-invertible} y \ref{th-dem-detAB} y .
\end{proof}

    \begin{corolario}\label{cor-det-propiedades}  Sean $A,B \in M_n(\K)$, entonces
        \begin{enumerate}
            \item\label{cor-det-inversa} si $A$ invertible $\det(A^{-1}) = \det(A)^{-1}$,
            \item\label{cor-det-prod} $\det (AB) = \det(BA)$.
            \end{enumerate}
    \end{corolario} 
    \begin{proof} 

        \ref{cor-det-inversa}  Por teorema \ref{mtrx-inv-equiv3}, $\det(AA^{-1}) = \det(A)\det(A^{-1})$. Como $AA^{-1} = \Id_n$, entonces $1 =\det(\Id_n) =  \det(AA^{-1}) = \det(A)\det(A^{-1})$. Por lo tanto  $\det(A^{-1}) = 1/\det(A)$.

        \ref{cor-det-prod}
        $\det (AB) = \det(A)\det(B) = \det(B)\det(A) = \det(BA)$.
    \end{proof}

    \begin{observacion*} Del  corolorario \ref{cor-det-propiedades} \ref{cor-det-prod} se deduce fácilmente,  por inducción, que si $A_1, \ldots, A_k$ son matrices $n \times n$,  y $A =  A_1 \cdots A_k$, entonces
    \begin{equation}\label{eq-prod-det}
        \det(A) =\det(A_1)\det(A_2) \ldots \det(A_k).
    \end{equation}
        
    \end{observacion*}
    
    \begin{corolario}\label{cor-det-merf-2}
        Sea $A$ matriz $n \times n$ y $E_1,E_2,\ldots,E_t$ matrices elementales tal que $E_tE_{t-1} \ldots E_1A =B$. Entonces, 
        \begin{equation}\label{det-elemen-mrf}
            \det(A) = \det(E_1)^{-1}\det(E_2)^{-1}\ldots \det(E_t)^{-1}\det(B). 
        \end{equation}
        En particular,  si $B$ tiene filas nulas, $\det(A) =0$ y  si $B$ es MERF y no tiene filas nulas 
        $$
        \det(A) = \det(E_1)^{-1}\det(E_2)^{-1}\ldots \det(E_t)^{-1}.
        $$ 
    \end{corolario}
    \begin{proof}
       Por \eqref{eq-prod-det}, tenemos
        \begin{equation*}
            \det(B) = \det(E_1)\det(E_2)\ldots \det(E_t) \det(A).
        \end{equation*}
        Por lo tanto, 
        \begin{equation*}
            \det(A) = \det(E_1)^{-1}\det(E_2)^{-1}\ldots \det(E_t)^{-1}\det(B).
        \end{equation*}
        Ahora bien, si $B$ tiene una fila nula, entonces su determinante es $0$ (corolario \ref{det-filas-iguales} \ref{cor-det-2}) y por lo tanto $\det(A) =0$. Si $B$ es MERF y no tiene filas nulas, entonces $B = \Id$, por lo tanto $\det(B)= 1$ y el resultado se deduce inmediatamente de \eqref{det-elemen-mrf}.
    \end{proof}

    El resultado anterior nos permite calcular determinantes reduciendo la matriz original a una matriz donde es más sencillo  calcular el determinate (por ejemplo, triangular). Esta reducción puede hacerse multiplicando por matrices elementales o,  equivalentemente,  realizando operaciones elementales de fila. 

    \begin{ejemplo*}
        Calcular el determinante de $A= \begin{bmatrix}
            1&1&2\\2&3&1\\3&4&-5
        \end{bmatrix}$.
    \end{ejemplo*}
    \begin{proof}[Solución] Mediante operaciones elementales de fila encontremos una matriz $B$ equivalente a  $A$ que sea triangular superior y apliquemos el corolario anterior,  sabiendo  que por proposición \ref{det-triang-sup} el determinante de $B$ es el producto de las entradas diagonales.  
        \begin{align*}
            A= \begin{bmatrix} 1&1&2\\2&3&1\\3&4&-5 \end{bmatrix}
            \stackrel{F_2-2F_1}{\stackrel{F_3-3F_1}{\longrightarrow}}
            \begin{bmatrix} 1&1&2\\0&1&-3\\0&1&-11 \end{bmatrix}
            \stackrel{F_3-F_2}{\longrightarrow}
            \begin{bmatrix} 1&1&2\\0&1&-3\\0&0&-8 \end{bmatrix} =B.
        \end{align*} 
    Como las operaciones elementales utilizadas (de tipo \ref{elem-2}) no cambian el determinante (teorema \ref{det-prop-fundamentales}), tenemos que
    $$
    \det(A) = \det(B) = 1 \cdot 1 \cdot (-8) = -8.
    $$ 
    \end{proof}

    \begin{definicion}\label{def-transpuesta-simetrica}
        Sea $A$ una matriz $m \times n$ con coeficientes en $\K$. La \textit{transpuesta}\index{matriz!transpuesta} de $A$, denotada $A^\t$, es la matriz  $n \times m$ que en la fila $i$ y columna $j$ tiene el coeficiente $[A]_{ji}$. Es decir
        \begin{equation*}
        [A^\t]_{ij} = [A]_{ji}.
        \end{equation*} 
        Si $A$ es una matriz $n \times n$, diremos que es \textit{simétrica}\index{matriz!simétrica} si $A^\t = A$. 
    \end{definicion}
    
    \begin{ejemplo*} Si
        $$A=\begin{bmatrix}a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33}\end{bmatrix},$$ 
        entonces
        $$A^\t=\begin{bmatrix}a_{11}&a_{21}&a_{31}\\a_{12}&a_{22}&a_{32}\\a_{13}&a_{23}&a_{33}\end{bmatrix}.$$ 
    \end{ejemplo*}
    
    \begin{ejemplo*}
        Si $$A=\begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix},$$ 
        entonces
        $$A^\t=\begin{bmatrix}1&3&5\\2&4&6\end{bmatrix}.$$ 
    \end{ejemplo*}
    
    
    En  general $A^\t$ es la matriz cuyas filas son las columnas de $A$ y viceversa. 
    
    \begin{ejemplo*}
        Si 
        \begin{equation*}
            A=\begin{bmatrix}1&2&3\\2&-1&4\\3&4&7\end{bmatrix},
        \end{equation*}
        entonces $A^\t =A$, es decir  $A$ es simétrica.
    \end{ejemplo*}
    
    \begin{proposicion}\label{prop-matriz-transpuesta} Sea $A$ matriz $m \times n$.
        \begin{enumerate}
            \item\label{transpuesta-1} $(A^\t)^\t = A$.
            \item\label{transpuesta-2}  Si $B$ matriz $n \times k$,  entonces
            \begin{equation*}
            (AB)^\t = B^\t A^\t.
            \end{equation*} 
            \item\label{transpuesta-3}  Sea $A$ matriz $n \times n$, entonces, $A$ invertible si y sólo si  $A^\t$ es invertible y  en ese caso $(A^\t)^{-1} = (A^{-1})^\t$.
        \end{enumerate}
    \end{proposicion}
    \begin{proof}
        \ref{transpuesta-1}     $[(A^\t)^\t]_{ij} = [A^\t]_{ji} = [A]_{ij}$. 

        \ref{transpuesta-2}    Por definición de transpuesta $(AB)^\t$ es una matriz $k \times m$.  Ahora observemos  que $B^\t$  es una matriz $k \times n$ y $A^\t$ es $n \times m$, luego tiene sentido multiplicar $B^\t$ por $A^\t$ y se obtiene también  una matriz  $k \times m$. La demostración de la proposición se hace comprobando que el coeficiente $ij$ de $(AB)^\t$ es igual al coeficiente $ij$ de $B^\t A^\t$ y se deja como ejercicio para el lector. 
        
        \ref{transpuesta-3} 
        \begin{align*}
            \text{$A$ invertible } &\Leftrightarrow \text{existe $B$ matriz $n \times n$ tal que $AB = \Id_n = BA$} \\
            &\Leftrightarrow (AB)^\t = \Id^\t_n = (BA)^\t\\
            &\Leftrightarrow B^\t A^\t = \Id_n = A^\t B^\t\\
            &\Leftrightarrow \text{$B^\t$ es  la inversa de $A^\t$}. 
        \end{align*}
        Es decir, $A$ invertible si y sólo si  $A^\t$ es invertible y si $B = A^{-1}$, entonces  $(A^\t)^{-1} = B^\t$.
    \end{proof}
    
    Observar que por inducción no es complicado probar que si $A_1,\ldots, A_k$ son matrices,  entonces 
    \begin{equation*}
    (A_1\ldots A_k)^\t = A_k^\t\ldots A_1^\t.
    \end{equation*}
    
    \begin{ejemplo*} Veamos las transpuesta de las matrices elementales  $2 \times 2$.
        \begin{enumerate}
            \item Si $c \not=0$, multiplicar por  $c$ la primera fila y multiplicar $c$ por la segunda fila son, respectivamente,
            \begin{equation*}
            E = \begin{bmatrix} c& 0\\ 0&1\end{bmatrix}\;\text{ y }\; E = \begin{bmatrix} 1& 0\\ 0&c\end{bmatrix},
            \end{equation*}
            por lo tanto $E^\t$ es la misma matriz en ambos casos. 
            \item si  $c \in \K$, sumar a la fila $2$ la fila $1$ multiplicada por $c$ o sumar a la fila $1$ la fila $2$ multiplicada por $c$ son, respectivamente,
            \begin{equation*}
            E_1 = \begin{bmatrix} 1& 0\\ c&1\end{bmatrix}\;\text{ y }\; E_2 = \begin{bmatrix} 1& c\\ 0&1\end{bmatrix},
            \end{equation*}
            por lo tanto 
            \begin{equation*}
            E_1^\t = \begin{bmatrix} 1& c\\ 0&1\end{bmatrix} = E_2 \;\text{ y }\; E_2^\t = \begin{bmatrix} 1& 0\\ c&1\end{bmatrix} =E_1,
            \end{equation*}
            \item Finalmente, intercambiando la fila $1$ por la fila $2$ obtenemos la matriz
            \begin{equation*}
            E = \begin{bmatrix} 0& 1\\ 1&0\end{bmatrix},
            \end{equation*}
            por lo tanto $E^\t =E$
        \end{enumerate}
    \end{ejemplo*}
    

\begin{observacion*}
    En  el caso  de matrices $2 \times 2$ podemos comprobar fácilmente que $\det A^\t = \det A$:
    $$
    \det A^\t = \det \begin{bmatrix} a_{11} & a_{21} \\ a_{12}& a_{22}\end{bmatrix} = 
    a_{11}a_{22} - a_{21}a_{12} = \det A. 
    $$
\end{observacion*}

También vale este resultado para matrices $n \times n$.

\begin{teorema}\label{det-a-trans} Sea $A \in M_n(\K)$,  entonces 
    $\det(A) = \det(A^\t)$
\end{teorema}
\begin{proof}
    Ver el teorema \ref{th-det-a-trans-app}.
\end{proof}
    
    El resultado anterior permite obtener resultados nuevos del cálculo de determinante a partir de resultados vistos anteriormente. 
    
    
    \begin{proposicion}\label{det-triang-inf}
    Sea $A \in M_n(\K)$ matriz triangular  inferior cuyos elementos en la diagonal son $d_1,\ldots,d_n$. Entonces $\det A = d_1.d_2.\ldots d_n$.
    \end{proposicion}
    \begin{proof}
        Si $A$ es triangular inferior con elementos en la diagonal $d_1,\ldots,d_n$,  entonces $A^\t$ es triangular superior con  elementos en la diagonal $d_1,\ldots,d_n$. Por la proposición \ref{det-triang-sup}, $\det A^\t = d_1\ldots d_n$. Por el teorema  \ref{det-a-trans} obtenemos el resultado. 
    \end{proof}


    
    \begin{teorema}\label{det-opr-col}
        Sea $A  \in M_n(\K)$ y sean $1 \le r,s \le n$.
        \begin{enumerate}
            \item Sea $c \in \K$ y $B$ la matriz que se obtiene de $A$ multiplicando la columna $r$ por $c$, entonces $\det B = c \det A$.
            \item  Sea $c \in \K$ y $B$ la matriz que se obtiene de $A$ sumando a la columna $r$ la columna $s$ multiplicada por $c$, entonces $\det B = \det A$.
            \item Sea $B$ la matriz que se obtiene de $A$ permutando la columna $r$ con la fila $s$, entonces $\det B = -\det A$.
        \end{enumerate}
    \end{teorema}
    \begin{proof}
        Las operaciones por columna del enunciado se traducen a operaciones por fila de la matriz $A^\t$. Luego, aplicando los resultados del teorema  \ref{det-prop-fundamentales} y usando el hecho de que $\det(A) = \det(A^\t)$ y $\det(B) = \det(B^\t)$ en cada caso, se deduce el corolario. 
    \end{proof}
    
    \begin{corolario} Sea $A  \in M_n(\K)$.
        \begin{enumerate}
            \item\label{det-1} Si $A$ tiene dos columnas iguales,  entonces $\det A=0$.
            \item\label{det-2} Si $A$ tiene una columna nula, entonces $\det A =0$.
        \end{enumerate}
    \end{corolario}
    \begin{proof}
        \ref{det-1} Si $A$ tiene dos columnas iguales,  entonces $A^\t$ tiene dos filas iguales, luego, por corolario \ref{det-filas-iguales} \ref{cor-det-1},  $\det A^\t =0$ y por lo tanto $\det A=0$.
        
        \ref{det-2} Si $A$ tiene una columna nula,  entonces $A^\t$ tiene una fila nula, luego, \ref{det-filas-iguales} \ref{cor-det-2}, $\det A^\t =0$ y por lo tanto $\det A=0$.
    \end{proof}
    
        El siguiente teorema nos dice  que es posible calcular el determinante desarrollándolo por cualquier fila o cualquier columna. 
    
    
    \begin{teorema} \label{th-expancion-cofactores} El determinante de una matriz $A$ de orden $n \times n$ puede ser calculado por la expansión de los cofactores en  cualquier columna o cualquier fila. Más específicamente, 
        \begin{enumerate}
            \item si usamos la expansión por la $j$-ésima columna, $1 \le j \le n$, tenemos
            \begin{align*}
            \det A &= \sum_{i=1}^{n} a_{ij} C_{ij} \\
            & = a_{1j}C_{1j}+a_{2j}C_{2j}+\cdots+a_{nj}C_{nj}.
            \end{align*} 
            \item si usamos la expansión por la $i$-ésima fila, $1 \le i \le n$, tenemos
            \begin{align*}
            \det A &= \sum_{j=1}^{n} a_{ij} C_{ij} \\
            & = a_{i1}C_{i1}+a_{i2}C_{i2}+\cdots+a_{in}C_{in};
            \end{align*} 
        \end{enumerate}
    \end{teorema}
\begin{proof}
    Ver la demostración de el teorema  \ref{th-dessarrollo-por-columnas}.
\end{proof}

\subsection*{$\S$ Ejercicios}
\begin{enumex}
    \item Calcular el determinante de las siguientes matrices.
        \begin{enumex}
            \begin{minipage}{0.4\textwidth}
            \item $A=\begin{bmatrix}
                1&-1\\-2&1 
            \end{bmatrix}$\,,
        \end{minipage}
        \begin{minipage}{0.4\textwidth}
            \item $B=\begin{bmatrix}
                1&1&2\\0&3&1\\3&4&-5
            \end{bmatrix}$\,,
        \end{minipage}

        \begin{minipage}{0.4\textwidth}
            \item $C=\begin{bmatrix}
                0&1&0&0\\2&0&0&0\\0&0&0&3\\0&0&1&0
            \end{bmatrix}$\,,
        \end{minipage}
        \begin{minipage}{0.4\textwidth}
            \item $D=\begin{bmatrix}
                0&0&3&3\\3&0&1&2\\1&0&2&4\\2&1&3&2
            \end{bmatrix}$\,.
        \end{minipage}
        \end{enumex}
    \item  Sea $A \in \R^{n \times n}$.
    \begin{enumex}
        \item Probar que $\det(-A) = (-1)^n \det(A)$. 
        \item Diremos que $A$  es \textit{antisimétrica} si $A^\t = -A$. Probar que si $n$  es impar y $A$ antisimétrica,  entonces $\det(A) =0$.
    \end{enumex}   
    \item  Sea $A \in \R^{n \times n}$. Diremos que $A$  es \textit{ortogonal} si $A^\t = A^{-1}$.
    \begin{enumex}
        \item Probar que si $A$ es ortogonal, entonces $\det(A) = \pm 1$. 
        \item Dar un ejemplo de una matriz ortogonal con $\det(A) = -1$. 
        \item Probar que $A$ es ortogonal si y solo si  existe una $\cB = \{u_1,\ldots,u_n\}$ BON de $\R^n$ tal que $A = [u_1 \, \cdots \, u_n]$ . Ver la sección  \ref{seccion-bases-ortonormales-rn} para la definición de BON.
    \end{enumex}   

    \item Sea $P$ una matriz de permutación $n \times n$ (ver sección \ref{seccion-matrices-elementales}, ejercicio \ref{matriz-de-permutacion}). Probar que $P$  es invertible y que $P^{-1} = P^\t$. 
    \item En  este ejercicio trabajaremos  con matrices de bloques $r,s$ es decir matrices del tipo 
    $$
    \begin{bmatrix} A &B \\ C & D \end{bmatrix}
    $$ 
    con $A \in \K^{r \times r}$ ,  $B \in \K^{r \times s}$ ,  $C \in \K^{s \times r}$ y  $D \in \K^{s \times s}$ (ver capítulo  \ref{capitulo-algebra-de-matrices},  ejercicio \ref{matriz-de-bloques}). En  este contexto, probaremos, paso a paso, que 
     \begin{equation}\label{det-matriz-bloques-triang}
        \det \begin{bmatrix} A &B \\ 0 & C \end{bmatrix} = \det(A)\det(C).
     \end{equation}
    \begin{enumex}
        \item Probar que 
        $$
        \det \begin{bmatrix} A &0 \\ 0 & B \end{bmatrix} = \det(A)\det(B).
        $$
        \item Sea $C \in \K^{s \times s}$  y sea $R$ la MERF de $C$ tal que $C = E_1\ldots E_k R$ donde $E_i$  es una matriz elemental. Probar que 
        $$
        \begin{bmatrix} A &B \\ 0 & C \end{bmatrix} =  \begin{bmatrix} \Id  &0 \\ 0 & E_1\end{bmatrix} \cdots \begin{bmatrix} \Id  &0 \\ 0 & E_k \end{bmatrix} \begin{bmatrix} A  &B \\ 0 & R \end{bmatrix}.
        $$
        \item Usando la notación y el resultado del ítem anterior, probar que 
        $$
        \det \begin{bmatrix} A &B \\ 0 & C \end{bmatrix} = \det(E_1) \ldots \det(E_k) \det \begin{bmatrix} A  &B \\ 0 & R \end{bmatrix}.
        $$ 
        \item Si $C$ invertible, probar que
        $$
        \det \begin{bmatrix} A &B \\ 0 & C \end{bmatrix} = \det(C)  \det \begin{bmatrix} A  &B \\ 0 & \Id \end{bmatrix}.
        $$ 
        \item Probar que 
        $$\det\begin{bmatrix} A  &B \\ 0 & \Id \end{bmatrix}= \det\begin{bmatrix} A  &0 \\ 0 & \Id \end{bmatrix}.$$
        [Ayuda: las operaciones elementales de fila de tipo E1 no cambian el determinanate.]
        \item Probar \eqref{det-matriz-bloques-triang}. 
    \end{enumex}

        
    \item\label{ejercicio-area-de un paralelogramo} Sean $v,w$ dos elementos no nulos de $\R^2$ tal que uno no es múltiplo del otro. El conjunto de elementos de $\R^2$ 
    $$
    \left\{ t_1 v + t_2 w: 0 \le t_1 \le 1, \quad 0 \le t_2 \le 1 \right\}
    $$
    se llama el \textit{paralelogramo generado por $v$ y $w$}. 
    
    Probaremos, paso a paso, que el área del paralelogramo generado por $v$ y $w$ es $\pm\det\begin{bmatrix} v \\ w \end{bmatrix}$ y usaremos para ello una mezcla de argumentos geométricos y algebraicos. 
    \begin{enumex}
        \item Sea $v = (a,b)$ y $w = (c,0)$ probar que el área del paralelogramo generado por $v$ y $w$ (un paralelogramo horizontal) es 
        $$
        |bc| = \det\begin{bmatrix}
            a&b\\c&0
        \end{bmatrix}. 
        $$
        Es decir,  es base $\times$ altura. 
        \item Sea $\theta$  es el ángulo comprendido entre $v =(a,b)$ y $w = (c,d)$. Usando la fórmula de $\cos(\theta)$  que se obtiene a partir del producto escalar (fórmula \eqref{eq-cos-theta}),  demostrar  que
        $$
        |\sin(\theta)| = \left|\frac{ad-bd}{||v||||w||}\right|.
        $$
        \item Teniendo en cuenta la propiedad de que  el área de un paralelogramo es la longitud de la base por la altura probar que el área del paralelogramo generado por $(a,b)$, $(c,d)$ es
        $$
        A \quad = \quad  \left|\det\begin{bmatrix}
            a&b\\ c&d
        \end{bmatrix}\right|. 
        $$
        \begin{observacion*} El volumen  de un paralelepípedo en $\R^3$ determinado por $3$ vectores $v_1=(a_1, a_2, a_3)$, $v_2= (b_1, b_2, b_3)$, $v_3= (c_1, c_2, c_3)$ también está dado por la fórmula 
            $$
            V = \left| \det \begin{bmatrix}
                a_1 & a_2 & a_3 \\
                b_1 & b_2 & b_3 \\
                c_1 & c_2 & c_3
        \end{bmatrix} \right|.
            $$
            La fórmula se generaliza a todas las dimensiones $n \ge 2$. 
        \end{observacion*}
    \end{enumex}
\end{enumex}


\end{section}    


\begin{section}{Autovalores y autovectores}\label{seccion-autovalores-y-autovectores-de-matrices}


    En el capítulo \ref{chap-trans-lin} veremos la definición y propiedades de las transformaciones lineales entre espacios vectoriales. Las transformaciones lineales juegan un rol muy importante en toda la matemática, pasando por el álgebra, el análisis, la geometría, etc. 

    Si $A$  es una matriz $m \times n$ y $v$  es una matriz $n \times 1$,  es decir un vector,  el producto $Av$ es un vector $ m \times 1$. Recordemos que, por comodidad, identificamos $\mathbb K^{n}$ con  $\mathbb K^{n \times 1}$,  es decir escribiremos indistintamente un vector columna o una $n$-upla. Esta multiplicación de matrices por vectores es una transformación lineal y veremos, también en el capítulo \ref{chap-trans-lin}, que toda transformación lineal puede ser  representada  como la multiplicación de una matriz por  un vector. En esta sección estudiaremos,  dada una matriz $A$, los vectores que $v \in \mathbb K^{n}$ tales que $Av = \lambda v$,  con  $\lambda \in \K$. Motiva el estudio de estos vectores el hecho de que es  sencillo multiplicar a izquierda por matrices diagonales:
    \begin{equation*}
        \begin{bmatrix}
            d_1 & 0 & \cdots &0 \\
            0 & d_2 & \cdots &0 \\
            \vdots & \vdots & \ddots &\vdots \\
            0 & 0 & \cdots & d_n 
            \end{bmatrix}
        \begin{bmatrix}
            x_1\\x_2 \\\vdots \\x_n
        \end{bmatrix}
        = \begin{bmatrix}
            d_1x_1\\d_2x_2 \\\vdots \\d_nx_n
        \end{bmatrix}
    \end{equation*} 
    (ver observación \ref{obs-mult-matrices-diagonales}). En particular,  si $e_i$  es el vector columna  que tiene un $1$ en la fila $i$ y todas las demás filas iguales a $0$,  entonces
    \begin{equation*}
        \begin{bmatrix}
            d_1 & 0 & \cdots &0 \\
            0 & d_2 & \cdots &0 \\
            \vdots & \vdots & \ddots &\vdots \\
            0 & 0 & \cdots & d_n 
            \end{bmatrix}
        e_i
        = d_ie_i
    \end{equation*} 
    y esta propiedad caracteríza las matrices diagonales, es decir una matriz $D$ es diagonal si  y solo si $De_i =d_ie_i$ para $1 \le i \le n$. Dicho de  otra forma una matriz es diagonal si y solo si  al aplicarla sobre algún $e_i$ obtenemos un múltiplo de $e_i$. 

    \begin{definicion}
        Sea $A\in\K^{n\times n}$. Se dice que {$\lambda\in\K$} es un \textit{autovalor}\index{autovalor de una matriz} de $A$ y si existe \textit{$v\in\K^{n}$} no nulo tal que 
        \begin{align*}
        Av=\lambda v .
        \end{align*}
        En ese caso decimos que $v$  es un \textit{autovector}\index{autovector de una matriz} asociado a $\lambda$
        \end{definicion}

        Aunque no siempre es posible caracterizar una matriz $A$  por sus autovectores, el estudio de este tipo de vectores  resulta importante para obtener información de la matriz y propiedades de la misma.  

        \begin{observacion*} Nada impide, por definición, que un autovalor pueda valer $0$, pero un autovector \textit{nunca} puede ser $0$. 
        \end{observacion*}

        \begin{ejemplo*}
        $1$ es un autovalor de $\Id_n$ y todo $v\in\K^n$ es un autovector asociado a $1$ pues
        \begin{align*}
        \Id_n v= v 
        \end{align*}
        \end{ejemplo*}
        
        \begin{ejemplo*}
        $0$ es un autovalor de  
        $\left[\begin{array}{cc}
                    0&1\\0&0
                    \end{array}
        \right]$
        y $\left[\begin{array}{c}
                    1\\0
                    \end{array}
        \right]$ es un autovector asociado a $0$ pues
        \begin{align*}
        \left[\begin{array}{cc}
                    0&1\\0&0
                    \end{array}
        \right]
        \left[
        \begin{array}{c}
        1\\0 
        \end{array}
        \right]
        =
        \left[
        \begin{array}{c}
        0\\0 
        \end{array}
        \right]=
        0\left[
        \begin{array}{c}
        1\\0 
        \end{array}
        \right]
        \end{align*}
        \end{ejemplo*}

        \begin{ejemplo}\label{autovalores-3x3} Sea 
            $$
            B = \begin{bmatrix}3&2&{ - 1}\\2&3&1\\0&0&5\end{bmatrix}.
            $$
            Esta matriz tiene autovalores $1$ y  $5$. El vector $\begin{bmatrix}1\\0\\-2\end{bmatrix}$  tiene autovalor 5, el  vector $\begin{bmatrix}0\\1\\2\end{bmatrix}$ tiene también autovalor 5 y el vector $\begin{bmatrix}1\\-1\\0\end{bmatrix}$ tiene autovalor 1. Comprobemos esto,  por ejemplo, para el vector $\begin{bmatrix}1\\0\\-2\end{bmatrix}$:                          
            \begin{align*}
            B\begin{bmatrix}1\\0\\-2\end{bmatrix} &= \begin{bmatrix}3&2&{ - 1}\\2&3&1\\0&0&5\end{bmatrix}\begin{bmatrix}1\\0\\-2\end{bmatrix} = \begin{bmatrix}3+0+2\\2+0-2\\0+0-10\end{bmatrix} = \begin{bmatrix}5\\0\\-10\end{bmatrix} = 5\begin{bmatrix}1\\0\\-2\end{bmatrix}.
            \end{align*}
            Es decir,  $5$ es autovalor de $B$ y $\begin{bmatrix}1\\0\\-2\end{bmatrix}$ es un autovector asociado a $5$.

            Análogamente, 
            \begin{align*}
            B\begin{bmatrix}0\\1\\2\end{bmatrix} &= \begin{bmatrix}3&2&{ - 1}\\2&3&1\\0&0&5\end{bmatrix}\begin{bmatrix}0\\1\\2\end{bmatrix} = \begin{bmatrix}0+2-2\\0+3+2\\0+0+10\end{bmatrix} = \begin{bmatrix}0\\5\\10\end{bmatrix} = 5\begin{bmatrix}0\\1\\2\end{bmatrix}.
            \end{align*}
            y
            \begin{align*}
                B\begin{bmatrix}1\\-1\\2\end{bmatrix} &= \begin{bmatrix}3&2&{ - 1}\\2&3&1\\0&0&5\end{bmatrix}\begin{bmatrix}1\\-1\\0\end{bmatrix} = \begin{bmatrix}3-2+0\\2-3+0\\0+0+0\end{bmatrix} = \begin{bmatrix}1\\-1\\0\end{bmatrix} = 1\begin{bmatrix}1\\-1\\0\end{bmatrix}.
            \end{align*}

            ¿Cómo hicimos en este ejemplo para encontrar los autovalores y algunos autovectores?  En lo que sigue de esta sección veremos un método general para encontrar los autovalores y autovectores de una matriz.
        \end{ejemplo}

        \begin{observacion*}
        La existencia de autovalores dependen del cuerpo donde estamos trabajando. 
        Por ejemplo sea $A \in \R^{n\times n}$,  definida por 
        $$A=
        \begin{bmatrix}
            0&-1\\1&0
        \end{bmatrix}
        .$$ 
        Entonces,  $A$ no tiene autovalores reales. Veamos por qué.
        \begin{equation}\label{eq-no-aut-1}
            \begin{bmatrix}
                0&-1\\1&0
            \end{bmatrix}
            \begin{bmatrix}
                x_1\\x_2
            \end{bmatrix} =
            \begin{bmatrix}
                -x_2\\x_1
            \end{bmatrix}
        \end{equation}
        Si $\lambda$ fuera un autovalor y $\begin{bmatrix}x_1\\x_2\end{bmatrix}$ fuera autovector, tendríamos
        \begin{equation}\label{eq-no-aut-2}
            \begin{bmatrix}
                0&-1\\1&0
            \end{bmatrix}
            \begin{bmatrix}
                x_1\\x_2
            \end{bmatrix} =
            \begin{bmatrix}
                \lambda x_1\\\lambda x_2
            \end{bmatrix}.
        \end{equation}
        Luego, por \eqref{eq-no-aut-1} y \eqref{eq-no-aut-2}, $-x_2 = \lambda x_1$ y $x_1 = \lambda x_2$,  entonces   $-x_2 = \lambda x_1 = \lambda^2 x_2$. Si  $\lambda \ne0$,  entonces $\lambda^2 >0$, y eso implica que $x_2=0$ y  en consecuencia $x_1=0$. Si $\lambda=0$, también $x_1=x_2 =0$. Es decir, en ambos casos $\begin{bmatrix}x_1\\x_2\end{bmatrix} = \begin{bmatrix}0\\0\end{bmatrix} $ y no es autovector.

        Veremos más adelante que si permitimos autovalores complejos entonces esta matriz tiene  autovalores.
        \end{observacion*}

        \begin{definicion}
        
        Dado $i\in\{1, ..., n\}$, como ya vimos se denota \textit{$e_i$} al vector columna de $\K^n$ cuyas coordenadas son todas ceros excepto la coordenada $i$ que es un $1$
        \begin{align*}
        e_i=\left[
        \begin{array}{c}
        0\\ \vdots\\1\\ \vdots\\0
        \end{array}
        \right]
        \end{align*}
        El conjunto $\{e_1, ..., e_n\}$ se llama \textit{base canónica}\index{base canónica} de $\K^n$.
        \end{definicion}

        \begin{ejemplo*}
        En $\K^3$ la base canónica es  $
        e_1=\left[
        \begin{array}{c}
        1\\0\\0
        \end{array}
        \right]$, $
        e_2=\left[
        \begin{array}{c}
        0\\1\\0
        \end{array}
        \right]$, 
        $
        e_3=\left[
        \begin{array}{c}
        0\\0\\1
        \end{array}
        \right]
        $
        \end{ejemplo*}

        \begin{ejemplo*}
        Sea $D\in\K^{n\times n}$ una matriz diagonal con entradas $\lambda_1$, $\lambda_2$, ..., $\lambda_n$. Entonces $e_i$ es un autovector con autovalor $\lambda_i$  $,\,\forall\, i\in\{1, ..., n\}$
        \end{ejemplo*}

        
        
        
        \vskip .2cm
        \begin{definicion}
        Sea $A\in\K^{n\times n}$ y $\lambda\in\K$ un autovalor de $A$. El \textit{autoespacio}\index{autoespacio} asociado a $\lambda$ es
        $$
        V_\lambda=
        \{v\in\K^n\mid Av=\lambda v\}.
        $$
        Es decir, \textit{$V_\lambda$} es el conjunto  formado por todos los autovectores asociados a $\lambda$ y el vector nulo.
        \end{definicion}

        El conjunto de todos los autovectores con un mismo autovalor es \textit{invariante por la suma y la multiplicación por escalares.}    En particular los múltiplos de un autovector son autovectores con el mismo autovalor.

        \begin{teorema} Sea $A$ matriz $n \times n$ y $\lambda \in \K$. 
        Si $v, w $ pertenecen a $V_\lambda$, el autoespacio de $A$ asociado a $\lambda$, entonces $v+tw \in V_\lambda$ para cualquier $\t \in \K$. 
        \end{teorema}
        
        \begin{proof}
         $$
        A(v+tw)=Av+tAw=\lambda v+t\lambda w=\lambda(v+tw).
        $$
        \end{proof}
        
        \begin{proposicion} Sea $A$ matriz  $n \times n$ y $v,w \in \K^n$ autovectores con autovalores $\lambda, \mu \in \K$,  respectivamente. Entonces.  $\lambda \ne \mu$ implica que $v \ne w$. Es decir,  autovectores con autovalores distintos son distintos.
        \end{proposicion}
        
       \begin{proof}
        Supongamos que $v = w$,  entonces  $Av=\lambda v$ y $Av=\mu v$. Luego, $\lambda v=\mu v$ y por lo tanto 
        $$
        (\lambda-\mu) v=
        \left[
        \begin{array}{c}
        (\lambda-\mu)v_1\\ \vdots\\(\lambda-\mu)v_n
        \end{array}
        \right]
        =
        \left[
        \begin{array}{c}
        0\\ \vdots\\0
        \end{array}
        \right]
        $$
        
        Como $v\neq0$ por ser autovector, alguna de sus coordenadas es no nula. Entonces $\lambda-\mu$ tiene que ser $0$ o dicho de otro modo $\lambda=\mu$, lo cual es un absurdo.
        \end{proof}
        
        
        \begin{problema*}
        Hallar los autovalores de $A\in\K^{n\times n}$ y para cada autovalor, describir explícitamente el autoespacio asociado.
    \end{problema*}
        
        \begin{itemize}
            \item En otras palabras nos preguntamos que $\lambda\in\K$ y  que $v\in\K^{n}$ satisfacen
            $$
            Av=\lambda v
            \Longleftrightarrow
            \lambda v-Av=0
            \Longleftrightarrow
            (\lambda \Id-A)v=0 .
            $$
            \item La última igualdad es un sistema de ecuaciones lineales. Queremos ver entonces si existe un $v\in\K^{n}$ no nulo que sea solución del sistema homogéneo
            \begin{equation*}
                (\lambda \Id-A)X=0.\tag{*}
            \end{equation*}
            \item  Un  sistema $BX =0$ tiene solución no trivial sii $\det(B)=0$. Por lo tanto (*) tiene  solución no trivial si y sólo si 
            $$
            \det(\lambda \Id-A)=0.
            $$ 
        \end{itemize}
        
        Estos sencillos pasos demuestran  lo siguiente.

        \begin{proposicion}
        $\lambda\in\K$ es un autovalor de $A$ y $v\in\K^n$ es un autovector asociado a $\lambda$ si y sólo si
    
        \begin{itemize}
            \item $\det(\lambda \Id-A)=0$
            \item $v$ es solución del sistema homogéneo $(\lambda \Id-A)X=0$
        \end{itemize}
    \end{proposicion}

        Esta es casi la respuesta a nuestro problema. Para dar una respuesta más operativa introducimos el siguiente polinomio.
    
        \begin{definicion}
        Sea $A\in\K^{n\times n}$. El \textit{polinomio característico}\index{polinomio característico} de $A$ es
        \begin{align*}
        \chi_A(x)=\det(x \Id-A)
        \end{align*}
        \end{definicion}

        \begin{ejemplo*}
        El polinomio  característico de $\Id_n$ es 
        \begin{align*}
        \chi_{\Id_n}(x)=(x-1)^n 
        \end{align*}
        \end{ejemplo*} \vskip -.6cm
        \begin{proof}
            $x\Id- \Id=(x-1)\Id$ es una matriz diagonal con $x-1$ en todas las entradas de la diagonal. Entonces el determinante es el producto de los valores de la diagonal. 
        \end{proof}

        En general,  si $A = [a_{ij}]$ matriz $n \times n$, tenemos que
            \begin{equation*}
            \chi_A(x) = \det(x\,\Id-A) = \det
            \begin{bmatrix}
            x-a_{11}&-a_{12}&\cdots&-a_{1n}\\
            -a_{21}&x-a_{22}&\cdots&-a_{2n}\\
            \vdots&\vdots&\ddots&\vdots\\
            -a_{n1}&-a_{n2}&\cdots&x-a_{nn}\\
            \end{bmatrix}
            \end{equation*} 
            y el polinomio característico de $A$ es un polinomio  de grado $n$,  más precisamente  
            $$
            \chi_A(x) =x^n + a_{n-1}x^{n-1}+ \cdots + a_1x + a_0.
            $$ 
            Esto se puede demostrar por inducción. 
            
        \begin{ejemplo*}
        El polinomio  característico de 
        $A=\begin{bmatrix} 0&1\\0&0 \end{bmatrix}$ es  $\chi_{A}(x)=x^2$.
        \end{ejemplo*}

        \begin{ejemplo*} Si $A=\begin{bmatrix}a&b\\c&d \end{bmatrix}$, entonces $\chi_{A}(x)=(x-a)(x-d) - bc $.
        \end{ejemplo*}
        \begin{proof}  $A-x \Id=\begin{bmatrix} x-a&-b\\-c&x-d \end{bmatrix}$ y usamos la fórmula del determinante de una matriz $2\times 2$. 
        \end{proof}

        \begin{proposicion}
                Sea $A\in \K^{n \times n}$.  Entonces $\lambda\in \K$ es autovalor si y sólo si $\lambda$ es raíz del polinomio característico de $A$. 
            \end{proposicion}
            \begin{proof}
                ~
                \begin{center}
                    \begin{tabular}{rl}
                        $\lambda$ es autovalor &$\Leftrightarrow$ existe $v \ne 0$ tal que $Av = \lambda v$ \\
                                    &\\
                                    &$\Leftrightarrow$ $0 = \lambda v -A  =   \lambda \Id v -Av =  (A-\lambda \Id)v$ \\
                                    &\\
                                    &$\Leftrightarrow$ $(\lambda \Id-A)X=0$ tiene solución no trivial \\
                                    &\\
                                    &$\Leftrightarrow$ $\chi_A(\lambda) = \det(\lambda \Id-A) =0$ \\
                                    &\\
                                    &$\Leftrightarrow$ $\lambda$ es raíz del polinomio característico.  
                                \end{tabular}
                \end{center}
            \end{proof}

        \begin{observacion*} Sea $A\in \K^{n \times n}$, entonces podemos aplicar el siguiente método para encontrar autovalores y autovectores de $A$.
            \begin{enumerate}
                \item Calcular $\chi_A(x) =\det(x \Id-A)$,
                \vskip .2cm
                \item Encontrar las raíces $\lambda_1,\ldots,\lambda_k$ de $\chi_A(x)$.
                
                \noindent No siempre es posible hacerlo, pues no hay una fórmula o método general para encontrar las raíces de polinomios de grado 5 o superior.
                \vskip .2cm
                \item Para cada $i$ con $1 \le i \le k$ resolver el sistema de ecuaciones lineales:
                \begin{equation*}
                    (\lambda_i \Id-A)X = 0.
                \end{equation*}
                Las soluciones no triviales  de este sistema son los autovectores con autovalor $\lambda_i$.
            \end{enumerate}
        \end{observacion*}
        
        \begin{ejemplo*}
            Encontrar autovalores y autovectores de la matriz 
            \begin{equation*}
                A=\begin{bmatrix}
                    3&-2 \\ 1&0
                \end{bmatrix}.
            \end{equation*}
        \end{ejemplo*}
        \begin{proof}[Solución] ~
            \begin{enumerate}
                \item  $\chi_A(x)= \det\left[\begin{matrix}
                x-3& 2 \\ -1 & x
                \end{matrix} \right]= x^2 -3x +2   =(x -1)(x -2)$.
                \vskip .2cm
                \item Los autovalores de $A$ son las raíces de  $\chi_A(x)$: $1$ y $2$.
                \vskip .2cm
                \item Debemos resolver los sistemas de ecuaciones:
                \begin{equation*}
                    (\Id-A)X = 0,\qquad (2 \Id-A)X = 0.
                \end{equation*}
            \end{enumerate}

        Es decir,  debemos resolver los sistemas
        \begin{align*}
            \begin{bmatrix}  1- 3&2 \\ -1&1\end{bmatrix}
            \begin{bmatrix}    x_1\\x_2\end{bmatrix}
            =
            \begin{bmatrix}    0\\0\end{bmatrix}
            \quad &\text{o, equivalentemente,} \quad 
            \begin{bmatrix} -2&2 \\ -1&1 \end{bmatrix}
            \begin{bmatrix}    x_1\\x_2\end{bmatrix}
            =
            \begin{bmatrix}    0\\0\end{bmatrix} \tag{S1}
            \\
            &
            \\
            \begin{bmatrix}    2-3&2 \\ -1&2\end{bmatrix}
            \begin{bmatrix}    x_1\\x_2\end{bmatrix}
            =
            \begin{bmatrix}    0\\0\end{bmatrix}
            \quad &\text{o, equivalentemente,} \quad 
            \begin{bmatrix}-1&2 \\ -1&2\end{bmatrix}
            \begin{bmatrix}    x_1\\x_2\end{bmatrix}
            =
            \begin{bmatrix}    0\\0\end{bmatrix} \tag{S2}
        \end{align*}

        (S1)\quad $\begin{bmatrix} -2&2 \\ -1&1 \end{bmatrix} \stackrel{F_1 -2F_2}{\longrightarrow} \begin{bmatrix} 0&0 \\ -1&1 \end{bmatrix}$  $\Rightarrow$ $-x_1+x_2=0$ $\Rightarrow$ $(t,t)$ es solución. 

        (S2)\quad  $\begin{bmatrix}-1&2 \\ -1&2\end{bmatrix} \stackrel{F_2 -F_1}{\longrightarrow} \begin{bmatrix}-1&2 \\ 0&0\end{bmatrix}$  $\Rightarrow$ $-x_1+2x_2=0$ $\Rightarrow$ $(2t,t)$ es solución. 
    
     
    
    
     De lo anterior concluimos:
        
            \begin{itemize}
                \item Los autovalores de $A$ son $1$ y $2$.
                \item El auto espacio correspondiente al  autovalor $1$ es
            \begin{equation*}
                V_1 = \{t(1,1): t \in \R\}.
            \end{equation*}
            \item El auto espacio correspondiente al  autovalor $2$ es
            \begin{equation*}
                V_2 = \{t(2,1): t \in \R\}.
            \end{equation*}
            \end{itemize}
    
            
        \end{proof}
    

    
        \begin{ejemplo*}
        Sea $A= \begin{bmatrix}0&-1\\1&0\end{bmatrix} \in \R^2$. Encontrar los autovalores \textit{reales} de $A$.    
        \end{ejemplo*}
        \begin{proof}[Solución]
            $x \Id-A= \begin{bmatrix}x&1\\-1&x\end{bmatrix}$, luego
            \begin{align*}
                \chi_A(x)=x^2+1.
                \end{align*}
            El polinomio no tiene raíces reales, por lo tanto no existen autovalores reales (y obviamente no hay autovectores).
        \end{proof}

        Sin embargo si nos dicen 
        \begin{center}
            \textit{Encontrar autovalores y autovectores complejos de la matriz  $A= \begin{bmatrix}0&-1\\1&0\end{bmatrix}$}, 
        \end{center}
        la respuesta va a ser diferente. 

        Lo  que ocurre es que 
        \begin{align*}
        \chi_A(x)=x^2+1=(x+i)(x-i),
        \end{align*}
        y este polinomio \textit{sí} tiene raíces complejas: $i$ y $-i$, por lo tanto  $i$ y $-i$ son los autovalores de $A$.

        Averigüemos los autovalores planteando los sistemas de ecuaciones correspondientes,  es decir $\lambda \Id x-A =0$ para $\lambda=i, -i$:

        \begin{align*}
            \begin{bmatrix}   i& 1\\ -1&i\end{bmatrix}
            \begin{bmatrix}    x_1\\x_2\end{bmatrix}
            =
            \begin{bmatrix}    0\\0\end{bmatrix},
            \tag{S1}
            \\
            &
            \\
            \begin{bmatrix}    -i&1 \\ -1&-i\end{bmatrix}
            \begin{bmatrix}    x_1\\x_2\end{bmatrix}
            =
            \begin{bmatrix}    0\\0\end{bmatrix}.
            \tag{S2}
        \end{align*}
        
        Resolvamos los sistemas:

        (S1)\quad $\begin{bmatrix} i& 1\\ -1&i\end{bmatrix} \stackrel{F_2 -i F_1}{\longrightarrow} \begin{bmatrix} i& 1\\ 0&0\end{bmatrix} $  $\Rightarrow$ $ix_1 +  x_2=0$ $\Rightarrow$ $(\omega ,-i\omega)$ es solución ($\omega \in \C$).

        (S2)\quad  $\begin{bmatrix}    -i&1 \\ -1&-i\end{bmatrix} \stackrel{F_1 -i F_2}{\longrightarrow}\begin{bmatrix}    0&0 \\ -1&-i\end{bmatrix}$  $\Rightarrow$ $-x_1-i x_2=0$ $\Rightarrow$ $(-i \omega,\omega)$ es solución ($\omega \in \C$).


        Luego $A$ tiene dos autovalores, $i$ y $-i$, y 
    \begin{equation*}
        V_i = \left\{ \omega(1, -i): \omega\in \C \right\}, \qquad  V_{-i} = \left\{ \omega(-i,1): \omega\in \C \right\}.
    \end{equation*} 
        
        Nunca está de más comprobar los resultados:
    
        \begin{align*}
            \begin{bmatrix}0&-1\\1&0\end{bmatrix}\begin{bmatrix}1\\-i\end{bmatrix} &= \begin{bmatrix}i\\1\end{bmatrix} = i \begin{bmatrix}1\\-i\end{bmatrix}.\\
            \\
            \begin{bmatrix}0&-1\\1&0\end{bmatrix}\begin{bmatrix}-i\\1\end{bmatrix} &= \begin{bmatrix}-1\\-i\end{bmatrix} = (-i) \begin{bmatrix}-i\\1\end{bmatrix}.
        \end{align*}

        \begin{ejemplo*}
            En  base a lo que sabemos ahora analicemos de nuevo  la matriz del ejemplo \ref{autovalores-3x3}. El polinomio característico es 
            \begin{align*}
            \chi_B(x)&=\det(x \Id-A)\\
            &=\det\begin{bmatrix}x-3&-2&{ 1}\\-2&x-3&-1\\0&0&x-5\end{bmatrix}\\
            &= x^3- 11 x^2+ 35 x-25 \\
            &=(x-5)^2(x-1).
            \end{align*}
        Por lo tanto los autovalores son $5$ y $1$. Averigüemos los autoespacios correspondientes a estos autovalores. Para $\lambda=5$ debemos resolver el sistema $(5\Id-B)X=0$, es decir
        \begin{align*}
        \begin{bmatrix}2&-2&{ 1}\\-2&2&-1\\0&0&0\end{bmatrix}
        \begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}
        =
        \begin{bmatrix}0\\0\\0\end{bmatrix}.
        \end{align*}
        Haciendo reducción por filas obtenemos el sistema equivalente
        \begin{align*}
        \begin{bmatrix}2&-2&{ 1}\\0&0&0\\0&0&0\end{bmatrix}
        \begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}
        =
        \begin{bmatrix}0\\0\\0\end{bmatrix},
        \end{align*}
        es decir $2x_1-2x_2+x_3=0$. Luego $x_1=x_2-\frac{1}{2}x_3$ y por lo tanto el autoespacio correspondiente a $5$ es  
        \begin{align*}
        V_5=\{(t-\frac{1}{2}s,t,s): t,s\in\K\}. 
        \end{align*}
        Para $t=0$, $s=-2$ obtenemos el autovector $(1,0,-2)$ y para $t=1$, $s=2$ obtenemos el autovector $(0, 1,2)$. Es decir,  los vectores con autovalor 5 del ejercicio  \ref{autovalores-3x3}. 

        Finalmente,  averigüemos el  autoespacio correspondiente al  autovalor $1$. En este caso  debemos resolver el sistema $(\Id-B)X=0$, es decir
        \begin{align*}
        \begin{bmatrix}-2&-2&{ 1}\\-2&-2&-1\\0&0&-4\end{bmatrix}
        \begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}
        =
        \begin{bmatrix}0\\0\\0\end{bmatrix}.
        \end{align*}
        Haciendo reducción por filas obtenemos el sistema equivalente
        \begin{align*}
        \begin{bmatrix}1&1&0\\0&0&1\\0&0&0\end{bmatrix}
        \begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}
        =
        \begin{bmatrix}0\\0\\0\end{bmatrix},
        \end{align*}
        es decir $x_1=-x_2$ y $x_3=0$. Luego $x_1=-x_2$ y por lo tanto el autoespacio correspondiente a $1$ es
        \begin{align*}
        V_1=\{(t,-t,0): t\in\K\}.
        \end{align*}
        Para $t=1$ obtenemos el autovector $(1,-1,0)$. Es decir,  el vector con autovalor 1 del ejercicio  \ref{autovalores-3x3}.


        \end{ejemplo*}
    
        \subsection*{$\S$ Ejercicios}

        \begin{enumex}
            \item Para cada una de las siguientes matrices calcule el polinomio característico, los autovalores y los autoespacios correspondientes.
                \begin{enumex}
                \begin{minipage}{0.25\textwidth}
                    \item $\begin{bmatrix} 10&-9 \\4 &-2\end{bmatrix}$,
                \end{minipage}
                \begin{minipage}{0.25\textwidth}
                    \item $\begin{bmatrix} 1&2 \\ 4&3\end{bmatrix}$,
                \end{minipage}
                \begin{minipage}{0.25\textwidth}
                    \item $\begin{bmatrix} 0&3 \\7 &0\end{bmatrix}$,
                \end{minipage}

                \begin{minipage}{0.25\textwidth}
                    \item $\begin{bmatrix} 0&0 \\0 &0\end{bmatrix}$,
                \end{minipage}
                \begin{minipage}{0.25\textwidth}
                    \item $\begin{bmatrix} 1&0 \\ 0&1\end{bmatrix}$.
                \end{minipage}
                \end{enumex}
            
            \item Para cada una de las siguientes matrices calcule el polinomio característico, los autovalores y los autoespacios correspondientes.
                \begin{enumex}
                    \begin{minipage}{0.3\textwidth}
                    \item $\begin{bmatrix} 3&-2&0 \\-2 &3&0\\0&0&5\end{bmatrix}$,\end{minipage}
                    \begin{minipage}{0.3\textwidth}
                    \item $\begin{bmatrix} 0&1&0 \\ 0&0&1\\4&-17&8 \end{bmatrix}$.\end{minipage}
                \end{enumex}
            
            \item   Sean $A$, $B$ matrices $n \times n$. Probar que si $A$ es semejante a $B$ (ver sección \ref{seccion-matrices-invertibles}, ejercicio \ref{matrices-semejantes}),  entonces $A$ y $B$ tienen los mismos autovalores.  
            \item\label{matriz-n2=0} Sea $N$ una matriz compleja $2 \times 2$ tal que $N^2 =0$. Probar que, o bien $N=0$, o bien $N$  es semejante a 
            $$
            \begin{bmatrix}
                0&0\\1&0
            \end{bmatrix}.
            $$ 
            \item Usar el resultado del ejercicio \ref{matriz-n2=0} para probar lo siguiente: si $A$ es una matriz compleja $2 \times 2$ ,  entonces $A$ es semejante sobre $\C$ a una de las dos matrices siguientes:
            $$
            \begin{bmatrix}
                a&0\\0&b
            \end{bmatrix}\qquad
            \begin{bmatrix}
                a&0\\1&a
            \end{bmatrix}. 
            $$
        \end{enumex}
    
\end{section}    

\end{chapter}

