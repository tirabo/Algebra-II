\documentclass[11pt,spanish,makeidx]{amsbook}
\usepackage{etex}
\tolerance=10000
\renewcommand{\baselinestretch}{1.3}

% Para hacer el  indice en linea de comando hacer 
% makeindex main
\usepackage{t1enc}
\usepackage[spanish,es-nodecimaldot]{babel}
\usepackage{latexsym}
\usepackage[utf8]{inputenc}
\usepackage{verbatim}
\usepackage{multicol}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amsfonts,amssymb}
\usepackage{amsthm}
\usepackage{calc}         % From LaTeX distribution
\usepackage{graphicx}     % From LaTeX distribution
\usepackage{ifthen}
\input{random.tex}        % From CTAN/macros/generic
\usepackage{subfigure} 
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{matrix}
\tikzset{
	every picture/.append style={
		execute at begin picture={\deactivatequoting},
		execute at end picture={\activatequoting}
	}
}
\usetikzlibrary{decorations.pathreplacing,angles,quotes}
%\usetikzlibrary{graphs}
%\usepackage{tikz-3dplot} %for tikz-3dplot functionality
%\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage{stackrel}
%\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{tkz-graph}

%% \theoremstyle{plain} %% This is the default
\oddsidemargin 0.0in \evensidemargin -1.0cm \topmargin 0in
\headheight .3in \headsep .2in \footskip .2in
\setlength{\textwidth}{16cm} %ancho para apunte
\setlength{\textheight}{21cm} %largo para apunte
%\leftmargin 2.5cm
%\rightmargin 2.5cm
\topmargin 0.5 cm

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
}
\usepackage{hypcap}


\renewcommand{\thesection}{\thechapter.\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}

\newtheorem{teorema}{Teorema}[section]
\newtheorem{proposicion}[teorema]{Proposici\'on}
\newtheorem{corolario}[teorema]{Corolario}
\newtheorem{lema}[teorema]{Lema}
\newtheorem{propiedad}[teorema]{Propiedad}

\theoremstyle{definition}

\newtheorem{definicion}{Definici\'on}[section]
\newtheorem{ejemplo}{Ejemplo}[section]
\newtheorem{ejemplos}{Ejemplos}[section]
\newtheorem{problema}{Problema}[section]
\newtheorem{ejercicio}{Ejercicio}[section]
\newtheorem{ejerciciof}{}[section]

\theoremstyle{remark}
\newtheorem{observacion}{Observaci\'on}[section]
\newtheorem{obs}{Observaci\'on}[section]
\newtheorem{nota}{Nota}[section]

\renewcommand{\abstractname}{Resumen}
\renewcommand{\partname }{Parte }
\renewcommand{\indexname}{Indice }
\renewcommand{\figurename }{Figura }
\renewcommand{\tablename }{Tabla }
\renewcommand{\proofname}{Demostraci\'on}
\renewcommand{\refname }{Referencias }
\renewcommand{\appendixname }{Ap\'endice }
\renewcommand{\contentsname }{Contenidos }
\renewcommand{\chaptername }{Cap\'\i tulo }
\renewcommand{\bibname }{Bibliograf\'\i a }


\setcounter{MaxMatrixCols}{20}


\newcommand{\img}{\operatorname{Im}}
\newcommand{\nuc}{\operatorname{Nu}}
\newcommand\im{\operatorname{Im}}
\renewcommand\nu{\operatorname{Nu}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\renewcommand{\t}{{\operatorname{t}}}
\renewcommand{\sin}{{\,\operatorname{sen}}}
\newcommand{\Q}{\mathbb Q}
\newcommand{\R}{\mathbb R}
\newcommand{\C}{\mathbb C}
\newcommand{\K}{\mathbb K}
\newcommand{\F}{\mathbb F}
\newcommand{\Z}{\mathbb Z}
\newcommand{\sen}{\operatorname{sen}}
\newcommand \circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\makeindex
\begin{document}
	\baselineskip=0.55truecm %original
	%\baselineskip=0.43truecm
	
	\pagenumbering{roman}
	
	\title{Álgebra II / Álgebra - Notas del teórico  \\ Año 2018 - 2º Cuatrimestre  \\ FAMAF  - UNC \\ Versión: \today
	}
	
	\maketitle
	
	\newpage
	
	\author{
		${}^{}$
		\\${}^{}$
		\\${}^{}$
		\center{\textbf{AUTORES/COLABORADORES}} \\${}^{}$\\ 
		\flushleft 
		\begin{itemize}
			\item \textbf{Obra original:} Silvina Riveros,  Alejandro Tiraboschi y Agustín García  Iglesias. 
			\item \textbf{Colaboración: } -.   
			\item \textbf{Correcciones y sugerencias:} -. 
		\end{itemize}
	}
	
	%\author{A. L. Tiraboschi}
	
	%\address{FaMAF}
	%\subjclass{47A56, 47A10; Secondary 47A55, 47A70, 47A68, 47B15}
	%\date{July 19, 1992}
	%\translator{H. H. McFaden}
	
	\vskip 2cm 
	\thanks{
		\center{\textbf{LEER}} \\
		${}^{}$\\
		{\flushleft 
			Este material es distribuido bajo la licencia Creative Commons} \\
		{\center \textbf{Atribución--CompartirIgual 4.0 Internacional}}
		\\ 
		\center  lo cual significa 
		\\
		\flushleft
		- En cualquier explotación de la obra autorizada por la licencia será necesario reconocer los autores, colaboradores, etc.\\
		- La distribución de la obra u obras derivadas se debe hacer con una licencia igual a la que regula la obra original.\\
		${}^{}$
		\\
		Los detalles de la licencia pueden encontrarse en \href{https://creativecommons.org/licenses/by/4.0/deed.es}{Creative Commons}
	}
	
	\tableofcontents 
	
	\pagenumbering{arabic}
	
	\chapter*{Prefacio} 

	
	Las siguientes notas se han utilizado para el dictado del curso ``Álgebra II / Álgebra'' del primer año de las licenciaturas y profesorados de FAMAF. 	Han sido las notas principales en el dictado del año 2018 y se limitan casi exclusivamente al contenido dictado en el curso. Las partes señaladas con (*) y los apéndices son optativos.
	
	Estas notas están basadas principalmente en \textit{Apuntes de Álgebra II - Año 2005} de Silvina Riveros y han sido revisadas, modificadas y ampliadas por Alejandro Tiraboschi  y Agustín García Iglesias. 
	
	También hemos utilizado como bibliografía de apoyo  los siguientes: 
	
	\begin{enumerate}
		\item [-] \textit{Álgebra Lineal.} Autores: Gabriela Jeronimo, Juan Sabia y Susana Tesauri. Año 2008. Puede descargarse del Departamento de Matemática de la UBA, en la dirección
		
		\href{http://mate.dm.uba.ar/~jeronimo/algebra_lineal/AlgebraLineal.pdf}	 {http://mate.dm.uba.ar/~jeronimo/algebra\_lineal/AlgebraLineal.pdf}
		\item [- ] \textit{Linear Algebra.} Autores: Jim Hefferon. Se descarga en
		
		\href{http://joshua.smcvt.edu/linearalgebra/}{http://joshua.smcvt.edu/linearalgebra/}
	\end{enumerate}
	
	
	Otra bibliografía de interés:
	\begin{enumerate}
		\item [-] \textit{Álgebra Lineal.}  Autor: Serge Lang. Año: 1976. Editorial: Fondo Educativo Interamericano. Ficha en biblioteca de FAMAF: \href{http://bit.ly/2stBTzs}{http://bit.ly/2stBTzs}
		\item [-] \textit{Álgebra Lineal.} Autores: Kenneth Hoffman y Ray Kunze.  Año: 1973. Editorial: Prentice Hall. Ficha en biblioteca de FAMAF:  \href{http://bit.ly/2tn3eRc}{http://bit.ly/2tn3eRc}
	\end{enumerate}
	
	\vskip .5cm
	
	\textbf{Contenidos mínimos}
	
	Resolución de ecuaciones lineales. Matrices. Operaciones elementales. Matriz inversa. Espacios vectoriales sobre $\mathbb R$ y $\mathbb C$. Subespacios. Independencia lineal. Bases y dimensión Rectas y planos en $\mathbb R^n$. Transformaciones lineales y matrices. Isomorfismos. Cambio de bases. Núcleo e imagen de transformaciones lineales. Rango fila y columna. Determinante de una matriz. Cálculo y propiedades básicas. Espacios con producto interno. Desigualdad de Cauchy-Schwartz. Desigualdad triangular. Teorema de Pitágoras. Ortonormalización de Gram-Schmidt. Ecuaciones de rectas y planos en $\mathbb R^n$. Distancias. Introducción a vectores y valores propios. Aplicaciones. Diagonalización de matrices simétricas.
	\pagenumbering{roman}


	\maketitle
	
	

	
	
	\pagenumbering{arabic}
	
	\setcounter{chapter}{0}
	
\begin{chapter}{Vectores}\label{chap-vectores}

	El concepto de vector es básico para el estudio de funciones de varias variables y proporciona la motivación geométrica para todo el curso. Por lo tanto, las propiedades de los vectores, tanto algebraicas como geométricas, serán discutidas en forma extensa en este capítulo.
	
	\begin{section}{Álgebra lineal en $\R^2$ y $\R^3$}  
		Sabemos que se puede usar un número para representar un punto en una línea, una vez que se selecciona la longitud de una unidad.
		
		\begin{figure}[h]\label{fig-punto-en-R}
			\begin{tikzpicture}
			\draw[thick,->] (-4.0,0) -- (4.0,0) node[right] {$x$}; % eje x
			\foreach \x in {-3,...,3}
			\draw (\x,3pt) -- (\x,-3pt)
			node[anchor=north] {\x};
			\end{tikzpicture}
			\caption{La recta real y algunos números enteros.}
		\end{figure}
		
		Se puede usar un par de números $(x, y)$ para representar un punto en el plano. Estos pueden ser representados como en la figura \ref{fig-punto-en-R2}.
		\begin{figure}[h]
			\begin{tikzpicture}
			\draw[thick,->] (-4.0,0) -- (4.0,0) node[right] {$x$}; % eje x
			\draw[thick,->] (0,-3) -- (0,3) node[above] {$y$}; % eje y
			\foreach \x in {-3,...,-1}
			\draw (\x,3pt) -- (\x,-3pt)
			node[anchor=north] {\x};
			\foreach \x in {1,...,3}
			\draw (\x,3pt) -- (\x,-3pt)
			node[anchor=north] {\x};
			\foreach \y in {-2,...,-1}
			\draw (3pt,\y) -- (-3pt,\y) 
			node[anchor=east] {\y}; 
			\foreach \y in {1,...,2}
			\draw (3pt,\y) -- (-3pt,\y)
			node[anchor=east] {\y}; 
			\draw[fill] (2,1) circle [radius=0.07];
			%\draw[thick,->] (0,0) -- (2,1);
			\node [right] at (2,1) {$(2,1)$};
			\draw [dashed] (0,1) -- (2,1);
			\draw [dashed] (2,0) -- (2,1);
			\draw[fill] (-1,2.5) circle [radius=0.07];
			%\draw[thick,->] (0,0) -- (-1,2.5);
			\node [left] at (-1,2.5) {$(-1,2.5)$};
			\draw [dashed] (0,2.5) -- (-1,2.5);
			\draw [dashed] (-1,0) -- (-1,2.5);
			\draw[fill] (-2.5,-2.5) circle [radius=0.07];
			%\draw[thick,->] (0,0) -- (-2.5,-2.5);
			\node [below] at (-2.5,-2.5) {$(-2.5,-2.5)$};
			\draw [dashed] (0,-2.5) -- (-2.5,-2.5);
			\draw [dashed] (-2.5,0) -- (-2.5,-2.5);
			\end{tikzpicture}
			\caption{Representación gráfica de los puntos $(2,1)$, $(-1,2.5)$ y  $(-2.5,-2.5)$ en $\R^2$.}
			\label{fig-punto-en-R2}
		\end{figure}
		
		
		Ahora observamos que un triple de números $(x, y, z)$ se puede usar para representar un punto en el espacio, es decir, espacio tridimensional, o 3-espacio. Simplemente introducimos un eje más. La figura \ref{fig-punto-en-R3} ilustra esto.
		\begin{figure}[h]
			\begin{tikzpicture}
			%draw the main coordinate system axes
			\draw[thick,->] (0,0,0) -- (5,0,0) node[right]{$y$};
			\draw[thick,->] (0,0,0) -- (0,5,0) node[above]{$z$};
			\draw[thick,->] (0,0,0) -- (0,0,5) node[anchor=north east]{$x$};
			\draw[fill] (3,2.5,3.5) circle [radius=0.07] node[anchor=west]{\,$v$};
			%\draw[thick,->] (0,0,0) -- (3,2.5,3.5) node[anchor=west]{$v$};
			\draw [dashed] (0,2.5,0) -- (3,2.5,0);
			\draw [dashed] (0,2.5,0) --  (0,2.5,3.5);
			\draw [dashed] (0,2.5,3.5) -- (3,2.5,3.5) ;
			\draw [dashed] (3,0,0) -- (3,2.5,0);
			\draw [dashed] (0,0,3.5) -- (3,0,3.5);
			\draw [dashed] (0,0,3.5) -- (0,2.5,3.5);
			\draw [dashed] (3,0,3.5) -- (3,0,0);
			\draw [dashed] (3,2.5,3.5) -- (3,2.5,0);
			\draw [dashed] (3,2.5,3.5) -- (3,0,3.5);
			\foreach \y in {1,...,4}
			\draw (\y,0.1,0) -- (\y,-0.1,0)
			node[anchor=north] {\y};
			\foreach \y in {1,...,4}
			\draw (-0.1,\y,0) -- (0.1,\y,0)
			node[anchor=east] {\y\;};
			\foreach \y in {1,...,4}
			\draw (0.1,-0.1,\y) -- (-0.1,0.1,\y)
			node[anchor=south east] {\y\,};
			\end{tikzpicture}
			\caption{Representación gráfica del punto $v = (3.5,\,3,\,2.5)$ en $\R^3$. }
			\label{fig-punto-en-R3}
		\end{figure}			
		
		En lugar de usar $(x,y,z)$, también suele usarse la notación $(x_1,x_2,x_3)$. La línea podría llamarse el 1-espacio, y el plano podría llamarse el 2-espacio. Por lo tanto, podemos decir que un solo número representa un punto en el 1-espacio. Un par representa un punto en el 2-espacio. Un triple representa un punto en el 3-espacio.
		
		Aunque no podemos hacer un dibujo para ir más lejos, no hay nada que nos impida considerar un cuádruple de números y decretar que este es un punto en el 4-espacio. Un quíntuple sería un punto en el 5-espacio, luego vendría un séxtuple, séptuple, óctuple, ...
		
		Podemos generalizar y definir un punto en el $n$-espacio, para $n$ un entero positivo, como una $n$-tupla de números. Vamos a denotar tal $n$-tupla con letras $v$, $w$, $u$, ... y usaremos otras letras minúsculas para los números. Si $v =(x_1,x_2,\ldots,x_n)$, llamamos a los números $x_1,x_2,\ldots,x_n$ las \textit{coordenadas} del punto $v$. Más precisamente, $x_i$ será la \textit{coordenada $i$-ésima} de $v$. Por ejemplo, en el 3-espacio, 2 es la primera coordenada del punto $(2,3, -4)$, y $-4$ es la tercera coordenada. Denotamos a los  $n$-espacios por $\R^n$. Para formalizar:
		\begin{definicion}
			Sea $\R$ el cuerpo de los números reales,  entonces
			\begin{equation*}
				\R^n:= \{(x_1,x_2,\ldots,x_n): x_i \in \R, 1 \le i \le n \}.
			\end{equation*}
			Tode $v$ en $\R^n$ será llamado {\em punto}\index{punto}.
		\end{definicion}
		
		La mayoría de nuestros ejemplos tendrán lugar cuando $n = 2$ o $n = 3$. Por lo tanto, el lector puede visualizar cualquiera de estos dos casos a lo largo del apunte. Para ello usaremos el {\em sistema  de coordenadas cartesianas}\index{sistema  de coordenadas cartesianas} para representar los elementos de  $\R^2$ y  $\R^3$, tal como se ha hecho en las figuras \ref{fig-punto-en-R2} y \ref{fig-punto-en-R3}.
		
		
	
	
		\begin{ejemplo} \label{ej-3espacio-industria}
			Un ejemplo clásico de 3-espacio es, por supuesto, el espacio en el que vivimos. Después de seleccionar un origen y un sistema de coordenadas, podemos describir la posición de un punto (cuerpo, partícula, etc.) mediante 3 coordenadas. Además, como se sabía hace mucho tiempo, es conveniente extender este espacio a un espacio de 4 dimensiones, donde la cuarta coordenada es el tiempo, seleccionándose el origen del tiempo, por ejemplo, como el nacimiento de Cristo, aunque esto es puramente arbitrario. Entonces, un punto con coordenada de tiempo negativo es un punto antes de Cristo, y un punto con coordenada  de tiempo positiva es un punto después de Cristo.
			
			Sin embargo, no es que obligatoriamente ``el tiempo es la cuarta dimensión''. El espacio 4-dimensional anterior es solo un ejemplo posible. Hagamos un ejemplo relacionado a la economía: tomamos como coordenadas la cantidad de dinero gastado por una industria a lo largo de un año. 
			Por ejemplo, podríamos tener un espacio de 6 dimensiones con coordenadas correspondientes a las siguientes industrias: 1. acero, 2. automotriz, 3. productos agrícolas,  4. productos químicos, 5. indumentaria y 6. transporte. Las coordenadas de las 6-tuplas representarían el gasto anual de las industrias correspondientes. Por  ejemplo, 
			\begin{equation*}
			(1000, 800, 550, 300, 700, 200)
			\end{equation*}
			significaría que la industria del acero gastó mil en un año determinado, y que la industria química gastó 300 en ese año.
		\end{ejemplo} 

		Los 3-espacios pueden ser visualizados como ``productos de espacios de dimensiones inferiores''. Por ejemplo, podemos ver las coordenadas de los 3-espacios como dos coordenadas en un 2-espacio acompañada por una coordenada en el 1-espacio. Esto se escribe como 
		\begin{equation*}
			\R^3 = \R^2 \times \R^1.
		\end{equation*}
		Utilizamos el signo del producto, que no debe confundirse con otros ``productos'', como el producto de los números. Del mismo modo, podemos escribir
		\begin{equation*}
		\R^4 = \R^3 \times \R^1.
		\end{equation*}
		Hay otras formas de expresar $\R^4$ como un producto, a saber:
		\begin{equation*}
		\R^4 = \R^2 \times \R^2.
		\end{equation*}
		Esto significa que vemos por separado las dos primeras coordenadas $(x_l, x_2)$ y
		las dos últimas coordenadas $(x_3, x_3)$. 
			
		Ahora vamos a definir cómo sumar los puntos de $\R^n$. Si $v$, $w$ son dos puntos, digamos en le 2-espacio,  definimos $v + w$ como el punto cuyas coordenadas son la suma de cada coordenada. Es decir, si, por ejemplo,  $v= (1, 2)$ y $w= (- 3, 5)$, entonces $v+w = (- 2, 7)$. En 3-espacios la definición es análoga. Por  ejemplo, si $v= (- 1, n, 3)$ y $w= (k, 7, - 2)$, entonces $v+w = (k - 1, n + 7, 1)$.
		
		Generalizando, podemos hacer la siguiente definición:
			\begin{definicion}
				Dados $(x_1,x_2), (y_1,y_2) \in \R^2$ o $(x_1,x_2,x_3), (y_1,y_2,y_3) \in \R^3$, definimos
				\begin{itemize}
					\item $(x_1,x_2)+ (y_1,y_2)=(x_1+y_1,x_2+y_2)$, 
					\item $(x_1,x_2,x_3)+ (y_1,y_2,y_3)=(x_1+y_1,x_2+y_2,x_3+y_3)$, 
				\end{itemize}
				y en general, si $(x_1,\ldots,x_n), (y_1,\ldots,y_n) \in \R^n$, definimos 
					\begin{itemize}
					\item $(x_1,\ldots,x_n)+ (y_1,\ldots,y_n)=(x_1+y_1,\ldots,x_n+y_n)$, 
				\end{itemize}
			\end{definicion}
			
		Observemos que se cumplen las siguientes reglas: sean $v,w,u$ en $\R^n$,  entonces
		\begin{enumerate}
			\item $v+ (w +u) = (v+w)+u$,
			\item $v + w = w + v$,
			\item si definimos
			\begin{equation*}
				0 = (0,\dots,0),
			\end{equation*}
			el punto cuyas coordenadas son todas $0$, entonces 
			\begin{equation*}
				v +0 = 0 +v = v,
			\end{equation*}
			\item si $v = (x_1,\ldots,x_n)$,  definimos $-v = (-x_1,\ldots, -x_n)$. Entonces 
			\begin{equation*}
				v + (-v) = (-v) + v = 0.
			\end{equation*}
		\end{enumerate}	
	Estas propiedades se deducen casi trivialmente de la definición de suma, coordenada a coordenada, y de la validez de las propiedades en el caso de la recta real. 
	
	
	
	
	
	\begin{ejemplo} 
		Vimos al final del ejemplo  \ref{ej-3espacio-industria} que una $n$-tupla puede representar cuestiones relacionadas con las finanzas. En  nuestro ejemplo una 6-tupla representaba el gasto anual de determinadas actividades económicas, por ejemplo los gastos  en los años 2000 y 2001 son 
		\begin{center}
			\begin{tabular}{lcl}
				2000 \quad&$\rightarrow$\quad &$(1000, 800, 550, 300, 700, 200)$ \\
				2001 \quad&$\rightarrow$\quad &$(1200, 700, 600, 300, 900, 250)$ 
			\end{tabular} 
		\end{center}
		Luego los costos totales en los dos años son 
		\begin{align*}
			(1000, 800, 550, 300, 700&, 200) + (1200, 700, 600, 300, 900, 250) = \\
			&=(1000+1200, 800+700, 550+600, 300+300, 700+900, 200+250) \\
			&= (2200, 1500, 1350, 600, 1600, 450). 
		\end{align*} 
	\end{ejemplo}
	
	
	En  el ejemplo anterior es claro que la suma de puntos se corresponde con lo que nosotros esperamos que ocurra. Ahora interpretaremos la suma  geométricamente en el plano.
	
	En  álgebra lineal a veces resultará  conveniente pensar a cada punto como un {\em vector} que comienza en el origen.  Los vectores en  $\R^2$ y $\R^3$ se pueden graficar como ``flechas'' que parten del origen y llegan a las coordenadas del punto. Veamos en los siguientes ejemplos que está interpretación es útil. 
	
	\begin{ejemplo}
		Sea $v =(2,3)$ y $w= (-1, 1)$. Entonces $v+w= (1, 4)$. En el dibujo de los puntos involucrados aparece un \textit{paralelogramo} (fig. \ref{fig-ley-del-paralelogramo})
		\begin{figure}[h]
			\begin{tikzpicture}[scale=1.0]
				\draw[thick,->] (-3.0,0) -- (3.0,0) node[right] {$x$}; % eje x
				\draw[thick,->] (0,-1) -- (0,4) node[above] {$y$}; % eje y
				\foreach \x in {-2,...,2}
				\draw (\x,3pt) -- (\x,-3pt);
				\foreach \y in {0,...,3}
				\draw (3pt,\y) -- (-3pt,\y) ;
				\draw[very thick, ->] (0,0) -- (-1,1);
				\node [left] at (-1,1) {$(-1,1)$};
				\draw[very thick,->] (0,0) -- (2,3);
				\node [right] at (2,3) {$(2,3)$};
				\draw[very thick,-] (0,0) -- (1,4);
				\node [above] at (1,4) {$(1,4)$};
				\draw[fill] (1,4) circle [radius=0.05];
				\draw[-] (-1,1) -- (1,4);
				\draw[-] (2,3) -- (1,4);
			\end{tikzpicture}
			\caption{\;}\label{fig-ley-del-paralelogramo}
		\end{figure} 
	\end{ejemplo}
	
	\begin{ejemplo}
		Sea $v= (3, 1)$ y $w=(1,2)$. Entonces
		\begin{equation*}
			v+w = (4,3). 
		\end{equation*}
		Esta suma la representamos en la fig. \ref{fig-ley-del-paralelogramo-2}. 
		\begin{figure}[h]
			\begin{tikzpicture}[scale=1.0]
				\draw[thick,->] (-3.0,0) -- (4.0,0) node[right] {$x$}; % eje x
				\draw[thick,->] (0,-1) -- (0,3) node[above] {$y$}; % eje y
				\foreach \x in {-2,...,3}
				\draw (\x,3pt) -- (\x,-3pt);
				\foreach \y in {0,...,2}
				\draw (3pt,\y) -- (-3pt,\y) ;
				\draw[very thick, ->] (0,0) -- (3,1);
				\node [right] at (3,1) {$v$};
				\draw[very thick,->] (0,0) -- (1,2);
				\node [above] at (0.9,2) {$w$};
				\draw[very thick,-] (0,0) -- (4,3);
				\node [above] at (4,3) {$v+w$};
				\draw[fill] (4,3) circle [radius=0.05];
				\draw[-] (3,1) -- (4,3);
				\draw[-] (1,2) -- (4,3);
			\end{tikzpicture}
			\caption{\;}\label{fig-ley-del-paralelogramo-2}
		\end{figure}
		
		Vemos de nuevo que en la representación geométrica aparece un paralelogramo. La razón por la cual la figura que aparece es un paralelogramo se puede dar en 	términos de la geometría plana de la siguiente manera. Obtenemos $v = (1, 2)$ comenzando desde el origen $0 = (0, 0)$, y moviéndonos 1 unidad hacia la derecha y 2 hacia arriba. Para obtener $v+w$, comenzamos desde $v$, y de nuevo nos movemos 1 unidad a la derecha y 2 hacia arriba. Así, el segmento entre 0 y $w$, y entre $v$ y $v+w$ son las hipotenusas de los triángulos rectángulos cuyos catetos  correspondientes son de la misma longitud y paralelos. Los segmentos anteriores son por lo tanto paralelos y de la misma longitud, como se ilustra en la fig. \ref{fig-ley-del-paralelogramo-3}.
		\begin{figure}[h]
			\begin{tikzpicture}[scale=1.0]
			\draw[thick,->] (-3.0,0) -- (4.0,0) node[right] {$x$}; % eje x
			\draw[thick,->] (0,-1) -- (0,3) node[above] {$y$}; % eje y
			\foreach \x in {-2,...,3}
			\draw (\x,3pt) -- (\x,-3pt);
			\foreach \y in {0,...,2}
			\draw (3pt,\y) -- (-3pt,\y) ;
			\draw[very thick,-] (0,0) -- (1,2);
			\node [above] at (0.9,2) {$w$};
			\draw[very thick,-] (3,1) -- (4,3);
			\node [above] at (4,3) {$v+w$};
			\node [below] at (3,1) {$v$};
			\node [left] at (0,-0.3) {$0$};
			\draw [dashed] (1,0) -- (1,2);
			\draw [dashed] (3, 1) -- (4,1) -- (4,3);
			\draw[fill] (0,0) circle [radius=0.07];
			\draw[fill] (1,2) circle [radius=0.07];
			\draw[fill] (3,1) circle [radius=0.07];
			\draw[fill] (4,3) circle [radius=0.07];
			\end{tikzpicture}
			\caption{\;}\label{fig-ley-del-paralelogramo-3}
		\end{figure}
	\end{ejemplo}
	
	
	\begin{ejemplo} Sea el punto $v =(3, 1)$ , entonces $-v = (- 3, - 1)$. Si dibujamos $v$ y $-v$ vemos que $-v$ es un vector del mismo ``tamaño'' que $v$ pero con la dirección opuesta. Podemos ver a $-v$ como la reflexión de $v$ a través del origen (fig. \ref{fig-vector-opuesto}). 
	\begin{figure}[h]
		\begin{tikzpicture}[scale=1.0]
		\draw[thick,->] (-4.0,0) -- (4.0,0) node[right] {$x$}; % eje x
		\draw[thick,->] (0,-2) -- (0,2) node[above] {$y$}; % eje y
		\foreach \x in {-3,...,3}
		\draw (\x,3pt) -- (\x,-3pt);
		\foreach \y in {-1,...,1}
		\draw (3pt,\y) -- (-3pt,\y) ;
		\draw[very thick, ->] (0,0) -- (3,1);
		\node [right] at (3,1) {$v$};
		\draw[very thick, ->] (0,0) -- (-3,-1);
		\node [left] at (-3,-1) {$-v$};
		\draw[fill] (0,0) circle [radius=0.1];
		\end{tikzpicture}
		\caption{\;}\label{fig-vector-opuesto}
	\end{figure}
	\end{ejemplo}

	Ahora consideraremos la multiplicación de un vector $v$ por un número. 
	
	\begin{definicion}
		Sea $v = (x_1,\ldots,x_n) \in \R^n$ y $\lambda \in \R$, entonces
		\begin{equation*}
			\lambda.v = (\lambda x_1,\ldots,\lambda x_n).
		\end{equation*}  
		También denotamos $\lambda v = \lambda.v$.
	\end{definicion}

	\begin{ejemplo}
		 Si $v= (2, -1,5)$  y $\lambda = 7$, entonces $\lambda v = (14, -7.35)$.
	\end{ejemplo}
	
	Es fácil verificar las siguientes reglas: dados $v,w \in \R^n$, 
	\begin{enumerate}
		\item[5.] si $\lambda \in \R$, $\lambda (v + w) = \lambda v + \lambda w$.
		\item[6.] Si $\lambda_1,\lambda_2 \in \R$, $(c_1+c_2)v= \lambda_1v + \lambda_2v$.
	\end{enumerate}

	También tengamos en cuenta que 
	\begin{equation*}
		(-1)v = -v.
	\end{equation*}
	
	¿Cuál es la representación geométrica de la multiplicación de un vector  por un número?
	\begin{ejemplo}
		Sea $v= (1,2)$ y $\lambda = 3$. Luego $\lambda v = (3,6)$ como en la fig. \ref{fig-homotecia}(a)
		\begin{figure}[h]
			\begin{tabular}{ccc}
				\begin{tikzpicture}[scale=1.0]
				\draw[thick,->] (-1.0,0) -- (4.0,0) node[right] {$x$}; % eje x
				\draw[thick,->] (0,-1) -- (0,6) node[above] {$y$}; % eje y
				\foreach \x in {0,...,3}
				\draw (\x,3pt) -- (\x,-3pt);
				\foreach \y in {-1,...,5}
				\draw (3pt,\y) -- (-3pt,\y) ;
				\draw[very thick, -] (0,0) -- (1,2);
				\node [right] at (1.1,1.9) {$v=(1,2)$};
				\draw[very thick, -] (0,0) -- (3,6);
				\node [right] at (3.1,5.9) {$3v=(3,6)$};
				\draw[very thick, -] (0,0) -- (0.5,1);
				\node [right] at (0.6,0.9) {$\frac12v=(0.5,1)$};
				\draw[fill] (0,0) circle [radius=0.07];
				\draw[fill] (0.5,1) circle [radius=0.07];
				\draw[fill] (3,6) circle [radius=0.07];
				\draw[fill] (1,2) circle [radius=0.07];
				\node [right] at (1.8,-1.5) {(a)};
				\end{tikzpicture}
				&\qquad &
				\begin{tikzpicture}[scale=0.58]
				\draw[thick,->] (-4.0,0) -- (4.0,0) node[right] {$x$}; % eje x
				\draw[thick,->] (0,-6) -- (0,6) node[above] {$y$}; % eje y
				\foreach \x in {-3,...,3}
				\draw (\x,3pt) -- (\x,-3pt);
				\foreach \y in {-5,...,5}
				\draw (3pt,\y) -- (-3pt,\y) ;
				\draw[very thick, -] (0,0) -- (1,2);
				\node [right] at (1.1,1.9) {$v$};
				\draw[very thick, -] (0,0) -- (3,6);
				\node [right] at (3.1,5.9) {$3v$};
				\draw[very thick, -] (0,0) -- (-3,-6);
				\node [right] at (-3.0,-5.9) {$-3v$};
				\draw[fill] (0,0) circle [radius=0.12];
				\draw[fill] (-3,-6) circle [radius=0.12];
				\draw[fill] (3,6) circle [radius=0.12];
				\draw[fill] (1,2) circle [radius=0.12];
				\node [right] at (-0.2,-7) {(b)};
				\end{tikzpicture}
			\end{tabular}
		\caption{\;}\label{fig-homotecia}
		\end{figure}
			
			
	\end{ejemplo}
	 La multiplicación por 3 equivale a ``estirar'' $v$ por 3. Del mismo modo, $\frac12v$ 	equivale a estirar $v$ en $\frac12$, es decir, reducir $v$ a la mitad de su tamaño. En general, si $t$ es un número con $t> 0$, interpretamos $tv$ como un punto en la misma dirección que $v$ con tamaño  $t$-veces el tamaño de $v$.  De hecho, decimos que $v$ y $w$ tienen la misma dirección si existe un número $\lambda> 0$ tal que $v = \lambda w$. La multiplicación por un número negativo invierte la dirección. Así, $-3 v$ se representa como en la fig. \ref{fig-homotecia}(b). Decimos que $v$ y $w$  (ninguno de los cuales es cero) tienen direcciones opuestas si existe un número $\lambda <0$ tal que $v =\lambda w$. Por lo tanto, $-v$ tiene dirección opuesta a $v$.
		\end{section}
	
	
	
	\begin{section}{Vectores afines}
		
	\end{section}
	
	
	
	\begin{section}{El producto escalar}
		
		En 2-espacios, dados dos vectores $v = (x_1, x_2)$ y $w= (y_l, y_2)$, definimos su \textit{producto escalar} como
		\begin{equation*}
			\langle v , w  \rangle :=x_1y_1 + x_2y_2.
		\end{equation*}
		Para el caso de 3-espacios, sean   $v = (x_1, x_2,x_3)$ y $w= (y_l, y_2,y_3)$,  entonces el \textit{producto escalar de $v$ y $w$} es
		\begin{equation*}
			\langle v , w  \rangle :=x_1y_1 + x_2y_2+x_3y_3.
		\end{equation*}
		Finalmente, en los $n$-espacios,  generalizamos la definición de la manera obvia: sean  $v = (x_1, \ldots,x_n)$ y $w= (y_l, \ldots,y_n)$,  entonces el \textit{producto escalar de $v$ y $w$} es
		\begin{equation*}
		\langle v , w \rangle :=x_1y_1 + x_2y_2+\cdots+x_ny_n.
		\end{equation*}
		
		Es importante notar  que este producto es un número real. Por ejemplo, si
		\begin{equation*}
			v= (1, 3, - 2) \quad\text{ y } \quad w= (- 1, 4, - 3),
		\end{equation*}
		entonces
		\begin{equation*}
			\langle v , w \rangle= - 1 + 12 + 6 = 17.
		\end{equation*}
		
		Por el momento, no le damos una interpretación geométrica a este producto escalar. Haremos esto más tarde. Primero derivaremos algunas propiedades importantes. Los básicas son: sean $v$, $w$, $u$  tres vectores, entonces
		
		
		\begin{enumerate}[label=\textbf{P\arabic*.},ref=P\arabic*]
			\item\label{prop-P1}	$\langle v , w \rangle = \langle w , v \rangle$.
			\item\label{prop-P2} 
			\begin{equation*}
				\langle v , w + u \rangle =\langle v , w \rangle + \langle v , u \rangle = \langle w +u , v \rangle.
			\end{equation*}
			\item\label{prop-P3} Si $\lambda$ es un número, entonces 
			\begin{equation*}
				\langle \lambda v , w \rangle = \lambda \langle v , w \rangle \quad \text{ y } \quad  \langle v , \lambda w \rangle = \lambda \langle v , w \rangle.
			\end{equation*}
			\item\label{prop-P4} Si $v=0$ es el vector cero, entonces $\langle v , v \rangle =0$,  de lo contrario
			\begin{equation*}
				\langle v , v \rangle >0
			\end{equation*}
		\end{enumerate}
		
		Ahora vamos a probar estas propiedades. En cuanto a la primera, tenemos que
		\begin{equation*}
			x_1y_1 + x_2y_2+\cdots+x_ny_n = y_1x_1 + y_2x_2+\cdots+y_nx_n
		\end{equation*}
		porque para cualquiera de los dos números $x, y$, tenemos que $xy=yx$. Esto prueba la propiedad \ref{prop-P1}. 
		
		Para \ref{prop-P2}, sea $u = (z_1, \ldots, z_n)$. Entonces
		\begin{equation*}
			w + u = (y_1+z_1, \ldots, y_n+ z_n)
		\end{equation*}
		y
		\begin{align*}
			\langle v , w + u \rangle &= \langle (x_1, \ldots,x_n) , (y_1+z_1, \ldots, y_n+ z_n) \rangle\\
			&= x_1(y_1+z_1) + \cdots x_n(y_n+z_n) \\
			&= x_1y_1+x_1z_1 + \cdots x_ny_n+x_nz_n
		\end{align*}
		Reordenando los términos obtenemos
		\begin{equation*}
				\langle v , w + u \rangle =  x_1y_1+\cdots +  x_ny_n +x_1z_1 + \cdots+x_nz_n,
		\end{equation*}
		que no es otra cosa que $\langle v , w \rangle + \langle v , u \rangle$.
		
		Dejamos la propiedad \ref{prop-P3} como ejercicio.
		
		Finalmente probemos \ref{prop-P4}. Observemos que 
		\begin{equation}\label{eq-v-escalar-v}
			\langle v , v \rangle = x_1^2 + x_2^2 + \cdots + x_n^2.
		\end{equation}
		Como $x_i^2 \ge 0$ para todo $i$,  entonces $\langle v , v \rangle \ge 0$. Además, es claro que si $v$ tiene todas las coordenadas iguales a 0,  entonces  $\langle v , v \rangle =0$. En  el caso que $v\not=0$, entonces,  existe algún $i$  tal que  $x_i \ne 0$, por lo tanto $x_i^2>0$ y por la ecuación (\ref{eq-v-escalar-v}), tenemos que  $\langle v , v \rangle>0$.
		
		
		
		
		El producto escalar $\langle v , w \rangle$ puede ser  igual a 0 para determinados  vectores,  incluso ambos distintos de 0.  Por ejemplo, si $v = (1,2,3)$ y $w = (2, 1, -\frac43)$,  entonces
		\begin{equation*}
			\langle v , w \rangle = 2 + 2 -4 =0.
		\end{equation*}
	
		Decimos que dos vectores $v$ y $w$ son  \textit{perpendiculares} u \textit{ortogonales} si  $\langle v , w \rangle=0$. Por el momento, no es claro que en el plano esta definición coincida con nuestra noción geométrica intuitiva de perpendicularidad. Esto lo veremos en la siguiente sección. Aquí nos limitaremos a observar un ejemplo. En  $\R^3$ consideremos los vectores
		\begin{equation*}
			e_1 = (1,0,0),\quad e_2 = (0, 1,0),\quad e_3 = (0,0,1),
		\end{equation*}
		representados en la fig. \ref{fig-canonicos-en-R3}
		\begin{figure}[h]
			\begin{tikzpicture}[scale=1.3]
			%draw the main coordinate system axes
			\draw[thick,->] (0,0,0) -- (2,0,0) node[right]{$y$};
			\draw[thick,->] (0,0,0) -- (0,2,0) node[above]{$z$};
			\draw[thick,->] (0,0,0) -- (0,0,2) node[anchor=north east]{$x$};
			\draw[very thick,->] (0,0,0) -- (1,0,0);
			\node [below] at (1,0,0.1) {$e_2$};
			\draw[very thick,->] (0,0,0) -- (0,1,0) node[anchor=west]{$e_3$};
			\draw[very thick,->] (0,0,0) -- (0,0,1);
			\node [below] at (0.2,0,1) {$e_1$};
			\draw [dashed] (0,0.8,0) -- (1.5,0.8,0);
			\draw [dashed] (1.5,0,0) -- (1.5,0.8,0);
			\draw[very thick,->] (0,0,0) -- (1.5,0.8,0) node[right]{$v$};
			\end{tikzpicture}
			\caption{}
			\label{fig-canonicos-en-R3}
		\end{figure}
				
		Luego, vemos que $\langle e_i , e_j \rangle = 0$, si $i \ne j$ y por lo tanto $e_i$  es perpendicular a $e_j$ si $i \ne j$, lo cual concuerda con nuestra intuición. 
		
		Observemos que si $v = (x_1,x_2,x_3)$,  entonces $\langle v , e_i \rangle = x_i$. Por lo tanto,  si la coordenada $i$-ésima de $v$ es cero,  $v$  es ortogonal a $e_i$. Esto nos dice,  por ejemplo,  que si $v$  es un vector contenido en el plano que incluye $e_2$ y $e_3$,  es decir si la primera coordenada es cero,  entonces $v$  es ortogonal a $e_1$. 
		
		
		
		
	\end{section}

	\begin{section}{La norma de un vector}
	Si $v$  es vector,  entonces $\langle v , v \rangle \ge 0$ y de definimos como la \textit{norma de $v$} o \textit{longitud de $v$} al número
	\begin{equation*}
		||v|| = \sqrt{\langle v , v \rangle}.
	\end{equation*}
	
	Cuando $v$ pertenece al plano  y $v =(x,y)$,  entonces $||v|| = \sqrt{x^2 + y^2}$ y si graficamos el vector en la fig. \ref{fig-pitagoras}, vemos que la noción de norma o longitud en $\R^2$ se deduce del teorema de Pitágoras.  
	\begin{figure}[h]
		\begin{tikzpicture}
		\draw[thick,->] (0,0) coordinate (O) -- (3,0);
		\draw[thick,->] (O) -- (0,4);
		\node[inner sep=1.5pt,fill,circle,label={60:$(x,y)$}] at (2,3) (point) {};
		\draw[very thick] (0,0) -- (point);
		%\draw (1.8,0) -- ++(0,0.2) -- ++(0.2,0);
		\draw[dashed] (2,0) coordinate (pointx) -- (point); 
		\draw[decoration={brace,mirror,raise=5pt},decorate]
		(2,0) -- node[right=6pt] {$y$} (point);
		\draw[decoration={brace,mirror,raise=5pt},decorate]
		(0,0) -- node[below=6pt] {$x$} (2,0); 
		\draw[decoration={brace,raise=5pt},decorate]
		(0,0) -- node[below=6pt] {${}$} (2,3);
		\node [right] at (0.5,1.8) {$r$};
		\end{tikzpicture}
		\caption{$r= \sqrt{x^2 + y^2}$.}
		\label{fig-pitagoras}
	\end{figure} 

	Si $n=3$,  el dibujo es como en la fig. \ref{fig-pitagoras-3d}, para $v =(x,y,z)$.  Es decir,  por la aplicación reiterada del teorema de Pitágoras obtenemos que la longitud de $v$ es $\sqrt{x^2 + y^2 +z^2}$.
	\begin{figure}[h]
		\begin{tikzpicture}[scale=0.8]
		\draw[thick,->] (0,0,0) -- (5,0,0) node[right]{$y$};
		\draw[thick,->] (0,0,0) -- (0,5,0) node[above]{$z$};
		\draw[thick,->] (0,0,0) -- (0,0,5) node[anchor=north east]{$x$};
		\draw[very thick,->] (0,0,0) -- (4,4.5,3) node[anchor=west]{$v$};
		\draw[dashed]  (0,0,0) -- (4,0,3); 
		\draw[dashed]  (0,0,3) -- (4,0,3); 
		\draw[dashed]  (4,0,0) -- (4,0,3); 
		\draw[dashed]  (4,0,3) -- (4,4.5,3); 
		%\draw[decoration={brace,raise=5pt},decorate]
		%(0,0,0) -- node[below=6pt] {${}$} (4,4.5,3);
		\node [right] at (2,2.2,1.5) {$r$};
		\node [right] at (1.2,-0.1,1.5) {$w$};
		\node [below] at (4.2,0.2,3.5) {$(x,y)$};
		\end{tikzpicture}
		\caption{$w= \sqrt{x^2 + y^2}$, $r = \sqrt{w^2 + z^2} = \sqrt{x^2 + y^2 +z^2}$.}
		\label{fig-pitagoras-3d}
	\end{figure} 
	
	En  general,  si $v =(x_1,x_2,\ldots,x_n) \in \R^n$,  entonces
	\begin{equation*}
		||v|| = \sqrt{x_1^2+x_2^2+\cdots+x_n^2}
	\end{equation*} 
	y la aplicación reiterada del  teorema de Pitágoras nos dice que esta es la definición correcta de longitud o norma de un vector. 
	
	\begin{proposicion}
		Sea $v \in \R^n$ y $\lambda \in \R$,  entonces
		\begin{equation*}
			||\lambda v|| = |\lambda|||v||.
		\end{equation*}
	\end{proposicion}
	\begin{proof}
		$||\lambda v||^2 = \la\lambda v, \lambda v \ra$, por la propiedad \ref{prop-P3} del producto escalar, 
		\begin{equation*}
			\la\lambda v, \lambda v \ra = \lambda\la v, \lambda v \ra = \lambda^2\la v, v  \ra.
		\end{equation*}
		Es decir 	$||\lambda v||^2 =  \lambda^2 ||v||^2$, por lo tanto (sacando raíz cuadrada), $||\lambda v|| = |\lambda|||v||$.
	\end{proof}
	
	\vskip .3cm
	
	Pero el producto escalar no sólo es útil para definir la longitud de un vector,  sino que también nos dice cual es el ángulo entre dos vectores.  Sean   $v_1= (x_1,y_1)$ y $v_2= (x_2,y_2)$ en $\R^2$; veremos a continuación que 
		\begin{equation*}
		\langle v_1 , v_2 \rangle = ||v_1||\, ||v_2|| \cos(\theta)
		\end{equation*}
		donde  $\theta$ es el ángulo comprendido entre $v_1$ y $v_2$.
	
		
		Sea $\alpha_1$ el ángulo comprendido  entre $v_1$ y el eje horizontal y $\alpha_2$ el ángulo comprendido  entre $v_2$ y el eje horizontal.  Entonces,
		\begin{equation*}
		v_1 = ||v_1||(\cos (\alpha_1), \sen(\alpha_1) ), \quad v_2 = ||v_2||(\cos (\alpha_2), \sen(\alpha_2) ),
		\end{equation*}
		por lo tanto
		\begin{equation*}
		\langle v_1 , v_2 \rangle =  ||v_1|| \,  ||v_2|| (\cos (\alpha_1)\cos(\alpha_2)+  \sen(\alpha_1)\sen\alpha_2)). 
		\end{equation*}
		Por otro  lado, por la propiedad de la suma de los cosenos tenemos que 
		\begin{equation}\label{eq-ley-cosenos}
		\cos (\alpha_1)\cos(\alpha_2)+  \sen(\alpha_1)\sen(\alpha_2) = \cos(\alpha_1- \alpha_2).
		\end{equation}
		Es decir, 
		\begin{equation*}
		\langle v_1,v_2 \rangle =   ||v_1|| \,  ||v_2|| \cos (\alpha_1-\alpha_2), 
		\end{equation*}
		y precisamente, el ángulo comprendido entre  $v_1$ y $v_2$ es $\theta =  \alpha_1-\alpha_2$.
		
		Esto se puede generalizar a $\R^3$  y ahí en vez de la fórmula (\ref{eq-ley-cosenos}) se debe usar la ley esférica de los cosenos. Estos resultados se puede generalizar a  $\R^n$ y  en general vale  que si $v_1, v_2 \in \R^n$,  entonces el ángulo comprendido entre $v_1$ y $v_2$ es 
		\begin{equation}\label{eq-ang-comprendido}
		\theta = 	\operatorname{arcos}\left(\frac{\langle v_1 , v_2 \rangle}{||v_1|| \,  ||v_2|| }\right).
		\end{equation}  
		
		\vskip .3cm 
		
		Terminaremos esta sección dando la noción de  distancia entre dos vectores o dos puntos. 
		
		\begin{definicion}
			Sea $v,w \in \R^n$, entonce las \textit{distancia} entre $v$ y $w$ es $||v-w||$.
		\end{definicion}

	\end{section}

	\begin{section}{Rectas en $\R^2$}
		En  el espacio $\R^n$, definimos la \textit{ecuación paramétrica} o la \textit{representación paramétrica} de la recta que pasa por el punto $v$ en la dirección del vector $w \ne 0$ como
		\begin{equation*}
			X =v +tw, \quad \text{para $t \in \R$.} 
		\end{equation*} 
		En $\R^2$ podemos representar una recta como en la figura \ref{fig-recta-parametrica}.
		\begin{figure}[h]
			\begin{tikzpicture}
			\draw[thick,-] (-4,0) -- (4,0);
			\draw[thick,-] (0,-1) -- (0,3);
			\draw[very thick,-] (-3,3) -- (4,-0.5);
			\node[inner sep=1.5pt,fill,circle] at (1,1) {};
			\draw[very thick,-] (0,0) -- (1,1) node[above]{$v$};
			\draw[very thick,->] (0,0) -- (-2,1) node[left]{$w$};
			\node[inner sep=1.5pt,fill,circle] at (-1,2)  {};
			\node[above] at (-0.8,2.2){$v+tw$};
			\end{tikzpicture}
			\caption{Una recta en el plano.}
			\label{fig-recta-parametrica}
		\end{figure} 
		Cuando damos tal representación paramétrica, podemos pensar en un móvil que comienza en el punto $v$ en el tiempo $t = 0$, y moviéndose en la dirección de $w$. En el momento $t$, el móvil está en la posición $v+tw$. Por lo tanto, podemos interpretar físicamente la representación paramétrica como una descripción del movimiento, en que $w$ se interpreta como la velocidad del móvil. En un momento dado $t$, el móvil está en el punto
		\begin{equation*}
		X(t) =v +tw
		\end{equation*}
		que es  llamada la \textit{posición} del móvil en el tiempo $t$.
		
		Esta representación paramétrica también es útil para describir el conjunto de los puntos que se encuentran en el segmento de línea entre dos puntos dados. Sean $v$, $u$ dos puntos, entonces el segmento entre $v$ y $u$ consiste en todos los puntos
		\begin{equation*}
			S(t) = v + t(u-v)\quad  \text{con}\quad 0 \le t \le 1.
		\end{equation*}
		Observar que en tiempo 0, $S(0) = v$ y  en tiempo 1, $S(1)= v + (u-v)= u$. Como $t$ ``va'' de 0 a 1,  el móvil va de $v$ a $u$,  en linea recta. 
		
		Extendiendo a ambos lados el segmento, podemos describir la recta que pasa por $v$ y $u$ por la ecuación paramétrica (fig. \ref{fig-recta-entre-puntos})
		\begin{equation*}
		S(t) = v + t(u-v)\quad  \text{con}\quad t \in \R.
		\end{equation*}
			\begin{figure}[h]
			\begin{tikzpicture}
			\draw[thick,-] (-4,0) -- (4,0);
			\draw[thick,-] (0,-1) -- (0,2);
			\draw[very thick,-] (-4,2) -- (4,-0.5);
			\node[inner sep=1.5pt,fill,circle] at (-4 + 8/4,2 -2.5/4) {};
			\draw[very thick,-] (0,0) -- (-4 + 8/4,2 -2.5/4) node[above]{$u$};
			\draw[very thick,-] (0,0) -- (-4 + 2.5*8/4,2 -2.5*2.5/4) node[above]{$v$};
			\node[inner sep=1.5pt,fill,circle] at (-4 + 2.5*8/4,2 -2.5*2.5/4)  {};
			\draw[very thick,-] (-4 + 8/4,2 -2.5/4) -- (-4 + 2.5*8/4,2 -2.5*2.5/4);
			\end{tikzpicture}
			\caption{La recta que pasa entre $v$ y $u$.}
			\label{fig-recta-entre-puntos}
		\end{figure} 
		
		\begin{ejemplo}
			Encontrar una representación paramétrica para la recta que contiene los puntos $(1, - 3, 1)$ y $(- 2,4,5)$.
		\end{ejemplo}	
		\begin{proof}[Solución] Llamemos $v = (1, - 3, 1)$ y $u =(- 2,4,5)$. Entonces $u-v = (- 2,4,5) - (1,-3,1) = (-3,7,4)$ y la representación paramétrica de la recta que pasa por $u$ y $v$  es 
		\begin{equation*}
			X (t) = v + t(u-v) = (1, -3,1) + t (-3, 7, 4),\quad t\in \R.
		\end{equation*}
		\end{proof}
	
		Ahora discutiremos la relación entre una representación paramétrica y la ecuación usual de una recta en el plano.
		
		Supongamos que trabajamos en el plano y tenemos una recta descrita en forma paramétrica:
		\begin{equation*}
		X =v +tw.
		\end{equation*}
		Sea $v = (x_1,y_1)$, $w = (x_2,y_2)$,  entonces, todo punto de la recta es de la forma
		\begin{equation*}
		(x,y) =(x_1,y_1) +t(x_2,y_2) = (x_1+tx_2,y_1+ty_2),
		\end{equation*}
		es decir, los puntos de la recta $X$ son los $(x,y)$ tal que
		\begin{equation*}
			x = x_1+tx_2, \qquad y = y_1+ty_2, \qquad
		\end{equation*}
		para $t \in \R$. Entonces,  podemos eliminar $t$ y obtener la ecuación usual que relaciona $x$ e $y$.
		
		\begin{ejemplo}
			Sean $v = (2, 1)$ y $w = (- 1, 5)$. Entonces la representación paramétrica de la línea a través de $v$ en la dirección de $w$ nos da
			\begin{equation}\label{eq-par-des-ej}
			x = 2 - t, \qquad\quad y = 1 + 5t. \qquad\quad\tag{*}
			\end{equation}
			Despejando $t$ de la primera ecuación obtenemos $t = 2-x$. Reemplazando este valor de $t$  en la segunda ecuación obtenemos $y = 1 + 5t=1 + 5(2-x)t= y = 11- 5x$, luego
			\begin{equation*}\label{eq-impl-rec-ej}
				5x + y = 11,\tag{**}
			\end{equation*}
			que es la ecuación usual de una recta.
			
			Esta eliminación de $t$ muestra que cada par $(x, y)$ que satisface la representación paramétrica (\ref{eq-par-des-ej}) para algún valor de $t$ también satisface la ecuación (\ref{eq-impl-rec-ej}). A la inversa, supongamos que tenemos un par de números $(x, y)$ que satisfacen (\ref{eq-impl-rec-ej}). Sea $t = 2 - x$. Entonces
			\begin{equation*}
				y = 11 - 5x = 11 - 5 (2 - t) = 1 + 5t.
			\end{equation*}
			Por lo tanto, existe algún valor de $t$ que satisface la ecuación (\ref{eq-par-des-ej}). Así, hemos demostrado que los pares $(x, y)$ que son soluciones de (\ref{eq-impl-rec-ej}) son exactamente los mismos pares de números que los obtenidos al dar valores arbitrarios para $t$ en (\ref{eq-par-des-ej}). Por lo tanto, la línea recta se puede describir paramétricamente como en (\ref{eq-par-des-ej}) o en términos de su ecuación habitual (\ref{eq-impl-rec-ej}).
		\end{ejemplo}
		
		El razonamiento anterior se puede aplicar a  cualquier expresión paramétrica  de una recta y así tenemos
		
		\begin{definicion}\label{def-eq-implicita-rect} Sean $a,b, c \in \R$ tal que $(a,b) \ne (0,0)$ y sea $L = \{(x,y): ax +by =c\}$. Entonces diremos que $ax +by =c$  es la \textit{ecuación implícita} de la recta $L$. 
		\end{definicion} 
		
		
		Debemos observar que en $\R^3$ no alcanza una sola ecuación para definir una recta, veremos en la sección siguiente que una ecuación lineal define un plano en $\R^3$. Genéricamente hablando, con las soluciones de una ecuación en $\R^n$ se obtiene un objeto ``con una dimensión menos''. Todo esto quedará claro al final de la materia. 
		

	
	\end{section}

	\begin{section}{Planos en $\R^3$} 
		En  la sección anterior vimos (aunque no lo demostramos) que son equivalentes la definición paramétrica y la definición implícita de la recta. Para definir un plano en $\R^3$ utilizaremos primero la definición implícita. 
		\begin{definicion}\label{def-eq-implicita-plano} Sean $a,b, c,d \in \R$ tal que $(a,b,c) \ne (0,0,0)$ y sea 
		\begin{equation*}
			P = \{(x,y,z): ax +by +cz =d\}.
		\end{equation*}
		Entonces diremos que $P$  es  un \textit{plano con ecuación implícita}  $ax +by +cz =d$. 
		\end{definicion} 
	
		Observar que $(a,b,c) \ne (0,0,0)$ en la definición implica que una de las tres componentes del vector no es cero. supongamos que $a\ne 0$,  entonces es fácil despejar $x$ en función de las constantes $a$, $b$, $c$ y $d$; y las variables $y$ y $z$, por lo tanto cada coordenada del plano depende paramétricamente de $y$ y $z$ y así obtenemos una ecuación paramétrica de $P$ (que depende de 2 parámetros). Se puede hacer de forma análoga cuando $b\ne 0$ o $c \ne 0$. 
		
		
	\end{section}

	\end{chapter}
	
	
	\begin{chapter}{Sistemas lineales}\label{chap-sist-lin}
		
		\begin{section}{Sistemas de ecuaciones lineales}
		
			El problema a resolver será el siguiente: buscamos números  $x_1,\ldots,x_n$ en el cuerpo $\K$ ($= \R$ o $\C$)  que satisfagan las siguientes condiciones
			\begin{equation}\label{sist-eq}
			\begin{matrix}
			a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &y_1\\
			\vdots&  &\vdots& &&  &\vdots \\
			a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&y_m
			\end{matrix}
			\end{equation}
			donde $y_1, \ldots,y_m$ y $a_{i,j}$ ($1 \le i \le m$, $1 \le j \le n$) son números en $\K$.
			
			Llamaremos a (\ref{sist-eq}) un \textit{sistema de $m$ ecuaciones lineales con $n$ incógnitas.}\index{sistema de ecuaciones lineales} A una  $n$-upla $(x_1,\ldots,x_n)$ de elementos de $\K^n$ que satisface cada una de las ecuaciones de  (\ref{sist-eq}) la llamaremos una \textit{solución del sistema}. Si $y_1 = \cdots = y_m=0$, el sistema se llamará \textit{homogéneo.} En caso contrario el sistema se denominará \textit{no-homogéneo.}
			
			\begin{ejemplo}
				Los siguientes son sistemas de 2 ecuaciones lineales con 2 incógnitas:
				\begin{equation*}
				\begin{matrix}
				\text{(1)} & 
				\begin{matrix}
				2x_1 + 8x_2 &= 0 &  \\
				2x_1 + x_2 &=  1& 
				\end{matrix} &\quad&
				\text{(2)} & 
				\begin{matrix}
				2x_1 + x_2 &= 0 &  \\
				2x_1 - x_2 &=  1& 
				\end{matrix} &\quad&
				\text{(3)} & 
				\begin{matrix}
				2x_1 + x_2 &= 1 &  \\
				4x_1 +2 x_2 &=  2& 
				\end{matrix} 
				\end{matrix}
				\end{equation*}
			\end{ejemplo} 
			
			Resolvamos ahora un sistema de ecuaciones homogéneo sencillo:
			\begin{equation*}
			\begin{matrix}
			\circled{1}&\qquad& 2x_1 -x_2 + x_3 &=& 0 \\
			\circled{2}&\qquad& x_1 + 3x_2 + 4x_3 &=& 0.
			\end{matrix}
			\end{equation*}
			Observar que $(0,0,0)$ es solución. Busquemos otras soluciones manipulado las ecuaciones.
			\begin{align*}
			\begin{matrix}
			&\text{Si  hacemos}&-2\,\circled{2} +  \circled{1}& \text{obtenemos:} & & \circled{$1^\prime$}&  -7x_2 - 7x_3&=&0 &\;\Rightarrow\;& x_2 &=& -x_3\\
			&\text{Si  hacemos}&3 \,\circled{1} + \circled{2}& \text{obtenemos:} && \circled{$2^\prime$}& 7x_1 + 7x_3&=&0&\;\Rightarrow\;& x_1 &=& -x_3,
			\end{matrix}
			\end{align*}
			y esto nos dice que las soluciones son de la forma $\{(-x_3,-x_3, x_3): x_3 \in \R \}$, por ejemplo $(-1,-1,1)$ es solución y $(1,2,3)$ no es solución.
			
			Hemos encontrado soluciones por \textit{eliminación de incógnitas},  es decir multiplicando por constantes adecuadas  las ecuaciones y sumándolas hemos eliminado en  \circled{$1^\prime$} a $x_1$ y en  \circled{$2^\prime$} a $x_2$, con lo cual el resultado se deduce inmediatamente por pasaje de término.   
			
			\vskip .3cm
			
			
			
			Veremos ahora que este método se puede extender a sistemas de ecuaciones más generales, como en  (\ref{sist-eq}): si en este sistema multiplicamos cada ecuación por $c_i$ ($1 \le i \le m$) y sumamos miembro a miembro obtenemos
			\begin{equation*}
			\sum_{i=1}^m c_i(a_{i1}x_1 + a_{i2}x_2 + \cdots + a_{in}x_n) = \sum_i c_iy_i.
			\end{equation*}
			Expandiendo la ecuación y tomando como factor común los $x_j$ ($1 \le j \le n$) obtenemos la ecuación
			\begin{multline*}
			(c_{1}a_{11} + c_2a_{21}+ \cdots + c_ma_{m1})x_1 + \cdots +  	(c_{1}a_{1n} + c_2a_{2n}+ \cdots + c_ma_{mn})x_n = \\ = 	c_{1}y_{1} + c_2y_{2}+ \cdots + c_my_{m},
			\end{multline*}
			o,  escrito de otra forma, 
			\begin{equation}\label{comb-lin-eq}
			\left(\sum_{i=1}^{m}c_{i}a_{i1}\right)x_1 + \cdots +  	\left(\sum_{i=1}^{m}c_{i}a_{in}\right)x_n = \sum_{i=1}^{m}	c_{i}y_{i},
			\end{equation}
			la cual es una \textit{combinación lineal} de las ecuaciones dadas en (\ref{sist-eq}). Observar que la ecuación (\ref{comb-lin-eq}),  es  una ecuación lineal con $n$ incógnitas, es decir  es del mismo tipo que cada una de las ecuaciones que componen el sistema de ecuaciones original.
			
			\begin{proposicion}\label{sist-impl}
				Si $(x_1,\ldots,x_n) \in \K^n$  es solución del sistema de ecuaciones (\ref{sist-eq}), entonces $(x_1,\ldots,x_n)$ también es solución de la ecuación (\ref{comb-lin-eq}). 
			\end{proposicion}
			\begin{proof}
				Por hipótesis
				\begin{equation*}
				a_{i1}x_1 + a_{i2}x_2 + \cdots + a_{in}x_n = y_i,\; \text{ para } 1 \le i \le m.
				\end{equation*}
				Luego, 
				\begin{equation*}
				\sum_{i=1}^m c_i(a_{i1}x_1 + a_{i2}x_2 + \cdots + a_{in}x_n) = \sum_i c_iy_i
				\end{equation*}
				y  esta es otra escritura de la ecuación (\ref{comb-lin-eq}).
			\end{proof}
			
			La idea de hacer combinaciones lineales de ecuaciones es fundamental en el proceso de eliminación de incógnitas. En principio no es cierto que si obtenemos un sistema de ecuaciones por combinaciones lineales de otro sistema, ambos tengan las mismas soluciones   (por ejemplo hacer combinaciones lineales triviales con todos los coeficientes iguales a 0).
			
			Decimos que dos sistemas son \textit{equivalentes}\index{sistemas lineales equivalentes} si cada ecuación de un sistema es combinación lineal del otro.
			
			\begin{teorema}\label{sistemas-equiv}
				Sistemas de ecuaciones lineales equivalentes tienen las mismas soluciones.
			\end{teorema}
			\begin{proof}
				Sea 
				\begin{equation}\label{sist-01} \tag{*}
				\begin{matrix}
				a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &y_1\\
				\vdots&  &\vdots& &&  &\vdots \\
				a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&y_m
				\end{matrix}
				\end{equation}
				equivalente a
				\begin{equation}\label{sist-02} \tag{**}
				\begin{matrix}
				b_{11}x_1& + &b_{12}x_2& + &\cdots& + &b_{1n}x_n &= &z_1\\
				\vdots&  &\vdots& &&  &\vdots \\
				b_{k1}x_1& + &b_{k2}x_2& + &\cdots& + &b_{kn}x_n &=&z_k,
				\end{matrix}.
				\end{equation}
				En particular, las ecuaciones  de (\ref{sist-02}) se obtienen a partir de combinaciones lineales de las ecuaciones del sistema  (\ref{sist-01}). Luego, por proposición \ref{sist-impl}, si  $(x_1,\ldots,x_n)$  es solución de  (\ref{sist-01}), también será solución de cada una de las ecuaciones de (\ref{sist-02}) y por lo tanto solución  del sistema. 
				
				Recíprocamente, como también las ecuaciones  de (\ref{sist-01}) se obtienen a partir de combinaciones lineales de las ecuaciones del sistema  (\ref{sist-02}), toda solución  de 	
				(\ref{sist-02}) es solución de  (\ref{sist-01}).
			\end{proof}
			
			
			
			
			
			
			\vskip .3cm  
			
			Observar que la equivalencia de sistemas lineales es una relación de equivalencia,  en particular  vale la propiedad transitiva: si el sistema (A) es equivalente al sistema (B) y  el sistema (B)  es equivalente al sistema (C),  entonces (A) es equivalente a (C). Esto nos permite, ir paso a paso para eliminar las incógnitas. 
			
			\begin{ejemplo}
				Encontrar las soluciones del siguiente sistema de ecuaciones
				\begin{equation*}\label{sist-000} \tag{S0}
				\begin{matrix}
				\circled{1}&\qquad& 2x_1 &+&4x_2 &-& 6x_3 &=& 0 \\
				\circled{2}&\qquad& 3x_1 &-& x_2 &+& 5x_3 &=& 0.
				\end{matrix}
				\end{equation*}
			\end{ejemplo}
			\begin{proof}[Solución]
				Si reemplazamos la ecuación $\circled{1}$ por $\circled{1}/2$, obtenemos el sistema
				\begin{align*}\label{sist-001} \tag{S1}
				\begin{matrix}
				\circled{$1^\prime$}&\qquad x_1 &+&2x_2 &-& 3x_3 &=& 0 \\
				\circled{$2^\prime$}&\qquad 3x_1 &-& x_2 &+& 5x_3 &=& 0.
				\end{matrix}
				\end{align*}
				Reemplazando \circled{$2^\prime$} por $\circled{$2^\prime$}-3\circled{$1^\prime$}$, obtenemos
				\begin{align*}\label{sist-002} \tag{S2}
				\begin{matrix}
				\circled{$1^{\prime\prime}$}&\qquad x_1 &+&2x_2 &-& 3x_3 &=& 0 \\
				\circled{$2^{\prime\prime}$}&\qquad &-&7 x_2 &+& 14x_3 &=& 0.
				\end{matrix}
				\end{align*} 
				Reemplazando \circled{$2^{\prime\prime}$} por $\circled{$2^{\prime\prime}$}/(-7)$, obtenemos
				\begin{align*}\label{sist-003} \tag{S3}
				\begin{matrix}
				\circled{$1^{\prime\prime\prime}$}&\qquad x_1 &+&2x_2 &-& 3x_3 &=& 0 \\
				\circled{$2^{\prime\prime\prime}$}&\qquad && x_2 &-& 2x_3 &=& 0.
				\end{matrix}
				\end{align*} 
				Reemplazando \circled{$1^{\prime\prime\prime}$} por $\circled{$1^{\prime\prime\prime}$}-2\circled{$2^{\prime\prime\prime}$}$, obtenemos
				\begin{align*}\label{sist-004} \tag{S4}
				\begin{matrix}
				\circled{$1^{\prime\prime\prime\prime}$}&\qquad x_1 && &+& x_3 &=& 0 \\
				\circled{$2^{\prime\prime\prime\prime}$}&\qquad && x_2 &-& 2x_3 &=& 0.
				\end{matrix}
				\end{align*} 
				
				Luego $x_1 = -x_3$ y $x_2 = 2x_3$, y esto nos dice que las soluciones son de la forma $\{(-x_3,2x_3, x_3): x_3 \in \R \}$.
				
				Por otro lado, observar que 
				\begin{itemize}
					\item a partir de (\ref{sist-004}) podemos obtener (\ref{sist-003}) reemplazando \circled{$1^{\prime\prime\prime\prime}$} por $\circled{$1^{\prime\prime\prime\prime}$}+2 \,\circled{$2^{\prime\prime\prime\prime}$}$;
					\item a partir de (\ref{sist-003}) podemos obtener (\ref{sist-002}) reemplazando \circled{$2^{\prime\prime\prime}$} por $-7\,\circled{$2^{\prime\prime\prime}$}$;
					\item a partir de (\ref{sist-002}) podemos obtener (\ref{sist-001}) reemplazando \circled{$2^{\prime\prime}$} por $\circled{$2^{\prime\prime}$}+3\, \circled{$1^{\prime\prime}$}$;
					\item a partir de (\ref{sist-001}) podemos obtener (\ref{sist-000}) reemplazando \circled{$1^{\prime}$} por $2\,\circled{$1^{\prime}$}$.
				\end{itemize}
				
				Es decir los sistemas  (\ref{sist-000}) y (\ref{sist-004}) son equivalentes y por lo tanto  tienen las mismas soluciones.  Como el conjunto de soluciones de (\ref{sist-004}) es $\{(-x_3,2x_3, x_3): x_3 \in \R \}$,  éste también es el conjunto de soluciones del sistema original. 
			\end{proof}
			
			\begin{ejemplo}
				Encontrar las soluciones del siguiente sistema de ecuaciones
				\begin{equation*}
				\begin{matrix}
				\circled{1}&\qquad& 2x_1 &-&x_2 &+& x_3 &=& 1 \\
				\circled{2}&\qquad& x_1 &+& 3x_2 &+& 3x_3 &=&  2 \\
				\circled{3}&\qquad& x_1 &+& &+& 2x_3 &=&   1 .
				\end{matrix}
				\end{equation*}
				(observar que en $\circled{3}$ el coeficiente de $x_2$ es cero.)
			\end{ejemplo}
			\begin{proof}[Solución] Si
				\begin{align*}
				\begin{matrix*}[l]
				&\text{reemplazamos \circled{1} por }&\circled{1}-2\,\circled{2} + & \text{obtenemos:} &\quad& \circled{$1^\prime$}&\; -7x_2 - 5x_3&=&-3, \\
				&\text{reemplazamos \circled{2} por }&\circled{2}+3 \,\circled{1}& \text{obtenemos:} &\quad& \circled{$2^\prime$}&\; 7x_1 + 6x_3&=& 5, \\
				&\text{no cambiamos}&\circled{3} & \text{y obtenemos:} &\quad& \circled{$3^\prime$}&\; x_1 + 2x_3&=& 1.
				\end{matrix*}
				\end{align*}
				Ahora reemplazando  $\circled{$2^\prime$}$ por  $ \circled{$2^\prime$}- 7\, \circled{$1^\prime$}$, obtenemos $- 8x_3= -2$, que es equivalente a que $x_3 = \frac14$. Por lo tanto, obtenemos el sistema
				\begin{align*}
				\begin{matrix*}[r]
				\circled{$1^{\prime\prime}$}&\qquad -7x_2 - 5x_3&=&-3 \\
				\circled{$2^{\prime\prime}$}&\qquad x_3&=& \frac14 \\
				\circled{$3^{\prime\prime}$}&\qquad x_1 + 2x_3&=& 1 ,
				\end{matrix*}
				\end{align*}
				Luego, reemplazando $x_3$ por $\frac14$ y  despejando en  \circled{$1^{\prime\prime}$} y \circled{$2^{\prime\prime}$}:
				\begin{align*}
				\begin{matrix*}[r]
				\circled{$1^{\prime\prime\prime}$}&\qquad -7x_2 &=&-3 +5\frac14 \\
				\circled{$2^{\prime\prime\prime}$}&\qquad x_3&=& \frac14 \\
				\circled{$3^{\prime\prime\prime}$}&\qquad x_1 &=& 1 -2\frac14.
				\end{matrix*}
				\end{align*}
				Por lo tanto, $x_1 = \frac12$, $x_2 = \frac14$, $x_3 = \frac14$. 
			\end{proof}
			
			\vskip .3cm 
		\end{section}
		
		
		
		\begin{section}{Matrices, operaciones elementales por fila y método de Gauss }
			Estudiaremos primeramente la solución de un sistema de ecuaciones lineales homogéneo
			\begin{equation}\label{sist-eq-hom}
			\begin{matrix}
			a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &0\\
			\vdots&  &\vdots& &&  &\vdots \\
			a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&0.
			\end{matrix}
			\end{equation}
			Observemos que podemos escribir el sistema omitiendo las incógnitas, las sumas y los iguales (que estarán implícitos), como un arreglo rectangular de $m$ filas y $n$ columnas:
			\begin{equation}\label{matriz}
			A = \begin{bmatrix}
			a_{11}& a_{12}& \cdots &a_{1n} \\
			\vdots&\vdots  &  &\vdots \\
			a_{m1} &a_{m2}&\cdots &a_{mn}
			\end{bmatrix}
			\end{equation} 
			
			
			\begin{definicion} Sea $\K$ cuerpo. Una \textit{matriz}\index{matriz} $m \times  n$ o de \textit{orden $m \times  n$} es un arreglo rectangular de elementos de $\K$ con $m$ filas y $n$ columnas. A cada elemento de la matriz la llamamos \textit{entrada} o \textit{coeficiente}. Si $A$ es una matriz $m \times  n$, denotamos $[A]_{ij}$ la entrada que se ubica en la fila $i$ y la columna $j$. Al conjunto de matrices de orden $m \times  n$ con entradas en $\K$ lo denotamos $M_{m\times n}(\K)$, o simplemente $M_{m\times n}$ si $\K$  está sobreentendido. 
			\end{definicion}
			
			\begin{observacion}
				Más formalmente, podemos ver una matriz como un elemento del producto cartesiano $(\K^n)^m$, es decir como $m$-uplas donde en cada coordenada hay una $n$-upla. Esta es la forma usual de describir una matriz en los lenguajes de programación modernos. 
			\end{observacion}
			
			\begin{ejemplo}
				El siguiente es un ejemplo de una matriz $2 \times 3$:
				\begin{equation*}
				A = \begin{bmatrix}
				2& -1& 4 \\
				-3 &0&1
				\end{bmatrix}
				\end{equation*} .
			\end{ejemplo}
			
			Usualmente escribiremos a una matriz $m \times  n$   con entradas  $[A]_{ij} = a_{ij}$ como en (\ref{matriz}). A esta matriz también la podemos denotar como $A = [a_{ij}]$. Dos matrices $A = [a_{ij}]$ y $B = [b_{ij}]$, de orden $m \times n$, son iguales si $a_{ij} = b_{ij}$ para todo $i = 1,\ldots,m$  y $j = 1,\ldots,n$. Es decir, dos matrices son iguales si los elementos que ocupan la misma posición en ambas matrices coinciden.
			
			\vskip .3cm
			
			Como hicimos al comienzo de la sección, a un sistema de $m$ ecuaciones con $n$ incógnitas le asignaremos una matriz $m \times  n$  lo cual nos permitirá trabajar en forma más cómoda y, como veremos en la próxima sección,  podremos resolver los sistemas en forma algebraica (usando ``sumas'' y ``productos''  entre matrices).
			En  esta sección haremos operaciones en la matriz asociada al sistema de ecuaciones para pasar a un sistema equivalente más ``fácil'' de resolver.
			
			Sean
			$$
			X = \begin{bmatrix}
			x_1 \\ \vdots \\ x_n
			\end{bmatrix}\qquad \text{ y } \qquad
			0 = \begin{bmatrix}
			0 \\ \vdots \\ 0
			\end{bmatrix},
			$$
			podemos escribir el sistema de ecuaciones (\ref*{sist-eq-hom}) en forma resumida como
			\begin{equation}\label{sis-eq-hom-2}
			AX = 0.
			\end{equation}
			Más adelante, veremos que esta notación tiene un sentido algebraico (el término de la izquierda es un ``producto de matrices'').
			
			\begin{subsection}{Operaciones elementales por fila}
				Sea $A = [a_{ij}]$ una matriz $m \times  n$,  entonces la fila $i$ es 
				$$
				\begin{bmatrix} a_{i1}& a_{i2}& \cdots &a_{in} 	\end{bmatrix},
				$$
				y la denotamos $F_i(A)$ o simplemente $F_i$ si $A$ está sobreentendido. Si $c\in \K$,  entonces 
				$$
				cF_i = \begin{bmatrix} ca_{i1}& ca_{i2}& \cdots &ca_{in} 	\end{bmatrix}
				$$
				y
				$$
				F_r + F_s = \begin{bmatrix} a_{r1}+a_{s1}& a_{i2}+a_{s2}& \cdots &a_{in}+a_{sn} 	\end{bmatrix}.
				$$ 
				Diremos que la fila $i$ es nula si 
				$$
				F_i = \begin{bmatrix} 0& 0& \cdots &0 	\end{bmatrix},
				$$
				
				\begin{definicion}
					Sea $A = [a_{ij}]$ una matriz $m \times  n$, diremos que $e$ es una  \textit{operación elemental por fila}\index{operación elemental por fila} si aplicada a la matriz $A$ se obtiene  $e(A)$ de la siguiente manera:
					\begin{enumerate}
						\item\label{elem-1} multiplicando una fila por una constante $c\not=0$, o
						\item\label{elem-2} cambiando la fila $F_r$ por $F_r + tF_s$ con $r\not=s$, para algún $t \in \K$, o
						\item\label{elem-3} permutando la fila $r$ por la fila $s$.   
					\end{enumerate}
					Más precisamente sea 
					\begin{equation*}
					A = \begin{bmatrix} 
					F_1 \\  \vdots \\	F_m
					\end{bmatrix},
					\end{equation*} entonces 
					\begin{enumerate}
						\item si multiplicamos la fila $r$ por $c \not=0$, 
						$$ e(A) = \begin{bmatrix} 
						F_1 \\ 	\vdots \\ cF_r \\ \vdots \\	F_m
						\end{bmatrix} $$ con $c \not=0$, o
						\item si $r\not=s$, multiplicamos la fila $s$ por  $t \in \K$ y la sumamos a la fila $r$, 
						$$ e(A)= \begin{bmatrix} 
						F_1 \\  \vdots \\ F_r + t F_s\\ \vdots \\	F_m
						\end{bmatrix}.$$
						\item La última operación elemental es  permutar la fila $r$ por la fila $s$:
						$$
						A= \begin{bmatrix} 
						F_1 \\ 	\vdots \\ F_r \\ \vdots \\ F_s\\ \vdots \\	F_m
						\end{bmatrix} \quad \Rightarrow \quad
						e(A)= \begin{bmatrix} 
						F_1 \\ 	\vdots \\ F_s \\ \vdots \\ F_r\\ \vdots \\	F_m
						\end{bmatrix}.$$
					\end{enumerate}
				\end{definicion} 
				
				Podemos describir en forma más compacta una operación elemental por fila de la matriz $A=[a_{ij}]$:
				\begin{enumerate}
					\item multiplicar la fila $r$ por $c \not=0$
					$$
					e(A)_{ij} = \left\{ \begin{matrix}
					a_{ij}& \quad &\text{si $i\not=r$} \\
					ca_{ij}& \quad &\text{si $i=r$}
					\end{matrix}\right.
					$$
					o
					\item si $r\not=s$, multiplicar la fila $s$ por $t \in \K$ y sumarla a la fila $r$
					$$
					e(A)_{ij} = \left\{ \begin{matrix}
					a_{ij}& \quad &\text{si $i\not=r$} \\
					a_{rj} + t a_{sj}& \quad &\text{si $i=r$}
					\end{matrix}\right.
					$$
					con $t \in \K$, o
					\item permutar la fila $r$ por la fila $s$
					$$
					e(A)_{ij} = \left\{ \begin{matrix}
					a_{ij}& \quad &\text{si $i\not=r,s$}
					\\ a_{sj}& \quad &\text{si $i=r$}
					\\ a_{rj}& \quad &\text{si $i=s$}
					\end{matrix}\right.
					$$
				\end{enumerate} 
				
				\begin{ejemplo}
					Sea 
					$$
					A = 
					\begin{bmatrix}
					2&1\\-1&0\\4&-5
					\end{bmatrix}.
					$$
					Ejemplificaremos las operaciones elementales
					
					(1) Multipliquemos la fila 2 por -2, obtenemos
					$$
					e(A) = 
					\begin{bmatrix}
					2&1\\2&0\\4&-5
					\end{bmatrix}.
					$$
					
					(2) Sumemos a la fila 3 dos veces la fila 1,
					$$
					e(A) = 
					\begin{bmatrix}
					2&1\\-1&0\\8&-3
					\end{bmatrix}.
					$$
					
					(3) Permutemos la fila 2 con la fila 3.
					$$
					e(A) = 
					\begin{bmatrix}
					2&1\\4&-5\\-1&0
					\end{bmatrix}.
					$$
				\end{ejemplo}
				
				Una característica importante de las operaciones elementales es que cada una tiene como ``inversa'' otra operación elemental. 
				
				\begin{teorema}\label{op-elem}
					A cada operación elemental por fila $e$ le corresponde otra operación elemental $e^\prime$ (del mismo tipo que $e$) tal que $e^\prime(e(A)) = A$ y $e(e^\prime(A)) = A$. En otras palabras, la operación inversa de una operación elemental es otra operación elemental del mismo tipo.  
				\end{teorema}
				\begin{proof} \
					\begin{enumerate}
						\item[(1)] La operación inversa de multiplicar la fila $r$ por $c\not=0$ es multiplicar la misma fila por $1/r$.
						\item[(2)] La operación inversa de multiplicar la fila $s$ por  $t \in \K$ y sumarla a la fila $r$ es multiplicar  la fila $s$ por  $-t \in \K$ y sumarla a la fila $r$.
						\item[(3)] La operación inversa de permutar la fila $r$ por la fila $s$ es la misma operación.
					\end{enumerate}
				\end{proof}
				
				\begin{definicion} 
					Sean $A$ y $B$ dos matrices $m \times n$. Diremos que $B$ es \textit{equivalente por filas}\index{matrices equivalentes por filas} a $A$, si $B$ se puede obtener de $A$ por un número finito de operaciones elementales por fila. 
				\end{definicion}
				
				\begin{obs} Denotamos $A \sim B$, si $B$ es equivalente a $A$ por filas. Entonces esta relación es una \textit{relación de equivalencia}\index{relación de equivalencia}, es decir es reflexiva, simétrica y transitiva. En  nuestro caso, sean $A$, $B$ y $C$ matrices $m \times n$, entonces ``$\sim$'' cumple: 
					\begin{enumerate}
						\item  $A \sim A$ (reflexiva), 
						\item $A \sim B$, entonces $B \sim A$ (simétrica), y
						\item si $A \sim B$ y $B \sim C$, entonces $A \sim C$.   
					\end{enumerate}
					Claramente ``$\sim$'' es reflexiva (admitamos que no hacer nada es una equivalencia por filas). 
					
					Si podemos obtener $B$ de $A$ por operaciones elementales por fila, entonces, 
					$$
					B = e_k(e_{k-1}(\cdots(e_1(A)))\cdots),
					$$
					con $e_1,\ldots,e_k$ operaciones elementales por fila. Por el teorema \ref{op-elem},  tenemos $e'_1,\ldots,e'_{k-1},e'_k$ operaciones elementales inversas de  $e_1,\ldots,e_{k-1},e_k$, respectivamente. Luego, 
					$$
					A = e'_1(e'_{2}(\cdots(e'_k(B)))\cdots).
					$$
					Es decir, podemos  obtener $A$ de $B$ por operaciones elementales por fila, luego ``$\sim$'' es simétrica. Observar que para obtener $A$ a partir de $B$ tenemos que hacer las operaciones inversas en orden inverso. 
					
					Finalmente,   si podemos obtener $B$ de $A$ por operaciones elementales por fila y  podemos obtener $C$ de $B$ por operaciones elementales por fila, entonces podemos obtener $C$ de $A$ por operaciones elementales por fila (haciendo las primeras operaciones y luego las otras).
				\end{obs}
				
				\begin{ejemplo}
					Veamos que la matriz 
					\begin{equation*}
					A= 	\begin{bmatrix}
					3 & 9 & 6 \\ 4&8&4 \\ 0&2&2
					\end{bmatrix}
					\end{equation*}
					es equivalente por fila a la matriz
					\begin{equation*}
					B = \begin{bmatrix}
					1&0&-1 \\ 0&0&0\\  0&-1&-1
					\end{bmatrix}.
					\end{equation*}
					Hasta ahora, no hemos aprendido ningún algoritmo o método que nos lleve una matriz a otra por operaciones elementales por fila, pero no es difícil, en este caso, encontrar una forma de llevar la matriz $A$ a la matriz $B$:
					\begin{multline*}
					\begin{bmatrix}
					3 & 9 & 6 \\ 4&8&4 \\ 0&2&2
					\end{bmatrix} \stackrel{F_1/3}{\longrightarrow}
					\begin{bmatrix}
					1 & 3 & 2 \\ 4&8&4 \\ 0&2&2
					\end{bmatrix} \stackrel{F_2 -4F_1}{\longrightarrow}
					\begin{bmatrix}
					1 & 3 & 2 \\ 0&-4&-4 \\ 0&2&2
					\end{bmatrix} \stackrel{F_2/4}{\longrightarrow}
					\\
					\stackrel{F_2/4}{\longrightarrow}  \begin{bmatrix}
					1 & 3 & 2 \\ 0&-1&-1 \\ 0&2&2 
					\end{bmatrix} \stackrel{F_1 + 3F_2}{\longrightarrow} 
					\begin{bmatrix}
					1 & 0& -1 \\ 0&-1&-1 \\ 0&2&2 
					\end{bmatrix} \stackrel{F_3 + 2F_2}{\longrightarrow} 
					\begin{bmatrix}
					1 & 0& -1 \\ 0&-1&-1 \\ 0&0&0 
					\end{bmatrix} \stackrel{F_3\leftrightarrow F_2}{\longrightarrow} 
					\begin{bmatrix}
					1 & 0& -1 \\ 0&0&0\\  0&-1&-1 
					\end{bmatrix}.
					\end{multline*}
					
					Comprobamos fácilmente la propiedad reflexiva, pues podemos llegar de la matriz $B$ a la matriz $A$ haciendo, sucesivamente, la operaciones inversas en orden inverso:
					\begin{multline*}
					\begin{bmatrix}1 & 0& -1 \\ 0&0&0\\  0&-1&-1 \end{bmatrix}
					\stackrel{F_3\leftrightarrow F_2}{\longrightarrow} 
					\begin{bmatrix}1 & 0& -1 \\ 0&-1&-1 \\ 0&0&0 \end{bmatrix}
					\stackrel{F_3 - 2F_2}{\longrightarrow} 
					\begin{bmatrix}1 & 0& -1 \\ 0&-1&-1 \\ 0&2&2 \end{bmatrix} 
					\stackrel{F_1 - 3F_2}{\longrightarrow}
					\\ \stackrel{F_1 - 3F_2}{\longrightarrow}
					\begin{bmatrix}1 & 3 & 2 \\ 0&-1&-1 \\ 0&2&2 \end{bmatrix}
					\stackrel{4F_2}{\longrightarrow} 
					\begin{bmatrix}1 & 3 & 2 \\ 0&-4&-4 \\ 0&2&2 \end{bmatrix} 
					\stackrel{F_2 +4F_1}{\longrightarrow} 
					\begin{bmatrix} 1 & 3 & 2 \\ 4&8&4 \\ 0&2&2\end{bmatrix} 
					\stackrel{3F_1}{\longrightarrow} 
					\begin{bmatrix} 3 & 9 & 6 \\ 4&8&4 \\ 0&2&2 \end{bmatrix}.
					\end{multline*}
				\end{ejemplo}
				
				\vskip .5cm 
				
				\begin{teorema}\label{eq-fila-impl-sol}
					Sean $A,B$ matrices $m \times n$ tal que $A$ y $B$ son equivalentes  por filas. Entonces, los sistemas homogéneos $AX =0$ y $BX =0$ tiene las mismas soluciones. 
				\end{teorema}
				\begin{proof}
					Recordemos que dos sistemas de ecuaciones lineales son equivalentes si cada ecuación de un sistema es combinación lineal de las ecuaciones del otro. 	El  teorema \ref{sistemas-equiv} nos dice que dos sistemas equivalentes tienen las mismas soluciones. Probaremos, entonces,  que los sistemas  $AX =0$ y $BX =0$ son equivalentes, es decir que cada ecuación de $AX$ es combinación lineal de las ecuaciones de $BX$ y viceversa.
					
					Ahora bien, como $A$ y $B$ son equivalentes por filas existe una sucesión de operaciones elementales por fila que lleva la matriz $A$ a la matriz $B$:
					$$
					A = A_0 \longrightarrow A_1 \longrightarrow \cdots \longrightarrow A_k = B,
					$$
					donde $A_{i} = e_i(A_{i-1})$ con $e_i$ es una operación elemental ($1 \le i \le k $). Como las operaciones elementales permutan filas o realizan combinaciones lineales de filas, es claro que las ecuaciones de $A_{i}X=0$ son combinación lineal de las ecuaciones de $A_{i-1}X = 0$.  Por teorema  \ref{op-elem}, existe una operación elemental $e'_i$ inversa de $e_i$ tal  que $A_{{i-1}} = e'_i(A_i)$, luego las ecuaciones de $A_{i-1}X=0$ son combinación lineal de las ecuaciones de $A_iX = 0$ y concluimos que  $ A_{i}$ y   $ A_{i-1}$ determinan sistemas de ecuaciones equivalentes para  $1 \le i \le k$.
					
					Luego $A_0X$ es equivalente a $A_1X$, que es equivalente a a $A_2X$ y así sucesivamente, concluyendo que $AX=A_0X$ es equivalente a $BX =A_kX$. Luego ambos sistemas tiene las mismas soluciones.   
				\end{proof}
				
				\begin{ejemplo}\label{ejemplo2.11}
					Resolvamos el siguiente sistema:
					\begin{align}\label{sist-eq-01}
					\begin{split}
					2x_1 - x_2 + x_3 + 2x_4 &= 0 \\
					x_1 - 4x_2 -x_4 &=0 \\
					2x_1 +6x_2 -x_3 +3x_4 &= 0,  
					\end{split}
					\end{align}
					para  $x_i \in \R$ ($1 \le i \le 4$). 
					En todo sistema homogéneo $x_i =0$ ($1 \le i \le 4$) es solución. Veamos si hay otra soluciones. 
					
					La matriz correspondiente a este sistema de ecuaciones es 
					$$
					\begin{bmatrix} 2& -1&1& 2 \\ 1&-4 &0&-1 \\ 2&6&-1&3 \end{bmatrix}.
					$$
					Encontraremos una matriz que nos dará un sistema de ecuaciones equivalente, pero con soluciones mucho más evidentes:
					\begin{multline*}
					\begin{bmatrix} 2& -1&1& 2 \\ 1&-4 &0&-1 \\ 2&6&-1&3 \end{bmatrix}
					\stackrel{F_1\leftrightarrow F_2}{\longrightarrow} 
					\begin{bmatrix} 1&-4 &0&-1 \\ 2& -1&1& 2 \\ 2&6&-1&3 \end{bmatrix}
					\stackrel{F_2-2 F_1}{\longrightarrow} 
					\begin{bmatrix} 1&-4 &0&-1 \\ 0& 7&1& 4 \\ 2&6&-1&3 \end{bmatrix}
					\\
					\stackrel{F_3-2 F_1}{\longrightarrow} 
					\begin{bmatrix} 1&-4 &0&-1 \\ 0& 7&1& 4 \\ 0&14&-1&5 \end{bmatrix} 
					\stackrel{F_3-2 F_2}{\longrightarrow} 
					\begin{bmatrix} 1&-4 &0&-1 \\ 0& 7&1& 4 \\ 0&0&-3&-3 \end{bmatrix} 
					\stackrel{F_3/(-3)}{\longrightarrow} 
					\begin{bmatrix} 1&-4 &0&-1 \\ 0& 7&1& 4 \\ 0&0&1&1 \end{bmatrix}
					\\
					\stackrel{F_2- F_3}{\longrightarrow} 
					\begin{bmatrix} 1&-4 &0&-1 \\ 0& 7&0& 3 \\ 0&0&1&1 \end{bmatrix}
					\stackrel{F_2/7}{\longrightarrow} 
					\begin{bmatrix} 1&-4 &0&-1\\ 0& 1&0& \frac37\\ 0&0&1&1\end{bmatrix}
					\stackrel{F_1 +4F_2}{\longrightarrow} 
					\begin{bmatrix}1&0&0&\frac{5}{7}\\0&1&0&\frac37 \\ 0&0&1&1\end{bmatrix}.
					\end{multline*}
					Volvamos a las ecuaciones: el nuevo sistema de ecuaciones, equivalente al original, es
					\begin{align*}
					x_1 +\frac{5}{7}x_4 &= 0 \\
					x_2 + \frac{3}{7}x_4 &=0 \\
					x_3 +x_4 &= 0, 
					\end{align*}
					luego 
					\begin{align*}
					x_1  &=-\frac{5}{7}x_4 \\
					x_2  &=- \frac{3}{7}x_4  \\
					x_3  &= -x_4. 
					\end{align*}
					Por lo tanto, el conjunto de soluciones del sistema de ecuaciones (\ref{sist-eq-01}) es
					$$
					\left\{(-\frac{5}{7}t,- \frac{3}{7}t, -t,t): t \in \R \right\}.
					$$
					Luego, el sistema tiene infinitas soluciones parametrizadas por una variable $t \in \R$.
				\end{ejemplo}
				
				
				\begin{ejemplo}
					Consideremos ahora el siguiente sistema sobre los números complejos:
					\begin{align}\label{sist-eq-03}
					\begin{split}
					2x_1 +i x_2 &= 0 \\
					-ix_1 +3x_2  &=0 \\
					x_1 +2x_2  &= 0.
					\end{split}
					\end{align}
					Al ser un sistema homogéneo $x_1=x_2 = 0$ es solución. Veamos si hay otras soluciones: 
					\begin{multline*}
					\begin{bmatrix} 2&i \\ -i&3 \\ 1&2 \end{bmatrix}
					\stackrel{F_1\leftrightarrow F_3}{\longrightarrow} 
					\begin{bmatrix} 1&2 \\ -i&3 \\ 2&i \end{bmatrix}
					\stackrel{F_2+iF_1}{\longrightarrow} 
					\begin{bmatrix} 1&2 \\ 0&3+2i \\ 2&i \end{bmatrix}
					\stackrel{F_3-2F_1}{\longrightarrow} 
					\begin{bmatrix} 1&2 \\ 0&3+2i \\ 0&-4+i \end{bmatrix}
					\\
					\stackrel{F_2/(3+2i)}{\longrightarrow} 
					\begin{bmatrix} 1&2 \\ 0&1 \\ 0&-4+i \end{bmatrix}
					\stackrel{F_3-(-4+i)F_2}{\longrightarrow} 
					\begin{bmatrix} 1&2 \\ 0&1 \\ 0&0 \end{bmatrix}
					\stackrel{F_1-2F_2}{\longrightarrow} 
					\begin{bmatrix} 1&0 \\ 0&1 \\ 0&0 \end{bmatrix}.
					\end{multline*}
					Luego  el sistema (\ref{sist-eq-03}) es equivalente al sistema  $x_1=x_2 = 0$, que resulta ser la única solución.
				\end{ejemplo}
			\end{subsection} 
			
			
			\begin{subsection}{Matrices reducidas por filas} Ahora avanzaremos en una forma sistemática para hallar todas las soluciones de un sistema de ecuaciones.
				
				\begin{definicion}
					Una matriz $A$ de $m \times n$ se llama \textit{reducida por filas}\index{matriz reducida por filas} o \textit{MRF}\index{MRF} si 
					\begin{enumerate}
						\item[(a)] la primera entrada no nula de una fila de $A$ es 1. Este 1 es llamado \textit{1 principal}\index{1 principal de una MRF}.
						\item[(b)] Cada columna de $A$ que contiene un  1 principal tiene todos los otros elementos iguales a 0. 
					\end{enumerate} 
				\end{definicion} 
				
				\begin{ejemplo} Las siguientes matrices son MRF:
					\begin{equation*}
					\begin{bmatrix}1 & 0& -1 \\ 0&1&3\\  0&0&0 \end{bmatrix}\; \qquad 
					\begin{bmatrix} 0&1&3 \\1 & 0& -1\\  0&0&0 \end{bmatrix};
					\end{equation*}
					y las siguientes matrices, no son MRF:
					\begin{equation*}
					\begin{bmatrix}1 & 0& 1 \\ 0&2&3\\  0&0&0 \end{bmatrix}\; \text{no cumple (a),} \qquad
					\begin{bmatrix}1 & 0& -1 \\ 0&1&3\\  0&0&1 \end{bmatrix}\; \text{no cumple (b)}. 
					\end{equation*}
				\end{ejemplo}
				
				\begin{definicion}
					Sea $I_n$ la matriz  $n \times n$ definida 
					\begin{equation*}
					[I_n]_{ij} = \left\{ \begin{matrix}
					1 && \text{si $i=j$,}\\
					0 && \text{si $i\not=j$,}
					\end{matrix}\right.\qquad \text{ o bien }\qquad I_n =
					\begin{bmatrix}
					1 & 0 & \cdots & 0 \\
					0 & 1 & \cdots & 0 \\
					\vdots & \vdots & \ddots & \vdots \\
					0 & 0 & \cdots & 1
					\end{bmatrix}
					\end{equation*}
					(la matriz cuadrada con 1's en la diagonal y 0's en las otras entradas). Llamaremos a $I_n$ la \textit{matriz identidad $n \times n$}\index{matriz identidad $n \times n$}.
				\end{definicion}
				
				Observar que $I_n$ es una matriz reducida por fila.
				
				\begin{teorema}\label{th-mrf}
					Toda matriz $m \times n$ sobre $\K$ es equivalente por fila a una matriz reducida por fila.
				\end{teorema}
				\begin{proof}[Demostración (*)]
					Sea $A = [a_{ij}]$ una matriz $m \times n$. Trabajaremos fila por fila, de la primera a la última, de tal forma de ir encontrando matrices equivalentes por fila en cada paso, con ciertas características que ya detallaremos. Cuando terminemos llegaremos a una MRF.  
					
					Si la primera fila es nula pasamos a la segunda fila. Si la primera fila no es nula sea $a_{1k}$ la primera entrada no nula, es decir
					\begin{equation*}
					A = \begin{bmatrix}
					0 & \cdots & 0 & a_{1k} & \cdots & a_{1n} \\
					a_{21}& \cdots & a_{2,k-1} & a_{2k} & \cdots & a_{2n} \\
					\vdots&  &  &  &  & \vdots \\
					a_{m1}& \cdots & a_{m,k-1} & a_{mk} & \cdots & a_{mn} \\
					\end{bmatrix}.
					\end{equation*} 
					Esta matriz es equivalente por fila a $A_1$ donde $A_1$ se obtiene dividiendo la fila 1 por $a_{1k}$. Luego 
					\begin{equation*}
					A_1 = \begin{bmatrix}
					0 & \cdots & 0 & 1 & \cdots & a_{1n} \\
					a_{21}& \cdots & a_{2,k-1} & a_{2k} & \cdots & a_{2n} \\
					\vdots&  &  &  &  & \vdots \\
					a_{m1}& \cdots & a_{m,k-1} & a_{mk} & \cdots & a_{mn} \\
					\end{bmatrix}
					\end{equation*}
					(donde los nuevos $a_{1j}$ son  los originales divididos por $a_{1k}$).  
					
					Haciendo $m-1$ equivalencias por fila (reemplazamos $F_i$ por $F_i - a_{ik}F_1$) podemos hacer nulas todas las entradas debajo del  1 principal y obtener la matriz equivalente por fila
					\begin{equation*}
					A_2 = \begin{bmatrix}
					0 & \cdots & 0 & 1 & \cdots & a_{1n} \\
					a_{21}& \cdots & a_{2,k-1} & 0 & \cdots & a_{2n} \\
					\vdots&  &  &  &  & \vdots \\
					a_{m1}& \cdots & a_{m,k-1} &0 & \cdots & a_{mn} \\
					\end{bmatrix}
					\end{equation*}
					(obviamente los nuevos $a_{ij}$ están transformados por las equivalencias).
					
					El mismo procedimiento que hicimos arriba lo podemos hacer en la fila 2,  de tal forma que la fila 2 es cero u obtenemos otro 1 principal y todas las demás entradas de la columna donde se encuentra el 1 principal son nulas. 
					
					Repitiendo este procedimiento en todas las filas hasta la última, obtenemos que cada fila es, o bien 0, o bien la primera entrada no nula es  1 y todas las entradas en la misma columna de este 1 principal son nulas.   Esto, claramente, nos dice que hemos obtenido una matriz reducida por fila.  
				\end{proof}
				
				El procedimiento que usamos en la demostración del teorema anterior nos da un algoritmo para obtener una MRF a partir de una matriz arbitraria.
				
				\begin{observacion}[\sc Método para reducir a MRF] \label{metodo-merf}  
					\
									
				\begin{enumerate}
					\item \label{paso-0} Nos ubicamos en la primera fila.
					\item \label{paso-1} Si la fila es 0 y no es la última, pasar a la fila siguiente.
					\item \label{paso-2} Si la fila no es 0, 
					\begin{enumerate}
						\item si el primera entrada no nula está en  la columna $k$ y su valor es $c$, dividir la fila por $c$ (ahora la primera entrada no nula vale 1),
						\item con operaciones elementales del tipo $F_r+ tF_s$ hacer 0  todas las entradas en la columna $k$ (menos la de la columna actual).    
					\end{enumerate}
					\item \label{paso-3} Si la fila es la última, terminar. Si la fila no es la última, pasar a la fila siguiente e ir al paso (\ref{paso-1}).  
				\end{enumerate}
				Observar  que en este procedimiento nunca se utiliza la operación elemental de permutar filas.
			\end{observacion}
				
				
				\begin{ejemplo} Ejemplifiquemos con la matriz que aparece en el ejemplo  \ref{ejemplo2.11},  es decir con la matriz
					$$
					\begin{bmatrix} 2& -1&1& 2 \\ 1&-4 &0&-1 \\ 2&6&-1&3 \end{bmatrix}.
					$$
					Siguiendo estrictamente el algoritmo:
					\begin{multline*}
					\begin{bmatrix*}[r] 2& -1&1& 2 \\ 1&-4 &0&-1 \\ 2&6&-1&3 \end{bmatrix*}
					\stackrel{F_1/2}{\longrightarrow} 
					\begin{bmatrix*}[r] 1& -\frac12&\frac12& 1 \\ 1&-4 &0&-1 \\ 2&6&-1&3 \end{bmatrix*}
					\underset{F_3-2F_1}{\stackrel{F_2- F_1}{\longrightarrow}} 
					\begin{bmatrix*}[r] 1& -\frac12&\frac12& 1 \\ 0&-\frac72 &-\frac12&-2 \\ 0&7&-2&1 \end{bmatrix*}
					\\
					\stackrel{F_2/(-\frac72)}{\longrightarrow} 
					\begin{bmatrix*}[r] 1& -\frac12&\frac12& 1 \\ 0&1 &\frac17&\frac47 \\ 0&7&-2&1 \end{bmatrix*}
					\underset{F_3-7F_2}{\stackrel{F_1 +\frac12 F_2}{\longrightarrow}} 
					\begin{bmatrix*}[r] 1& 0&\frac47& \frac97 \\ 0&1 &\frac17&\frac47 \\ 0&0&-3&-3 \end{bmatrix*}
					\\
					\stackrel{F_3/(-3)}{\longrightarrow} 
					\begin{bmatrix*}[r] 1& 0&\frac47& \frac97 \\ 0&1 &\frac17&\frac47 \\ 0&0&1&1 \end{bmatrix*}
					\underset{F_2-\frac17F_3}{\stackrel{F_1 -\frac47 F_3}{\longrightarrow}} 
					\begin{bmatrix*}[r] 1& 0&0& \frac{5}7 \\ 0&1 &0&\frac37 \\ 0&0&1&1 \end{bmatrix*}.
					\end{multline*}
					Observemos que llegamos a la misma matriz que en el ejemplo \ref{ejemplo2.11}, pese a que hicimos otras operaciones elementales.
				\end{ejemplo}
				
				\begin{definicion}
					Una matriz $A$ de $m \times n$ es \textit{escalón reducida por fila}\index{matriz escalón reducida por fila} o \textit{MERF}\index{MERF} si
					\begin{enumerate}
						\item[\textit{a})] $A$ es reducida por fila,
						\item[\textit{b})] todas las filas cuyas entradas son todas iguales a cero están al final de la matriz, y
						\item[\textit{c})] en dos filas consecutivas no nulas el 1 principal de la fila inferior está más a la derecha que el 1 principal de la fila superior. 
					\end{enumerate}
				\end{definicion}
				
				\begin{ejemplo}
					Las siguientes son MERF:
					\begin{equation*}
					\begin{bmatrix} 1&0&0&2 \\ 0&1&0&5 \\ 0&0&1&4\end{bmatrix}, \qquad
					\begin{bmatrix} 1&0&0 \\ 0&1&0 \\ 0&0&0\end{bmatrix}, \qquad
					\begin{bmatrix} 0&0 \\ 0&0\end{bmatrix}, \qquad
					\begin{bmatrix} 0&1&2&0&1 \\ 0&0&0&1&0 \\ 0&0&0&0&0 \\ 0&0&0&0&0\end{bmatrix}.
					\end{equation*}
					Las siguientes no son MERF:
					\begin{equation*}
					\begin{bmatrix} 1&0&2&2 \\ 0&1&0&5 \\ 0&0&1&4\end{bmatrix}, \qquad
					\begin{bmatrix} 1&1&0 \\ 0&1&0 \\ 0&0&0\end{bmatrix}, \qquad
					\begin{bmatrix} 0&1&0 \\ 1&0&0 \\ 0&0&0\end{bmatrix}\qquad,
					\begin{bmatrix} 1&0&0 \\ 0&0&0 \\ 0&0&2\end{bmatrix}.
					\end{equation*}
				\end{ejemplo}
				
				
				En general una matriz  MERF tiene la forma
				\begin{equation}\label{merf-general}
				\begin{bmatrix}
				0& \cdots & 1 & *&0&* &* &0& * &*\\
				0& \cdots &0 &\cdots & 1& *  & * & 0& * &*\\
				\vdots&  &\vdots &  &\vdots && &\vdots& &\vdots\\
				0& \cdots &0& \cdots & 0&\cdots &\cdots & 1& * &* \\
				0& \cdots &0& \cdots & 0&\cdots &\cdots & 0& \cdots &0 \\
				\vdots&  &\vdots &  &\vdots && &\vdots& &\vdots\\
				0& \cdots &0& \cdots & 0&\cdots &\cdots & 0& \cdots &0
				\end{bmatrix}
				\end{equation}
				
				\vskip .5cm
				
				\begin{teorema}\label{th-merf}
					Toda matriz $m \times n$ es equivalente por filas a una matriz escalón reducida por fila.
				\end{teorema}
				\begin{proof}
					Ya vimos en el teorema \ref{th-mrf} que toda matriz es equivalente por fila a una matriz reducida por fila. Intercambiando filas podemos entonces obtener una matriz escalón reducida por fila.
				\end{proof}
				
				
				Llevar una matriz a una matriz  escalón reducida por fila equivalente nos provee un método para encontrar todas las soluciones de sistemas de ecuaciones homogéneos.
				
				\begin{ejemplo}
					Supongamos que queremos encontrar las soluciones del sistema $AX = 0$ donde $A$ es una matriz $4 \times 5$ y que $A$ es equivalente por filas a la matriz
					\begin{equation*}
					\begin{bmatrix} 1&0&0&2&-3 \\ 0&1&0&0&-5 \\ 0&0&1&-1&2 \\ 0&0&0&0&0\end{bmatrix}.
					\end{equation*} 
					Entonces el sistema de ecuaciones original, de 4 ecuaciones y 5 incógnitas,  es equivalente al sistema
					\begin{equation*}
					\begin{matrix*}[r]
					x_1 +2x_4 -3x_5 &= 0 \\ x_2 -5x_5 &= 0 \\ x_3-x_4+2x_5 &= 0
					\end{matrix*} 
					\qquad \Rightarrow \qquad 
					\begin{matrix*}[l]
					x_1  &= -2x_4 +3x_5 \\ x_2  &= 5x_5 \\ x_3 &= x_4-2x_5
					\end{matrix*}. 
					\end{equation*}
					Por  lo tanto el conjunto de soluciones del sistema es
					\begin{equation*}
					\{(-2s +3t,  5t,  s-2t, s, t): s,t \in \R\}
					\end{equation*}
				\end{ejemplo}   
			\end{subsection}
			
			
			\begin{subsection}{Método de Gauss} Consideremos el siguiente sistema homogéneo de $m$ ecuaciones lineales con $n$ incógnitas:
				\begin{equation*}
				\begin{matrix}
				a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &0\\
				\vdots&  &\vdots& &&  &\vdots \\
				a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&0
				\end{matrix}
				\end{equation*}
				con $a_{ij} \in \K$. Sea $A= [a_{ij}]$ la matriz asociada a este sistema. Mediante operaciones elementales de fila obtenemos una matriz $B= [b_{ij}]$ escalón reducida por fila (teoremas \ref{th-mrf} y \ref{th-merf}). Por el teorema \ref{eq-fila-impl-sol} los sistemas de ecuaciones $AX=0$ y $BX=0$ tiene las mismas soluciones. 
				
				
				 Veamos ahora formalmente cuales son las soluciones del sistema $BX=0$. Sea $r$ el número de filas no nulas de $B$ y $k_1,\ldots, k_r$ las columnas donde aparecen los primeros 1's en las primeras $r$ filas. Entonces, $k_1 < k_2 < \cdots< k_r$ y el sistema de ecuaciones asociado a $B$ es:
				\begin{equation}
				\begin{matrix}
				&x_{k_1}& + &\sum_{j \not= k_1,\ldots, k_r} b_{1j}\,x_j= &0\\
				&x_{k_2}& + &\sum_{j \not= k_1,\ldots, k_r} b_{2j}\,x_j= &0\\
				& \vdots& &  &\vdots \\
				&x_{k_r}& + &\sum_{j \not= k_1,\ldots, k_r} b_{rj}\,x_j= &0\\
				\end{matrix}
				\end{equation}  
				y, por lo tanto,
				\begin{equation}
				\begin{matrix}\label{sist-eq-hom-merf0}
				x_{k_1} &= -\sum_{j \not= k_1,\ldots, k_r} b_{1j}\,x_j\\
				x_{k_2} &= -\sum_{j \not= k_1,\ldots, k_r} b_{2j}\,x_j\\
				\vdots& \vdots \\
				x_{k_r}  &= -\textstyle\sum_{j \not= k_1,\ldots, k_r} b_{rj}\,x_j
				\end{matrix}.
				\end{equation} 
				Llamaremos a  $x_{k_1}, x_{k_2}, \ldots, x_{k_r}$ las \textit{variables principales}\index{sistema de ecuaciones lineales!variables principales} del sistema  y  las $n-r$ variables restantes son las \textit{variables libres}.\index{sistema de ecuaciones lineales!variables libres} Es claro entonces que variando de forma arbitraria todas las variables libres obtenemos todas las soluciones del sistema.
				
				 Las soluciones del sistema son, entonces,  los $(x_1,\ldots,x_n)\in \K^n$ tal que $x_{k_1},\ldots,x_{k_r}$ satisfacen las ecuaciones (\ref{sist-eq-hom-merf0}) y $x_i \in \K$ para $x_i$ libre. 
				
				
				El método anterior es denominado \textit{método de Gauss.}\index{sistema de ecuaciones lineales!método de Gauss} En particular,  hemos probado
				
				\begin{teorema}\label{inf-sol}
					Sea $A$ matriz $n \times n$ con coeficientes en $\K$. Entonces, si $A$ es equivalente por filas a  $R$ una  MERF y $R$ tiene filas nulas, el sistema $AX=0$ tiene más de una solución. 
				\end{teorema}
				\begin{proof}
					Sea $r$  el número de filas no nulas de $R$, como $R$ tiene $n$ filas y  tiene filas nulas, $r<n$. Por ser $A$ y $R$ equivalentes por fila, $AX$ y $RX$ tienen las mismas soluciones. Como $r <n$, entonces $n-r \ge 1$, por lo tanto existe al menos una variable libre y por lo tanto alguna solución no trivial.  
				\end{proof}
				
				\begin{teorema}\label{soluciones-m-menor-n}
					Sea $A$ una matriz $m \times n$ con  $m < n$. Entonces, el sistema de ecuaciones lineales asociado a esta matriz  tiene soluciones no triviales.
				\end{teorema}
				\begin{proof}
					La matriz $A$ es equivalente por fila a una matriz $R$ escalón reducida por fila 
					y $R$ tiene $r$ filas no nulas con $r \le m <n$. Por lo tanto hay $n-r \ge 1$ variables libres, y en consecuencia $RX$ (y $AX$) tiene otras soluciones además de la trivial $(0,\ldots,0)$.
				\end{proof}
				
				\begin{lema}\label{mtrx-merf-id}
					Sea $R$ una matriz $n \times n$ escalón reducida por fila tal que no tiene filas nulas. Entonces $R=I_n$. 
				\end{lema}
				\begin{proof}
					Como  $R$ es reducida por fila y no tiene filas nulas, cada fila tiene  un 1 en alguna entrada y en la columna donde está el 1 todos las otras entradas son nulas, por  lo tanto hay $n$ 1's principales distribuidos en $n$ columnas. Concluyendo: hay un 1 por columna y en esa columna todas las demás entradas son nulas. 
					
					Ahora bien como $R$ es una MERF, la primera fila contiene el 1 que está más a la izquierda, que no puede estar en otra ubicación que no sea la primera (pues si no la primera columna sería nula). Con el mismo razonamiento vemos que en la segunda fila hay un 1 en la columna 2 y en general en la fila $k$-ésima hay un 1 en la columna $k$. Luego $R=I_n$.
				\end{proof}			
				
				\begin{teorema}
					Sea $A$ una matriz $n \times n$. Entonces, $A$ es equivalente por filas a la matriz $I_n$  si y sólo si el sistema $AX = 0$ tiene un única solución. 
				\end{teorema}
				\begin{proof}
					($\Rightarrow$) Como $A$ es equivalente por filas a la matriz $I_n$, las soluciones de $AX =0$ son las mismas que las de $I_nX=0$. Ahora bien,  en la fila $i$ de la matriz $I_n$ tenemos $[I_n]_{ii} =1$ y las otras entradas son cero, luego la ecuación correspondiente a esa fila es $x _i =0$, y esto ocurre en todas las filas, luego el sistema de ecuaciones es
					\begin{align*}
					x_1 &=0 \\ x_2 &= 0 \\ \vdots \\ x_n &= 0
					\end{align*}
					cuya única solución es la solución trivial.
					
					($\Leftarrow$) Sea $R$ la matriz escalón reducida por filas asociada a $A$. Por hipótesis, $AX=0$ tiene una sola solución y  por lo tanto $RX=0$ tiene una sola solución. Como la solución trivial es la única que existe, no hay variables libres, es decir hay $n$ filas no nulas en $R$, como $R$ tiene $n$ filas, lo anterior implica que $R$ no tiene filas nulas. Entonces, por el lema anterior, $R=I_n$.
				\end{proof}
			\end{subsection}
			
			
			
			\begin{subsection}{Sistemas no homogéneos} Estudiaremos ahora sistemas lineales de ecuaciones donde las ecuaciones no están necesariamente igualadas a 0. En general, un  sistema de este tipo se escribe 
				\begin{equation}\label{sist-eq-gen-2}
				\begin{matrix}
				a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &y_1\\
				\vdots&  &\vdots& &&  &\vdots \\
				a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&y_m
				\end{matrix}
				\end{equation}
				donde $y_1, \ldots,y_m$ y $a_{i,j}$ ($1 \le i \le m$, $1 \le j \le n$) son números en $\K$. Si denotamos
				$$
				X = \begin{bmatrix}
				x_1 \\ \vdots \\ x_n 
				\end{bmatrix},
				\qquad 
				Y = \begin{bmatrix}
				y_1 \\ \vdots \\ y_n
				\end{bmatrix},
				$$
				entonces el sistema, para simplificar, se puede escribir como
				\begin{equation*}
				AX = Y.
				\end{equation*}
				
				A diferencia  de los sistemas homogéneos, los sistemas no homogéneos a veces no tiene solución.
				
				\begin{ejemplo}
					Comprobemos que el sistema
					\begin{align*}
					2x_1 +x_2 &= 0 \\ 2x_1 +x_2 &= 1 
					\end{align*}
					no tiene solución. Supongamos que $(x_1,x_2)$ es solución del sistema. Entonces $0= 2x_1+x_2=1$, absurdo. 
				\end{ejemplo}
				
				
				\begin{definicion}
					Consideremos un sistema como en (\ref{sist-eq-gen-2}) y sea  $A$ la matriz correspondiente al sistema. La \textit{matriz  ampliada}\index{sistema de ecuaciones lineales!matriz  ampliada} del sistema es 
					\begin{equation}\label{mtrx-ampliada}
					A' = \left[\begin{array}{ccc|c}  
					a_{11} & \cdots & a_{1n} &  y_1 \\
					a_{21} & \cdots & a_{2n} &  y_2 \\
					\vdots &  & \vdots  &  \vdots  \\
					a_{m1} & \cdots & a_{mn} &  y_m 
					\end{array}\right]
					\end{equation}
					que también podemos denotar
					\begin{equation*}
					A' = [A | Y].
					\end{equation*}
				\end{definicion}
				
				El método de resolución de los sistemas no homogéneos es similar al de los sistemas homogéneos. Utilizamos el método de Gauss para la matriz $A$ pero realizando las operaciones en toda la fila, inclusive en los elementos a la derecha de la línea: el método de Gauss se basa en intercambiar y operar con ecuaciones, así que para no cambiar las soluciones debemos trabajar con ambos miembros de las ecuaciones. En el caso homogéneo, esto no era necesario porque siempre los segundos miembros eran idénticamente iguales a cero. 
				
				\begin{teorema} Sea $[A | Y]$ la matriz ampliada de un sistema no homogéneo y sea $[B | Z]$ una matriz que se obtiene a partir de $[A | Y]$ por medio de una operación elemental. Entonces, los sistemas correspondientes a $[A | Y]$ y  $[B | Z]$ tiene las mismas soluciones. 
				\end{teorema}
				\begin{proof}
					Como $[B | Z]$ se obtiene por una operación elemental por fila a partir de $[A | Y]$,  entonces las ecuaciones de $[B | Z]$ son combinaciones lineales de las ecuaciones de $[A | Y]$. Como toda operación elemental por fila tiene inversa, podemos obtener $[A | Y]$ a partir de $[B | Z]$ y por lo tanto las ecuaciones de $[A | Y]$ son combinaciones lineales de las ecuaciones de $[B | Z]$. Es decir $[A | Y]$ y $[B | Z]$ determinan sistemas de ecuaciones lineales equivalentes y por lo tanto tiene las mismas soluciones (teorema \ref{sistemas-equiv}).
				\end{proof}
				
				\begin{corolario}
					Sean  $[A | Y]$  y  $[B | Z]$  matrices ampliadas equivalentes por fila.  Entonces, los sistemas correspondientes tiene las mismas soluciones.  
				\end{corolario}
				
				Estos resultados nos permiten usar el  método de Gauss para encontrar las soluciones de un sistema no homogéneo: hacemos operaciones elementales por fila en la matriz ampliada $[A | Y]$ hasta obtener $[B | Z]$,  donde $B = [b_{ij}]$ es la MERF equivalente por filas a $A$. Traducido a ecuaciones, obtenemos 
				\begin{equation}
				\begin{matrix}
				&x_{k_1}& + &\sum_{j \not= k_1,\ldots, k_r} b_{1j}\,x_j&=& &z_1\\
				&x_{k_2}& + &\sum_{j \not= k_1,\ldots, k_r} b_{2j}\,x_j&=& &z_2\\
				& \vdots& &  &\vdots \\
				&x_{k_r}& + &\sum_{j \not= k_1,\ldots, k_r} b_{rj}\,x_j&=& &z_r\\
				&&&0 &=& &z_{r+1}  \\
				& & & \vdots &&&\vdots \\
				&&&0 &=& &z_{m} 
				\end{matrix}.
				\end{equation}  
				En  el caso que para algún $i>r$ se cumpla que $z_r\ne 0$, entonces el sistema no tiene ninguna solución. Si $z_{r+1}= \cdots =z_m = 0$, las soluciones son
				\begin{equation}
				\begin{matrix}\label{sist-eq-nohom-merf0}
				x_{k_1} &= -\sum_{j \not= k_1,\ldots, k_r} b_{1j}\,x_j + z_1\\
				x_{k_2} &= -\sum_{j \not= k_1,\ldots, k_r} b_{2j}\,x_j + z_2\\
				\vdots& \vdots \\
				x_{k_r}  &= -\textstyle\sum_{j \not= k_1,\ldots, k_r} b_{rj}\,x_j + z_r
				\end{matrix}.
				\end{equation} 
				Como en el caso homogéneo, $x_{k_1},\ldots x_{k_r}$ serán llamadas las variables principales  y $\{x_j\}$ con  $j \not= k_1,\ldots, k_r$ son las variables libres.
				
				\begin{ejemplo} Resolvamos el sistema
					\begin{align*}
					x_1 -2x_2 + x_3  &= 1\\
					2x_1 +x_2 + x_3  &= 2\\
					5x_2 - x_3  &= 0.
					\end{align*}
					La matriz  aumentada correspondiente a este sistema es
					\begin{equation*}
					 \left[\begin{array}{rrr|r} 
					 1 & -2 & 1 &  1 \\ 2 & 1 & 1 &  2 \\ 0 & 5 & -1 &  0 
					\end{array}\right]
					\end{equation*}
					apliquemos el método de Gauss:
					\begin{multline*}
					\left[\begin{array}{rrr|r}1 & -2 & 1 &  1 \\ 2 & 1 & 1 &  2 \\	0 & 5 & -1 &  0  \end{array}\right]
					\stackrel{F_2 - 2F_1}{\longrightarrow} 
					\left[\begin{array}{rrr|r}1 & -2 & 1 &  1 \\ 0 & 5 & -1 &  0 \\	0 & 5 & -1 &  0  \end{array}\right]
					\stackrel{F_3-F_1}{\longrightarrow} 
					\left[\begin{array}{rrr|r}1 & -2 & 1 &  1 \\ 0 & 5 & -1 &  0 \\	0 & 0 & 0 & 0  \end{array}\right]
					\\
					\stackrel{F_2/5}{\longrightarrow} 
					\left[\begin{array}{rrr|r}1 & -2 & 1 &  1 \\ 0 & 1 & -1/5 &  0 \\	0 & 0 & 0 &  0  \end{array}\right]
					\stackrel{F_1+2 F_2}{\longrightarrow} 
					\left[\begin{array}{rrr|r}1 & 0 & 3/5 & 1 \\ 0 & 1 & -1/5 &  0 \\	0 & 0 & 0 &  0  \end{array}\right].
					\end{multline*}	
					Luego,  el sistema se reduce  a
					\begin{align*}
					x_1  + 3/5x_3  &= 1\\
					x_2 -1/5 x_3  &= 0.
					\end{align*}
					Es decir, 
					\begin{align*}
					x_1    &= - 3/5x_3 + 1\\
					x_2   &= 1/5 x_3.
					\end{align*}
					En consecuencia,  las soluciones de esta ecuación son
					\begin{equation*}
					\left\{(-\frac{3}5s + 1, \frac15 s, s ): s \in \K \right\}.
					\end{equation*}
				\end{ejemplo} 
				
			\end{subsection}
			
			
			
		\end{section}
		
		
		
		
		
		
		
		
		\begin{section}{Álgebra de matrices}
			
			Ahora estudiaremos propiedades algebraicas de las matrices,  en particular veremos que dado $n\in \mathbb N$, entonces podemos definir una suma y un producto en el conjunto de matrices $n \times n$ con la propiedad de que estas operaciones satisfacen muchos de los axiomas que definen a  $\mathbb Z$.  
			
			
			\begin{subsection}{Algunos tipos de matrices}
				
				\
				\vskip .3cm
				
				\textbf{Matriz cuadrada.} Es aquella que tiene igual número de filas que de columnas, es decir si es una matriz $n \times n$ para algún $n \in\mathbb N$. En ese caso, se dice que la matriz es de orden $n$. Por ejemplo, la matriz
				\begin{equation*}
				A= \begin{bmatrix}
				1&3&0\\-1&4&7\\-2&0&1
				\end{bmatrix}
				\end{equation*}
				es cuadrada de orden 3.
				
				Denotaremos el conjunto de todas las matrices cuadradas de orden $n$ con entradas en $\K$ por $M_n(\K)$ o simplemente $M_n$ si $\K$  está sobreentendido. Así, en el 	ejemplo anterior $A \in M_3$.
				
				Los elementos de la \textit{diagonal principal} de una matriz\index{diagonal principal de una matriz} cuadrada son aquellos que están situados
				en la diagonal que va desde la esquina superior izquierda hasta la inferior derecha. En otras
				palabras, la diagonal principal de una matriz $A=[a_{ij}]$ está formada por los elementos
				$a_{11},a_{22},\ldots,a_{nn}$.  En el ejemplo anterior la diagonal principal está compuesta por los elementos: $a_{11} = 1$, $a_{22} = 4$ , $a_{33} = 1$.
				
				\vskip .3cm
				
				\textbf{Matriz diagonal y matriz escalar.} Una matriz cuadrada, $A=[a_{ij}]$ de orden $n$, es \textit{diagonal}\index{matriz diagonal} si $a_{ij}  =0$ , para $i \not= j$ . Es decir, si todos los elementos situados fuera de la diagonal principal son cero. Por ejemplo, la siguiente matriz es diagonal:
				\begin{equation}
				\begin{bmatrix}
				2&0&0&0\\0&-1&0&0\\0&0&5&0\\0&0&0&3
				\end{bmatrix}.
				\end{equation}
				
				Un matriz $n \times n$  es \textit{escalar}\index{matriz escalar} si  es diagonal y todos los elementos de la diagonal son iguales, por ejemplo, en el caso $4 \times 4$ las matrices escalares son 
				
				\begin{equation}
				\begin{bmatrix}
				c&0&0&0\\0&c&0&0\\0&0&c&0\\0&0&0&c
				\end{bmatrix},
				\end{equation}
				con $c \in \K$.
				
				\vskip .3cm
				
				\textbf{Matriz unidad o identidad.} Esta matriz ya la hemos definido anteriormente. Recordemos que es una matriz diagonal cuya diagonal principal está compuesta de 1's. 
				
				Más adelante veremos que la matriz identidad, respecto a la multiplicación de matrices, juega un papel similar al número 1 respecto a la multiplicación de números reales o enteros (elemento neutro del producto).
				
				\vskip .3cm
				
				\textbf{Matriz nula}. La \textit{matriz nula}\index{matriz nula} de orden $m\times n$, denotada $0_{m \times n}$ o simplemente $0$ si $m$ y $n$ están sobreentendidos,  es la  matriz $m \times n$ cuyas entradas son todas nulas ($=0$). 
				
				Por ejemplo, la matriz nula $2 \times 3$ es
				\begin{equation*}
				\begin{bmatrix} 0&0&0\\ 0&0&0\end{bmatrix}.
				\end{equation*}
				Veremos luego que la matriz nula juega un papel similar al número 0 en el álgebra de matrices (elemento neutro de la suma).  
				
				\textbf{Matriz triangular.} Una matriz cuadrada es \textit{triangular superior} o \textit{escalón}\index{matriz triangular superior}\index{matriz escalón} si todos los elementos situados por debajo de la diagonal principal son cero. Por ejemplo, la siguiente matriz es triangular superior:
				\begin{equation}
				\begin{bmatrix}
				2&-1&3&1\\0&-1&0&2\\0&0&5&1\\0&0&0&3
				\end{bmatrix}.
				\end{equation}
				Análogamente, una matriz cuadrada es \textit{triangular inferior}\index{matriz triangular inferior} si todos los elementos situados por encima de la diagonal principal son cero. Un matriz triangular (superior o inferior) se dice \textit{estricta} si la diagonal principal es 0.
				
				En forma más precisa, sea $A =[a_{ij}]\in M_n(\K)$,  entonces
				\begin{itemize}
					\item $A$ es \textit{triangular superior} (\textit{triangular superior estricta}) si $a_{ij} =0$ para $i < j$ (resp. $i \le j$),
					\item $A$ es \textit{triangular inferior} (\textit{triangular inferior estricta}) si $a_{ij} =0$ para $i > j$ (resp. $i \ge j$).
				\end{itemize}
				
				Por  ejemplo, cualquier matriz diagonal es triangular superior y también triangular inferior. 
				
				No es difícil comprobar que si $R$ es una matriz cuadrada $n \times n$  que es una MERF,  entonces $R$  es triangular superior. 
				
			 
			\end{subsection}
			
			
			\begin{subsection}{Suma  de matrices}
				Sean $A=[a_{ij}]$, $B=[b_{ij}]$ matrices  $m \times n$. La matriz $C= [a_{ij} + b_{ij}]$ de orden $m \times n$,  es decir la matriz cuyo valor en la posición $ij$ es  $a_{ij} + b_{ij}$, es llamada \textit{la suma de las matrices $A$ y $B$} y se denota $A+B$. En otras palabras, la suma de dos matrices es la matriz que resulta de sumar ``coordenada a coordenada'' ambas matrices. 
				
				Veamos un ejemplo, consideremos las siguientes matrices:
				\begin{equation*}
				A = \begin{bmatrix} -1&3&5 \\ 2&0&-1  \end{bmatrix}, \qquad
				B = \begin{bmatrix} 5&2&-3 \\ 0&-1&1  \end{bmatrix}, \qquad
				M = \begin{bmatrix} 2&1&-3 \\ 3&0&-1 \\ 0&8&5 \end{bmatrix}.
				\end{equation*}
				Las matrices $A$ y $B$ son de orden $2 \times 3$, mientras la matriz $M$ es cuadrada de orden 3. Por tanto, no podemos calcular la suma de $A$ y $M$ y tampoco la suma de $B$ y $M$, en cambio, sí podemos sumar $A$ y $B$ ya que tienen el mismo orden. Esto es,
				\begin{equation*}
				A + B = \begin{bmatrix} -1&3&5 \\ 2&0&-1  \end{bmatrix}+
				\begin{bmatrix} 5&2&-3 \\ 0&-1&1  \end{bmatrix} =
				\begin{bmatrix} -1+5&3+2&5-3 \\ 2+0&0-1&-1+1  \end{bmatrix}=
				\begin{bmatrix} 4&5&2 \\ 2&-1&0  \end{bmatrix}
				\end{equation*}
				
				
				Dadas $A$, $B$ y $C$ matrices $m \times n$, podemos deducir fácilmente las siguientes propiedades de la suma de matrices de matrices:
				\begin{itemize}
					\item Conmutativa: $A + B = B + A$,
					\item Asociativa: $A + (B + C) = (A + B) + C$,
					\item Elemento neutro (la matriz nula): $A + 0 = 0 + A = A$,
					\item Elemento opuesto: existe una matriz $-A$ de orden $m \times n$ tal que  $A + (-A) = (-A) + A = 0$.
				\end{itemize}
				Debemos explicitar la matriz opuesta: si $A = [a_{ij}]$,  entonces $-A = [-a_{ij}]$. Usualmente denotaremos $A + (-B)$ como $A-B$ y $(-A)+B$ como $-A+B$. 
				
				La demostración de las propiedades anteriores se deduce de que las mismas propiedades valen coordenada a coordenada y se dejan a cargo del lector. 
			\end{subsection}		
			
			\begin{subsection}{Multiplicación de matrices} Sean $A=[a_{ij}]$ matriz $m \times n$ y 		 $B=[b_{ij}]$ matriz $n \times p$, entonces $C=[c_{ij}]$ matriz $m \times p$  es el \textit{producto} de $A$ y $B$, si 
				\begin{equation}\label{mtrx-mult}
				c_{ij} = a_{i1}b_{1j}+a_{i2}b_{2j}+\cdots+a_{in}b_{nj}= \sum_{k=1}^{n}a_{ik}b_{kj}.
				\end{equation}
				Es decir, los elementos que ocupan la posición $ij$ en la matriz producto, se obtienen sumando 	los productos que resultan de multiplicar los elementos de la fila $i$ en la primera matriz por los
				elementos de la columna $j$ de la segunda matriz. Al producto de $A$ por $B$ lo denotamos $AB$. Observar que una matriz $m \times n$ por una $n \times p$ resulta en una $m \times p$.
				
				\begin{obs}\label{mtrx-filasxcols}
					 Sean $A=[a_{ij}]$ matriz $m \times n$ y $B=[b_{ij}]$ matriz $n \times p$, entonces si 
					 multiplicamos la matriz que se forma con la fila $i$ de $A$ por la matriz que determina la columna $j$ de $B$, obtenemos el coeficiente $ij$ de $AB$.
					Esquemáticamente
					\begin{equation*}
					\begin{bmatrix} a_{i1}& a_{i2}& \cdots &a_{in}\end{bmatrix}
					\begin{bmatrix} b_{1j}\\ b_{2j}\\ \vdots \\b_{nj}\end{bmatrix} =  \sum_{k=1}^{n}a_{ik}b_{kj} = c_{ij}.
					\end{equation*}
					Por lo tanto diremos a veces, que el coeficiente $ij$ de la matriz $AB$ es la fila $i$ de $A$ por la columna $j$ de $B$. 
				\end{obs}	
				
				Veamos un ejemplo: si 
				\begin{equation*}
				A = \begin{bmatrix}1&0\\-3&1\end{bmatrix}, \qquad B = \begin{bmatrix}5&-1&2\\15&4&8\end{bmatrix},
				\end{equation*}
				como $A$ es $2 \times 2$ y $B$ es $2 \times 3$, la matriz $AB$ será $2 \times 3$ y  aplicando la regla (\ref{mtrx-mult}), obtenemos:
				\begin{equation*}
				AB = \begin{bmatrix}1\times  5 + 0\times 15&1\times (-1) + 0\times 4&1\times 2 + 0\times 8
				\\-3\times 5 + 1\times 15&-3\times (-1) + 1\times 4&-3\times 2 + 1\times 8
				\end{bmatrix} =
				\begin{bmatrix} 5 &-1 &2 
				\\ 0 &7 &2
				\end{bmatrix}.
				\end{equation*}
				Observemos que, debido a nuestra definición, no es posible multiplicar $B$ por $A$, pues no está definido multiplicar una matriz $2 \times 3$ por una $2 \times 2$. Por la definición, se puede multiplicar una matriz $m \times n$ por una matriz $r \times s$, sólo si $n=r$ y en ese caso, la multiplicación resulta ser una matriz $m \times s$. 	
				
				Hay casos, como veremos en el siguiente ejemplo, en los que se pueden calcular ambos productos aunque se obtienen resultados diferentes. Consideremos las siguientes matrices:
				\begin{equation*}
				A = \begin{bmatrix}2&1\\-3&1\end{bmatrix}, \qquad 
				B = \begin{bmatrix}1&3\\-1&1\end{bmatrix}
				\end{equation*}
				Entonces, por un lado,
				\begin{equation*}
				AB = \begin{bmatrix}2&1\\-3&1\end{bmatrix}
				\begin{bmatrix}1&3\\-1&1\end{bmatrix} =
				\begin{bmatrix}1&7\\-4&-8\end{bmatrix}, 
				\end{equation*}
				y por otro lado,
				\begin{equation*}
				BA = \begin{bmatrix}1&3\\-1&1\end{bmatrix} \begin{bmatrix}2&1\\-3&1\end{bmatrix}
				=\begin{bmatrix}-7&4\\-5&0\end{bmatrix}.
				\end{equation*}
				
				Según se pudo comprobar a través del ejemplo anterior, la multiplicación de matrices
				no cumple la propiedad conmutativa. Veamos algunas propiedades que sí cumple esta operación:
				\begin{itemize}
					\item Asociativa: 
					\begin{equation*}
					A (B C) = (A B) C, \quad\forall\, A \in M_{m \times n}, \;B \in M_{n \times p}, \;C  \in M_{p \times q},
					\end{equation*}
					
					\item Elemento neutro: si  $A$ es matriz $m \times n$,  entonces
					\begin{equation*}
					AI_{n} = A = I_{m}A,
					\end{equation*}
					\item Distributiva:
					\begin{equation*}
					A(B + C) = AB + AC,\qquad \forall\, A \in M_{m \times n}, \;B, C \in M_{n \times p},
					\end{equation*}
					y
					\begin{equation*}
					(A+ B)C = AC + BC,\qquad \forall\, A, B \in M_{m \times n}, \; C \in M_{n \times p}.
					\end{equation*}
				\end{itemize}	
				
				Como en el caso de la suma,  la demostración las propiedades anteriores se deja a cargo del lector. 
				
				En virtud de estas propiedades y de las anteriores de la suma de matrices, resulta que el conjunto $(M_n ,+,.)$  de las matrices cuadradas de orden $n$, respecto a las dos leyes de composición interna, ``+'' y ``·'', tiene estructura de anillo unitario no conmutativo (ver \href{ https://es.wikipedia.org/wiki/Anillo\_(matemática)}{ https://es.wikipedia.org/wiki/Anillo\_(matemática)}) . 
				
				Cuando las matrices son cuadradas podemos multiplicarlas por si mismas y  definimos,  de forma análoga a lo que ocurre en los productos de números, la potencia de una matriz: sea $A$ matriz $n \times n$, y sea $m \in \mathbb N$ entonces
				\begin{equation*}
				A^0 = I_{n}, \quad A^m = A^{m-1}A,		
				\end{equation*}
				es decir $A^m$ es multiplicar $A$ consigo mismo $m$-veces.  
				
				\begin{observacion}
					Un  caso especial de multiplicación es la multiplicación por matrices diagonales. Sea 
				\begin{equation*}
					D = \begin{bmatrix}
					d_1 & 0 & \cdots &0 \\
					0 & d_2 & \cdots &0 \\
					\vdots & \vdots & \ddots &\vdots \\
					0 & 0 & \cdots & d_n 
					\end{bmatrix}
				\end{equation*}
				matriz $n \times n$ diagonal con valor $d_i$  en la posición $ii$,  entonces si $A$ es matriz $n \times p$, tenemos que $DA$  es la matriz que en la fila $i$ tiene a la fila $i$ de $A$ multiplicada por $d_i$.  Es decir,
				\begin{equation*}
				\begin{bmatrix}
				d_1 & 0 & \cdots &0 \\
				0 & d_2 & \cdots &0 \\
				\vdots & \vdots & \ddots &\vdots \\
				0 & 0 & \cdots & d_n 
				\end{bmatrix}
				\begin{bmatrix}
				a_{11} & a_{12} & \cdots &a_{1p} \\
				a_{21} & a_{22} & \cdots &a_{2p}\\
				\vdots & \vdots & \ddots &\vdots \\
				a_{n1} & a_{n2} & \cdots & a_{np} 
				\end{bmatrix}
				=
				\begin{bmatrix}
				d_1a_{11} & d_1a_{12} & \cdots &d_1a_{1p} \\
				d_2a_{21} & d_2a_{22} & \cdots &d_2a_{2p}\\
				\vdots & \vdots & \ddots &\vdots \\
				d_n a_{n1} & d_n a_{n2} & \cdots & d_n a_{np} 
				\end{bmatrix}.
				\end{equation*}
				Esto es claro, pues el coeficiente $ij$  de $DA$ es la fila $i$ de $D$ por la columna $j$ de $A$, es decir
				$$
				[DA]_{ij} = 0.a_{1j}+\cdots + 0.a_{i-1,j}+d_i.a_{ij}+0.a_{i+1,j}+\cdots + 0.a_{nj} = d_ia_{ij}. 
				$$
				Observar que en el caso de que $D$ sea una matriz escalar (es decir $d_1=d_2=\cdots=d_n$), $DA$ es multiplicar por el mismo número todos los coeficientes de $A$. En particular, en este caso, si $A$  es $n \times n$,  $DA = AD$.
				 
				
				
				 Si $B$ es $m \times n$, ¿qué se obtiene haciendo $BD$? 
				 
				
				\end{observacion}
				
				\vskip .3cm
				
				Otras observaciones importantes: 
				
				\begin{itemize}
					\item multiplicar cualquier matriz por la matriz nula resulta la matriz nula,
					\item existen divisores de cero: en general, $AB = 0$ no implica que $A = 0$ o $B = 0$  o,  lo que es lo mismo, el producto de matrices no nulas puede resultar en una matriz nula. Por
					ejemplo,
					\begin{equation*}
					\begin{bmatrix}1&0\\2&0\end{bmatrix} \begin{bmatrix}0&0\\8&1\end{bmatrix} = \begin{bmatrix}0&0\\0&0\end{bmatrix}.
					\end{equation*}
					\item En general no se cumple la propiedad cancelativa: si $A\not=0$ y  $AB = AC$ no necesariamente se cumple que $B = C$. Por
					ejemplo,
					\begin{equation*}
					\begin{bmatrix}2&0\\4&0\end{bmatrix}=
					\begin{bmatrix}1&0\\2&0\end{bmatrix} \begin{bmatrix}2&0\\8&1\end{bmatrix} =
					\begin{bmatrix}1&0\\2&0\end{bmatrix} \begin{bmatrix}2&0\\5&3\end{bmatrix}
					\end{equation*}
					\item No se cumple la fórmula del binomio: 	sean $A, B$ matrices $n \times n$, entonces		
					\begin{align*}
					(A+B)^2 &= (A+B)(A+B) \\&= A(A+B) + B(A+B) \\&= AA + AB + BA + BB \\&= A^2 + AB + BA + B^2,
					\end{align*}
					y  esta última expresión puede no ser  igual a $A^2 + 2AB + B^2$ ya que el producto de matrices no es conmutativo (en general). 
				\end{itemize}
			\end{subsection}
		
		
		\begin{subsection}{Multiplicación de una matriz por un escalar} Otra operación importante es la multiplicación de una matriz por un elemento de $\K$: sea $A=[a_{ij}]$ matriz $m \times n$ y $c \in \K$,  entonces el producto de $c$ por $A$ es la matriz
			$$
			cA=[ca_{ij}].
			$$ 
		Por ejemplo, 
		$$
		2\begin{bmatrix*}[r]
		-1& 0& 3 & 4\\
		5& 1& -2 & 1\\
		3& 2& 1 & -3
		\end{bmatrix*} =
		\begin{bmatrix*}[r]
		-2& 0& 6& 8\\
		10& 2& -4 & 2\\
		6& 4& 2 & -6
		\end{bmatrix*}.
		$$
		
		Observar que multiplicar por $c$ una matriz $m \times n$,  es lo mismo que multiplicar por la matriz escalar $m \times m$ con los coeficientes de la diagonal iguales a $c$,  es decir
		\begin{equation}
			cA = \begin{bmatrix*}[r]
			c     &     0& \cdots & 0\\
			0     &     c& \cdots & 0\\
			\vdots&\vdots&  \ddots      &\vdots\\
			0     &    0  & \cdots & c
			\end{bmatrix*}
			 \begin{bmatrix}
			a_{11}&a_{12}& \cdots & & a_{1n}\\
			a_{21}&a_{22}& \cdots & &a_{2n}\\
			\vdots&\vdots&  & &\vdots\\
			a_{m1}&a_{m2}& \cdots & & a_{mn}
			\end{bmatrix} =
			 \begin{bmatrix}
			ca_{11}&ca_{12}& \cdots & & ca_{1n}\\
			ca_{21}&ca_{22}& \cdots & &ca_{2n}\\
			\vdots&\vdots&  & &\vdots\\
			ca_{m1}&ca_{m2}& \cdots & & ca_{mn}
			\end{bmatrix}
		\end{equation}
		
		Debido a esta observación y a las propiedades del producto de matrices, se cumple lo siguiente:
			\begin{align*}
			 c(A B) = (cA) B,\qquad &\forall\, c \in \K,  A \in M_{m \times n}, \;B \in M_{n \times p}, \\
			1.A = A ,\qquad&\forall\, c \in \K, A \in M_{m \times n}\\
			c(A + B) = cA + cB,\qquad& \forall\, c \in \K, A,B \in M_{m \times n}, \\
			(c+ d)A = cA + dA,\qquad& \forall\, c,d \in \K, \; A \in M_{m \times n}.
			\end{align*}
			
			Si $A$ es $n \times n$,  entonces $DA = AD$ cuando $D$  es una matriz escalar. Por lo tanto
			\begin{align*}
			c(A B) = (cA) B = A(cB),\qquad &\forall\, c \in \K,  A \in M_{n \times n}, \;B \in M_{n \times n}.
			\end{align*} 

		\end{subsection}
			
			
		\end{section}
		
		\begin{section}{Matrices elementales}
			Veremos ahora la relación entre el álgebra de matrices y la solución de sistemas de ecuaciones lineales. 
			
			Primero recordemos que dado un sistema de $m$  ecuaciones lineales con $n$ incógnitas
			\begin{equation}\label{sist-eq-gen-3}
			\begin{matrix}
			a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &y_1\\
			\vdots&  &\vdots& &&  &\vdots \\
			a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&y_m
			\end{matrix}
			\end{equation}
			donde $y_1, \ldots,y_m$ y $a_{i,j}$ ($1 \le i \le m$, $1 \le j \le n$) son números en $\K$. Si denotamos
			\begin{equation*}
			A = \begin{bmatrix}
			a_{11}& a_{12}& \cdots &a_{1n} \\
			\vdots&\vdots  &  &\vdots \\
			a_{m1} &a_{m2}&\cdots &a_{mn}\end{bmatrix},\qquad
			X = \begin{bmatrix}
			x_1 \\ \vdots \\ x_n 
			\end{bmatrix},
			\qquad 
			Y = \begin{bmatrix}
			y_1 \\ \vdots \\ y_n
			\end{bmatrix},
			\end{equation*}
			entonces 
			\begin{equation*}
			AX = \begin{bmatrix}
			a_{11}& a_{12}& \cdots &a_{1n} \\
			\vdots&\vdots  &  &\vdots \\
			a_{m1} &a_{m2}&\cdots &a_{mn}\end{bmatrix} \begin{bmatrix} 
			x_1 \\ \vdots \\ x_n 
			\end{bmatrix}
			=
			\begin{bmatrix}
			a_{11}x_1+ a_{12}x_2+ \cdots +a_{1n}x_n \\
			\vdots \\
			a_{m1}x_1 +a_{m2}x_2+\cdots +a_{mn}x_n\end{bmatrix} =
			\begin{bmatrix}
			y_1 \\ \vdots \\ y_n
			\end{bmatrix} = Y
			\end{equation*}
			(producto de matrices). Es decir, la notación antes utilizada es consistente con el, ahora definido, producto de matrices.  
			
			\begin{definicion} Una matriz $m \times m$  se dice  \textit{elemental}\index{matriz elemental} si fue obtenida por medio de una única operación elemental a partir de la matriz identidad $I_m$.
			\end{definicion}
			
			\begin{ejemplo} Veamos cuales son las matrices elementales  $2 \times 2$:
				\begin{enumerate}
					\item Si $c \not=0$, multiplicar por  $c$ la primera fila y multiplicar $c$ por la segunda fila son, respectivamente,
					\begin{equation*}
					\begin{bmatrix} c& 0\\ 0&1\end{bmatrix}\;\text{ y }\; \begin{bmatrix} 1& 0\\ 0&c\end{bmatrix},
					\end{equation*}
					\item si  $c \in \K$, sumar a la fila 2 la fila 1 multiplicada por $c$ o sumar a la fila 1 la fila 2 multiplicada por $c$ son, respectivamente,
					\begin{equation*}
					\begin{bmatrix} 1& 0\\ c&1\end{bmatrix}\;\text{ y }\; \begin{bmatrix} 1& c\\ 0&1\end{bmatrix}.
					\end{equation*}
					\item Finalmente, intercambiando la fila 1 por la fila 2 obtenemos la matriz
					\begin{equation*}
					\begin{bmatrix} 0& 1\\ 1&0\end{bmatrix}.
					\end{equation*}
				\end{enumerate}
			\end{ejemplo}
			
			\vskip .5cm
			
			En el caso de matrices $m \times m$ tampoco es difícil encontrar las matrices elementales:
			\begin{enumerate}
				\item Si $c \not=0$, multiplicar por  $c$ la fila $k$ de la matriz identidad, resulta en la matriz elemental que tiene todos 1's en la diagonal, excepto en la posición $k,k$ donde vale $c$,  es decir si $e(I_m) = [a_{ij}]$,  entonces
				\begin{equation}\label{elem-tipo-1}
				a_{ij} = \left\{ 
				\begin{matrix*}[l]
				1 &\text{si $i=j$ e $i\ne k$,}\\
				c &\text{si $i=j=k$,} \\
				0 \quad&\text{si $i \ne j$.}
				\end{matrix*}\right.
				\end{equation}
				Gráficamente,
				\begin{align*}
				&\begin{matrix}
				{}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{k}{\downarrow}&{}^{}&{}^{}&{}^{}
				\end{matrix} \\
				\begin{matrix}
				{}^{}\\
				{}^{}\\
				\overset{k}{\to}\\
				{}^{}\\
				{}^{}
				\end{matrix}
				&\begin{bmatrix}
				1 & 0 &  &\cdots & 0  \\
				\vdots  & \ddots  & & & \vdots \\
				0 & \cdots &c &\cdots &0 \\
				\vdots  &   & &\ddots & \vdots \\
				0  & \cdots  & &\cdots & 1
				\end{bmatrix}
				\end{align*}
				\item si  $c \in \K$, sumar a la fila $r$  la fila $s$ multiplicada por $c$, resulta en la matriz elemental que tiene todos 1's en la diagonal, y todos los demás coeficientes son 0,  excepto en la fila  $r$ y columna $s$ donde  vale $c$,  es decir si $e(I_m) = [a_{ij}]$,  entonces
				\begin{equation}\label{elem-tipo-2}
				a_{ij} = \left\{ 
				\begin{matrix*}[l]
				1 &\text{si $i=j$}\\
				c &\text{si $i=r$, $j=s$,} \\
				0 \quad&\text{otro caso.}
				\end{matrix*}\right.
				\end{equation}
				Gráficamente, 
				\begin{align*}
				&\begin{matrix}
				{}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{r}{\downarrow}&{}^{}&{}^{}&{}^{}{}^{}\overset{s}{\downarrow}&{}^{}
				\end{matrix} \\
				\begin{matrix}
				{}^{}\\{}^{}\\
				\overset{r}{\to}\\
				{}^{}\\
				{}^{}\\
				{}^{}
				\end{matrix}
				&\begin{bmatrix}
				1 & 0 &  &\cdots &&& 0  \\
				\vdots  & \ddots  & & &&& \vdots \\
				0 & \cdots &1 &\cdots&c&\cdots &0 \\
				\vdots  &   & &\ddots &&& \vdots \\
				\vdots  &   & & &\ddots&& \vdots \\
				0  & \cdots  & &\cdots &&& 1
				\end{bmatrix}
				\end{align*}
				
				
				\item Finalmente, intercambiar la fila $r$ por la fila $s$ resulta ser
				\begin{equation}\label{elem-tipo-3}
				a_{ij} = \left\{ 
				\begin{matrix*}[l]
				1 &\text{si ($i=j$, $i \ne r$, $i \ne s$) o ($i=r$, $j=s$) o ($i=s$, $j=r$) }\\
				0 \quad&\text{otro caso.}
				\end{matrix*}\right.
				\end{equation}
					Gráficamente, 
				\begin{align*}
				&\begin{matrix}
				{}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{r}{\downarrow}&{}^{}&{}^{}&{}^{}{}^{}\overset{s}{\downarrow}&{}^{}
				\end{matrix} \\
				\begin{matrix}
				{}^{}\\{}^{}\\
				\overset{r}{\to}\\
				{}^{}\\
				\overset{s}{\to}\\{}^{}\\
				{}^{}
				\end{matrix}
				&\begin{bmatrix}
				1 & \cdots &  &\cdots &&\cdots& 0  \\
				\vdots  & \ddots  & & &&& \vdots \\
				0 & \cdots &0 &\cdots&1&\cdots &0 \\
				\vdots  &   & &\ddots &&& \vdots \\
				0  & \cdots  &1 &\cdots &0& \cdots& 0 \\
				\vdots  &   & & &&\ddots& \vdots \\
				0  & \cdots  & &\cdots &&\cdots& 1
				\end{bmatrix}
				\end{align*}
			\end{enumerate}
			
			Veamos ahora que, dada una matriz $A$,   hacer una operación elemental en $A$ es igual a multiplicar $A$ a izquierda por una matriz elemental. Más precisamente:
			
			\begin{teorema}\label{th-mrtx-elem}
				Sea $e$ una operación elemental por fila y sea $E$ la matriz elemental $E=e(I)$. Entonces $e(A) = EA$.
			\end{teorema}
			\begin{proof}
				Hagamos la prueba para matrices $2 \times 2$. La prueba en general es similar, pero requiere de un complicado manejo de índices.
				\begin{enumerate}
					\item Si $c \not=0$, multiplicar $I_2$ por  $c$ la primera fila resulta en la matriz elemental
					\begin{equation*}
					\begin{bmatrix} c& 0\\ 0&1\end{bmatrix}.
					\end{equation*}
					Luego
					\begin{equation*}
					\begin{bmatrix} c& 0\\ 0&1\end{bmatrix}
					\begin{bmatrix} a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix} = 
					\begin{bmatrix} 
					c\,.\,a_{11} + 0 \,.\,a_{21}&c\,.\,a_{12}+0\,.\,a_{22}\\
					0\,.\,a_{11} + 1 \,.\,a_{21}&0\,.\,a_{12}+1\,.\,a_{22}\end{bmatrix}
					=
					\begin{bmatrix} 
					c\,.\,a_{11}&c\,.\,a_{12}\\
					a_{21}&a_{22}\end{bmatrix} = e(A).
					\end{equation*}
					De forma análoga se demuestra en el caso que la operación elemental sea  multiplicar la segunda fila por $c$.
					\item Si  $c \in \K$, cambiar en $I_2$ la fila 2 por la fila 1 multiplicada por $c$ resulta en la matriz elemental 
					\begin{equation*}
					\begin{bmatrix} 1& 0\\ c&1\end{bmatrix}.
					\end{equation*}
					Luego 
					\begin{equation*}
					\begin{bmatrix} 1& 0\\ c&1\end{bmatrix}	\begin{bmatrix} a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix} = 
					\begin{bmatrix} 
					a_{11} &a_{12}\\
					c\,.\,a_{11} + a_{21}&c\,.\,a_{12}+a_{22}\end{bmatrix} = e(A).
					\end{equation*}
					La demostración es análoga si la operación elemental es cambiar la fila 1 por la fila 2 multiplicada por $c$.
					\item Finalmente, si intercambiamos en $I_2$ la fila 1 por la fila 2 obtenemos la matriz elemental
					\begin{equation*}
					\begin{bmatrix} 0& 1\\ 1&0\end{bmatrix}.
					\end{equation*}
					Luego
					\begin{equation*}
					\begin{bmatrix} 0& 1\\ 1&0\end{bmatrix}	\begin{bmatrix} a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix} = 
					\begin{bmatrix}
					a_{21} &a_{22}\\
					a_{11} &a_{12}\end{bmatrix} = e(A).
					\end{equation*}
				\end{enumerate}
				
				\begin{comment}
				\begin{enumerate}
				\item[(1)] Analicemos primero la operación elemental $e$ que es multiplicar la fila $r$ por $c\not=0$. En  ese caso  
				\begin{equation*}
				e(I) = \begin{bmatrix} 1&0& \cdots& &0 \\ &\ddots& & & \\ 0& & c & &0 \\ &&  &\ddots& \\ 0&& \cdots& &1		 		
				\end{bmatrix}
				\begin{matrix} && \\ && \\ \leftarrow&\text{fila $r$,}& \\ && \\ &&	 		
				\end{matrix}
				\end{equation*}
				es decir, la matriz diagonal con 1's en la diagonal excepto en la fila $r$ y columna $r$ donde vale $c$, en símbolos
				\begin{equation*}
				e(I)_{ij} = \left\{ \begin{matrix*}[l]
				1& \quad &\text{si $i=j$, $i\not=r$} \\
				c& \quad &\text{si $i=j$, $i=r$} \\
				0 & \quad &\text{si $i\not=j$}
				\end{matrix*}\right..
				\end{equation*}
				
				
				\item[(2)] Si la operación elemental es multiplicar la fila $s$ por  $c$ y sumarla a la fila $r$ obtenemos
				\begin{equation*}
				e(I) = \begin{bmatrix} 
				1 & 0 &\cdots &&    0 \\
				& \ddots & &&  \\
				0 &\cdots& 1 &\cdots\; c \;\cdots& 0	\\
				&  & & \ddots &  \\
				0		&  & \cdots& & 1	
				\end{bmatrix}
				\begin{matrix} && \\ && \\ \leftarrow&\text{fila $r$.}& \\ && \\ &&	 		
				\end{matrix} 
				\end{equation*}
				Es decir en la fila $r$ hay un 1 en la posición $r$ y una $c$  en la posición $s$. En símbolos, 
				\begin{equation*}
				e(I)_{ij} = \left\{ \begin{matrix*}[l]
				1& \quad &\text{si $i=j$} \\
				c& \quad &\text{si $i=r$, $j=s$} \\
				0 & \quad &\text{otro caso}
				\end{matrix*}\right..
				\end{equation*}
				
				
				\item[(3)] Si la operación elemental es permutar la fila $r$ por la fila $s$ obtenemos 
				\begin{equation*}
				e(I) = \begin{bmatrix} 1&0& \cdots& &0 \\ &\ddots& & & \\ 0& & c & &0 \\ &&  &\ddots& \\ 0&& \cdots& &1		 		
				\end{bmatrix}.
				\end{equation*}
				
				\begin{equation*}
				e(I)_{ij} = \left\{ \begin{matrix*}[l]
				1& \quad &\text{si $i=j$, $i,j \not= r,s$ } \\
				1& \quad &\text{si $i=r$, $j=s$ o $i=s$, $j=r$} \\
				0 & \quad &\text{otro caso}
				\end{matrix*}\right..
				\end{equation*}
				\end{enumerate}
				\end{comment} 
			\end{proof}
			
			
			
			
			\begin{corolario}\label{coro-mrtx-elem}
				Sean $A$ y $B$ matrices $m \times n$. Entonces $B$ equivalente por filas a $A$ si y sólo si $B=PA$ donde $P$ es  producto de matrices elementales. Más aún, si $B = e_k(e_{k-1}(\cdots(e_1(A))\cdots))$ con $e_1,e_2,\ldots,e_k$ operaciones elementales de fila y $E_i=e_i(I)$ para $i=1,\ldots,k$,  entonces $B =  E_kE_{k-1}\cdots E_1A$.
			\end{corolario}
			\begin{proof} 
				\
				
				($\Rightarrow$) Si $B$ equivalente por filas a $A$  existen operaciones elementales $e_1,\ldots,e_k$ tal que $B = e_k(e_{k-1}(\cdots(e_1(A))\cdots))$, más formalmente
				\begin{equation*}
				\text{si } A_1 = e_1(A)\text{ y } A_i = e_i(A_{i-1})\text{ para }i=2,\ldots,k\text{, entonces } e_k(A_{k-1})= B.
				\end{equation*}
				Sea $E_i = e_i(I_m)$, entonces, por el teorema anterior $A_1= E_1A$ y $A_i = E_iA_{i-1}$ ($i=2,\ldots,k$). Por  lo tanto $B=E_kA_{k-1}$, en otras palabras $B =  E_kE_{k-1}\cdots E_1A$, luego $P = E_kE_{k-1}\cdots E_1$.
				
				($\Leftarrow$) Si $B= PA$, con  $P = E_kE_{k-1}\cdots E_1$ donde $E_i=e_i(I_m)$ es una matriz elemental,  entonces
				$$
				B = PA = E_kE_{k-1}\cdots E_1A \stackrel{\text{Teor. \ref{th-mrtx-elem}}}{=\joinrel=}  e_k(e_{k-1}(\cdots(e_1(A))\cdots)). 
				$$
				Por lo tanto, $B$ es equivalente por filas a $A$.
			\end{proof} 
			
		\end{section}
		
		\begin{section}{Matrices inversibles}
			\begin{definicion} Sea $A$ una  matriz $n \times n$ con coeficientes en $\K$. Una matriz $B \in M_{n\times n}(\K)$  es \textit{inversa  de $A$}\index{matriz inversa} si $BA=AB=I_n$. En  ese caso,  diremos que  $A$ es \textit{inversible}.\index{matriz inversible}
			\end{definicion}
			
			\begin{ejemplo}
				La matriz $\begin{bmatrix} 2&-1\\0&1\end{bmatrix}$ tiene inversa 
				$\begin{bmatrix} \frac12&\frac12\\0&1\end{bmatrix}$ pues es fácil comprobar que 
				\begin{equation*}
				\begin{bmatrix} 2&-1\\0&1\end{bmatrix}
				\begin{bmatrix} \frac12&\frac12\\0&1\end{bmatrix} =
				\begin{bmatrix} 1&0\\0&1\end{bmatrix}\quad\text{ y } \quad
				\begin{bmatrix} \frac12&\frac12\\0&1\end{bmatrix} 
				\begin{bmatrix} 2&-1\\0&1\end{bmatrix}=
				\begin{bmatrix} 1&0\\0&1\end{bmatrix}.
				\end{equation*}
			\end{ejemplo}
			
			\vskip .5cm
			
			\begin{proposicion}
				Sea $A  \in M_{n\times n}(\K)$, 
				\begin{enumerate}
					\item	sean $B, C \in M_{n \times n}(\K)$ tales que $BA =I_n$ y $AC = I_n$, entonces $B=C$;
					\item  si $A$ inversible la inversa es única.
				\end{enumerate}
			\end{proposicion}
			\begin{proof} (1)
				\begin{equation*}
				B = BI_n = B(AC) = (BA)C = I_nC = C.
				\end{equation*}
				
				(2) Sean $B$ y $C$ inversas de $A$, es decir $BA=AB=I_n$ y  $CA=AC=I_n$. En particular, $BA=I_n$ y $AC= I_n$, luego, por (1), $B=C$.  
			\end{proof}
			
			
			
			\begin{definicion}
				Sea $A\in M_{n\times n}(\K)$ inversible. A la única matriz inversa de $A$ la llamamos \textit{la matriz inversa de $A$} y la denotamos $A^{-1}$.
			\end{definicion}
			
			Veremos más adelante que si una matriz $n \times n$ admite una inversa a izquierda,  es decir si existe $B$ tal que $BA=I_n$, entonces la matriz es inversible. Lo mismo vale si $A$  admite inversa a derecha.
			
			\begin{ejemplo}
				Sea $A$ la matriz 
				\begin{equation*}
				\begin{bmatrix} 2&1&-2\\ 1&1&-2\\ -1&0&1
				\end{bmatrix}.
				\end{equation*}
				Entonces,  $A$ es inversible y su inversa es
				\begin{equation*}
				A^{-1} = \begin{bmatrix} 1&-1&0\\ 1&0&2\\ 1&-1&1
				\end{bmatrix}.
				\end{equation*}
				Esto se resuelve comprobando que $AA^{-1}=I_3$ (por lo dicho más arriba es innecesario comprobar que $A^{-1}A=I_3$).
			\end{ejemplo} 
			
			\begin{observacion}
				No toda matriz tiene inversa, por ejemplo  la  matriz nula (cuyos coeficientes son todos iguales a $0$) no tiene inversa pues $0\,.\, A= 0 \not= I$.  También existen matrices no nulas no inversibles,  por ejemplo la matriz 
				\begin{equation*}
				A = \begin{bmatrix} 2&1\\ 0&0\end{bmatrix}
				\end{equation*}
				no tiene inversa.
				Si  multiplicamos a $A$ por una cualquier matriz  $B =[b_{ij}]$ obtenemos
				\begin{equation*}
				AB = \begin{bmatrix} 2&1\\ 0&0\end{bmatrix}
				\begin{bmatrix} b_{11}&b_{12}\\ b_{21}&b_{22}\end{bmatrix} =
				\begin{bmatrix} 2b_{11}+b_{21}&2b_{12}+b_{22}\\0 &0\end{bmatrix}.
				\end{equation*}
				Luego $AB$, al tener una fila identicamente nula, no puede ser nunca la identidad. 
			\end{observacion}
			
			
			
			\begin{teorema}\label{th-prod-inv-impl-inv}
				Sean $A$ y $B$ matrices $n \times n$ con coeficientes en $\K$. Entonces
				\begin{enumerate}
					\item \label{inv-itm1} si $A$ inversible,  entonces $A^{-1}$  es inversible y su inversa es $A$,  es decir $(A^{-1})^{-1}=A$;
					\item \label{inv-itm2} si $A$ y $B$ son inversibles, entonces $AB$ es inversible y $(AB)^{-1} = B^{-1}A^{-1}$.
				\end{enumerate}
				\begin{proof}
					(\ref{inv-itm1}) La inversa a izquierda de $A^{-1}$ es $A$, pues $AA^{-1}=I_n$. Análogamente, la inversa a derecha de $A^{-1}$ es $A$, pues $A^{-1}A = I_n$. Concluyendo: $A$  es la inversa de $A^{-1}$.
					
					(\ref{inv-itm2}) Simplemente debemos comprobar que $B^{-1}A^{-1}$ es inversa a izquierda y derecha de $AB$:
					\begin{equation*}
					(B^{-1}A^{-1})AB = B^{-1}(A^{-1}A)B = B^{-1}I_nB =B^{-1}B = I_n,
					\end{equation*}
					y,  análogamente,
					\begin{equation*}
					AB(B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = AI_nA^{-1} =AA^{-1} = I_n.
					\end{equation*} 
				\end{proof}
			\end{teorema}
			
			\begin{observacion}
				Si $A_1,\ldots,A_k$  son inversibles,  entonces $A_1\ldots A_k$ es inversible y su inversa es  $$(A_1\ldots A_k)^{-1} = A_k^{-1}\ldots A_1^{-1} .$$
				El resultado es una generalización del punto (2) del teorema anterior y su demostración se hace por inducción en $k$ (usando (2)  del teorema anterior). Se deja como ejercicio al lector. 
			\end{observacion}
			
			\begin{observacion}
				La suma de matrices inversibles no necesariamente es inversible, por ejemplo $A+ (-A)= 0$ que no es inversible. 
			\end{observacion}	
			
			
			\begin{teorema}\label{th-elmental-impl-inversible}
				Una matriz elemental es inversible.
			\end{teorema}
			\begin{proof}
				Sea $E$ la matriz elemental que se obtiene a partir de $I_n$ por la operación elemental $e$. Se $e_1$ la operación elemental inversa (teorema \ref{op-elem}) y $E_1 = e_1(I_n)$. Entonces 
				\begin{align*}
				EE_1 &= e(e_1(I_n)) = I_n \\
				E_1E &= e_1(e(I_n)) = I_n.
				\end{align*}
				Luego  $E_1 = E^{-1}$. 
			\end{proof}	
			
			
			\begin{ejemplo}
				Es fácil encontrar explícitamente la matriz inversa de una matríz elemental, por ejemplo, en el caso $2 \times 2$ tenemos:
				\begin{enumerate}
					\item Si $c \not=0$,
					\begin{equation*}
					\begin{bmatrix} c& 0\\ 0&1\end{bmatrix}^{-1}=\begin{bmatrix} 1/c& 0\\ 0&1\end{bmatrix}
					\;\text{ y }\; \begin{bmatrix} 1& 0\\ 0&c\end{bmatrix}^{-1}=\begin{bmatrix} 1& 0\\ 0&1/c\end{bmatrix},
					\end{equation*}
					\item si  $c \in \K$, ,
					\begin{equation*}
					\begin{bmatrix} 1& 0\\ c&1\end{bmatrix}^{-1}=\begin{bmatrix} 1& 0\\ -c&1\end{bmatrix}
					\;\text{ y }\; \begin{bmatrix} 1& c\\ 0&1\end{bmatrix}^{-1}=\begin{bmatrix} 1& -c\\ 0&1\end{bmatrix}.
					\end{equation*}
					\item Finalmente, 
					\begin{equation*}
					\begin{bmatrix} 0& 1\\ 1&0\end{bmatrix} ^{-1}= 	\begin{bmatrix} 0& 1\\ 1&0\end{bmatrix}.
					\end{equation*}
				\end{enumerate}
			En  el caso general tenemos:
			
			(1) 
			\begin{align*}
			&\begin{matrix}
			{}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{k}{\downarrow}&{}^{}&{}^{}&{}^{}
			\end{matrix} \\
			\text{La inversa de \quad}\begin{matrix}
			{}^{}\\
			{}^{}\\
			\overset{k}{\to}\\
			{}^{}\\
			{}^{}
			\end{matrix}
			&\begin{bmatrix}
			1 & 0 &  &\cdots & 0  \\
			\vdots  & \ddots  & & & \vdots \\
			0 & \cdots &c &\cdots &0 \\
			\vdots  &   & &\ddots & \vdots \\
			0  & \cdots  & &\cdots & 1
			\end{bmatrix}
			\text{\quad es \quad}
			\begin{bmatrix}
			1 & 0 &  &\cdots & 0  \\
			\vdots  & \ddots  & & & \vdots \\
			0 & \cdots &1/c &\cdots &0 \\
			\vdots  &   & &\ddots & \vdots \\
			0  & \cdots  & &\cdots & 1
			\end{bmatrix}
			\end{align*}
			
			(2)
			\begin{align*}
			&\begin{matrix}
			{}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{r}{\downarrow}&{}^{}&{}^{}&{}^{}{}^{}\overset{s}{\downarrow}&{}^{}
			\end{matrix} \\
			\text{La inversa de \quad}\begin{matrix}
			{}^{}\\{}^{}\\
			\overset{r}{\to}\\
			{}^{}\\
			{}^{}\\
			{}^{}
			\end{matrix}
			&\begin{bmatrix}
			1 & 0 &  &\cdots &&& 0  \\
			\vdots  & \ddots  & & &&& \vdots \\
			0 & \cdots &1 &\cdots&c&\cdots &0 \\
			\vdots  &   & &\ddots &&& \vdots \\
			\vdots  &   & & &\ddots&& \vdots \\
			0  & \cdots  & &\cdots &&& 1
			\end{bmatrix}
			\text{\quad es \quad}
			\begin{bmatrix}
			1 & 0 &  &\cdots &&& 0  \\
			\vdots  & \ddots  & & &&& \vdots \\
			0 & \cdots &1 &\cdots&-c&\cdots &0 \\
			\vdots  &   & &\ddots &&& \vdots \\
			\vdots  &   & & &\ddots&& \vdots \\
			0  & \cdots  & &\cdots &&& 1
			\end{bmatrix}
			\end{align*}
			
			(3)
			
			\begin{align*}
			&\begin{matrix}
			{}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{r}{\downarrow}&{}^{}&{}^{}&{}^{}{}^{}\overset{s}{\downarrow}&{}^{}
			\end{matrix} \\
			\text{La inversa de \quad}\begin{matrix}
			{}^{}\\{}^{}\\
			\overset{r}{\to}\\
			{}^{}\\
			\overset{s}{\to}\\{}^{}\\
			{}^{}
			\end{matrix}
			&\begin{bmatrix}
			1 & \cdots &  &\cdots &&\cdots& 0  \\
			\vdots  & \ddots  & & &&& \vdots \\
			0 & \cdots &0 &\cdots&1&\cdots &0 \\
			\vdots  &   & &\ddots &&& \vdots \\
			0  & \cdots  &1 &\cdots &0& \cdots& 0 \\
			\vdots  &   & & &&\ddots& \vdots \\
			0  & \cdots  & &\cdots &&\cdots& 1
			\end{bmatrix}
			\text{\quad es la misma matriz.\quad}
			\end{align*}
		
						
		
	
\end{ejemplo}
		
		
			
			
			\begin{teorema}\label{mtrx-inv-equiv} Sea $A$ matriz $n \times n$ con coeficientes en $\K$. Las siguientes afirmaciones son equivalentes
				\begin{enumerate}
					\item[\textit{i})] $A$ es inversible,
					\item[\textit{ii})] $A$  es equivalente por filas a $I_n$, 
					\item[\textit{iii})] $A$ es producto de matrices elementales.
				\end{enumerate}
			\end{teorema}
			\begin{proof}
				
				\
				
				\textit{i}) $\Rightarrow$ \textit{ii}) Sea $R$ la matriz escalón reducida por fila equivalente por filas a $A$. Entonces,  existen $E_1,\ldots,E_k$ matrices elementales tal que $E_1,\ldots,E_kA = R$. Como las matrices elementales son inversibles, el producto de matrices elementales es inversible, luego  $E_1,\ldots,E_k$ es inversible y por lo tanto $R=E_1,\ldots,E_kA$ es inversible. 
				
				Recordemos que las matrices escalón reducidas por fila si tienen filas nulas, ellas se encuentran al final.  Ahora bien,  si la última fila de $R$ es nula entonces,  $RB$ tiene la última fila nula también y por lo tanto no puede ser igual a la identidad, es decir, en ese caso $R$ no es inversible, lo cual produce un absurdo. Concluyendo: la última fila (la fila $n$) de $R$ no es nula y como es MERF, $R$ no tiene filas nulas. Por lo tanto $R=I_n$ (lema \ref{mtrx-merf-id}) y,  entonces, $A$ es equivalente por filas a $I_n$. 
				
				\textit{ii}) $\Rightarrow$ \textit{iii}) Como $A$  es equivalente por filas a $I_n$, al ser la equivalencia por filas una relación de equivalencia,  tenemos que $I_n$ es equivalente por filas a $A$, es decir  existen $E_1,\ldots,E_k$ matrices elementales, tales que $E_1E_2,\ldots,E_kI_n = A$. Por lo tanto, $A =E_1E_2,\ldots,E_k$ producto de matrices elementales.
				
				\textit{iii}) $\Rightarrow$ \textit{i}) Sea $A = E_1E_2,\ldots,E_k$ donde $E_i$  es una matriz elemental ($i=1,\ldots,k$). Como cada $E_i$ es inversible,  el producto de ellos es inversible,  por lo tanto $A$ es inversible.
			\end{proof}	
			
			\begin{corolario}
				Sean $A$ y $B$ matrices $m \times n$. Entonces,   $B$ es equivalente por filas a $A$ si y sólo si existe matriz inversible $P$ de orden $m \times m$ tal que $B =PA$ . 
			\end{corolario}
			\begin{proof}
				
				\
				
				($\Rightarrow$) $B$ es equivalente por filas a $A$,  luego existe $P$ matriz producto de matrices elementales tal que $B =PA$. Como cada matriz elemental es inversible (teorema \ref{th-elmental-impl-inversible}) y el producto de matrices inversibles es inversible (teorema  \ref{th-prod-inv-impl-inv}(\ref{inv-itm2})), se deduce que $P$ es inversible. 
				
				($\Leftarrow$) Sea  $P$  matriz inversible tal que $B =PA$. Como $P$ es inversible, por el teorema anterior, $P$ es producto de matrices elementales, luego $B =PA$ es equivalente por filas a $A$.
			\end{proof}	
			
			\begin{corolario}\label{mtrx-inv-gauss}
				Sea $A$ matriz $n \times n$. Sean $e_1,\ldots,e_k$ las operaciones elementales por filas que reducen a $A$  a una MERF y esta MERF es la identidad,  es decir $e_1(e_{2}(\cdots(e_k(A))\cdots)) =I_n$. Entonces, $A$ inversible y  las mismas operaciones elementales aplicadas a $I_n$ nos llevan a $A^{-1}$,  es decir $e_1(e_{2}(\cdots(e_k(I_n))\cdots)) =A^{-1}$.
			\end{corolario}
			\begin{proof} Por el teorema anterior, al ser $A$ equivalente por filas a la identidad, $A$ es inversible.  
				Sean las matrices elementales  $E_i = e_i(I_n)$ para $i=1,\ldots,k$,  entonces (ver corolario \ref{coro-mrtx-elem}) $E_1E_2\ldots E_kA = I_n$, por lo tanto, multiplicando por $A^{-1}$ a derecha en ambos miembros,    
				\begin{align*}
				E_1E_2\ldots E_kA A^{-1}&= I_nA^{-1} \quad \Leftrightarrow \\
				E_1E_2\ldots E_kI_n&= A^{-1} \quad \Leftrightarrow \\
				e_1(e_{2}(\cdots(e_k(I_n))\cdots)) &=A^{-1}.
				\end{align*}
			\end{proof}
			
			Este último corolario nos provee un método sencillo para calcular la inversa de una matriz $A$ (inversible). Primero,  encontramos $R = I_n$ la MERF  equivalente por filas a $A$, luego, aplicando la mismas operaciones elementales a $I_n$, obtenemos la inversa de $A$. Para facilitar el cálculo es  conveniente comenzar con $A$ e $I_n$ e ir aplicando paralelamente las operaciones elementales por fila. Veamos un ejemplo.

			\begin{ejemplo}
				Calculemos la inversa (si tiene) de 
				\begin{equation*}
				A=\begin{bmatrix}2&-1\\1&3 \end{bmatrix}.
				\end{equation*}
			\end{ejemplo}
			\begin{proof}[Solución] Por lo que ya hemos demostrado 1) si $A$ tiene inversa es reducible por filas a la identidad, 2) las operaciones que llevan a $A$ a la identidad, llevan también la identidad  a $A^{-1}$. Luego  trataremos de reducir por filas a $A$ y todas las operaciones elementales las haremos en paralelo partiendo de la matriz identidad:
				%$$
				%\left[\begin{array}{cccc|c}
				%1 & 0 & 3 & -1 & 0 \\
				%0 & 1 & 1 & -1 & 0 \\
				%0 & 0 & 0 & 0 & 0 \\
				%\end{array}\right]
				%$$
				\begin{align*}
				[A|I] &= \left[\begin{array}{cc|cc}2&-1 &  1&0\\1&3& 0&1\end{array}\right] 
				\stackrel{F_1\leftrightarrow F_2}{\longrightarrow} 
				\left[\begin{array}{cc|cc}1&3& 0&1\\2&-1 &  1&0 \end{array}\right]
				\stackrel{F_2-2 F_1}{\longrightarrow}
				\left[\begin{array}{cc|cc}1&3& 0&1\\0&-7 &  1&-2 \end{array}\right]
				\stackrel{F_2/(-7)}{\longrightarrow}\\
				&\quad\longrightarrow 
				\left[\begin{array}{cc|cc}1&3& 0&1\\0&1 &  -\frac17&\frac27\end{array}\right]
				\stackrel{F_1-3 F_2}{\longrightarrow}
				\left[\begin{array}{cc|cc}1&0&  \frac37&\frac17\\0&1 &  -\frac17&\frac27 \end{array}\right].
				\end{align*}
				Luego, como $A$ se reduce por filas a la identidad, $A$ es inversible y su inversa es  
				\begin{equation*}
				A^{-1}=\begin{bmatrix}\frac37&\frac17\\-\frac17&\frac27 \end{bmatrix}.
				\end{equation*}
				El lector desconfiado  podrá comprobar, haciendo el producto de matrices, que $AA^{-1} = A^{-1}A=I_2$.
			\end{proof}
			
			
			\begin{teorema}\label{mtrx-inv-equiv2} 
				Sea $A$ matriz $n \times n$ con coeficientes en $\K$. Entonces,  las siguientes afirmaciones son equivalentes. 
				\begin{enumerate}
					\item[\textit{i})] $A$ es inversible.
					\item[\textit{ii})] El sistema $AX=Y$ tiene una única solución para toda matriz $Y$ de orden $n \times 1$. 
					\item[\textit{iii})] El sistema homogéneo $AX=0$ tiene una única solución trivial.
				\end{enumerate}
			\end{teorema}
			\begin{proof}
				
				\
				
				\textit{i}) $\Rightarrow$  \textit{ii}) Sea $X_0$ solución del sistema $AX=Y$, luego
				\begin{equation*}
				AX_0=Y  \quad \Rightarrow \quad  A^{-1}AX_0 = A^{-1}Y  \quad \Rightarrow \quad  X_0 = A^{-1}Y.
				\end{equation*}
				Es decir, $X_0$ es único (siempre igual  a $A^{-1}Y$).  
				
				
				\textit{ii}) $\Rightarrow$  \textit{i}) Es trivial, tomando $Y =0$.
				
				
				
				\textit{iii}) $\Rightarrow$  \textit{i}) Sea $R$ la matriz escalón reducida por filas equivalente a $A$, es decir $R=PA$ con $P$ inversible y $R$ es MERF. Si $R$ tiene una fila nula, entonces por teorema \ref{inf-sol},  el sistema $AX =0$ tiene más de una solución, lo cual es absurdo.  Por lo tanto, $R$ no tiene filas nulas. Como es una matriz cuadrada y es MERF, tenemos que $R=I_n$. Luego $A$ es equivalente por filas a $I_n$ y por teorema \ref{mtrx-inv-equiv} se deduce que $A$ es inversible. 			
				
			\end{proof}
			
			\begin{corolario}
				Sea $A$ una matriz $n \times n$ con coeficientes  en $\K$. Si $A$ tiene inversa a izquierda,  es decir si existe $B$ matriz $n \times n$ tal que $BA=I_n$,   entonces $A$ es inversible.  Lo mismo vale si $A$ tiene inversa a derecha. 
			\end{corolario}	
			\begin{proof}
				Supongamos que  $A$ tiene inversa a izquierda y  que $B$ sea la inversa a izquierda,  es decir $BA=I_n$. El sistema $AX=0$ tiene una única solución, pues $AX_0=0 \Rightarrow BAX_0=B0 \Rightarrow X_0=0$. Luego, $A$  es inversible (y su inversa es $B$). 
				
				Supongamos que  $A$ tiene inversa a derecha y  que $C$ sea la inversa a derecha, es decir $AC=I$. Por lo anterior, $C$ es inversible y  su inversa es $A$, es decir $AC=I$ y $CA=I$, luego $A$  es inversible. 
			\end{proof}	
			
			
			
			Terminaremos la  sección calculando algunas matrices inversas usando el corolario  \ref{mtrx-inv-gauss}. 
			
			\begin{ejemplo}
				Calcular la inversa (si tiene) de la matriz $A=\begin{bmatrix}
				1&-1&2\\ 3&2&4\\ 0&1&-2
				\end{bmatrix}$. 
			\end{ejemplo}
			\begin{proof}[Solución]
				%\left[\begin{array}{cc|cc}2&-1 &  1&0\\1&3& 0&1\end{array}\right] 
				\begin{align*} 
				&\left[\begin{array}{rrr|rrr}	1&-1&2&1&0&0\\ 3&2&4&0&1&0\\ 0&1&-2&0&0&1 \end{array}\right]
				\stackrel{F_2-3 F_1}{\longrightarrow}
				\left[\begin{array}{rrr|rrr}	1&-1&2
				&1&0&0\\ 0&5&-2&-3&1&0\\ 0&1&-2&0&0&1 \end{array}\right]
				\stackrel{F_2\leftrightarrow F_3}{\longrightarrow} \\
				&\longrightarrow 
				\left[\begin{array}{rrr|rrr}	1&-1&2&1&0&0\\ 0&1&-2&0&0&1 \\ 0&5&-2&-3&1&0 \end{array}\right]
				\stackrel{F_1 + F_2}{\longrightarrow}
				\left[\begin{array}{rrr|rrr}	1&0&0&1&0&1\\ 0&1&-2&0&0&1 \\ 0&5&-2&-3&1&0 \end{array}\right]
				\stackrel{F_3-5F_2}{\longrightarrow} \\
				&\longrightarrow
				\left[\begin{array}{rrr|rrr}	1&0&0&1&0&1\\ 0&1&-2&0&0&1 \\ 0&0&8&-3&1&-5 \end{array}\right]
				\stackrel{F_3/8}{\longrightarrow}
				\left[\begin{array}{rrr|rrr}	1&0&0&1&0&1\\ 0&1&-2&0&0&1 \\ 0&0&1&-\frac38&\frac18&-\frac58 \end{array}\right]
				\stackrel{F_2+2F_3}{\longrightarrow} \\
				&\longrightarrow
				\left[\begin{array}{rrr|rrr}	1&0&0&1&0&1\\ 0&1&0&-\frac34&\frac14&-\frac14 \\ 0&0&1&-\frac38&\frac18&-\frac58 \end{array}\right].
				\end{align*}
				
				Por lo tanto 
				\begin{equation*}
				A^{-1} = 	\begin{bmatrix*}[r]	1&0&1\\ -\frac34&\frac14&-\frac14 \\ -\frac38&\frac18&-\frac58 \end{bmatrix*}
				\end{equation*}
			\end{proof}
			
			\begin{ejemplo}\label{inv-2x2-0}
				Dados $a,b,c,d \in \mathbb R$, determinar cuando la matriz $A = \begin{bmatrix*} a&b\\c&d\end{bmatrix*}$  es inversible y en ese caso,  cual es su inversa. 
			\end{ejemplo}
			\begin{proof}[Solución] Para poder aplicar el método de Gauss, debemos ir haciendo casos. 
				
				1) Supongamos que  $a\not=0$, entonces 
				\begin{equation*}
				\begin{bmatrix}a&b\\c&d\end{bmatrix} \stackrel{F_1/a}{\longrightarrow}
				\begin{bmatrix}1&\dfrac{b}{a}\\[6pt]c&d\end{bmatrix} \stackrel{F_2 -cF_1}{\longrightarrow}
				\begin{bmatrix}1&\dfrac{b}{a}\\[6pt]0&d- c\dfrac{b}{a}\end{bmatrix} =
				\begin{bmatrix}1&\dfrac{b}{a}\\[6pt]0&\dfrac{ad-bc}{a}\end{bmatrix}
				\end{equation*}   
				Si $ad-bc=0$,  entonces la matriz se encuentra reducida por filas y la última fila es $0$, luego en ese caso no es inversible.  Si $ad-bc\not=0$, entonces
				\begin{equation*}
				\begin{bmatrix}1&\dfrac{b}{a}\\[8pt]0&\dfrac{ad-bc}{a}\end{bmatrix} \stackrel{a/(ad-bc)\,F_2}{\longrightarrow}
				\begin{bmatrix}1&\dfrac{b}{a}\\[6pt]0&1\end{bmatrix}
				\stackrel{F1-b/a\,F_2}{\longrightarrow}
				\begin{bmatrix}1&0\\0&1\end{bmatrix}.
				\end{equation*} 
				Luego, en el caso $a\not=0$, $ad-bc\not=0$ hemos reducido por filas la matriz $A$  a la identidad y por lo tanto $A$  es inversible. Además, podemos encontrar $A^{-1}$ aplicando a $I$ las mismas operaciones elementales que reducían $A$ a la identidad:
				\begin{align*}
				\begin{bmatrix}1&0\\0&1\end{bmatrix} 
				&\stackrel{F_1/a}{\longrightarrow}
				\begin{bmatrix}\dfrac1a&0\\[8pt]0&1\end{bmatrix} 
				\stackrel{F_2 -cF_1}{\longrightarrow}
				\begin{bmatrix*}[r]\dfrac1a&0\\[8pt]-\dfrac{c}{a}&1\end{bmatrix*}
				\stackrel{a/(ad-bc)\,F_2}{\longrightarrow}
				\begin{bmatrix*}[c]\dfrac1a&0\\[8pt]-\dfrac{c}{ad-bc}&\dfrac{a}{ad-bc}\end{bmatrix*}
				\stackrel{F1-b/a\,F_2}{\longrightarrow}
				\\&\longrightarrow			
				\begin{bmatrix*}[c]\dfrac1a+\dfrac{bc}{a(ad-bc)}&-\dfrac{b}{ad-bc}\\[8pt]-\dfrac{c}{ad-bc}&\dfrac{a}{ad-bc}\end{bmatrix*} 
				=
				\begin{bmatrix*}[c]\dfrac{d}{ad-bc}&-\dfrac{b}{ad-bc}\\[8pt]-\dfrac{c}{ad-bc}&\dfrac{a}{ad-bc}\end{bmatrix*} .
				\end{align*}  
				Concluyendo, en el caso $a\not=0$, $ad-bc\not=0$, $A$  es inversible y 
				\begin{equation}\label{inv-2x2}
				A^{-1} = \dfrac{1}{ad-bc}
				\begin{bmatrix*}[c]d&-b\\-c&a\end{bmatrix*}.
				\end{equation}
				
				2) Estudiemos el caso $a=0$. Primero observemos que si $c=0$ o $b=0$ , entonces la matriz no es inversible, pues en ambos casos nos quedan matrices que no pueden ser reducidas por fila a la identidad. Luego la matriz puede ser inversible si $bc\not=0$ y en este caso la reducción por filas es:
				\begin{equation*}
				\begin{bmatrix}0&b\\c&d\end{bmatrix} \stackrel{F_1\leftrightarrow F_2}{\longrightarrow}
				\begin{bmatrix}c&d\\0&b\end{bmatrix} \stackrel{F_1/c}{\longrightarrow}
				\begin{bmatrix}1&\dfrac{d}{c}\\[6pt]0&b\end{bmatrix} \stackrel{F_2/b}{\longrightarrow}
				\begin{bmatrix}1&\dfrac{d}{c}\\[6pt]0&1\end{bmatrix}
				\stackrel{F_1 - d/c F_2}{\longrightarrow}
				\begin{bmatrix}1&0\\0&1\end{bmatrix}.
				\end{equation*}   
				Luego $A$  es inversible y aplicando estas mismas operaciones elementales a la identidad  obtenemos la inversa:
				\begin{equation*}
				\begin{bmatrix}1&0\\0&1\end{bmatrix} \stackrel{F_1\leftrightarrow F_2}{\longrightarrow}
				\begin{bmatrix}0&1\\1&0\end{bmatrix} \stackrel{F_1/c}{\longrightarrow}
				\begin{bmatrix}0&\dfrac{1}{c}\\[6pt]1&0\end{bmatrix} \stackrel{F_2/b}{\longrightarrow}
				\begin{bmatrix}0&\dfrac{1}{c}\\[6pt]\dfrac{1}{b}&0\end{bmatrix}
				\stackrel{F_1 - d/c F_2}{\longrightarrow}
				\begin{bmatrix}-\dfrac{d}{bc}&\dfrac{1}{c}\\[6pt]\dfrac{1}{b}&0\end{bmatrix}.
				\end{equation*}  
				Luego, en el caso  que $a=0$, entonces $A$ inversible si  $b c\not=0$ y su inversa es
				\begin{equation*}
				A^{-1} = \begin{bmatrix}-\dfrac{d}{bc}&\dfrac{1}{c}\\[6pt]\dfrac{1}{b}&0\end{bmatrix} = 
				\dfrac{1}{-bc}
				\begin{bmatrix*}[c]d&-b\\-c&0\end{bmatrix*}.
				\end{equation*}
				Es decir, la expresión de la inversa es igual a (\ref{inv-2x2}) (considerando que  $a=0$).
				
				Reuniendo los dos casos:  $A$ es inversible si $a\not=0$ y  $ad-bc\not=0$ o si $a=0$ y $bc\not=0$, pero esto es lógicamente equivalente a pedir solamente  $ad-bc\not=0$, es decir
				\begin{equation*}
				(a\not=0 \wedge ad-bc\not=0) \vee (a=0 \wedge bc\not=0)\; \Leftrightarrow\; ad-bc\not=0
				\end{equation*}
				(ejercicio).
				
				Resumiendo, $\begin{bmatrix*} a&b\\c&d\end{bmatrix*}$ es inversible  $\Leftrightarrow ad-bc\not=0$  y en ese caso,  su inversa viene dada por 
				\begin{equation}
				\begin{bmatrix*} a&b\\c&d\end{bmatrix*}^{-1} =  \dfrac{1}{ad-bc}
				\begin{bmatrix*}[c]d&-b\\-c&a\end{bmatrix*}
				\end{equation} 
				
				\vskip 6pt
				
				Veremos en la próxima sección que el uso de determinantes permitirá establecer la generalización de este resultado para matrices $n \times n$ con $n\ge 1$.
				
			\end{proof}
			
		\end{section}
		
		
	\begin{section}{Determinante}\label{seccion-determinate}
	El determinante puede ser pensado como una función que a cada matriz cuadrada $n \times n$ con coeficientes en $\K$,  le asocia un elemento de $\K$. En  esta sección veremos como se define esta función y algunas propiedades de la misma. Algunas  demostraciones se omitirán, pues se pondrá énfasis en los usos del determinante y no tanto en sus propiedades teóricas. Las demostraciones faltantes se pueden ver en el Apéndice \ref{apend.Determinante}.
	
	\vskip .5cm
	
	
	
	Una forma de definir determinante es mediante una definición recursiva,  es decir para calcular el determinante de una matriz $n \times n$, usaremos el cálculo  del determinante para matrices $n-1 \times n-1$,  que a su vez se calcula usando el determinante de matrices $n-2 \times n-2$ y así sucesivamente hasta llegar al caso base, que es el caso  de matrices $1 \times 1$.
	
	
	\vskip .5cm
	
	\begin{definicion} Sea $A \in M_n(\K)$. Sean  $i, j$ tal que  $1 \le i,j \le n$. Entonces
		$A(i|j)$ es la matriz $n-1 \times n-1$ que se obtiene eliminando la fila $i$ y la columna $j$ de $A$.
	\end{definicion}
	
	\begin{ejemplo}
		Sea $A= \begin{bmatrix}1&-1&3\\4&2&-5 \\ 0&7&3\end{bmatrix}$,  entonces
		\begin{equation*}
		A(1|1)= \begin{bmatrix} 2&-5 \\ 7&3\end{bmatrix}, \qquad
		A(2|3)= \begin{bmatrix} 1&-1\\0&7\end{bmatrix}, \qquad
		A(3|1)= \begin{bmatrix} -1&3\\2&-5 \end{bmatrix}.
		\end{equation*}
	\end{ejemplo}		
	
	\begin{definicion}
		Sea $n \in \mathbb N$ y $A =[a_{ij}] \in M_n(\K)$ , entonces el \textit{determinante de $A$}\index{determinante}, denotado $\det(A)$ se define como:
		\begin{enumerate}
			\item[(1)] si $n=1$,  $\det([a]) =a$;
			\item[($n$)] si $n >1$, 
			\begin{align*}
			\det(A) &=  a_{11}\det A(1|1) - a_{21}\det A(2|1) + \cdots + (-1)^{1+n}  a_{n1}\det A(n|1) \\
			&= \sum_{i=1}^{n} (-1)^{1+i}  a_{i1}\det A(i|1).
			\end{align*}
		\end{enumerate}
		Si  $1 \le i,j \le n$, al número $\det A(i|j)$ se lo llama el \textit{menor $i,j$ de $A$} y a $C^A_{ij}:= (-1)^{i+j} \det A(i|j)$ se lo denomina  el \textit{cofactor $i,j$ de $A$.} Si la matriz $A$ está sobreentendida se denota, a veces, $C_{ij} := C^A_{ij}$.
	\end{definicion}
	
	Observemos, que con las definiciones introducidas  tenemos
	\begin{equation}\label{def-determinante}
	\det(A) = \sum_{i=1}^{n}  a_{i1}C^A_{i1}.
	\end{equation}
	A este cálculo  se lo denomina \textit{calculo del determinante por desarrollo por la primera columna}, debido  a que usamos los coeficientes de la primera columna,  multiplicados por los cofactores correspondientes. A veces, para simplificar,  denotaremos
	$$
	|A| := \det A.
	$$
	
	\begin{obs}\textbf{Determinantes $\mathbf{2 \times 2}$.} Calculemos el determinante de las matrices $2 \times 2$. Sea 
		$$A=\begin{bmatrix}a&b\\c&d\end{bmatrix},$$  entonces
		$$
		\det A = a \det [d] - c \det [b] = ad-bc.
		$$
		
		Hemos visto en el ejemplo \ref{inv-2x2-0} que $A$ es inversible si y solo si $ ad-bc \not=0$,  es decir 
		\begin{equation}
		\text{\textit{$A$ es inversible si y solo si $ \det A \not=0$.}}
		\end{equation}
		Este resultado se generaliza para matrices $n \times n$. Más aún, la fórmula (\ref{inv-2x2}) se generaliza también para matrices cuadradas de cualquier dimensión (ver el corolario \ref{cor-inv-x-det}).
	\end{obs}
	
	
	\begin{observacion} \textbf{Determinantes $\mathbf{3 \times 3}$.} Calculemos el determinante de las matrices $3 \times 3$. Sea 
		$$A=\begin{bmatrix}a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33}\end{bmatrix},$$  entonces
		\begin{align*}
		\det A &= a_{11}\left|\begin{matrix}a_{22}&a_{23}\\a_{32}&a_{33}\end{matrix}\right|
		- a_{21}\left|\begin{matrix}a_{12}&a_{13}\\a_{32}&a_{33}\end{matrix}\right|
		+ a_{31}\left|\begin{matrix}a_{12}&a_{13}\\a_{22}&a_{23}\end{matrix}\right|\\
		&= a_{11}(a_{22}a_{33}- a_{23}a_{32})
		- a_{21}(a_{12}a_{33}-a_{13}a_{32}) 
		+ a_{31}(a_{12}a_{23} - a_{13}a_{22}) \\
		&=a_{11}a_{22}a_{33}- a_{11}a_{23}a_{32} 
		- a_{12}a_{21}a_{33}+ a_{13}a_{21}a_{32}+ a_{12}a_{23}a_{31}
		- a_{13}a_{22}a_{31}.	
		\end{align*}
		
		Observar que  el determinante de una matriz $3 \times 3$ es una sumatoria de seis términos cada uno de los cuales es de la forma $\pm a_{1\,i_1}a_{2\,i_2}a_{3\,i_3}$ e $i_1i_2i_3$ puede ser cualquier permutación de $123$. La fórmula 
		\begin{equation}\label{det3x3}
		\det A =a_{11}a_{22}a_{33}- a_{11}a_{23}a_{32} 
		- a_{12}a_{21}a_{33}+ a_{13}a_{21}a_{32}+ a_{12}a_{23}a_{31}
		- a_{13}a_{22}a_{31},
		\end{equation} 
		no es fácil de recordar, pero existe un procedimiento sencillo que nos permite obtenerla: 1) a la matriz original le agregamos las dos primeras filas al final, 2) ``sumamos'' las diagonales descendentes y ``restamos'' las diagonales ascendentes,
		
		
		
		\begin{equation}
		\begin{tikzpicture}[baseline=(A.center)]
		\tikzset{node style ge/.style={circle}}
		\tikzset{BarreStyle/.style =   {opacity=.4,line width=0.5 mm,line cap=round,color=#1}}
		\tikzset{SignePlus/.style =   {above left,,opacity=1}}
		\tikzset{SigneMoins/.style =   {below left,,opacity=1}}
		% les matrices
		\matrix (A) [matrix of math nodes, nodes = {node style ge},,column sep=0 mm] 
		{ a_{11} & a_{12} & a_{13}  \\
			a_{21} & a_{22} & a_{23}  \\
			a_{31} & a_{32} & a_{33}  \\
			a_{11} & a_{12} & a_{13} \\
			a_{21} & a_{22} & a_{13}\\
		};
		
		\draw [BarreStyle=blue] (A-1-1.north west) node[SignePlus=blue] {$+$} to (A-3-3.south east) ;
		\draw [BarreStyle=blue] (A-2-1.north west) node[SignePlus=blue] {$+$} to (A-4-3.south east) ;
		\draw [BarreStyle=blue] (A-3-1.north west) node[SignePlus=blue] {$+$} to (A-5-3.south east) ;
		\draw [BarreStyle=red]  (A-3-1.south west) node[SigneMoins=red] {$-$} to (A-1-3.north east);
		\draw [BarreStyle=red]  (A-4-1.south west) node[SigneMoins=red] {$-$} to (A-2-3.north east);
		\draw [BarreStyle=red]  (A-5-1.south west) node[SigneMoins=red] {$-$} to (A-3-3.north east);
		\end{tikzpicture}
		\end{equation}
		
		Es decir,
		\begin{enumerate}
			\item[(a)]  se suman  $a_{11}a_{22}a_{33}$, $a_{21}a_{32}a_{13}$, $a_{31}a_{12}a_{23}$,  y
			\item[(b)]  se restan  $a_{31}a_{22}a_{13}$,  $a_{11}a_{32}a_{23}$,  $a_{21}a_{12}a_{33}$. 
		\end{enumerate}
	\end{observacion}
	
	\begin{ejemplo}
		Calcular el determinante de 
		$$ A = \begin{bmatrix}1&-2&2\\3&-1&1\\2&5&4\end{bmatrix}.$$
		
		La forma más sencilla es ampliando la matriz y calculando:
		\begin{equation*}
		\begin{tikzpicture}[baseline=(A.center)]
		\tikzset{node style ge/.style={circle}}
		\tikzset{BarreStyle/.style =   {opacity=.4,line width=0.5 mm,line cap=round,color=#1}}
		\tikzset{SignePlus/.style =   {above left,,opacity=1}}
		\tikzset{SigneMoins/.style =   {below left,,opacity=1}}
		% les matrices
		\matrix (A) [matrix of math nodes, nodes = {node style ge},,column sep=0 mm] 
		{ 1&-2&2  \\
			3&-1&1  \\
			2&5&4  \\
			1&-2&2 \\
			3&-1&1\\
		};
		
		\draw [BarreStyle=blue] (A-1-1.north west) node[SignePlus=blue] {$+$} to (A-3-3.south east) ;
		\draw [BarreStyle=blue] (A-2-1.north west) node[SignePlus=blue] {$+$} to (A-4-3.south east) ;
		\draw [BarreStyle=blue] (A-3-1.north west) node[SignePlus=blue] {$+$} to (A-5-3.south east) ;
		\draw [BarreStyle=red]  (A-3-1.south west) node[SigneMoins=red] {$-$} to (A-1-3.north east);
		\draw [BarreStyle=red]  (A-4-1.south west) node[SigneMoins=red] {$-$} to (A-2-3.north east);
		\draw [BarreStyle=red]  (A-5-1.south west) node[SigneMoins=red] {$-$} to (A-3-3.north east);
		\end{tikzpicture}.
		\end{equation*}
		Luego 
		\begin{equation*}
		\begin{matrix*}[l]
		\det A &= 1\times (-1) \times 4 \quad\;\,\,+ 3\times5 \times2\quad\;+ 2\times (-2) \times 1\\
		&\quad- 2\times (-1) \times 2\quad - 1 \times 5 \times 1 \quad - 3 \times (-2) \times 4\\
		&= -4+ 30 -4 +4 -5 +24 \\
		&= 35.
		\end{matrix*}
		\end{equation*}
	\end{ejemplo}
	
	\begin{obs}
		La regla para calcular el determinante de matrices $3 \times 3$ \textbf{\large no} se aplica a matrices $n \times n$ con $n \ne 3$.
	\end{obs}
	
	\begin{comment}
		\begin{proposicion}
		Sea $A \in M_n(\K)$ matriz triangular  cuyos elementos en la diagonal son $d_1,\ldots,d_n$. Entonces $\det A = d_1.d_2.\ldots d_n$.
		\end{proposicion}
		\begin{proof} Si $A$ es triangular superior podemos demostrar el resultado por inducción sobre $n$: es claro que si $n=1$,  es decir si $A = [d_1]$, el determinante vale $d_1$. Por otro lado, si $n>1$,  observemos que $A(1|1)$ es también triangular superior con valores $d_2,\ldots,d_n$  en la diagonal principal. Entonces,  usamos la definición de la fórmula (\ref{def-determinante}) y observamos que el desarrollo por la primera  columna solo tiene un término, pues esta columna solo tiene un coeficiente no nulo, el $d_1$ en la primera posición. Por lo tanto, 
		\begin{equation*}
		\det(A) = d_1 \det(A(1|1)) \stackrel{\text{(HI)}}{=} d_1.(d_2.\ldots.d_n).
		\end{equation*}
		
		En  el caso  que  $A$ sea triangular inferior también lo demostraremos por inducción. El caso $n=1$ es trivial. Si $n >1$, observemos que  $A(1|1)$ es también triangular inferior con valores $d_2,\ldots,d_n$  en la diagonal principal y si $i >1$, $A(i|1)$ es triangular inferior con el primer valor en la diagonal igual a 0. Por hipótesis inductiva $\det(A(1|1)) = d_2.\ldots.d_n$ y si $i >1$, $\det(A(i|1)) = 0\times \ldots = 0$, por lo tanto 
		\begin{equation*}
		\det(A) = d_1 \det(A(1|1))+ \sum_{i=2}^{n} a_{i1}\det(A(i|1)) =  d_1.(d_2.\ldots.d_n) + 0.
		\end{equation*} 
		\end{proof}
	\end{comment}

	\begin{proposicion}\label{det-triang-sup}
	Sea $A \in M_n(\K)$ matriz triangular  superior cuyos elementos en la diagonal son $d_1,\ldots,d_n$. Entonces $\det A = d_1.d_2.\ldots d_n$.
	\end{proposicion}
	\begin{proof} Podemos demostrar el resultado por inducción sobre $n$: es claro que si $n=1$,  es decir si $A = [d_1]$, el determinante vale $d_1$. Por otro lado, si $n>1$,  observemos que $A(1|1)$ es también triangular superior con valores $d_2,\ldots,d_n$  en la diagonal principal. Entonces,  usamos la definición de la fórmula (\ref{def-determinante}) y observamos que el desarrollo por la primera  columna solo tiene un término, pues esta columna solo tiene un coeficiente no nulo, el $d_1$ en la primera posición. Por lo tanto, 
		\begin{equation*}
		\det(A) = d_1 \det(A(1|1)) \stackrel{\text{(HI)}}{=} d_1.(d_2.\ldots.d_n).
		\end{equation*}
	\end{proof}
	
	\begin{corolario}
		$det I_n = 1$.
	\end{corolario}
	\begin{proof}
		Se deduce del hecho que $I_n$  es triangular superior y todo coeficiente de la diagonal principal vale 1.
	\end{proof}
	
	\begin{corolario}
		Si $R$ es una MERF, entonces 
		\begin{align*}
		\det R = \left\{ \begin{matrix*}[l]
		1 \;&\text{si $R$ no tiene filas nulas,}\\
		0&\text{si $R$ tiene filas nulas.}
		\end{matrix*}\right.  
		\end{align*}
	\end{corolario}
	\begin{proof}
		Si $R$ no tiene filas nulas es igual a $I_n$ (lema \ref{mtrx-merf-id}), luego $\det R = 1$. En general, $R$ es una matriz triangular superior y si tiene alguna fila nula $r$, entonces el coeficiente ${rr}$ es igual a $0$ y por lo tanto 	$\det R = 0$.
	\end{proof}

	\begin{ejemplo} Veamos,  en el caso de una matriz $A= [a_{ij}]$ de orden  $2 \times 2$ que ocurre con el determinante cuando hacemos una operación elemental. 
			\begin{enumerate}
			\item Si $c \not=0$, multiplicar por  $c$ la primera fila y multiplicar $c$ por la segunda fila obtenemos, respectivamente,
			\begin{equation*}
			e(A) = \begin{bmatrix} ca_{11}& ca_{12}\\ a_{21}&a_{22}\end{bmatrix}\;\text{ y }\; e(A) = \begin{bmatrix} a_{11}& a_{12}\\ ca_{21}&ca_{22}\end{bmatrix},
			\end{equation*}
			luego 
			\begin{equation*}
			\det \begin{bmatrix} ca_{11}& ca_{12}\\ a_{21}&a_{22}\end{bmatrix} = ca_{11}a_{22} - ca_{12}a_{21}\;\text{ y }\;\det \begin{bmatrix} ca_{11}& ca_{12}\\ a_{21}&a_{22}\end{bmatrix} = ca_{11}a_{22} - ca_{12}a_{21}. 
			\end{equation*}
			Por lo tanto,  en ambos casos, $\det e(A) = c \det A$. 
			\item Sea  $c \in \K$, si sumamos a la fila 2 la fila 1 multiplicada por $c$ o sumamos a la fila 1  la fila 2 multiplicada por $c$ obtenemos, respectivamente,
			\begin{equation*}
			e(A) = \begin{bmatrix} a_{11}& a_{12}\\ a_{21}+ ca_{11}&a_{22}+ ca_{12}\end{bmatrix}\;\text{ y }\; 
			e(A) = \begin{bmatrix} a_{11} + ca_{21}& a_{12} + ca_{22}\\ a_{21}&a_{22}\end{bmatrix}.
			\end{equation*}
			Por lo tanto, 
			\begin{align*}
				\det \begin{bmatrix} a_{11}& a_{12}\\ a_{21}+ ca_{11}&a_{22}+ ca_{12}\end{bmatrix} &=
				a_{11}(a_{22}+ ca_{12})- a_{12}( a_{21}+ ca_{11}) \\
				&= a_{11}a_{22}+ ca_{11}a_{12}- a_{12} a_{21}- ca_{12} a_{11}\\
				&= a_{11}a_{22}- a_{12} a_{21} \\
				&= \det A.
			\end{align*}
			En  el otro caso también se comprueba que $\det e(A) = \det A$. 
			\item Finalmente, intercambiando la fila 1 por la fila 2 obtenemos la matriz
			\begin{equation*}
			e(A)=\begin{bmatrix} a_{21}& a_{22}\\ a_{11}&a_{12}\end{bmatrix},
			\end{equation*}
			por lo tanto 
			\begin{equation*}
			\det e(A)= \det \begin{bmatrix} a_{21}& a_{22}\\ a_{11}&a_{12}\end{bmatrix} = a_{21}a_{12} - a_{22}a_{11} = -\det A.
			\end{equation*}
		\end{enumerate}
	\end{ejemplo}
	
	Todos los resultado del ejemplo anterior se pueden generalizar. 
	
	\begin{teorema} \label{det-prop-fundamentales}
		Sea $A  \in M_n(\K)$ y sean $1 \le r,s \le n$.
		\begin{enumerate}
			\item Sea $c \in \K$ y $B$ la matriz que se obtiene de $A$ multiplicando la fila $r$ por $c$, es decir $A  \stackrel{cF_r}{\longrightarrow} B$, entonces $\det B = c \det A$.
			\item  Sea $c \in \K$, $r \ne s$ y $B$ la matriz que se obtiene de $A$ sumando a la fila $r$ la fila $s$ multiplicada por $c$, es decir  $A  \stackrel{F_r + cF_s}{\longrightarrow} B$, entonces $\det B = \det A$.
			\item Sea $r \ne s$ y sea $B$ la matriz que se obtiene de $A$ permutando la fila $r$ con la fila $s$, es decir  $A  \stackrel{F_r \leftrightarrow F_s}{\longrightarrow}B$, entonces $\det B = -\det A$.
			
		\end{enumerate}
	\end{teorema}
	

	
	
	Este resultado nos permite calcular el determinante de matrices elementales.
	
	
	
	
	\begin{corolario}\label{det-mtrx-elem} Sea $n \in \mathbb N$ y $c \in \K$. Sean $1 \le r,s \le n$,  con $r \ne s$.
		\begin{enumerate}
			\item Si $c \not=0$, la matriz elemental que se obtiene de multiplicar por  $c$ la fila $r$ de $I_n$, tiene determinante igual a $c$.
			\item Sea $r \ne s$. La matriz elemental que se obtiene de sumar a la fila $r$ de $I_n$  la fila $s$ multiplicada por $c$, tiene determinante $1$.
			\item Finalmente, si $r \ne s$, la matriz elemental que se obtiene de intercambiar la fila $r$ por la fila $s$ de $I_n$ tiene determinante $-1$.
		\end{enumerate}	
	\end{corolario} 
	\begin{proof}
		Se deduce fácilmente del teorema anterior y del hecho de que $det I_n =1$.			
	\end{proof}
	
	
	\begin{corolario}\label{det-filas-iguales} Sea $A  \in M_n(\K)$.
		\begin{enumerate}
			\item Si $A$ tiene dos filas iguales,  entonces $\det A=0$.
			\item Si $A$ tiene una fila nula, entonces $\det A =0$.
		\end{enumerate}
	\end{corolario}
	\begin{proof}
		(1) Sea $A$ matriz donde $F_r = F_s$ con $r\ne s$. Luego, intercambiando la fila $r$ por la fila $s$ obtenemos la misma matriz. Es decir $A  \stackrel{F_r \leftrightarrow F_s}{\longrightarrow} A$. Por el teorema \ref{det-prop-fundamentales} (3), tenemos entonces que $\det A = - \det A$, por lo tanto  $\det A =0$. 
		
		(2) Sea $F_r$ una fila nula de $A$, por lo tanto multiplicar por 2 esa fila no cambia la matriz. Es decir $A  \stackrel{2F_r}{\longrightarrow} A$. Por el teorema \ref{det-prop-fundamentales} (1), tenemos entonces que $\det A = 2\det A$, por lo tanto  $\det A =0$.
	\end{proof}
	
	
	
	\begin{teorema} \label{mtrx-inv-equiv3} Sean $A,B \in M_n(\K)$, entonces
		\begin{enumerate}
			\item $\det (A B) = \det(A)\det(B)$. 
			\item $A$ inversible si y solo si  $\det(A)\ne0$.
		\end{enumerate}
	\end{teorema}
		
	\begin{corolario}
		$\det (AB) = \det(BA)$.
	\end{corolario}
	\begin{proof}
		$\det (AB) = \det(A)\det(B) = \det(B)\det(A) = \det(BA)$.
	\end{proof}
	
	\begin{definicion}
		Sea $A$ una matriz $m \times n$ con coeficientes en $\K$. La \textit{transpuesta}\index{matriz transpuesta} de $A$, denotada $A^\t$, es la matriz  $n \times m$ que en la fila $i$ y columna $j$ tiene el coeficiente $[A]_{ji}$. Es decir
		\begin{equation*}
		[A^\t]_{ij} = [A]_{ji}.
		\end{equation*}  
	\end{definicion}
	
	\begin{ejemplo} Si
		$$A=\begin{bmatrix}a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33}\end{bmatrix},$$ 
		entonces
		$$A^\t=\begin{bmatrix}a_{11}&a_{21}&a_{31}\\a_{12}&a_{22}&a_{32}\\a_{13}&a_{23}&a_{33}\end{bmatrix}.$$ 
	\end{ejemplo}
	
	\begin{ejemplo}
		Si $$A=\begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix},$$ 
		entonces
		$$A^\t=\begin{bmatrix}1&3&5\\2&4&6\end{bmatrix}.$$ 
	\end{ejemplo}
	
	
	En  general $A^\t$ es la matriz cuyas filas son las columnas de $A$. 
	
	\begin{proposicion}\label{prop-matriz-transpuesta} Sea $A$ matriz $m \times n$.
		\begin{enumerate}
			\item $(A^\t)^\t = A$.
			\item  Si $B$ matriz $n \times k$,  entonces
			\begin{equation*}
			(AB)^\t = B^\t A^\t.
			\end{equation*} 
			\item  Sea $A$ matriz $n \times n$, entonces, $A$ inversible si y sólo si  $A^\t$ es inversible y  en ese caso $(A^\t)^{-1} = (A^{-1})^\t$.
		\end{enumerate}
	\end{proposicion}
	\begin{proof}
		(1) 	$[(A^\t)^\t]_{ij} = [A^\t]_{ji} = [A]_{ij}$. 
		
		\vskip .3cm 
		
		(2)	Por definición de transpuesta $(AB)^\t$ es una matriz $k \times m$.  Ahora observemos  que $B^\t$  es una matriz $k \times n$ y $A^\t$ es $n \times m$, luego tiene sentido multiplicar $B^\t$ por $A^\t$ y se obtiene también  una matriz  $k \times m$. La demostración de la proposición se hace comprobando que el coeficiente $ij$ de $(AB)^\t$ es igual al coeficiente $ij$ de $B^\t A^\t$ y se deja como ejercicio para el lector. 
		
		(3) 
		\begin{align*}
			\text{$A$ inversible } &\Leftrightarrow \text{existe $B$ matriz $n \times n$ tal que $AB = I_n = BA$} \\
			&\Leftrightarrow (AB)^\t = I^\t_n = (BA)^\t\\
			&\Leftrightarrow B^\t A^\t = I_n = A^\t B^\t\\
			&\Leftrightarrow \text{$B^\t$ es  la inversa de $A^\t$}. 
		\end{align*}
		Es decir, $A$ inversible si y sólo si  $A^\t$ es inversible y si $B = A^{-1}$, entonces  $(A^\t)^{-1} = B^\t$.
	\end{proof}
	
	Observar que por inducción no es complicado probar que si $A_1,\ldots, A_k$ son matrices,  entonces 
	\begin{equation*}
	(A_1\ldots A_k)^\t = A_k^\t\ldots A_1^\t.
	\end{equation*}
	
	\begin{ejemplo} Veamos las transpuesta de las matrices elementales  $2 \times 2$.
		\begin{enumerate}
			\item Si $c \not=0$, multiplicar por  $c$ la primera fila y multiplicar $c$ por la segunda fila son, respectivamente,
			\begin{equation*}
			E = \begin{bmatrix} c& 0\\ 0&1\end{bmatrix}\;\text{ y }\; E = \begin{bmatrix} 1& 0\\ 0&c\end{bmatrix},
			\end{equation*}
			por lo tanto $E^\t$ es la misma matriz en ambos casos. 
			\item si  $c \in \K$, sumar a la fila 2 la fila 1 multiplicada por $c$ o sumar a la fila 1 la fila 2 multiplicada por $c$ son, respectivamente,
			\begin{equation*}
			E_1 = \begin{bmatrix} 1& 0\\ c&1\end{bmatrix}\;\text{ y }\; E_2 = \begin{bmatrix} 1& c\\ 0&1\end{bmatrix},
			\end{equation*}
			por lo tanto 
			\begin{equation*}
			E_1^\t = \begin{bmatrix} 1& c\\ 0&1\end{bmatrix} = E_2 \;\text{ y }\; E_2^\t = \begin{bmatrix} 1& 0\\ c&1\end{bmatrix} =E_1,
			\end{equation*}
			\item Finalmente, intercambiando la fila 1 por la fila 2 obtenemos la matriz
			\begin{equation*}
			E = \begin{bmatrix} 0& 1\\ 1&0\end{bmatrix},
			\end{equation*}
			por lo tanto $E^\t =E$
		\end{enumerate}
	\end{ejemplo}
	

\begin{obs}
	En  el caso  de matrices $2 \times 2$ podemos comprobar fácilmente que $\det A^\t = \det A$:
	$$
	\det A^\t = \det \begin{bmatrix} a_{11} & a_{21} \\ a_{12}& a_{22}\end{bmatrix} = 
	a_{11}a_{22} - a_{21}a_{12} = \det A. 
	$$
\end{obs}

También vale este resultado para matrices $n \times n$.

\begin{teorema}\label{det-a-trans} Sea $A \in M_n(\K)$,  entonces 
	$\det(A) = \det(A^\t)$
\end{teorema}
	
	\vskip .3cm
	
	El resultado anterior permite obtener resultados nuevos del cálculo de determinante a partir de resultados vistos anteriormente. 
	
	
	\begin{proposicion}\label{det-triang-inf}
	Sea $A \in M_n(\K)$ matriz triangular  inferior cuyos elementos en la diagonal son $d_1,\ldots,d_n$. Entonces $\det A = d_1.d_2.\ldots d_n$.
	\end{proposicion}
	\begin{proof}
		Si $A$ es triangular inferior con elementos en la diagonal $d_1,\ldots,d_n$,  entonces $A^\t$ es triangular superior con  elementos en la diagonal $d_1,\ldots,d_n$. Por la proposición \ref{det-triang-sup}, $\det A^\t = d_1\ldots d_n$. Por el teorema  \ref{det-a-trans} obtenemos el resultado. 
	\end{proof}


	
	\begin{teorema}\label{det-opr-col}
		Sea $A  \in M_n(\K)$ y sean $1 \le r,s \le n$.
		\begin{enumerate}
			\item Sea $c \in \K$ y $B$ la matriz que se obtiene de $A$ multiplicando la columna $r$ por $c$, entonces $\det B = c \det A$.
			\item  Sea $c \in \K$ y $B$ la matriz que se obtiene de $A$ sumando a la columna $r$ la columna $s$ multiplicada por $c$, entonces $\det B = \det A$.
			\item Sea $B$ la matriz que se obtiene de $A$ permutando la columna $r$ con la fila $s$, entonces $\det B = -\det A$.
		\end{enumerate}
	\end{teorema}
	\begin{proof}
		Las operaciones por columna del enunciado se traducen a operaciones por fila de la matriz $A^\t$. Luego, aplicando los resultados del teorema  \ref{det-prop-fundamentales} y usando el hecho de que $\det(A) = \det(A^\t)$ y $\det(B) = \det(B^\t)$ en cada caso, se deduce el corolario. 
	\end{proof}
	
	\begin{corolario} Sea $A  \in M_n(\K)$.
		\begin{enumerate}
			\item Si $A$ tiene dos columnas iguales,  entonces $\det A=0$.
			\item Si $A$ tiene una columna nula, entonces $\det A =0$.
		\end{enumerate}
	\end{corolario}
	\begin{proof}
		(1) Si $A$ tiene dos columnas iguales,  entonces $A^\t$ tiene dos filas iguales, luego, por corolario \ref{det-filas-iguales} (1),  $\det A^\t =0$ y por lo tanto $\det A=0$.
		
		(2) Si $A$ tiene una columna nula,  entonces $A^\t$ tiene una fila nula, luego, \ref{det-filas-iguales} (2), $\det A^\t =0$ y por lo tanto $\det A=0$.
	\end{proof}
	
		El siguiente teorema nos dice  que es posible calcular el determinante desarrollándolo por cualquier fila o cualquier columna. 
	
	
	\begin{teorema} \label{th-expancion-cofactores} El determinante de una matriz $A$ de orden $n \times n$ puede ser calculado por la expansión de los cofactores en  cualquier columna o cualquier fila. Más específicamente, 
		\begin{enumerate}
			\item si usamos la expansión por la $j$-ésima columna, $1 \le j \le n$, tenemos
			\begin{align*}
			\det A &= \sum_{i=1}^{n} a_{ij} C_{ij} \\
			& = a_{1j}C_{1j}+a_{2j}C_{2j}+\cdots+a_{nj}C_{nj}.
			\end{align*} 
			\item si usamos la expansión por la $i$-ésima fila, $1 \le i \le n$, tenemos
			\begin{align*}
			\det A &= \sum_{j=1}^{n} a_{ij} C_{ij} \\
			& = a_{i1}C_{i1}+a_{i2}C_{i2}+\cdots+a_{in}C_{in};
			\end{align*} 
		\end{enumerate}
	\end{teorema}




\end{section}	
	\end{chapter}




	\begin{chapter}{Espacios vectoriales}\label{chap-esp-vect} En este capítulo estudiaremos en forma general las combinaciones lineales sobre conjuntos abstractos.
		En el primer capítulo desarrollamos el método de Gauss para la resolución de sistemas de ecuaciones lineales. En el método de Gauss se usan sistemáticamente las combinaciones lineales de las filas de una matriz. Podemos ver estas filas como elementos de $\K^n$  y nuestro primer impulso para el estudio de las combinaciones lineales sería trabajar en este contexto, es decir  en $\K^n$. Eso es lo que haremos, en forma introductoria, en la primera sección, donde trabajaremos restringidos a   $\R^2$ y $\R^3$.   Sin embargo, muchos de los resultados sobre combinaciones lineales en $\K^n$ son aplicables también a conjuntos más generales y de gran utilidad en la matemática. Por lo tanto,  apartir de la segunda sección,  nuestros ``espacios vectoriales'' (espacios donde pueden hacerse combinaciones lineales de vectores) serán espacios abstractos. 
		

		

		
		\begin{section}{Definición y ejemplos de espacios vectoriales}
			
			\begin{definicion} Sea $\K$ cuerpo. Un \textit{espacio vectorial}\index{espacio vectorial} sobre $\K$ o un \textit{$\K$-espacio vectorial }, consiste de  un  conjunto $V$ no vacío, cuyos elementos son llamados \textit{vectores}, junto a  '$+$' y '$.$' tal que
				\begin{enumerate}
					\item[(\textit{a})] '$+\colon V\times V\to V$' es una operación, llamada \textit{adición} o  \textit{suma de vectores,} tal que a dos vectores $v,w \in V$ les asigna otro vector $v+w \in V$,
					\item[(\textit{b})]  '$.
                     \colon \K\times V\to V$' es una operación tal que a $\lambda \in \K$ y $v \in V$ le asigna el vector $\lambda.v$ (o simplemente $\lambda v$).  '$.$' es llamada  el \textit{producto por escalares}.
					
				\end{enumerate}	 
				Además, estas operaciones deben satisfacer 
				\begin{enumerate}
					\item[\textbf{S1.}] $v + w = w + v$, para $v,w \in V$ (\textit{conmutatividad de la suma}),
					\item[\textbf{S2.}] $(v+ w)+ u = v + (w+u)$, para $v,w,u \in V$ (\textit{asociatividad de la suma}),
					\item[\textbf{S3.}] existe un único vector $0$, llamado \textit{vector cero}, tal que $0+ v = v + 0 =v$, para todo $v \in V$ (\textit{existencia de elemento neutro de la suma}).
					\item[\textbf{S4.}] Para cada $v \in V$, existe un  único vector $-v$ tal que  $v + (-v) = (-v)+ v =0$ (\textit{existencia de opuesto} o  \textit{inverso aditivo}).
					\item[\textbf{P1.}] $1.v=v$ para todo $v \in V$.
					\item[\textbf{P2.}] $\lambda_1(\lambda_2v) = (\lambda_1\lambda_2)v$, para todo $\lambda_1,\lambda_2 \in \K$ y todo $v \in V$.
					\item[\textbf{D1.}] $\lambda(v+w) = \lambda v +\lambda w$, para todo $\lambda \in \K$ y todo $v,w \in V$ (\textit{propiedad distributiva}).
					\item[\textbf{D2.}] $(\lambda_1+\lambda_2)v = \lambda_1v + \lambda_2 v$ para todo $\lambda_1,\lambda_2 \in \K$ y todo $v \in V$ (\textit{propiedad distributiva}).
				\end{enumerate}
			\end{definicion}
			
			Debido a la ley de asociatividad para la suma $(v+w)+u$ es igual a $v+(w+u)$ y por lo tanto podemos eliminar los paréntesis sin ambigüedad. Es decir, 	$\forall\, v,w,u \in V$ denotamos 
			$$
			v + w + u := (v+w)+u =v+(w+u).
			$$
			
			De forma análoga, $\forall\, \lambda_1,\lambda_2 \in V$, $\forall \, v \in V$ usaremos la notación
			$$
			\lambda_1\lambda_2v  = (\lambda_1\lambda_2)v = \lambda_1(\lambda_2v).
			$$
			
			Otra notación importante, e intuitiva, es la siguiente	$\forall\, v,w \in V$
			$$
			v-w := v+(-w),
			$$
			y a menudo diremos que $v-w$ es la \textit{resta} de $v$ menos $w$. 
			
			\begin{ejemplo} {\sc $\K^n$.} Sea $\K$ cuerpo, y sea
				\begin{equation*}
				V = \{(x_1,x_2,\ldots,x_n): x_i \in \K, 1 \le i \le n \} = \K^n.
				\end{equation*}	
				Entonces $V$ es espacio vectorial con las siguientes operaciones: si $(x_1,x_2,\ldots,x_n) \in \K^n$, $(y_1,y_2,\ldots,y_n) \in K^n$, $\lambda \in \K$
				\begin{enumerate}
					\item[(\textit{a})] $(x_1,x_2,\ldots,x_n)+ (y_1,y_2,\ldots,y_n) = (x_1+y_1,x_2+y_2,\ldots,x_n+y_n)$,
					\item[(\textit{b})] $\lambda(x_1,x_2,\ldots,x_n) = (\lambda x_1,\lambda x_2,\ldots,\lambda x_n)$.
				\end{enumerate}
				Observar que las sumas y productos son coordenada a coordenada y, por lo tanto, en cada coordenada son sumas y productos en $\K$.
				
				Comprobemos las propiedades necesarias para que $V$ sea un espacio vectorial. Como la suma de vectores y el producto por escalares es coordenada a coordenada, las propiedades se deducirán fácilmente de los axiomas para la suma y el producto en los cuerpos.  Sean $x =  (x_1,\ldots,x_n)$, $y = (y_1,\ldots,y_n)$, $z = (z_1,\ldots,z_n)$ en $V$ y $\lambda, \lambda_1,\lambda_2 \in \K$:
				\begin{enumerate}
					\item[S1.] $x + y = y +x$, pues $x_i + y_i = y_i + x_i$, $1 \le i \le n$.
					\item[S2.]  $(x+ y)+ z = x + (y+z)$,  pues $(x_i + y_i) + z_i = x_i + (y_i + z_i)$, $1 \le i \le n$. 
					\item[S3.] Sea $0 = (0,\ldots,0)$,  entonces $0+x = (0+x_1,\ldots, 0+x_n) =(x_1,\ldots,x_n) =x$.
					\item[S4.] Sea $-x = (-x_1,\ldots,-x_n)$,  entonces $x + (-x) = (x_1-x_1,\ldots,x_n-x_n) =  (0,\ldots,0)$.
					\item[P1.] $1.x=(1.x_1,\ldots,1.x_n) = (x_1,\ldots,x_n)=x$.
					\item[P2.] $\lambda_1(\lambda_2x) = (\lambda_1\lambda_2)x$ pues $\lambda_1(\lambda_2x_i) =(\lambda_1\lambda_2)x_i$, $1 \le i \le n$.
					\item[D1.] $\lambda(x+y) = \lambda x +\lambda y$,  pues $\lambda(x_i+y_i) = \lambda x_i + \lambda y_i$, $1 \le i \le n$.
					\item[D2.] $(\lambda_1+\lambda_2)x = \lambda_1x + \lambda_2 x$, pues $ (\lambda_1+\lambda_2)x_i = \lambda_1x_i + \lambda_2 x_i$, $1 \le i \le n$.
				\end{enumerate} 
				
				
			\end{ejemplo}
			
			\medspace
			
			\begin{ejemplo}{\sc Matrices $m \times n$.} Sea $\K$ cuerpo,  definimos en $M_{m \times n}(\K)$ la suma  y el producto por escalares de la siguiente forma. Sean $A = [a_{ij}]$, $B = [b_{ij}]$ matrices $m \times n$ y $ \lambda \in \K$, entonces $A+B$, $\lambda A$ son matrices en $M_{m \times n}(\K)$ con coeficientes:
				\begin{align*}
				[A + B]_{ij} &= [a_{ij} + b_{ij}], \\
				[\lambda A]_{ij} &= [\lambda a_{ij}]. 
				\end{align*}
				Es decir, la suma es coordenada a coordenada y el producto es multiplicar el escalar en cada coordenada. Este caso no es más que $\K^{mn}$ presentado de otra manera. 
				
				Ejemplifiquemos, con casos sencillos, la suma de matrices y el producto por escalares
				\begin{equation*}
				\begin{bmatrix} -2&1\\0&4 \end{bmatrix} + \begin{bmatrix} 5&1\\2&-5 \end{bmatrix} =
				\begin{bmatrix} 3&2\\2&-1 \end{bmatrix}, \qquad 
				3\begin{bmatrix} -2&1\\0&4 \end{bmatrix} = \begin{bmatrix} -6&3\\0&12 \end{bmatrix}.
				\end{equation*}	
			\end{ejemplo}
			
			\medspace
			
			\begin{ejemplo}{\sc Polinomios.} Sea 
				\begin{equation*}
				\K[x] = \{a_nx^n + \cdots + a_1x + a_0: n \in \mathbb N_0, a_i \in \K\text{, para } 0\le i \le n  \}
				\end{equation*}
				el conjunto de polinomios sobre $\K$. Entonces si $P(x), Q(x) \in \K[x]$, definimos la suma de polinomios de la siguiente manera: sea $P(x) = a_nx^n + \cdots + a_1x + a_0$ y $Q(x)= b_nx^n + \cdots + b_1x + a_0$ (completamos coeficientes con 0 hasta que ambos tengan el mismo $n$), entonces
				\begin{equation*}
				(P+Q)(x) = (a_n+b_n)x^n + \cdots + (a_1+ b_1)x + (a_0+b_0).
				\end{equation*}
				Si $\lambda \in \K$, 
				\begin{equation*}
				(\lambda P)(x) = \lambda a_nx^n + \cdots + \lambda a_1x + \lambda a_0.
				\end{equation*}
				Por ejemplo,
				\begin{align*}
				(3x^2 + 1)+(x^4 + 2x^3 + 5x^2-x) &= x^4 + 2x^3 + 8x^2-x +1, \qquad \text{ y } \\
				3(x^4 + 2x^3 + 5x^2-x) &= 3x^4 + 6x^3 + 15x^2-3x.
				\end{align*}
			\end{ejemplo}
			
			
			\medspace
			
			\begin{ejemplo}{\sc Espacios de funciones.} Sean
				\begin{align*}
				\operatorname{F}(\R) &= \{f: \R \to \R: \text{ tal que } f \text{ es una función} \},\\
				\operatorname{C}(\R) &= \{f: \R \to \R: \text{ tal que }  f  \text{ es una función continua} \}.
				\end{align*}
				Recordemos que si $f,g$ son funciones, entonces la función suma de $f$ y $g$ está definida por
				\begin{equation*}
				(f+g)(x) = f(x) + g(x).
				\end{equation*}
				Por otro lado, si $\lambda \in \R$, la función multiplicar $f$ por $\lambda $ está definida por
				\begin{equation*}
				(\lambda f)(x) = \lambda f(x).
				\end{equation*}			
								
				Es sencillo ver que con estas dos operaciones, $\operatorname{F}(\R)$ es un $\R$-espacio vectorial.
				
				Con respecto a $\operatorname{C}(\R)$, hemos visto en el primer curso de análisis matemático  que la suma de funciones continuas es una función continua y, por lo tanto, $f+g$ es continua si $f$ y $g$ lo son.  
				
				El producto de un escalar $c$ por una función continua $f$,  puede ser visto como el producto de una función que es constante y vale $\lambda $ (y es continua) y la función $f$. Por lo tanto, $\lambda f$ es producto de funciones continuas y, en consecuencia, es una función continua.  Resumiendo,
				\begin{equation*}
				f, g \in \operatorname{C}(\R) \Rightarrow f+g \in \operatorname{C}(\R), \qquad \lambda \in \R, f \in \operatorname{C}(\R) \Rightarrow \lambda f \in \operatorname{C}(\R).
				\end{equation*}
				No es difícil ver que con estas definiciones $\operatorname{C}(\R)$  es un $\R$-espacio vectorial.
			\end{ejemplo}
			
            
			
			\medspace
			
			\begin{ejemplo}
            Consideremos el conjunto de los números reales positivos:
            \[
            R_{>0}=\{x\in\R : x>0\}.
            \]
            Entonces $V=R_{>0}$ es un $\R$-espacio vectorial con la suma $\oplus:V\times V\to V$ y  y el producto $\odot:\R\times V\to V$ dados por
            \begin{align*}
            x\oplus y&=x\cdot y, & c\odot x&=x^c,
            \end{align*}
            para cada $c\in\R$, $x,y\in R_{>0}$.
Es fácil ver que los axiomas \textbf{S1.} y \textbf{S2.} sobre la conmutatividad y asociatividad, respectivamente, de la suma $\oplus$ se siguen de las propiedades de conmutatividad y asociatividad del producto $\cdot$ en $\R$.

La existencia del vector $\bf0$ del axioma \textbf{S3.}, neutro para la suma $\oplus$, requiere de cierto cuidado. Notar que este vector debe ser un elemento $\bf0$ en $V$ (un real positivo) que cumpla $x\oplus \bf 0=x$ para todo $x$. Ahora, $x\oplus \bf0=x\cdot \bf0$ por definición, de donde se desprende que debemos tomar $bf0=1$. Es decir, {\it el vector cero es el número 1}. De manera similar, se sigue que el opuesto indicado en el  axioma \textbf{S4.} debe estar dado por $-x=x^{-1}$.

Finalmente, las propiedades de los axiomas \textbf{P1.}, \textbf{P2.}, \textbf{D1.} y \textbf{D2.} se siguen de las propiedades conocidas de la exponenciación en $\R$ y quedan a cargo del lector, como un interesante desafío para terminar de comprender este ejemplo.
\end{ejemplo}            
            
			\vskip .3cm
			
			
			\begin{proposicion}\label{prop-basicas-ev} Sea $V$ un espacio vectorial sobre el cuerpo $\K$. Entonces,
				\begin{enumerate}
					\item\label{c*0=0} $\lambda.0=0$, para todo $\lambda \in \K$;
					\item\label{0*c=0} $0.v = 0$, para todo $v \in V$;
					\item\label{c*v=0} si $\lambda \in \K$, $v \in V, v\ne 0$ y $\lambda.v=0$,  entonces $\lambda =0$;
					\item\label{-1*v=-v} $(-1).v = -v$, para todo $v \in V$.
				\end{enumerate}
			\end{proposicion}
			\begin{proof} Tanto la prueba de (1), como la de (2) son similares a la demostración de que $0.a=0$  en $\mathbb Z$ (o en  $\R$).
				 
				(1) Como $0$ es el elemento neutro de la suma en $V$, entonces $0 = 0 +0$, luego 
				\begin{equation*}
					\begin{array}{rllll}
					\lambda.0 &=& \lambda.(0+0)&\quad&\text{(propiedad distributiva $\Rightarrow$)}  \\
					\lambda.0&=& \lambda.0 + \lambda.0 && \text{(sumando a la izquierda $-\lambda.0$ $\Rightarrow$)} \\
				\lambda.0 - \lambda.0 &=& \lambda.0 + \lambda.0 -\lambda.0 && \text{(opuesto $\Rightarrow$)}\\
					0 &=& \lambda.0 +0  &&\text{(elemento neutro $\Rightarrow$)}\\
					0 &=& \lambda.0.
					\end{array}
				\end{equation*}
				
				(2) Análoga a (1).
				
				(3) Supongamos que  $\lambda.v=0$ y  $\lambda \ne 0$, entonces, por (1), $ \lambda^{-1}(\lambda.v)=0$, pero $\lambda^{-1}(\lambda.v) = (\lambda^{-1}\lambda).v = 1.v = v$. Luego $0=v$,  que contradice la hipótesis. El absurdo vino de suponer que $\lambda \ne 0$.
				
				(4)  $(-1).v +v = (-1).v +1.v = (-1+1).v$,  esto último es por la propiedad distributiva. Ahora bien $(-1+1).v = 0.v \overset{(2)}{=} 0$. Es decir   $(-1).v +v = 0$ y por lo tanto  $(-1).v$ es el opuesto de $v$ (que es $-v$).  
			
			\end{proof}
		
		\begin{definicion}
			Sea $V$ espacio vectorial sobre $\K$ y $v_1,\ldots,v_n$ vectores en $V$. Dado $v \in V$, diremos que\textit{ $v$  es combinación lineal de los $v_1,\ldots,v_n$ }\index{combinación lineal} si existen escalares $\lambda_1,\ldots,\lambda_n$ en $\K$,  tal que 
			$$
			v = \lambda_1v_1+\cdots+\lambda_nv_n.
			$$
		\end{definicion}
	
	\begin{ejemplo}
		\begin{enumerate}
			\item Sean $v_1 = (1,0)$, $v_2 = (0,1)$ en $\C^2$ ¿es $v = (i,2)$ combinación lineal de $v_1,v_2$? La respuesta es sí, pues 
			$$
			v = iv_1+2v_2.
			$$
			Observar además que es la única combinación lineal posible, pues si 
			$$
			v = \lambda_1v_1+ \lambda_2 v_2,
			$$
			entonces
			$$
			(i,2) = (\lambda_1,0)+(0,\lambda_2) = (\lambda_1,\lambda_2),
			$$
			luego $\lambda_1=i$ y $\lambda_2= 2$.
			
			Puede ocurrir que un vector sea combinación lineal de otros vectores de varias formas diferentes. Por ejemplo,   si   $v = (i,2)$ y $v_1 = (1,0)$, $v_2 = (0,1)$, $v_3 = (1,1)$,  tenemos que
			\begin{align*}
				v &= iv_1+2v_2+0v_3,\quad\quad\quad\text{y también}\\
				v &= (i-1)v_1 + v_2 + v_3.  
			\end{align*}
			
			\item Sean $(0,1,0)$, $(0,1,1)$ en $\C^3$ ¿es $(1,1,0)$ combinación lineal de $(0,1,0)$, $(0,1,1)$? La respuesta es no, pues si 
			$$
			(1,1,0) = \lambda_1(0,1,0)+ \lambda_2(0,1,1) = (0,\lambda_1,0)+ (0,\lambda_2,\lambda_2) = (0,\lambda_1+\lambda_2,\lambda_2),
			$$ 
			luego, la primera coordenada nos dice que $1=0$, lo cual es absurdo. Por lo tanto,  no existe un par $\lambda_1,\lambda_2 \in \K$ tal que  $(1,1,0) = \lambda_1(0,1,0)+ (0,1,1)$.
			
		\end{enumerate}
	\end{ejemplo}
		
		\vskip .3cm
		
		\begin{observacion}
			La pregunta de si un vector $v =(b_1,\ldots,b_m) \in \K^m$ es combinación lineal de vectores $v_1,\ldots,v_n \in \K^m$ se resuelve con un sistema de ecuaciones lineales: si 
			$$
			v_i = (a_{1i},\ldots,a_{mi}), \quad \text{para $1 \le i \le n$,}
			$$
			entonces $v = \lambda_1v_1 + \cdots +\lambda_nv_n$ se traduce, en coordenadas, a
			\begin{align*}
				(b_1,\ldots,b_m) &= \lambda_1(a_{11},\ldots,a_{m1}) + \cdots +\lambda_n(a_{1n},\ldots,a_{mn}) \\
				&= (\lambda_1a_{11} + \cdots+ \lambda_na_{1n}, \ldots, \lambda_1a_{m1} + \cdots+ \lambda_na_{mn}).
			\end{align*}
			Luego, $v$  es combinación lineal de los vectores $v_1,\ldots,v_n \in \K^n$ si y sólo si el tiene solución el siguiente sistema de ecuaciones con incógnitas $\lambda_1,\ldots, \lambda_n$: 
			\begin{equation*}
			\begin{matrix}
			a_{11}\lambda_1& + &a_{12}\lambda_2& + &\cdots& + &a_{1n}\lambda_n &= &b_1\\
			\vdots&  &\vdots& &&  &\vdots \\
			a_{m1}\lambda_1& + &a_{m2}\lambda_2& + &\cdots& + &a_{mn}\lambda_n &=&b_m
			\end{matrix}.
			\end{equation*}
		\end{observacion}
		
		
		\begin{ejemplo} Demostrar que $(5,12,5)$  es combinación lineal de los vectores $(1,-5,2), (0,1,-1), (1,2,-1)$. Planteamos la ecuación:
			\begin{align*}
				(5,12,5) &= \lambda_1(1,-5,2)+\lambda_2 (0,1,-1)+\lambda_3 (1,2,-1) 
				\\&= (\lambda_1+\lambda_3,-5\lambda_1+\lambda_2+2\lambda_3,2\lambda_1-\lambda_2-\lambda_3).
			\end{align*}
			Por consiguiente,  esta ecuación se resuelve con el siguiente sistema de ecuaciones
			\begin{align*}
				\lambda_1+\lambda_3 &= 5 \\
				-5\lambda_1+\lambda_2+2\lambda_3 &= 12 \\
				2\lambda_1-\lambda_2-\lambda_3 &= 5.
			\end{align*}
			Ahora bien, usando el método de Gauss
								\begin{multline*}
			\left[\begin{array}{rrr|r}1 & 0 & 1 &  5 \\ -5 & 1 & 2 &  12 \\	2 & -1 & -1 &  5  \end{array}\right]
			\stackrel[F_3 -2F_1]{F_2 + 5F_1}{\longrightarrow} 
			\left[\begin{array}{rrr|r}1 & 0 & 1 &  5 \\ 0 & 1 & 7 &  37 \\	0 & -1 & -3 &  -5  \end{array}\right]
			\stackrel{F_3+F_1}{\longrightarrow} 
			\left[\begin{array}{rrr|r}1 & 0 & 1 &  5 \\ 0 & 1 & 7 &  37 \\	0 & 0 & 4 & 32  \end{array}\right]
			\\
			\stackrel{F_3/4}{\longrightarrow} 
			\left[\begin{array}{rrr|r}1 & 0 & 1 & 5 \\ 0 & 1 & 7 & 37 \\	0 & 0 & 1& 8  \end{array}\right]
			\stackrel[F_2 -7F_3]{F_1 - F_3}{\longrightarrow}
			\left[\begin{array}{rrr|r}1 & 0 & 0 & -3 \\ 0 & 1 & 0 &  -19 \\	0 & 0 & 1& 8  \end{array}\right].
			\end{multline*}	
			Luego $\lambda_1= -3$, $\lambda_2 = -19$ y $\lambda_3=8$,  es decir
			\begin{align*}
			(5,12,5) &= -3(1,-5,2)-19 (0,1,-1)+8 (1,2,-1).
			\end{align*}
		\end{ejemplo}
\end{section}
	
\begin{section}{Subespacios vectoriales}
	\begin{definicion}
		Sea $V$ un espacio vectorial sobre $\K$. diremos que $W \subset V$ es \textit{subespacio de $V$}\index{subespacio} si $W \not= \emptyset$ y
		\begin{enumerate}
			\item[(a)] si para cualesquiera $w_1,w_2 \in W$, se cumple que $w_1+w_2 \in W$ y
			\item[(b)] si $\lambda \in \K$ y  $w \in W$, entonces $\lambda w \in W$.
		\end{enumerate}
	\end{definicion}

	\begin{observacion} Si $W$ subespacio de $V$,  entonces $0 \in W$: como  $W \ne \emptyset$, tomo  cualquier $w \in W$ y  por (a) tenemos que $0.w \in W$. Ya vimos, proposición \ref{prop-basicas-ev}(\ref{0*c=0}),  que $0.w =0$ y por lo tanto $0 \in W$. 
	\end{observacion}

	\begin{observacion}
		 Si $W$ subespacio de $V$ y $w_1,\ldots,w_k \in W$,  entonces cualquier combinación lineal de los $w_1,\ldots,w_k$ pertenece a $W$. Es decir, para cualesquiera $\lambda_1,\ldots,\lambda_k \in \K$, se cumple que  $\lambda_1w_1+\cdots\lambda_kw_k \in W$. Esto se debe a que $\lambda_iw_i \in W$ para $1\le i \le k$. Por un argumento inductivo, no es difícil probar que la suma de $k$ términos en $W$ es un elemento de $W$, por lo tanto $\lambda_1w_1+\cdots\lambda_kw_k \in W$.  
	\end{observacion}

	\begin{teorema}
		Sea $V$ un espacio vectorial sobre $\K$ y $W$ subespacio de $V$. Entonces $W$ con las operaciones suma y producto por escalares de $V$ es un espacio vectorial.
	\end{teorema}
	\begin{proof} Debemos verificar las propiedades de la definición de espacio vectorial.
		\begin{enumerate}
			\item[S1.] $w,w' \in W$, entonces $w + w' = w' + w$, pues conmutan en $V$;
			\item[S2.] si $w_1,w_2,w_3 \in W$, entonces $w_1+(w_2+w_3) = (w_1+w_2)+w_3$, pues la suma es asociativa en $V$;
			\item[S3.] ya vimos que $0 \in W$, y como $0$ es el elemento neutro de la suma para $V$, también lo es para $W$.
			\item[S4.] Sea $w \in W$, entonces $(-1)w \in W$ (por SE2). Pero, por proposición \ref{prop-basicas-ev}(\ref{-1*v=-v}), $(-1)w=-w$,  es decir $(-1)w$ es el opuesto aditivo de $w$  en $V$, luego es el opuesto aditivo de $w$ en $W$.
			\item[P1.] $1.v=v$ para todo $v \in V$, luego $1.w =w$ para todo $w \in W$.
			\item[P2.] $\lambda_1(\lambda_2v) = (\lambda_1\lambda_2)v$, para todo $\lambda_1,\lambda_2 \in \K$ y todo $v \in V$, luego $\lambda_1(\lambda_2w) = (\lambda_1\lambda_2)w$, para todo $\lambda_1,\lambda_2 \in \K$ y todo $w \in W$.
			\item[D1.] $\lambda(v_1+v_2) = \lambda v_1 +\lambda v_2$, para todo $\lambda \in \K$ y cualesquiera $v_1,v_2 \in V$, luego   $\lambda(w_1+w_2) = cw_1 +cw_2$, para todo $\lambda \in \K$ y cualesquiera $w_1,w_2 \in W$.
			\item[D2.] $(\lambda_1+\lambda_2)v = \lambda_1v + \lambda_2 v$ para todo $\lambda_1,\lambda_2 \in \K$ y todo $v \in V$, luego $(\lambda_1+\lambda_2)w = \lambda_1w + \lambda_2 w$ para todo $\lambda_1,\lambda_2 \in \K$ y todo $w \in W$.
		\end{enumerate} 
	\end{proof}



\begin{ejemplo} Veremos ahora una serie de ejemplos de subespacios vectoriales.
	\begin{enumerate}
		\item Sea $V$ un $\K$-espacio vectorial, entonces $0$ y $V$ son subespacios vectoriales de $V$. Suelen ser llamados los \textit{subespacios triviales}\index{subespacios triviales} de $V$.
		
		\item Sea $V$ un $\K$-espacio vectorial y sea $v \in V$, entonces
		$$
		W = \{\lambda v: \lambda \in \K \}
		$$
		es un subespacio vectorial. En  efecto
		\begin{enumerate}
			\item si $\lambda_1v,\lambda_2v \in W$, con $\lambda_1,\lambda_2 \in \K$,  entonces $\lambda_1v + \lambda_2v = (\lambda_1+\lambda_2)v \in W$;
			\item  $\lambda_1v \in W$, con $\lambda_1 \in \K$ y $\lambda \in \K$,  entonces $\lambda(\lambda_1v) = (\lambda \lambda_1)v \in W$.
		\end{enumerate}
		El subespacio $W$ suele ser denotado $\K v$.
	
		
		\item Sean $V=\K^n$ y $1\le j \le n$. Definimos 
		$$
		W = \left\{ (x_1,x_2,\ldots,x_n): x_i \in \K\; (1 \le i \le n), x_j =0\right\}.
		$$
		Es decir $W$  es el subconjunto de $V$ de todas las $n$-uplas con la coordenada $j$ igual a 0. Por ejemplo  si $j=1$ 
		$$
		W = \left\{ (0,x_2,\ldots,x_n): x_i \in \K \;(2 \le i \le n)\right\}.
		$$
		Veamos que este último es un subespacio: 
		\begin{enumerate}
			\item si $(0,x_2,\ldots,x_n), (0,y_2,\ldots,y_n) \in W$,  entonces
			$(0,x_2,\ldots,x_n)+ (0,y_2,\ldots,y_n) = (0,x_2+y_2,\ldots,x_n+y_n)$, el cual pertenece a $W$. 
			\item 	Por otro lado, si $\lambda \in \K$, $\lambda(0,x_2,\ldots,x_n) = (0,\lambda x_2,\ldots,\lambda x_n) \in W$.
		\end{enumerate}
		La demostración para $j >1$ es completamente análoga. 
		\item El  conjunto $\R[x] = \left\{P(x): P(x) \text{ es polinomio en $\R$ } \right\}$, es subespacio de $\operatorname{F}(\R)$, pues $\R[x] \subset \operatorname{F}(\R)$ y las operaciones de suma y producto por un escalar son cerradas en $\R[x]$.
		\item De forma análoga, el conjunto $\R[x]$ es subespacio de $\operatorname{C}(\R)$,  el espacio de funciones continuas de  $\R$.
		\item Sea $W= \left\{A \in M_n(\K): A^\t = A \right\}$. Es claro que  $A \in W$ si y sólo si $[A]_{ij} = [A]_{ji}$. Veamos que  $W$ es subespacio de $M_n(\K)$: 
		\begin{enumerate}
			\item sean $A=[a_{ij}]$, $B= [b_{ij}]$ tales que $A=A^\t$ y $B=B^\t$, entonces debemos verificar que $A+ B \in W$,  es decir que la transpuesta de $A+B$  es la misma matriz: ahora bien, $[A+B]_{ij} = a_{ij}+b_{ij}$, luego 
			\begin{equation*}
				[(A+B)^\t]_{ij} =a_{ji}+b_{ji} = [A]_{ji} + [B]_{ji} = [A]_{ij} + [B]_{ij} = [A+B]_{ij},
			\end{equation*}
			por lo tanto $A+B \in W$.
			\item Si $\lambda \in \K$, $[\lambda A]_{ij} = \lambda a_{ij}$, luego, 
			$$
			[\lambda A^\t]_{ij} = \lambda a_{ji} = \lambda a_{ij} = [\lambda A]_{ij},
			$$
			por lo tanto $\lambda A \in W$.
		\end{enumerate} 	

		\item Sea $A \in M_{m \times n}(\K)$. Si $x = (x_1,\ldots,x_n) \in \K^n$,  entonces $Ax$ denotará la multiplicación de $A$ por la matriz columna formada por $x_1,\ldots,x_n$,  es decir
		$$
		Ax = A\begin{bmatrix} x_1 \\ \vdots \\ x_n\end{bmatrix}.
		$$
		Sea 
		$$
		W = \left\{x \in \K^n: Ax=0 \right\}.
		$$
		Es decir, $W$  es el subconjunto de $\K^n$ de las soluciones del sistema $Ax=0$. Entonces, $W$  es un subespacio de $\K^n$:
		\begin{enumerate}
			\item si $x,y \in W$, es decir si $Ax=0$ y $Ay=0$,  entonces $A(x+y) = Ax + Ay = 0 + 0 = 0$, luego , $x+y \in W$;
			\item si $\lambda \in \K$ y $x \in W$, entonces $A(\lambda x) = \lambda Ax =\lambda.0 =0$, luego $\lambda x \in W$. 
		\end{enumerate}
	\end{enumerate}
\end{ejemplo}

\vskip .5cm

	\begin{teorema}
	Sea $V$ un espacio vectorial sobre $\K$ y sean $v_1,\ldots,v_k \in V$. Entonces
	$$
	W = \{\lambda_1v_1+\cdots+\lambda_kv_k: \lambda_1,\ldots,\lambda_k \in \K \}
	$$
	es un subespacio vectorial. Es decir,  el conjunto de las combinaciones lineales de $v_1,\ldots,v_k$ es un subespacio vectorial.
	\end{teorema}
	\begin{proof}
		\begin{enumerate}
			\item[(a)] Sean $\lambda_1v_1+\cdots+\lambda_kv_k$ y $\mu_1v_1+\cdots+\mu_kv_k$ dos combinaciones lineales de $v_1,\ldots,v_k$, entonces 
			\begin{align*}
				(\lambda_1v_1+\cdots+\lambda_kv_k)+(\mu_1v_1+\cdots+\mu_kv_k) &=  \lambda_1v_1+\mu_1v_1+\cdots+\lambda_kv_k+\mu_kv_k\\
				&= (\lambda_1+\mu_1)v_1+\cdots+(\lambda_k+\mu_k)v_k,
			\end{align*}
			que es  una combinación lineal de  $v_1,\ldots,v_k$ y por lo tanto pertenece a $W$.
			\item[(b)] Si $\lambda \in \K$ y $\lambda_1v_1+\cdots+\lambda_kv_k$  es una combinación lineal de $v_1,\ldots,v_k$,  entonces
			\begin{align*}
			\lambda (\lambda_1v_1+\cdots+\lambda_kv_k) &=  \lambda(\lambda_1v_1)+\cdots+\lambda(\lambda_kv_k)\\
			&= (\lambda\lambda_1)v_1+\cdots+(\lambda\lambda_k)v_k,
			\end{align*}
				que es  una combinación lineal de  $v_1,\ldots,v_k$ y por lo tanto pertenece a $W$.
		\end{enumerate}
	\end{proof}
	


	\begin{definicion}
	Sea $V$ un espacio vectorial sobre $\K$ y sean $v_1,\ldots,v_k \in V$. Al  subespacio vectorial $	W = \{\lambda_1v_1+\cdots+\lambda_kv_k: \lambda_1,\ldots,\lambda_k \in \K \}$ de las combinaciones lineales de $v_1,\ldots,v_k$ se lo denomina \textit{subespacio generado por $v_1,\ldots,v_k$}\index{subespacio generado} y se lo denota  
	\begin{equation*}
		\begin{array}{rcll}
		W &=& \langle v_1,\ldots,v_k \rangle, &\quad \text{ o } \\
		W &=&  \operatorname{gen}\left\{ v_1,\ldots,v_k\right\} &\quad \text{ o } \\
		W &=&  \operatorname{span}\left\{ v_1,\ldots,v_k\right\}.&
		\end{array}
	\end{equation*}
	Además, en este caso, diremos que el conjunto $S = \left\{ v_1,\ldots,v_k \right\}$ \textit{genera} al subespacio $W$.
	
\end{definicion}

\vskip.5cm

		\begin{teorema}
			Sea $V$ un espacio vectorial sobre $\K$. Entonces la intersección de subespacios vectoriales es un subespacio vectorial. 
		\end{teorema}
	\begin{proof}
		Sea $\{W_i \}_{i \in I}$ una familia   de subespacios vectoriales y sea 
		$$W = \bigcap_{i\in I} W_i,$$
		entonces
		\begin{enumerate}
			\item[(a)] si $w_1,w_2 \in W$, tenemos que $w_1, w_2 \in W_i$ para todo $i \in I$, luego, como $W_i$ es subespacio vectorial,  $w_1+ w_2 \in W_i$ para todo $i \in I$, por lo tanto $w_1+ w_2 \in W$;
			\item[(b)] si  $\lambda \in \K$ y $w \in W$,  $w \in W_i$ para todo $i \in I$ y, por lo tanto, $cw \in W_i$ para todo $i \in I$. En consecuencia $w \in W$.
		\end{enumerate}
	\end{proof}
		
		
		\begin{obs}
			Si $V$ es un $\K$-espacio vectorial, $S$ y $T$ subespacios de $V$, entonces $S \cup T$ no es necesariamente un subespacio de $V$. En efecto, consideremos en $\R^2$ los subespacios 
			$S = \R(1,0)$ y $T = \R(0,1)$. 	Observamos que $(1,0)\in  S$ y $(0,1) \in  T$; luego, ambos  pertenecen a $S \cup T$. Pero $(1,0) + (0,1) = (1,1) \not\in S \cup T$, puesto que $(1,1) \not\in S$ y $(1,1) \not\in T$.
		\end{obs}
	
	\begin{teorema}
		Sea $V$ un espacio vectorial sobre $\K$ y sean $v_1,\ldots,v_k \in V$. Entonces,  la intersección de todos los subespacios vectoriales que contienen  a $v_1,\ldots,v_k$ es igual a $\langle v_1,\ldots,v_k \rangle$.	
	\end{teorema}	
	\begin{proof}
		Denotemos $W_1 = \langle v_1,\ldots,v_k \rangle$ y $W_2$ la intersección de todos los subespacios vectoriales que contienen  a $v_1,\ldots,v_k$. Probaremos que  $W_1 = W_2$ con la doble inclusión,  es decir probando que $W_1 \subseteq W_2$ y  $W_2 \subseteq W_1$.
		
		\vskip.3cm
		
		($W_1 \subseteq W_2$). Sea $W$  subespacio vectorial que contiene $v_1,\ldots,v_k$. Como $W$ es subespacio,  entonces  $W$ contiene a cualquier combinación lineal de los $v_1,\ldots,v_k$, por lo tanto $W$ contiene a $W_1$. Es decir, cualquier subespacio que contiene a $v_1,\ldots,v_k$,  también contiene a  $W_1$, por lo tanto la intersección de todos los subespacios que  contienen a $v_1,\ldots,v_k$, contiene a $W_1$. Luego $W_2 \supseteq W_1$.
		
		\vskip.3cm
		
		($W_2 \subseteq W_1$). $W_1$ es un subespacio que contiene a $v_1,\ldots,v_k$, por lo tanto  la intersección de todos los subespacios que  contienen a $v_1,\ldots,v_k$  está contenida en $W_1$. Es decir, $W_2 \subseteq W_1$.
	\end{proof}
		
		
	\begin{definicion} Sea $V$ un espacio vectorial sobre $\K$ y sean $S_1,\ldots,S_k$ subconjuntos  de $V$.
		definimos 
		\begin{equation*}
			S_1+  \cdots +S_k := \left\{s_1+\cdots+s_k: s_i \in S_i, 1 \le i \le k \right\},
		\end{equation*}
		el conjunto \textit{suma de los  $S_1,\ldots,S_k$.}
	\end{definicion}	

	\begin{teorema}
		 Sea $V$ un espacio vectorial sobre $\K$ y sean $W_1,\ldots,W_k$ subespacios  de $V$. Entonces $W= W_1+\cdots+W_k$ es un subespacio de $V$.
	\end{teorema}
	\begin{proof}
		Sean $v = v_1+\cdots+v_k$ y $w = w_1+\cdots+w_k$ en $W$ y $\lambda \in \K$. Entonces
		\begin{enumerate}[label=(\alph*),ref=\alph*]
			\item $v+w =  (v_1+w_1)+\cdots+(v_k+w_k) \in W_1+\cdots+W_k$, pues como $W_i$ es subespacio de $V$, tenemos que $v_i+w_i \in W_i$.
			\item  $\lambda v = \lambda(v_1+\cdots+v_k) = \lambda v_1+\cdots+\lambda v_k \in W_1+\cdots+W_k$, pues como $W_i$ es subespacio de $V$, tenemos que $\lambda v_i \in W_i$.
		\end{enumerate}
	\end{proof}

	\begin{proposicion}
		Sea $V$ un espacio vectorial sobre $\K$ y sean $v_1,\ldots,v_r$ elementos de   de $V$. Entonces
		\begin{equation*}
			\langle v_1,\ldots,v_r \rangle = \langle v_1 \rangle+ \cdots + \langle v_r \rangle.
		\end{equation*}
	\end{proposicion}
	\begin{proof}
		Probemos el resultado viendo que los dos conjuntos se incluyen mutuamente.
		
		($\subseteq$) Sea $w \in \langle v_1,\ldots,v_r \rangle$, luego $w = \lambda_1 v_1 +\cdots+ \lambda_r v_r$. Como $ \lambda_i v_i \in \langle v_i \rangle$, $1 \le i \le r$ ,  tenemos que  $w \in \langle v_1 \rangle+ \cdots + \langle v_r \rangle$.  En  consecuencia, $\langle v_1,\ldots,v_r \rangle \subseteq \langle v_1 \rangle+ \cdots + \langle v_r \rangle$. 
		
		($\supseteq$) Si $w \in \langle v_1 \rangle+ \cdots + \langle v_r \rangle$, entonces $w = w_1 + \cdots+w_r$ con $w_i \in \langle v_i\rangle$ para todo $i$. Por lo tanto, $w_i = \lambda_i v_i$ para algún $\lambda_i \in \K$ y  $w = \lambda_1 v_1 +\cdots+ \lambda_r v_r \in \langle v_1,\ldots,v_r \rangle $. En  consecuencia, $\langle v_1 \rangle+ \cdots + \langle v_r \rangle \subseteq \langle v_1,\ldots,v_r \rangle$. 
	\end{proof}

	\begin{ejemplo}\label{ejemplos2} Veremos una serie de ejemplos de subespacios,  suma e intersección de subespacios.
		\begin{enumerate}
			\item\label{ejemplos2-1} Sea $\K = \C$ y $V= \C^5$. Consideremos los vectores
			\begin{equation*}
				v_1 = (1,2,0,3,0), \qquad v_2 = (0,0,1,4,0), \qquad v_3 = (0,0,0,0,1),
			\end{equation*}
			y sea $W= \langle v_1,v_2,v_3 \rangle$. 
			
			Ahora bien, $w \in W$, si y sólo si $w = \lambda_1 v_1+\lambda_2 v_2+\lambda_3 v_3$, con $\lambda_1,\lambda_2,\lambda_3 \in \C$. Es decir
			\begin{align*}
				w &= \lambda_1 (1,2,0,3,0)+\lambda_2 (0,0,1,4,0)+\lambda_3 (0,0,0,0,1) \\
				&=  (\lambda_1,2\lambda_1,0,3\lambda_1,0)+ (0,0,\lambda_2,4\lambda_2,0)+ (0,0,0,0,\lambda_3) \\ &=(\lambda_1,2\lambda_1,\lambda_2,3\lambda_1+4\lambda_2,\lambda_3)
			\end{align*} 
			Luego,  también podríamos escribir
			\begin{equation*}
				W = \left\{(x_1,x_2,x_3,x_4,x_5)\in \C^5: x_2 = 2x_1, x_4 = 3x_1+4x_3 \right\}.
			\end{equation*}
			
			\item Sea $V = M_2(\C)$ y sean 
			\begin{equation*}
				W_1 = \left\{\begin{bmatrix} x&y\\z&0 \end{bmatrix}: x,y,z \in \C \right\}, 
				\qquad W_2 = \left\{\begin{bmatrix} x&0\\0&y \end{bmatrix}: x,y \in \C \right\}.
			\end{equation*} 
			Es claro que cada uno de estos conjuntos es un subespacio, pues,
			$$
			W_1 = \C \begin{bmatrix} 1&0\\0&0 \end{bmatrix} +
			\C \begin{bmatrix} 0&1\\0&0 \end{bmatrix} +
			\C \begin{bmatrix} 1&0\\0&0 \end{bmatrix}, \quad 
			W_2 = \C \begin{bmatrix} 1&0\\0&0 \end{bmatrix} +
			\C \begin{bmatrix} 0&0\\0&1 \end{bmatrix}. 
			$$
			Entonces, $W_1 + W_2 = V$. En  efecto, sea 
			$\begin{bmatrix} a&b\\c&d\end{bmatrix} \in V$, entonces
			$$
			\begin{bmatrix} a&b\\c&d\end{bmatrix} =  \begin{bmatrix} x_1&y_1\\z_1&0 \end{bmatrix} + 
			\begin{bmatrix} x_2&0\\0&y_2 \end{bmatrix} =
			\begin{bmatrix} x_1+x_2&y_1\\z_1&y_2 \end{bmatrix},
			$$
			y esto se cumple  tomando $x_1 = a, x_2 =0,  y_1 = b, z_1= c, y_2 =d$.
			
			Por otro lado
			\begin{align*}
				W_1 \cap W_2 &= \left\{\begin{bmatrix} a&b\\c&d\end{bmatrix}:  \begin{bmatrix} a&b\\c&d\end{bmatrix} \in W_1, \begin{bmatrix} a&b\\c&d\end{bmatrix} \in W_2\right\} \\
				&= \left\{\begin{bmatrix} a&b\\c&d\end{bmatrix}:  \begin{matrix}
				(a =x_1, b = y_1, c = z_1,d =0)\wedge \\(a = x_2, b=c=0, d= y_2)
				\end{matrix} \right\} \\
				&= \left\{\begin{bmatrix} a&0\\0&0\end{bmatrix}:  a \in \C\right\} .
			\end{align*}
		\end{enumerate}
	\end{ejemplo}

\vskip .3cm

	\begin{definicion}\label{espacio-fila-columna}
		Sea $A = [a_{ij}] \in M_{m \times n}(\K)$. El \textit{vector fila $i$} es el vector  $(a_{i1},\ldots,a_{in}) \in \K^n$. El \textit{espacio fila} de $A$ es el subespacio de $\K^n$ generado por los $m$ vectores fila de $A$.  De forma análoga, se define  el vector columna $i$ al vector $(a_{11},\ldots,a_{mi}) \in \K^m$ y  el \textit{espacio columna} de $A$ es el subespacio de $\K^m$ generado por los $n$ vectores columna de $A$.  
	\end{definicion}

	\begin{ejemplo}
		Sea
		$$
		A = \begin{bmatrix}
		1&2&0&3&0\\ 0&0&1&4&0 \\0&0&0&0&1
		\end{bmatrix},
		$$
		entonces, como vimos en el ejemplo \ref{ejemplos2}(\ref{ejemplos2-1}), el espacio fila es
		\begin{equation*}
		W = \left\{(x_1,x_2,x_3,x_4,x_5)\in \C^5: x_2 = 2x_1, x_4 = 3x_1+4x_3 \right\}.
		\end{equation*}
	\end{ejemplo}
\end{section}		
		
\begin{section}{Bases y dimensión}
	\begin{definicion}
		Sea $V$ un espacio vectorial sobre $\K$. Un subconjunto $S$ de $V$ se dice \textit{linealmente dependiente}\index{linealmente dependiente} (o simplemente, \textit{dependiente}) si existen vectores distintos $v_1,\ldots,v_n \in S$  y escalares $\lambda_1,\ldots,\lambda_n$ de $\K$, no todos nulos, tales que 	
		\begin{equation*}
			\lambda_1v_1+\cdots+\lambda_nv_n=0.
		\end{equation*}
		Un conjunto que no es linealmente dependiente se dice \textit{linealmente independiente}\index{linealmente independiente} (o simplemente, \textit{independiente}).
		
		Si el conjunto $S$ tiene solo un número finito de vectores $v_1,\ldots,v_n$, se dice,
		a veces, que los $v_1,\ldots,v_n$ son dependientes (o independientes), en vez de decir
		que $S$ es dependiente (o independiente).
		
		Cuando un conjunto $S$ es dependiente (independiente), diremos,  para simplificar,  que $S$ es LD (resp. LI).
	\end{definicion}

\begin{observacion} Por definición, un conjunto $S = \left\{v_1,\ldots,v_n \right\}$ es independiente si se cumple cualquiera de las dos afirmaciones siguientes:
	\begin{enumerate}[label=(\alph*),ref=\alph*]
		\item\label{li1} dados $\lambda_1,\ldots,\lambda_n$ en $\K$ tal que $\lambda_i \ne 0$ para algún $i$,  entonces  $\lambda_1v_1+\cdots+\lambda_nv_n\not=0$, o ,
		\item \label{li2}dados $\lambda_1,\ldots,\lambda_n$ en $\K$ tal que $\lambda_1v_1+\cdots+\lambda_nv_n=0$,  entonces $0=\lambda_1=\cdots=\lambda_n$
	\end{enumerate}
	El enunciado (\ref{li1}) se deduce intuitivamente negando  la definición de linealmente dependiente y el resultado (\ref{li2}) es el contrarrecíproco  de (\ref{li1}).
	
	
Para los interesados, esto es un ejercicio de lógica: ser linealmente dependiente se puede enunciar 
$$
(\exists \lambda_1,\ldots,\lambda_n: (\exists i: \lambda_i \ne 0) \wedge (\lambda_1v_1+\cdots+\lambda_nv_n=0 )),
$$
luego, su negación  (es decir ser LI), es 
$$
(\forall \lambda_1,\ldots,\lambda_n:  (\exists i: \lambda_i \ne 0) \Rightarrow \lambda_1v_1+\cdots+\lambda_nv_n\ne 0),
$$
(observar  que $\neg(P \wedge Q) \,\equiv\, \neg P \vee \neg Q \,\equiv\, P \Rightarrow \neg Q$). 

Como $(P \Rightarrow Q) \equiv (\neg Q \Rightarrow \neg P)$, la propiedad anterior  es equivalente a
$$
(\forall \lambda_1,\ldots,\lambda_n:  (\lambda_1v_1+\cdots+\lambda_nv_n=0 ) \Rightarrow (\forall i: \lambda_i = 0)),
$$	
	
\end{observacion}

\vskip .5cm

Las siguientes afirmaciones son consecuencias fáciles de la definición.
\begin{enumerate}
	\item Todo conjunto que contiene un conjunto linealmente dependiente es linealmente dependiente.
	\item  Todo subconjunto de un conjunto linealmente independiente es linealmente independiente.
	\item  Todo conjunto que contiene el vector $0$ es linealmente dependiente; en efecto, $1.0 = 0$.
\end{enumerate}

\begin{ejemplo}
	En $\R^3$  los vectores $(1,-1,1)$ y $(-1,1,1)$ son LI, pues si $\lambda_1(1,-1,1)+\lambda_2(-1,1,1) =0$,  entonces $0= (\lambda_1,-\lambda_1,\lambda_1)+(-\lambda_2,\lambda_2,\lambda_2) =  (\lambda_1-\lambda_2,-\lambda_1+\lambda_2,\lambda_1+\lambda_2)$, y esto es cierto si 
	\begin{equation*}
		\begin{array}{rcl}
		\lambda_1-\lambda_2 &=& 0 \\
		-\lambda_1+\lambda_2 &=& 0 \\
		\lambda_1+\lambda_2 &=& 0 
		\end{array}.
	\end{equation*} 
	Luego $\lambda_1 = \lambda_2$ y $\lambda_1 = -\lambda_2$, por lo tanto $\lambda_1 = \lambda_2 =0$. Es decir,  hemos visto que 
	$$
	\lambda_1(1,-1,1)+\lambda_2(-1,1,1) =0 \quad \Rightarrow \quad\lambda_1 = \lambda_2 =0,
	$$
	y, por lo tanto,  $(1,-1,1)$ y $(-1,1,1)$ son LI.
\end{ejemplo}

\begin{ejemplo} Sea $\K$  cuerpo. En $\K^3$ los vectores
	\begin{align*}
	v_1 &= (\;\;3,\;0,-3) \\
	v_2 &= (-1,\;1,\;\;2) \\
	v_3 &= (\;\;4,\;2,-2) \\
	v_4 &= (\;\;2,\;1,\,\;\;1)
	\end{align*}
	
	son linealmente dependientes, pues
	$$
	2v_1+2v_2 -v_3 +0.v_4 =0.
	$$
	Por otro lado, los vectores
	\begin{align*}
	e_1 &= (1,0,0) \\
	e_2 &= (0,1,0) \\
	e_3 &= (0,0,1) 
	\end{align*}
	son linealmente independientes.
\end{ejemplo}


\begin{observacion}
	En  general,  en $\K^m$, si queremos determinar si  $v_1,\ldots,v_n$ es LI, planteamos la ecuación  
	\begin{equation*}
	\lambda_1v_1+\cdots+\lambda_nv_n=(0,\ldots,0),
	\end{equation*}
	que, viéndola coordenada a coordenada, es equivalente a un sistema de $m$ ecuaciones lineales con  $n$ incógnitas (que son $\lambda_1,\ldots,\lambda_n$). Si  la única solución es la trivial entonces $v_1,\ldots,v_n$ es LI. Si hay alguna solución no trivial, entonces $v_1,\ldots,v_n$ es LD. 
\end{observacion}
 
 \begin{definicion}
 	Sea $V$ un espacio vectorial. Una \textit{base}\index{base de un espacio vectorial} de $V$ es un conjunto $\mathcal B \subseteq V$ tal que
 	\begin{enumerate}
 		\item $\mathcal B$ genera a $V$, y
 		\item $\mathcal B$ es LI.
 	\end{enumerate}
 	 El espacio $V$ es de \textit{dimensión finita} si tiene una base finita,  es decir con  un número finito de elementos.
 \end{definicion}



\begin{ejemplo}[{\sc Base canónica de $\K^n$}] Sea el espacio vectorial $\K^n$ y sean
	\begin{equation*}
	\begin{array}{rcl}
	e_1 &=& (1,0,0,\ldots,0) \\
	e_2 &=& (0,1,0,\ldots,0) \\
	&&\qquad.\,.\,.\,.\,.\,.\,\\ 
	e_n&=& (0,0,0,\ldots,1)
	\end{array}
	\end{equation*}
	($e_i$ es el vector con todas sus coordenadas iguales a cero,  excepto  la coordenada $i$ que vale 1). Entonces veamos que  $e_1,\ldots,e_n$ es una base de $\K^n$.
	\begin{enumerate}
		\item Si $(x_1,\ldots,x_n) \in \K^n$,  entonces
		$$
		(x_1,\ldots,x_n) = x_1e_1+\cdots+x_ne_n.
		$$
		Por lo tanto, $e_1,\ldots,e_n$ genera a  $\K^n$.
		\item Si 
		$$
		x_1e_1+\cdots+x_ne_n =0,
		$$
		entonces
		\begin{align*}
			(0,\ldots,0) &= x_1(1,0,\ldots,0)+ x_2(0,1,\ldots,0)+\cdots+x_n(0,0,\ldots,1)\\ 
			&=  (x_1,0,\ldots,0)+(0,x_2,\ldots,0)+\cdots+(0,0,\ldots,x_n)\\ &= (x_1,x_2,\ldots,x_n).
		\end{align*}
		Luego, $x_1= x_2=\cdots=x_n =0$ y por lo tanto $e_1,\ldots,e_n$ es LI.
		
		Para $1 \le i \le n$, al vector $e_i$ se lo denomina el \textit{$i$-ésimo vector canónico}  y a la base $\mathcal B_n = \left\{e_1,\ldots,e_n \right\}$ se la denomina la \textit{base canónica}\index{base canónica} de $\K^n$. 
		
		
	\end{enumerate}
\end{ejemplo}
 
 
 \begin{ejemplo}
 	Sea $P$ una matriz $n \times n$ inversible con elementos en el cuerpo $\K$. Entonces si $C_1,\ldots,C_n$ son los vectores columna de $P$ (ver definición  \ref{espacio-fila-columna}), estos forman una base de $\K^n$. Eso se verá como sigue. Si $X = (x_1,\ldots,x_n) \in \K^n$, lo podemos ver como columna y 
 	$$
 	PX=x_1C_1+\cdots+x_nC_n.
 	$$
 	Como $PX=0$ tiene solo la solución trivial $X= 0$, se sigue que $\{C_1,\ldots,C_n\}$ es un conjunto linealmente independiente. ¿Por qué generan $\K^n$? Sea $Y \in \K^n$, si $X = P^{-1} Y$, entonces $Y = PX$, esto es
 	$$
 	Y=x_1C_1+\cdots+x_nC_n.
 	$$
 	Así, $\{C_1,\ldots,C_n\}$ es una base de $\K^n$.
 \end{ejemplo}


\begin{ejemplo}
	Sea $\K_n[x]$  el conjunto de polinomios de grado menor  que $n$ con coeficientes en $\K$:
	$$
	\K_n[x] = \left\{a_0 + a_1 x + a_2x^2+\cdots+a_{n-1}x^{n-1}: a_0,\ldots,a_{n-1} \in \K  \right\}.
	$$
	Entonces $1,x,x^2,\ldots,x^n$  es una base de $\K_n[x]$. Es claro que los $1,x,x^2,\ldots,x^n$ generan $\K_n[x]$. Por otro lado, si  $\lambda_0 + \lambda_1 x + \lambda_2x^2+\cdots+\lambda_{n-1}x^{n-1} =0$, tenemos que $\lambda_0=\lambda_1 = \lambda_2 =\cdots =\lambda_{n-1} =0$.
\end{ejemplo}

\begin{ejemplo}[{\sc Base canónica de $M_{m \times n}(\K)$}] 
	Sean $1 \le i \le m$, $1\le j \le m$ y $E_{ij} \in M_{m \times n}(\K)$ definida por
	\begin{equation*}
		[E_{ij}]_{kl} = \left\{ 
		\begin{array}{lll}
		1& &\text{si $i=k$ y $j=l$,} \\
		0& &\text{otro caso}. 
		\end{array}
		 \right.
	\end{equation*}
	Es decir $E_{ij}$  es la matriz cuyas entradas son todas iguales a 0,  excepto la entrada $ij$ que vale 1. En el caso $2 \times 2$  tenemos la matrices
	\begin{equation*}
		E_{11} = \begin{bmatrix} 1&0\\0&0\end{bmatrix}, \quad
		E_{12} = \begin{bmatrix} 0&1\\0&0\end{bmatrix}, \quad
		E_{21} = \begin{bmatrix} 0&0\\1&0\end{bmatrix}, \quad
		E_{22} = \begin{bmatrix} 0&0\\0&1\end{bmatrix}.
	\end{equation*}
	Volviendo al caso general,  es claro que si $A= [a_{ij}] \in M_{m \times n}(\K)$,  entonces
	\begin{equation}\label{base-canonica-matrices}
		A = \sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}E_{ij},
	\end{equation}
	luego $\{E_{ij} \}_{1 \le i \le m, 1\le j \le n}$ genera $M_{m \times n}(\K)$. También, por la ecuación (\ref{base-canonica-matrices}), es claro que si $\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}E_{ij}=0$,  entonces $a_{ij}=0$ para todo $i$ y $j$. Luego,  $\{E_{ij} \}_{1 \le i \le m, 1\le j \le n}$ es LI. 
	
	Concluyendo,  $\{E_{ij} \}_{1 \le i \le m, 1\le j \le n}$ es una base de  $M_{m \times n}(\K)$ y se la denomina la \textit{base canónica} de  $M_{m \times n}(\K)$.
\end{ejemplo}



Si $S$ es un conjunto finito denotemos $|S|$  al \textit{cardinal} de  $S$ es decir, la cantidad de elementos de $S$. 

\begin{teorema}\label{indep-menorigual-gen}
	Sea $V$ un espacio vectorial generado por un conjunto finito de vectores $w_1,\ldots,w_m$. Entonces todo conjunto independiente de vectores de $V$ es finito y  contiene a lo más $m$ elementos. 
\end{teorema}
\begin{proof} Sea $V = \la w_1,\ldots,w_m\ra$ y  $S \subset V$.   El  enunciado del teorema es equivalente a decir:
	$$
	\text{si }S \text{ es LI } \Rightarrow |S| \le m.
	$$
	Para demostrar este teorema es suficiente probar el contrarrecíproco del enunciado, es decir:
	$$
	\text{si }|S| > m \Rightarrow S \text{ es LD},
	$$
	o, dicho  de otra forma, todo subconjunto $S$ de $V$ que contiene más de $m$ vectores es linealmente dependiente. Sea $S$ un tal conjunto,  entonces $S = \{v_1,\ldots,v_n\}$ con $n >m$.  Como  $w_1,\ldots,w_m$ generan $V$, existen escalares $a_{ij}$ en $\K$ tales que
	\begin{equation*}
		v_j = \sum_{i=1}^{m}a_{ij}w_i, \qquad (1 \le j \le n).
	\end{equation*}
	Probaremos ahora que existen $x_1,\ldots,x_n \in \K$ no todos nulos, tal que $x_1v_1 + \cdots+x_nv_n =0$. Ahora bien, para cualesquiera $x_1,\ldots,x_n \in \K$ tenemos
	\begin{align*}
		x_1v_1 + \cdots+x_nv_n &= \sum_{j=1}^{n} x_jv_j& \\
		& = \sum_{j=1}^{n}x_j \sum_{i=1}^{m}a_{ij}w_i& \\
		& = \sum_{j=1}^{n} \sum_{i=1}^{m}(x_ja_{ij})w_i& \\ 
		& = \sum_{i=1}^{m}(\sum_{j=1}^{n} x_ja_{ij})w_i.&  (*)
	\end{align*}
	El sistema de ecuaciones
	\begin{equation*}
		\sum_{j=1}^{n} x_ja_{ij} = 0, \qquad (1 \le i \le m) 
	\end{equation*}
	tiene $m$ ecuaciones  y $n > m$ incógnitas, luego, por el teorema \ref{soluciones-m-menor-n}, existen escalares $x_1,\ldots,x_n \in \K$ no todos nulos, tal que $\sum_{j=1}^{n} x_ja_{ij} = 0$, ($1 \le i \le m$) y, por $(*)$, tenemos que  $x_1v_1 + \cdots+x_nv_n =0$. Esto quiere decir que los $v_1,\ldots,v_n$ son LD.
\end{proof}


\begin{corolario}
	Si $V$ es un espacio vectorial de dimensión finita, entonces dos bases cualesquiera de $V$ tienen el mismo número de elementos.
\end{corolario}
\begin{proof}
		Como $V$ es de dimensión finita, tiene una base finita $\mathcal B$  de $m$ vectores, es decir,  $\mathcal B$ es base de $V$ y  $|\mathcal B| = m.$
		Sea $\mathcal B'$  otra base de $V$, como $\mathcal B$  genera $V$ y  $\mathcal B'$ es  un conjunto LI, entonces, por el teorema anterior, $|\mathcal B'| \le m$. Sea $n = |\mathcal B'|$,  entonces $n \le m$. Por otro lado $\mathcal B'$ es base y, por lo tanto,  genera $V$ y  $\mathcal B$ es LI, luego, por el teorema anterior nuevamente,  $m \le n$, y en consecuencia $m=n$.
\end{proof}

\begin{definicion}
	Sea $V$ espacio vectorial de dimensión finita. Diremos que $n$  es \textit{la dimensión de $V$}\index{dimensión de un espacio vectorial} y  denotaremos $\dim V =n$,  si existe una base de $V$  de $n$  vectores. Si $V = \{0\}$,  entonces definimos $\dim V =0$.
\end{definicion}


Como ya demostramos, si $V$  es un espacio vectorial de dimensión finita y $\mathcal B,\mathcal B' $ dos bases de $V$,  entonces $|\mathcal B|=|\mathcal B'|$.



\begin{ejemplo} Sean $m,n \in \mathbb N$. 
	\begin{enumerate}
		\item $\dim \K^n= n$, pues  la base canónica tiene $n$ elementos.
		\item $\dim M_{m \times n}(\K) = mn$, pues la base canónica de $M_{m \times n}(\K)$ tiene $mn$  elementos.  
		\item $\dim \K_n[x] =n$, pues $1,x,x^2,\ldots,x^{n-1}$ es una base.   
	\end{enumerate}
\end{ejemplo}
		
		
		\begin{corolario}
			Sea $V$ un espacio vectorial de dimensión finita y sea $n = \dim V$. 
			Entonces
			\begin{enumerate}
				\item cualquier subconjunto de $V$ con más de $n$ vectores es linealmente
				dependiente;
				\item ningún subconjunto de $V$ con menos de $n$ vectores puede generar $V$.
			\end{enumerate} 
		\end{corolario}
	\begin{proof}
		\begin{enumerate}
			\item Sea $\{v_1,\ldots,v_n\}$ una base de $V$, entonces $v_1,\ldots,v_n$ generan $V$, luego, por el teorema \ref{indep-menorigual-gen}, cualquier subconjunto de $V$ que contenga más de $n$ vectores es LD.
			\item Sea $S$  subconjunto de $V$ con $m < n$ vectores. Si $S$  genera $V$,  entonces todo subconjunto de más de $m$  vectores es LD (teorema \ref{indep-menorigual-gen}), por lo tanto, un subconjunto de $n$ vectores es LD. En  consecuencia, no puede haber una base de $n$  elementos, lo cual contradice la hipótesis.
		\end{enumerate} 
	\end{proof}

	\begin{lema}\label{li+vec=li}
		Sea $S$ un subconjunto linealmente independiente de un espacio vectorial $V$. Supóngase que $w$ es un vector de $V$ que no pertenece al subespacio generado por $S$. Entonces el conjunto que se obtiene agregando $w$  a $S$, es linealmente independiente.
	\end{lema}
	\begin{proof}
		Supóngase que $v_1,\ldots,v_n$  son vectores distintos de $S$ y sean $\lambda_i,\lambda \in \K$  tales que
		\begin{equation}\label{eq-dep-lin}
			\lambda_1 v_1 + \cdots + \lambda_n v_n + \lambda w =0 .
		\end{equation}
		Debemos probar que $\lambda_i=0$, $1 \le i \le n$, y $\lambda =0$.
		Supongamos que $\lambda \ne 0$, entonces podemos dividir la ecuación por $\lambda$ y haciendo  pasaje de término  obtenemos
		$$
		w = \left(-\frac{\lambda_1}{\lambda}\right) v_1 + \cdots   \left(-\frac{\lambda_n}{\lambda}\right) v_n.
		$$
		Luego $w$ estaría  en el subespacio generado por $S$, lo cual contradice la hipótesis. 
		
		Por  lo tanto $\lambda =0$ y, en consecuencia  
		$$
		\lambda_1 v_1 + \cdots + \lambda_n v_n =0.
		$$ 
		Como $S$ es un conjunto linealmente independiente, todo $\lambda_i = 0$. 
	\end{proof}



	\begin{teorema}\label{completar-bases}
	Sea $V$ espacio vectorial de dimensión finita $n$ y $S_0$ un subconjunto LI de $V$. Entonces $S_0$  es finito  y existen $w_1,\ldots,w_m$ vectores en  $V$ tal que  $S_0 \cup \{w_1,\ldots,w_m\}$ es una base de $V$. 
\end{teorema}
\begin{proof}
	Se extiende $S_0$ a una base de $V$, como sigue. Si $S_0$ genera $V$, entonces $S_0$ es una base de $V$ y está demostrado. Si $S_0$ no genera $V$, por el lema anterior se halla un vector $w_1$ en $V$ tal que el conjunto $S_1 = S_0 \cup \{v_1\}$ es independiente.			
	Si $S_1$ genera $V$, está demostrado. Si no, se aplica el lema para obtener un vector $w_2$ en $V$ tal que el conjunto $S_2 = S_1 \cup \{w_2\} = S_0 \cup \{w_1,w_2\}$ es independiente. Si se continúa de este modo, entonces (y en no más de $\dim V$ de etapas) se llega a un conjunto 
	$$
	S_m =  S_0 \cup \{w_1,\ldots,w_m\}
	$$
	que es independiente y que genera $V$ (si no, continuamos), por lo tanto $S_m$ es base de $V$. 
\end{proof}

Es decir, todo subconjunto LI de un espacio vectorial de dimensión finita se puede completar a una base.  

	\begin{corolario}
		Sea $W$ es un subespacio de un espacio vectorial con de dimensión finita $n$ y $S_0$ un subconjunto LI de $W$. Entonces, $S_0$ se puede completar a una base de $W$. 
	\end{corolario}
	\begin{proof}
		Como $S_0$ es un conjunto linealmente independiente de $W$, entonces $S_0$ es también un subconjunto linealmente independiente de $V$; como $V$ es de dimensión finita, $S_0$ no tiene más de $n$ elementos y por lo tanto es finito.
		
		Como $W$ es un espacio vectorial, aplicando el teorema anterior completamos a una base de $W$. 
	\end{proof}

	\begin{corolario}
		Sea $V$ espacio vectorial de dimensión finita y $V \ne \{0\}$, entonces $\dim V >0$.
	\end{corolario}
	\begin{proof}
		Como $V \ne \{0\}$,  existe $v \in V$ con $v \ne 0$. Entonces, $S_0 = \{v\}$ es LI, pues $\lambda v =0 \Rightarrow \lambda =0$. Por el teorema anterior, $S_0$ se extiende a una base $\mathcal B$. Como $|\mathcal B| \ge |S_0| =1$, tenemos que $\dim V >0$.    
	\end{proof}
	
	\begin{corolario}\label{dimw-menor-dimv}
		Si $W$ es un subespacio propio de un espacio vectorial de dimensión finita $V$, entonces $W$ es de dimensión finita y  $\dim W < \dim V$.
	\end{corolario}
	\begin{proof} Si $W = \{0\}$, entonces $\dim W = 0$,  como $W \subsetneq V$,  tenemos que $V$  es no nulo y por lo tanto $\dim W = 0 < \dim V$. 
		
		Si $W \ne \{0\}$,  sea $w \in W$, $w\not=0$. Por el teorema anterior existe una base $\mathcal B$ de $W$ que contiene a $w$. Como $\mathcal B$ es un conjunto LI en $W$, lo es también en $V$ y por teorema  \ref{indep-menorigual-gen}, $ \dim W = |\mathcal B| \le \dim V$.
		
		Como $W$ es un subespacio propio de $V$ existe un vector $v$ en $V$ que
		no está en $W$. Agregando $v$ a cualquier base de $W$ se obtiene un subconjunto
		linealmente a independiente de $V$ (lema \ref{li+vec=li}). Así, $\dim W < \dim V$.
	\end{proof}

	\begin{comment}
	
	\begin{corolario}
	 En un espacio vectorial V de dimensión finita todo conjunto linealmente independiente de vectores es parte de una base.
	\end{corolario}
	\begin{proof}
		Sea $S$ un conjunto LI en $V$, por el teorema \ref{completar-bases}, existen $v_1,\ldots,v_m$ tal que  $S \cup \{v_1,\ldots,v_m\}$ es una base de $V$. 
	\end{proof}

	\begin{corolario}
		Sea A una matriz $n \times n$ sobre el cuerpo $\K$, y supóngase que los vectores fila de $A$ forman un conjunto linealmente independiente de vectores de $\K^n$. Entonces $A$ es inversible.
	\end{corolario}
	\begin{proof}
		Sean $v_1,\ldots,v_n$ los  vectores fila de $A$, y sea $W = \langle v_1,\ldots,v_n \rangle$.  Como los $v_1,\ldots,v_n$ son linealmente independientes, la dimensión de $W$ es $n$. Por el corolario  \ref{dimw-menor-dimv} se tiene 	que $W$ no es propio y por lo tanto $W = \K^n$.  Luego existen escalares $b_{ij}$ en $\K$ tales que
		$$
		e_i = \sum_{j=1}^{n} b_{ij} v_j, \qquad 1 \le i \le n
		$$
		donde $\{e_1, e_2,\ldots ,e_n\}$ es la base canónica de $\K^n$. 
		Como $v_j= (a_{j1},\ldots,a_{jn})$,  tenemos
		\begin{align*}
			e_i &= \sum_{j=1}^{n} b_{ij} (a_{j1},\ldots,a_{jn}) \\
			&=  (\sum_{j=1}^{n} b_{ij}a_{j1},\ldots,\sum_{j=1}^{n} b_{ij}a_{jn}).
		\end{align*}
		
		Así, para la matriz $B= [b_{ij}]$ se tiene
		$$
		BA=Id.
		$$
	\end{proof}
	
	\end{comment}

Hemos visto que si  $V$  es un espacio de dimensión finita,  entonces todo conjunto LI se puede extender a una base. Veremos ahora que dado un conjunto finito de generadores,  existe un subconjunto que es una base. 


\begin{teorema}\label{gen->base}
	Sea $V \ne 0$ espacio vectorial y $S$ un conjunto finito de generadores de $V$,  entonces existe un subconjunto $\mathcal B$  de $S$ que es una base.  
\end{teorema} 
\begin{proof}
	Sea
	$$
	C = \{|R|: R \subseteq S \;\wedge\; R \text{ es LI}\}.
	$$
	Como $V$ no es nulo  y $S$ genera $V$, $C \ne \emptyset$. $C$  es un subconjunto no vacío de  $\mathbb N$,  acotado superiormente por $|S|$ y por lo tanto tiene máximo. Sea $n$ el máximo de $C$ y sea $\mathcal B \subseteq S$ tal que $|\mathcal B| =n$  y $\mathcal B$ es LI. Veremos que $\mathcal B$  es una base. Para ello, como $\mathcal B $ es LI, sólo falta ver que $\mathcal B$ genera a $V$. 
	
	Supongamos que existe $v \in S$ tal que $ v \not\in \langle \mathcal B \rangle$. Por el lema \ref{li+vec=li},  entonces  $\mathcal B \cup \{v\}$ es LI y este subconjunto LI de $S$ tiene $n+1$  elementos, lo cual contradice la maximalidad de $n$. Es claro entonces, que $v \in S \Rightarrow v \in \mathcal B$,  es decir $S \subset \langle B \rangle$. Como $S \subset \langle B \rangle$,  entonces
	$V = \langle S \rangle \subset \langle B \rangle$, es decir $V =  \langle B \rangle$.
\end{proof}


	\begin{teorema}
		Si $W_1$, y $W_2$ son subespacios de dimensión finita de un espacio vectorial, entonces $W_1+ W_2$ es de dimensión finita y 
		$$\dim W_1 + \dim W_2 = \dim (W_1 \cap W_2) + \dim (W_1 + W_2).$$
	\end{teorema}
	\begin{proof}
		 El  conjunto $W_1 \cap W_2$  es un subespacio de $W_1$ y $W_2$ y por lo tanto un espacio vectorial de dimensión finita. Sea  $u_1,\ldots,u_k$ una base de $W_1 \cap W_2$, por el teorema \ref{completar-bases},  existen $v_1,\ldots,v_n$ vectores en $W_1$ y $w_1,\ldots,w_m$ vectores en $W_2$  tal que
		 $$
		 \{u_1,\ldots,u_k,v_1,\ldots,v_n\}\quad  \text{es una base de $W_1$,}
		 $$
		  y
		  $$
		 \{u_1,\ldots,u_k,w_1,\ldots,w_m\}\quad \text{es una base de $W_2$.}
		 $$
		Es claro que, el subespacio $W_1+ W_2$ es generado por los vectores 
		$$
		u_1,\ldots,u_k,v_1,\ldots,v_n,w_1,\ldots,w_m.
		$$
		Veamos que estos  vectores forman un conjunto independiente. En efecto, supóngase que
		\begin{equation}\label{tres-sumas}
			\sum \lambda_i u_i + \sum \gamma_i v_i + \sum \mu_i w_i =0,
		\end{equation}
		luego
		$$
		\sum \mu_i w_i = - \sum \lambda_i u_i - \sum \gamma_i v_i.
		$$
		Por  lo tanto, $\sum \mu_i w_i \in (W_1 \cap W_2) + W_1 = W_1$. Es  decir,  $\sum \mu_i w_i \in W_2$ y $\sum \mu_i w_i \in W_1$, por lo tanto $\sum \mu_i w_i \in (W_1 \cap W_2)$, y entonces
		$$
		\sum \mu_i w_i  = \sum \alpha_i u_i \Rightarrow 0 = \sum \alpha_i u_i - \sum \mu_i w_i .
		$$
		Como $\{u_1,\ldots,u_k,w_1,\ldots,w_m\}$ es una base y por lo tanto LI, tenemos que $0=\alpha_i=\mu_j$, para todo $i,j$. Por lo tanto, por (\ref{tres-sumas}), 
		\begin{equation}\label{dos-sumas}
		\sum \lambda_i u_i + \sum \gamma_i v_i  =0.
		\end{equation}
		Como $ \{u_1,\ldots,u_k,v_1,\ldots,v_n\}$  es una base de $W_1$,  tenemos que también $0=\lambda_i = \gamma_j$ para todo $i,j$. Luego  $0=\lambda_i = \gamma_j = \mu_r$, para cualesquiera $i,j,r$ y por lo tanto $u_1,\ldots,u_k,v_1,\ldots,v_n,w_1,\ldots,w_m$ es LI y  como  generaban a  $W_1+W_2$ resultan ser una base de $W_1+W_2$, por lo tanto $\dim (W_1+W_2) = k+n+m$.
		
		Finalmente,
		\begin{align*}
			\dim W_1 + \dim W_2&= (k+n)+(k+m) \\&	=k+(k+n+m)\\ &=  \dim (W_1 \cap W_2) + \dim (W_1 + W_2) .
		\end{align*}
	\end{proof}
	\end{section}



	
	\begin{section}{Dimensiones de subespacios}\label{sec-dimensiones-de-subespacios}
		
			
		Dada $A \in M_{m\times n}(\K)$,  ya hemos visto que  las soluciones del sistema $AX=0$ forman un subespacio vectorial. Sea $R $ la MERF equivalente por filas a $A$ y $r$ la cantidad de filas no nulas de $R$. Ahora bien, cada fila no nula está asociada  a una  variable principal y las  $n-r$ variables restantes son variables libres  que generan  todas las soluciones.
		El  hecho de que tenemos $n-r$ variables libres no dice que hay $n-r$ vectores LI que generan $W$, y por lo tanto,  $\dim W = n-r$. Esto lo veremos en ejemplos.
		
		\begin{ejemplo}
			Encontrar una base del subespacio 
			$$
			W = \left\{(x,y,z,w) \in \mathbb{R}: \quad\begin{array}{rcl}
			x-y -3z +\;\;w &=& 0 \\ y +5z +3w &=& 0
			\end{array} \right\}.
			$$
		\end{ejemplo}
		\begin{proof}[Solución]
			$W$  está definido implícitamente y usando el método de Gauss podemos describirlo paramétricamente, pues:
			\begin{equation*}
			\begin{bmatrix}1&-1&-3&1 \\ 0&1&5&3  \end{bmatrix}
			\stackrel{F_1+F_2}{\longrightarrow} 
			\begin{bmatrix}1&0&2&4 \\ 0&1&5&3  \end{bmatrix}.
			\end{equation*}
			Por lo tanto, el sistema de ecuaciones que define $W$ es equivalente a 
			\begin{equation*}
			\begin{array}{rcl}
			x  +2z +4w &=& 0 \\ y +5z +3w &=& 0,
			\end{array}
			\end{equation*}
			es decir 
			\begin{equation*}
			\begin{array}{rcl}
			x  &=& -2z - 4w  \\ y &=& -5z -3w ,
			\end{array}
			\end{equation*}
			y por lo tanto
			\begin{align*}
			W &= \left\{(-2z -4w,-5z -3w,z,w) : z,w\in \mathbb{R} \right\} \\
			&= \left\{(-2,-5,1,0)z+(-4, -3,0,1)w : z,w\in \mathbb{R} \right\}\\
			&= \langle (-2,-5,1,0),(-4, -3,0,1)\rangle.
			\end{align*}
			Concluimos entonces que $(-2,-5,1,0),(-4, -3,0,1)$  es una base de $W$ y, por lo tanto,  su dimensión es 2.
		\end{proof}
		
		\vskip .5cm 
		
	
		
		\begin{teorema}
			Sean $A$ matriz $m \times n$ con coeficientes en $\K$, $P$ matriz $m\times m$ inversible y $B =PA$. Entonces el el espacio fila de $A$ es igual al espacio fila de $B$.
		\end{teorema}
		\begin{proof}
			Sea $A= [a_{ij}]$, $P =[p_{ij}]$ y $B = [b_{ij}]$. Como  $B= PA$, tenemos que la fila $i$ de $B$ es
			\begin{align*}
				(b_{i1},\ldots,b_{in})&= (F_i(P).C_1(A),\ldots,F_i(P).C_n(A)) \\
				&= (\sum_{j=1}^{m} p_{ij}a_{j1}, \ldots, \sum_{j=1}^{m} p_{ij}a_{jn}) \\
				&= \sum_{j=1}^{m} p_{ij}(a_{j1}, \ldots,a_{jn}).
			\end{align*}
		Luego, cada vector fila de $B$ se puede obtener como combinación lineal de los vectores fila de $A$, y por lo tanto el espacio fila de $B$ está incluido en el espacio fila de $A$. 
		
		Ahora bien, como $P$ inversible, podemos multiplicar por $P^{-1}$ a izquierda la fórmula $B= PA$, y obtenemos $P^{-1}B = P^{-1}P A = A$. Haciendo el mismo razonamiento que arriba concluimos que  también el espacio fila de $A$ está incluido en el espacio fila de $B$ y por lo tanto son iguales. 
		\end{proof}
	
	\begin{corolario}\label{subesp-merf}
			Sean $A$ matriz $m \times n$ y $R$ la MERF equivalente por filas a $A$. Entonces, el espacio fila de $A$ es igual al espacio fila de $R$ y las filas no nulas de $R$ forman una base del espacio fila de $A$. 
	\end{corolario}
	\begin{proof}
		$R=PA$, donde $P$ es una matriz $m \times m$ inversible, luego, por el teorema anterior, el espacio fila de $A$  es igual al espacio fila de $R$. Calculemos ahora cual es la dimensión del espacio fila de $R$. Veamos que filas no nulas de $R$ son LI. 
		
		Recordemos  que por definición de MERF cada fila no nula comienza con un 1 y en esa coordenada  todas las demás filas tienen un 0, por lo tanto una combinación lineal no trivial resulta en un vector no nulo: si $v$ es una fila no nula de $R$, con el 1 principal en la coordinada $i$ y $\lambda \ne0$,  entonces $\lambda v$ vale $\lambda$ en la posición $i$ y esta coordenada no puede ser anulada por la combinación de otras filas.  
	\end{proof}

	\begin{corolario}\label{inv-impl-filasgen}
			Sean $A$ matriz $n \times n$. Entonces, $A$ es inversible si y sólo si las filas de $A$ son una base de $\K^n$.
	\end{corolario}
	\begin{proof}
		Si $A$ es inversible entonces la MERF de $A$ es la identidad, por lo tanto  el espacio fila de $A$ genera $\K^n$.
		
		Por otro lado, si el espacio fila de $A$  genera $\K^n$, el espacio fila de  la  MERF es $\K^n$ y por lo tanto  la MERF de $A$ es la identidad y en consecuencia $A$ es inversible.
		
		Hemos probado que $A$ es inversible si y sólo si las $n$ filas de $A$ generan $\K^n$. Como $\dim \K^n = n$,  todo conjunto de $n$ generadores es una base. 
	\end{proof}
	
	\vskip .5cm
	
	El  corolario  \ref{subesp-merf} nos provee un método para encontrar una base de un  subespacio de $\K^n$ generado por $m$ vectores: si $v_1,\ldots,v_m \in \K^n$ y $W = \langle v_1,\ldots,v_m\rangle$, consideramos la matriz 
	$$
	A = \begin{bmatrix}
	v_1 \\ v_2 \\ \vdots \\ v_m
	\end{bmatrix}
	$$
	donde las filas son los vectores $v_1,\ldots,v_m$. Luego calculamos $R$, la MERF equivalente por filas a $A$, y si $R$ tiene $r$ filas no nulas, las $r$ filas no nulas son una base de $W$ y, por consiguiente, $\dim W = r$. 
	
	\begin{ejemplo}\label{ej-4.5}
		Encontrar una base  de $W= \langle (1,0,1), (1,-1,0), (5,-3,2)\rangle$. 
	\end{ejemplo}
	\begin{proof}[Solución]
		Formemos la matriz cuyas filas son los vectores que generan $W$,  es decir 
		$$
		A = \begin{bmatrix} 1&0&1 \\ 1&-1&0 \\ 5&-3&2 \end{bmatrix}.
		$$
		Entonces
		\begin{equation*}
		\begin{bmatrix}1&0&1 \\ 1&-1&0 \\ 5&-3&2  \end{bmatrix}
		\underset{F_3-5F_1}{\stackrel{F_2- F_1}{\longrightarrow}} 
		\begin{bmatrix}1&0&1 \\ 0&-1&-1 \\ 0&-3&-3\end{bmatrix}
		\stackrel{-F_2}{\longrightarrow} 
		\begin{bmatrix}1&0&1 \\ 0&1&1 \\ 0&-3&-3\end{bmatrix}
		\stackrel{F_3 - 3F_2}{\longrightarrow}
		\begin{bmatrix}1&0&1 \\ 0&1&1 \\ 0&0&0\end{bmatrix}.
		\end{equation*}
		Por lo tanto, $\dim W =2$ y $(1,0,1), (0,1,1)$ es una base de  $W$.
	\end{proof}
	
	El método que nos provee el corolario  \ref{subesp-merf} nos permite encontrar una base de un subespacio vectorial de $\K^n$ a partir de un conjunto de generadores del subespacio. Como vimos en el teorema \ref{gen->base},  en todo conjunto finito de generadores existe un subconjunto que es una base. El siguiente teorema nos permite encontrar uno de tales subconjuntos. 

	
	\begin{teorema}
		Sea $v_1,\ldots, v_r$ vectores en $\K^n$ y $W = \langle  v_1,\ldots, v_r \rangle$. Sea $A$ la matriz formada por las filas $v_1,\ldots, v_r$ y $R$ una MRF equivalente por filas a $A$ que se obtiene sin el uso de permutaciones de filas (ver observación \ref{metodo-merf}). Si $i_1,i_2,\ldots,i_s$ son las filas no nulas de $R$,  entonces $v_{i_1},v_{i_2},\ldots,v_{i_s}$ es una base de $W$.
	\end{teorema}
	\begin{proof}
		Se hará por inducción sobre $r$. 
		
		Si $r = 1$ es trivial ver que vale la afirmación. 
		
		Supongamos que tenemos el resultado probado para $1\le k < r$ (hipótesis inductiva). 		
		Sea $W' = \langle  v_1,\ldots, v_{r-1} \rangle$ y sea $A'$ la matriz formada por las $r-1$ filas $v_1,\ldots, v_{r-1}$. 		
		Sea $R'$ la MRF equivalente por filas a $A'$ que se obtiene sin usar permutaciones de filas. Por hipótesis inductiva, si $i_1,i_2,\ldots,i_s$ son las filas no nulas de $R'$,  entonces $v_{i_1},v_{i_2},\ldots,v_{i_s}$ es una base de $W'$.
		
		Sea
		\begin{equation*}
			R_0 = \begin{bmatrix}
			R' \\ v_r
			\end{bmatrix}
		\end{equation*}
		donde $R'$ es la MRF de $A'$. 
		
		Si $v_r \in W'$, entonces  $v_{i_1},v_{i_2},\ldots,v_{i_s}$ es una base de $W$ y 
		\begin{equation*}
		R = \begin{bmatrix}
		R' \\ 0
		\end{bmatrix}
		\end{equation*}
		es la MRF de $A$.
		
		Si $v_r \not\in W'$, entonces  $v_{i_1},v_{i_2},\ldots,v_{i_s}, v_r$ es una base de $W$ (lema \ref{li+vec=li}) y la MRF de $A$ tiene la última fila no nula.  
	\end{proof}


	\begin{ejemplo}
		Sea $S = \{(1,0,1),(1,-1,0), (5,-3,2)\}$ y $W = \langle S\rangle$. Encontrar una base de $W$ que sea un subconjunto de $S$. 
	\end{ejemplo}
	\begin{proof}[Solución] Hemos visto en el ejemplo \ref{ej-4.5} que una  MRF de $A$  es 
		\begin{equation*}
			\begin{bmatrix} 1&0&1\\0&1&1\\0&0&0 \end{bmatrix},
		\end{equation*}
		y que la misma se obtiene sin usar permutaciones. Esta matriz tiene las dos primeras filas no nulas, por lo tanto, $\{(1,0,1),(1,-1,0)\}$ es una base de $W$.
	\end{proof}

Finalmente, terminaremos esta sección con un teorema que resume algunas equivalencias respecto a matrices inversibles.

\begin{teorema}
	Sea $A$ matriz $n \times n$ con coeficientes en $\K$. Entonces son equivalentes
	\begin{enumerate} 
		\item $A$ es inversible.
		\item $A$  es equivalente por filas a $I_n$.
		\item $A$ es producto de matrices elementales.
		\item El sistema $AX=Y$ tiene una única solución para toda matriz $Y$ de orden $n \times 1$. 
		\item El sistema homogéneo $AX=0$ tiene una única solución trivial.
		\item $\det A \ne 0$.
		\item Las filas de $A$ son LI.
		\item Las columnas de $A$ son LI.
	\end{enumerate}
\end{teorema}
\begin{proof} Por teoremas \ref{mtrx-inv-equiv} y  \ref{mtrx-inv-equiv2}, tenemos que 	(1) $\Leftrightarrow$ 	(2) $\Leftrightarrow$ 	(3) $\Leftrightarrow$ 	(4) $\Leftrightarrow$ 	(5).
	
	(1) $\Leftrightarrow$ (6). Por teorema \ref{mtrx-inv-equiv3}.
	
	(1) $\Leftrightarrow$ (7). Por corolario \ref{inv-impl-filasgen}.
	
	(1) $\Leftrightarrow$ (8). $A$ inversible $\Leftrightarrow$ $A^\t$ inversible $\Leftrightarrow$  las filas de $A^\t$ son LI $\Leftrightarrow$  las columnas de $A$ son LI. 
\end{proof}
	
	\end{section}




	\begin{section}{Coordenadas}
		Una de las características útiles de una base $\mathcal B$ en un espacio vectorial  $V$ de dimensión $n$ es que permite introducir coordenadas en $V$ en forma análoga a las ``coordenadas naturales'', $x_i$, de un vector $v = (x_l,\ldots, x_n)$ en el espacio $\K^n$. En este esquema, las coordenadas de un vector $v$ en $V$, respecto de la base $\mathcal B$, serán los escalares que sirven para expresar $v$ como combinación lineal de los vectores de la base. En  el caso  de la base canónica $e_1,\ldots,e_n$ de $\K^n$ tenemos
		$$
		v = (x_1,\ldots,x_n) = \sum_{i=1}^{n} x_ie_i.
		$$
		por lo tanto $x_i$  es la coordenada $i$-ésima de $v$ respecto a la base canónica. 
		
		En  forma análoga veremos que si $v_1,\ldots,v_n$  es una base de $V$,  entonces existe una única forma de  escribir 
		$$
		v =  \sum_{i=1}^{n} x_iv_i,
		$$ 
		y los valores  $x_i$  serán las \textit{coordenadas de $v$}\index{coordenadas de un vector} en la base dada. 
		
		\begin{definicion}
			Si $V$ es un espacio vectorial de dimensión finita, una \textit{base ordenada}\index{base ordenada} de $V$ es una sucesión finita de vectores linealmente independiente y que genera $V$.
		\end{definicion}
		
		
		La diferencia entre la definición de ``base'' y la de ``base ordenada'',  es que en la última es  importante el orden de los vectores de la base. Si la sucesión $v_1,\ldots,v_n$ es una base ordenada de $V$, entonces el conjunto $\{v_1,\ldots,v_n\}$ es una base de $V$. La base ordenada es el conjunto, juntamente con el orden dado. Se incurrirá en un pequeño abuso de notación y se escribirá
		$$
		\mathcal{B} = \{v_1,\ldots,v_n\}
		$$
		diciendo que $\mathcal{B}$ es una base ordenada de $V$.
		
		\begin{proposicion}
			Sea $V$  espacio vectorial de dimensión finita y sea $\mathcal{B} = \{v_1,\ldots,v_n\}$ una base ordenada de $V$. Entonces, para cada $v \in V$,  existen únicos $x_1,\ldots,x_n \in \K$ tales que $$v =   x_1v_1 + \cdots +x_nv_n.$$
		\end{proposicion}
		\begin{proof}
			Como $v_1,\ldots,v_n$  generan $V$,  es claro que existen $x_1,\ldots,x_n \in \K$ tales que $v =   x_1v_1 + \cdots +x_nv_n$. Sean $y_1,\ldots,y_n \in \K$ tales que $v =   y_1v_1 + \cdots +y_nv_n$. Veremos que $x_i = y_i$ para $1 \le i \le n$.
			
			Como $v =  \sum_{i=1}^{n} x_iv_i$ y $v =  \sum_{i=1}^{n} y_iv_i$,  restando miembro a miembro obtenemos 
			$$
			0 =   \sum_{i=1}^{n} (x_i-y_i)v_i.
			$$
			Ahora bien,  $v_1,\ldots,v_n$ son  LI, por lo tanto todos los coeficientes de la ecuación anterior son nulos, es decir $x_i-y_i=0$ para $1 \le i \le n$ y entonces $x_i = y_i$ para $1 \le i \le n$.
		\end{proof}
	
	La proposición anterior permite asociar a cada vector una $n$-upla: sea $V$  espacio vectorial de dimensión finita y sea $\mathcal{B} = \{v_1,\ldots,v_n\}$ una base ordenada de $V$, si $v \in V$ y $$v =   x_1v_1 + \cdots +x_nv_n,$$  entonces \textit{$x_i$ es la coordenada $i$-ésima de $v$} y denotamos
	$$
	v = (x_1,\ldots,x_n)_{\mathcal B}.
	$$
	También nos será útil describir a $v$ como una matriz $n \times 1$: \textit{la matriz de $v$  en la base  $\mathcal{B}$}  es
	$$
	[v]_\mathcal{B} = \begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix}.
	$$
	
	\begin{ejemplo}
		Sea $\mathcal B = \{(1,-1),(2,3)\}$ base ordenada de $\R^2$. Encontrar las coordenadas  de $(1,0)$ y $(0,1)$ en la base $\mathcal B$.
	\end{ejemplo}
	\begin{proof}[Solución] Debemos encontrar $x_1, x_2 \in \R$ tal que 
		$$
		(1,0) = x_1(1,-1)+ x_2(2,3).
		$$
		Es decir 
		\begin{align*}
			x_1+ 2x_2 &= 1\\
			-x_1 + 3x_2 &= 0.
		\end{align*}
		Resolviendo el sistema de ecuaciones obtenemos $x_1 = \frac35$ y $x_2 = \frac15$,  es decir
		$$
		(1,0) =\; \frac35(1,-1)+ \frac15(2,3)\quad \text{o equivalentemente} \quad (1,0) = (\;\frac35,\frac15)_{\mathcal B}.
		$$ 
		De forma análoga podemos ver que
		$$
		(0,1) = -\frac25(1,-1)+ \frac15(2,3)\quad \text{o equivalentemente} \quad (0,1) = (-\frac25,\frac15)_{\mathcal B}.
		$$
	\end{proof}
	
	\begin{proposicion}\label{vectorbase->lineal}
		Sea $\mathcal{B}=\{v_1,\ldots,v_n\}$ una base ordenada de $V$ un $\K$-espacio vectorial. Entonces
		\begin{enumerate}
			\item $[v + w]_\mathcal{B} = [v]_\mathcal{B} +[w]_\mathcal{B}$, para $v,w \in V$,
			\item $[\lambda v]_\mathcal{B} = \lambda[v]_\mathcal{B}$, para $\lambda \in \K$ y $v \in V$.
		\end{enumerate}
	\end{proposicion} 
	\begin{proof}
		(1) Si $v = x_1v_1 + \cdots +x_nv_n$ y $w = y_1v_1 + \cdots +y_nv_n$, entonces 
		$$
		v + w = (x_1+y_1)v_1 + \cdots +(x_n+y_n)v_n,
		$$
		luego
		$$
		[v + w]_\mathcal{B} = \begin{bmatrix}x_1+y_1 \\ \vdots \\ x_n+y_n\end{bmatrix}
		= \begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix}+\begin{bmatrix}y_1 \\ \vdots \\ y_n\end{bmatrix} = [v]_\mathcal{B} +[w]_\mathcal{B}.
		$$
		
		(2) Si $v = x_1v_1 + \cdots +x_nv_n$ y $\ \in \K$, entonces 
		$$
		\lambda v = (\lambda x_1)v_1 + \cdots +(\lambda x_n)v_n,
		$$
		luego
		$$
		[\lambda v ]_\mathcal{B} = \begin{bmatrix}\lambda x_1 \\ \vdots \\ \lambda x_n\end{bmatrix}
		= \lambda \begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix} = \lambda [v]_\mathcal{B}.
		$$
	\end{proof}
	
	\begin{teorema}
			Sea $V$  espacio vectorial de dimensión $n$  sobre el cuerpo $\K$, 	y sean $	\mathcal{B} = \{v_1,\ldots,v_n\}$ y $\mathcal{B'} = \{v'_1,\ldots,v'_n\}$ bases ordenadas de $V$. Entonces existe una única matriz $P$ de orden $n \times n$, inversible, tal que para todo $v \in V$
			\begin{enumerate}
				\item $[v]_\mathcal{B} = P[v]_\mathcal{B'}$ y
				\item $[v]_\mathcal{B'} = P^{-1}[v]_\mathcal{B}$.
			\end{enumerate} 
			Las columnas de $P$ están dadas por
			$$
			C_j = [v'_j]_\mathcal{B},\qquad 1 \le j \le n,
			$$
			es decir, los coeficientes de la columna $j$ de $P$ son las coordenadas de $v'_j$ en la base $\mathcal{B}$. 
	\end{teorema}
	\begin{proof} (1) Tenemos
		\begin{align*}
			v &=   x_1v_1 + \cdots +x_nv_n, \\
			v &=  x'_1v'_1 + \cdots +x'_nv'_n.
		\end{align*}
		 También podemos escribir cada $v'_j$ en coordenadas respecto a $\mathcal B$:
		 $$
		 v'_j =  p_{1j}v_1 + \cdots +p_{nj}v_n, \qquad  1 \le j \le n.
		 $$ 
		Luego, 
		\begin{align*}
			 \sum_{i=1}^{n}x_iv_i &=  \sum_{j=1}^{n}x'_jv'_j \\
			&=  \sum_{j=1}^{n}x'_j(\sum_{i=1}^{n}p_{ij}v_i) \\
			&=  \sum_{i=1}^{n}\sum_{j=1}^{n}x'_jp_{ij}v_i \\
			&=  \sum_{i=1}^{n}(\sum_{j=1}^{n}p_{ij}x'_j)v_i.			
		\end{align*}
		Es decir, $x_i = \sum_{j=1}^{n}p_{ij}x'_j$. Por lo tanto, si $P = [p_{ij}]$ tenemos que
		$$
		\begin{bmatrix}x_1 \\ x_2\\ \vdots \\ x_n\end{bmatrix} = 
		\begin{bmatrix}p_{11} &p_{12} & \ldots & p_{1n} \\p_{21} &p_{22} & \ldots & p_{2n} \\ \vdots &\vdots && \vdots\\ p_{n1} &p_{n2} & \ldots & p_{nn}\end{bmatrix}
		 \begin{bmatrix}x'_1 \\ x'_2\\ \vdots \\ x'_n\end{bmatrix}.
		$$
		
		Veamos que $P$  es inversible: sea $(x_1,\ldots,x_n)$ solución del sistema de ecuaciones $PX =0$. Eso implica que si $v =\sum x_i v'_i$,  entonces $P[v]_{\mathcal B'}=0$.     Eso quiere decir que el vector  escrito en la base $\mathcal B$ tiene coordenadas $0$,  es decir $v = \sum 0. v_i =0$ y entonces $v = \sum 0. v'_i$ por la unicidad de escritura. Por lo tanto, la única solución del sistema $PX=0$ es 0 y en consecuencia $P$  es inversible.
		
		(2)  Como $P$ es inversible, multiplicamos por $P^{-1}$  a izquierda la ecuación $[v]_\mathcal{B} = P[v]_\mathcal{B'}$ y obtenemos el resultado.
	\end{proof}

	Observemos que, por simetría,  los coeficientes de la columna $j$ de $P^{-1}$ son las coordenadas de $v_j$ en la base $\mathcal{B'}$. 
	
	\begin{definicion} 
		Sea $V$  espacio vectorial de dimensión $n$  sobre el cuerpo $\K$, 	y sean $\mathcal{B} = \{v_1,\ldots,v_n\}$ y $\mathcal{B'} = \{v'_1,\ldots,v'_n\}$ bases ordenadas de $V$. Entonces la matriz
		$$
		P = \begin{bmatrix}
		 C_1 &C_2 &\cdots &C_n
		\end{bmatrix}
		$$
		con 
		$$
			C_j = [v'_j]_\mathcal{B},\qquad 1 \le j \le n,
		$$
		es la matriz de \textit{cambio de coordenadas}\index{cambiode coordenadas} de la base $\mathcal{B'}$  a la base $\mathcal{B}$.
	\end{definicion}
	
	
	\begin{ejemplo}
		 $\mathcal{B} = \{v_1,v_2,v_3 \}=\{(1,0,-1),(1,1,1),(1,0,0) \}$ en $\R^3$.
		\begin{enumerate}
			\item Probar que $\mathcal{B}$ es una base de $\R^3$, y 
			\item encontrar las coordenadas del vector  $(2,3,5)$ respecto a la base  $\mathcal{B}$. 
		\end{enumerate}
	\end{ejemplo}
\begin{proof}[Solución]
	(1) Sea $A$ la matriz cuyas filas son los vectores de $\mathcal{B}$ :
	$$
	A = \begin{bmatrix} 1&0&-1\\1&1&1\\1&0&0	\end{bmatrix}.
	$$
	Entonces,  calculando el determinante  por desarrollo de la última fila, obtenemos
	$$
	\det A = 1.(-1)^{3+1}. 
	\left| \begin{matrix} 0&-1\\1&1	\end{matrix} \right| = 1.1.1 =1.
	$$ 
	Por  lo tanto $A$ es inversible y en consecuencia el espacio fila de $A$ es $\R^3$. Como $\mathcal{B}$ es un conjunto de 3  generadores de  $\R^3$, $\mathcal{B}$ es base.
	
	(2) Si $x_1,x_2,x_3$  son las coordenadas de $(2,3,5)$ respecto a la base  $\mathcal{B}$, entonces
	$$
	(2,3,5) = x_1(1,0,-1)+x_2(1,1,1)+x_3(1,0,0)
	$$
	y esto es un sistema de tres ecuaciones y tres incógnitas.  Resolviendo este sistema encontramos la solución buscada. 
	
	Sin embargo,  nos interesa conocer una solución que sirva para cualquier vector, no solo para $(2,3,5)$, es decir buscamos la matriz $P = [p_{ij}]$ de cambio de coordenadas de la base canónica a la base $\mathcal B$. En particular,  esta matriz cumple 
	$$
	\begin{bmatrix} x_1\\x_2\\x_3 \end{bmatrix} = 
	\begin{bmatrix}p_{11} &p_{12}& p_{13} \\p_{21} &p_{22} & p_{23}\\ p_{31} &p_{32}& p_{33}\end{bmatrix}
	\begin{bmatrix} 2\\3\\5 \end{bmatrix}.
	$$
	con $(x_1,x_2,x_3)_{\mathcal B} = (2,3,5)$.
	
	
	Ahora bien, sabemos que $P^{-1}$ es la matriz cuya columna $j$ es la matriz de $v_j$ en la base canónica (fácil de calcular) y luego calculamos su inversa que es $P$.
	
	Por lo dicho en el párrafo anterior, $P^{-1}$  es la matriz cuya  columna $1$ es $(1,0,-1)$,  la columna $2$ es $(1,1,1)$ y la columna $3$ es $(1,0,0)$, luego 
	$$
	P^{-1} = \begin{bmatrix} 1&1&1\\0&1&0\\-1&1&0	\end{bmatrix}.
	$$ 
	Encontremos su inversa,
		\begin{multline*} 
	\left[\begin{array}{rrr|rrr}	1&1&1&1&0&0\\ 0&1&0&0&1&0\\ -1&1&0&0&0&1 \end{array}\right]
	\stackrel{F_3+ F_1}{\longrightarrow}
	\left[\begin{array}{rrr|rrr}	1&1&1&1&0&0\\ 0&1&0&0&1&0\\ 0&2&1&1&0&1 \end{array}\right]
	\underset{F_3-2F_2}{\stackrel{F_1 - F_2}{\longrightarrow}}\\
	\longrightarrow 
	\left[\begin{array}{rrr|rrr}	1&0&1&1&-1&0\\ 0&1&0&0&1&0\\ 0&0&1&1&-2&1 \end{array}\right]
	\stackrel{F_1 - F_3}{\longrightarrow}
	\left[\begin{array}{rrr|rrr}	1&0&0&0&1&-1\\ 0&1&0&0&1&0\\ 0&0&1&1&-2&1 \end{array}\right]
	.
	\end{multline*}
	Entonces 
	$$
	P = \left[\begin{array}{rrr}	0&1&-1\\ 0&1&0\\ 1&-2&1 \end{array}\right]
	$$
	y 
	$$
	\left[\begin{array}{rrr}	0&1&-1\\ 0&1&0\\ 1&-2&1 \end{array}\right]
	\left[\begin{array}{r} 2\\3\\5 \end{array}\right] = 
	\left[\begin{array}{r} -2\\3\\1 \end{array}\right].
	$$
	Por  lo tanto las coordenadas de   $(2,3,5)$ respecto a la base  $\mathcal{B}$ son $(-2,3,1)_{\mathcal B}$ o,  equivalentemente, 
	$$
	(2,3,5) = -2(1,0,-1)+3(1,1,1)+(1,0,0).
	$$
	
	  
\end{proof}
	
	\end{section}

	\end{chapter}
	
		
	\begin{chapter}{Transformaciones lineales}\label{chap-trans-lin}
		Las transformaciones lineales son las funciones con las que trabajaremos en álgebra lineal. Se trata de funciones entre espacios vectoriales que son compatibles con la estructura,  es decir con la suma y el producto por escalares.
		
		
		\begin{section}{Transformaciones lineales}
			\begin{definicion}
				Sean $V$ y $W$ dos espacios vectoriales sobre el cuerpo $\K$. Una 				\textit{transformación lineal}\index{transformación lineal} de $V$ en $W$ es una función $T:V \to W$  tal que
				\begin{enumerate}
					\item $T(v+v') = T(v)+ T(v')$, para $v,v' \in V$,
					\item $T(\lambda v) = \lambda T(v)$, para $v \in V$, $\lambda \in \K$.
				\end{enumerate}
			\end{definicion}
		
			\begin{obs}
				$T:V \to W$ es transformación lineal si y sólo si
				\begin{enumerate}
					\item[({a})]  $T(\lambda v+v') = \lambda T(v)+ T(v')$, para $v,v' \in V$, $\lambda \in \K$.
				\end{enumerate}
			Algunas veces usaremos esto último para comprobar si una aplicación de $V$ en $W$ es una transformación lineal. 
			\end{obs}
			
			\begin{ejemplo}
				Si $V$ es cualquier espacio vectorial, la transformación identidad $I$, definida por $Iv = v$ ($v \in V$), es una transformación lineal de $V$ en $V$. La transformación cero $0$, definida por $0v = 0$, es una transformación lineal de $V$ en $V$.
			\end{ejemplo}
		
			\begin{ejemplo}\label{ejemplo-lineal-3}
				Sea $T : \K^3 \to \K^2$ definida por
				$$
				T(x_1,x_2,x_3) = (2x_1 - x_3, -x_1+3x_2+x_3).
				$$
				Entonces, $T$  es una transformación lineal. La demostración la veremos en la observación que sigue a este ejemplo. 
				
			Observar que si 
			$$
			 A = \begin{bmatrix}
			 2&0&-1 \\ -1&3&1
			 \end{bmatrix},
			$$
			entonces
			$$
			\begin{bmatrix}
			2&0&-1 \\ -1&3&1
			\end{bmatrix} 
			\begin{bmatrix}
			x_1\\x_2\\x_3
			\end{bmatrix} =
			\begin{bmatrix}
			2x_1 - x_3 \\ -x_1+3x_2+x_3
			\end{bmatrix}.
			$$
			Es decir,  si $\mathcal C_n$  es la báse canónica de $\K^n$ y $[x]_{\mathcal C_3}$ es la matriz de $x$ en la base canónica,  entonces 
			$$
			A.[x]_{\mathcal C_3} = [T(x)]_{\mathcal C_2}.
			$$ 
			\end{ejemplo}
		
			\begin{obs}\label{obs-tl-1.5} 	Sea $T: \K^n \to \K^m$. En  general si $T(x_1,\ldots,x_n)$ en cada coordenada tiene una combinación lineal de los $x_1,\ldots,x_n$,  entonces $T$ es una transformación lineal. Mas precisamente, si $T$ está definida por
				\begin{align*}
				T(x_1,\ldots,x_n) &= (a_{11}x_1+\cdots + a_{1n}x_n,\, \ldots\,,a_{m1}x_1+\cdots + a_{mn}x_n )\\
				&=(\sum_{j=1}^n a_{1j} x_j,\ldots,\sum_{j=1}^n a_{mj} x_j),
				\end{align*}
				con $a_{ij} \in \K$, entonces $T$  es lineal. 
			\begin{proof}
				Sean $(x_1,\cdots,x_n), (y_1,\cdots,y_n) \in \K^n$ y $\lambda \in \K$,  entonces
				\begin{align*}
				T(\lambda(x_1,\cdots,x_n)+ (y_1,\cdots,y_n)) &= T(\lambda x_1+y_1,\ldots,\lambda x_n+y_n) \\
				&= (\sum_{j=1}^n a_{1j} (\lambda x_j+y_j),\ldots,\sum_{j=1}^n a_{mj} (\lambda x_j+y_j)) \\
				&= (\lambda\sum_{j=1}^n a_{1j} x_j+\sum_{j=1}^n a_{1j} y_j,\ldots,\lambda\sum_{j=1}^n a_{mj} x_j+\sum_{j=1}^n a_{mj} y_j) \\
				&= \lambda(\sum_{j=1}^n a_{1j} x_j,\ldots,\sum_{j=1}^n a_{mj} x_j) +(\sum_{j=1}^n a_{1j} y_j,\ldots,\sum_{j=1}^n a_{mj} y_j) \\
				& =\lambda T(x_1,\cdots,x_n)+ T(y_1,\cdots,y_n).
				\end{align*}
			\end{proof}
			\end{obs}
			
			\begin{ejemplo}[Transformaciones de $\R^2$ en $\R^2$] Las rotaciones y reflexiones en $\R^2$ son transformaciones lineales. Sea $\theta \in \R$ tal que  $0 \le \theta \le 2\pi$,  definimos
				\begin{equation*}
					\begin{array}{llll}
					R_\theta:&\R^2 &\to &\R^2 \\
					&(x,y) &\mapsto & (x \cos\theta - y \sin\theta, y\cos\theta+ x\sin\theta)
					\end{array}
				\end{equation*}
				Observemos que si escribimos el vector $(x,y)$  en coordenadas polares,  es decir  si 
				$$
				(x,y)= r(\cos\alpha,\sin\alpha),\quad r> 0, \; 0 \le \alpha < 2\pi, 
				$$
				entonces
				\begin{align*}
					R_\theta(x,y) &= R_\theta(r\cos\alpha,r\sin\alpha) \\
					&= (r\cos\alpha \cos\theta - r\sin\alpha \sin\theta, r\sin\alpha\cos\theta+ r\cos\alpha\sin\theta) \\
					&= (r\cos(\alpha+\theta) , r\sin(\alpha+\theta)) \\
					&= r(\cos(\alpha+\theta) , \sin(\alpha+\theta)).
				\end{align*}
				Por lo tanto $R_\theta(x,y)$ es el vector $(x,y)$ rotado $\theta$ grados en sentido antihorario y en consecuencia  $R_\theta$ es denominada la \textit{rotación antihoraria en $\theta$ radianes}. No es difícil verificar que $R_\theta$ es una transformación lineal. 
				
				
					\begin{figure}[h]	
					\begin{tikzpicture}[scale=5]
					\draw[->] (-0.1,0)  -- (1.1,0) node(xline)[right]	{$x$};
					\draw[->] (0,-0.1) -- (0,1.1) node(yline)[above] {$y$};
					\draw[->] (0,0) -- (0.5,0.866) node(yline)[above] {$R_{\theta}(v)$};
					\draw[->] (0.9238,0.3826) arc[start angle=22.5, end angle=60, radius=1 ];
					\draw[->] (0,0) -- (0.9238,0.3826) node(yline)[right] {$v$};
					\draw[->] (0.7,0) arc[start angle=0, end angle=22.5, radius=0.7 ];
					\draw[->] (0.9238/2,0.3826/2) arc[start angle=22.5, end angle=60, radius=0.5 ];
					\node [right] at (0.3,0.3) {$\theta$};
					\node [right] at (0.59,0.12) {$\alpha$};
					\draw (1,1pt) -- (1,-1pt);
					\node [right] at (0.95,-0.07) {$r$};
					% Lines
					\end{tikzpicture}
					\caption{Rotación  $\theta$ grados. }
					\label{rotacion-theta}
				\end{figure}
				
				
				
				Otras transformaciones lineales importantes de $\R^2$ en $\R^2$ son 
				$$
				S_h(x,y) = (x,-y) \quad \text{ y } \quad S_v(x,y) = (-x,y).
				$$
				La primera es la reflexión en el eje $x$ y la segunda la reflexión en el eje $y$. Las siguientes afirmaciones se comprueban algebraicamente en forma sencilla, pero nos podemos convencer de ellas por su interpretación geométrica:
				\begin{equation*}
				\begin{array}{ll}
				R_\theta \circ R_\varphi = R_{\theta +\varphi}, \quad &\text{(rotar $\varphi$ y $\theta$ $=$  rotar $\theta+\varphi$)} \\
				R_{\pi/2} \circ S_h \circ R_{-\pi/2} = S_v&
				\end{array}
				\end{equation*}
				
			\end{ejemplo}
	
		
			\begin{ejemplo}
			Sea $V = \R[x]$ el espacio vectorial de los polinomios con coeficientes reales. Definimos $D:V \to V$, por
			$$
			D(P)(x) = P'(x),\quad x \in \R. 
			$$
			Observemos primero que la derivada de un polinomio es un polinomio, pues 
			$$
			(a_nx^n+ a_{n-1}x^{n-1}+\cdots + a_1 x + a_0)' = na_nx^{n-1}+ (n-1)a_{n-1}x^{n-2}+\cdots + a_1.
			$$
			Además  $D$  es lineal, pues $(f+g)' = f' + g'$ y $(\lambda f)' = \lambda f'$, para$f,g$ funciones derivables y $\lambda \in \R$.
			\end{ejemplo} 
		
		\begin{obs}	Sean $V$ y $W$ dos espacios vectoriales sobre el cuerpo $\K$ y 	 $T:V \to W$			un transformación lineal. Entonces $T(0) =0$
		\end{obs}
		\begin{proof} $T(0) = T(0+0) = T(0) + T(0)$, por lo tanto 
			\begin{align*}
				-T(0) + T(0) = -T(0) + T(0)+T(0) \;\Rightarrow\; 0 = 0 +T(0) \;\Rightarrow\; 0= T(0).
			\end{align*}
		\end{proof}
		
		\begin{obs}
			Las transformaciones lineales preservan  combinaciones lineales, es decir si $T:V \to W$ es una transformación lineal, $ v_1,\ldots,v_k \in V$ y $\lambda_1, \ldots+ \lambda_k \in \K$,  entonces
			$$
			T(\lambda_1 v_1 + \cdots+ \lambda_k v_k) = \lambda_1 T(v_1) + \cdots+ \lambda_k T(v_k).
			$$
			Observar que el caso $k=2$ se demuestra de la siguiente manera
			$$
			T(\lambda_1 v_1 +  \lambda_2 v_2) =T(\lambda_1 v_1) + T(\lambda_2 v_2) =\lambda_1 T(v_1) + \lambda_2T( v_2).
			$$
			El caso general se demuestra por inducción. 
		
		\end{obs}
		
		
		\begin{teorema}\label{th-tl-definida-en-base}
			Sean $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y $\{v_1,\ldots,v_n\}$  una base ordenada de $V$. Sean $W$ un espacio vectorial sobre el mismo cuerpo y $\{w_1,\ldots,w_n\}$, vectores cualesquiera de $W$. Entonces existe una única transformación  lineal $T$ de $V$ en $W$ tal que
			\begin{equation*}
			T(v_j) = w_j, \quad j=1,\ldots,n.
			\end{equation*}
		\end{teorema}
			\begin{proof}
				Recordemos que si $v \in V$,  existen únicos $a_1,\ldots,a_n \in \K$ (las coordenadas de $v$) tal que $$v = a_1v_1 + \cdots+a_n v_n.$$  Luego para este vector $v$  definimos
				\begin{equation*}
					T(v) = a_1w_1 + \cdots+a_n w_n.
				\end{equation*}
				Entonces, $T$ es una correspondencia bien definida que asocia a cada vector $v$ 
				de $V$ un vector $T(v)$ de $W$. De la definición queda claro que $T(v_j) = w_j$ para cada $j$. Para ver que $T$ es lineal, sea
				\begin{equation*}
					w = b_1v_1 + \cdots+b_n v_n,
				\end{equation*}
				y sea  $\lambda \in \K$. Ahora
				\begin{align*}
					\lambda v+w &= \lambda(a_1v_1 + \cdots+a_n v_n) + b_1v_1 + \cdots+b_n v_n \\
					&= (\lambda a_1+b_1)v_1 + \cdots+(\lambda a_n+b_n)v_n
				\end{align*}
				con lo que, por definición
				\begin{equation*}
					T(\lambda v+w) =(\lambda a_1+b_1)w_1 + \cdots+(\lambda a_n+b_n)w_n. 
				\end{equation*}
				Por otra parte
				\begin{align*}
				\lambda  T(v) + T(w) &= \lambda (a_1w_1 + \cdots+a_n w_n)+b_1w_1 + \cdots+b_n w_n	 \\
				 &=(\lambda a_1+b_1)w_1 + \cdots+(\lambda a_n+b_n)w_n ,			
				\end{align*}
				y así
				\begin{equation*}
					T(\lambda v+w) = 	\lambda  T(v) + T(w).
				\end{equation*}
				
				Finalmente,  debemos probar la unicidad de $T$. Sea $S: V \to W$ transformación lineal tal que $S(v_j) = w_j$ para $1 \le j \le n$. Entonces,  si $v \in V$ un vector arbitrario, $v = \sum_i a_i v_i$ y
				\begin{equation*}
					S(v) = S(\sum_i a_i v_i)\sum_i a_i S( v_i) = \sum_i a_iw_i = 
					 \sum_i a_i T( v_i) =  T(\sum_i a_i v_i) = T(v)
				\end{equation*}
			\end{proof}
		
		El teorema \ref{th-tl-definida-en-base} es muy elemental, pero por su importancia ha sido presentado
		detalladamente. 
		
			\begin{ejemplo} Usando el teorema \ref{th-tl-definida-en-base}, podemos demostrar la observación \ref{obs-tl-1.5} de la siguiente manera: sea  $\mathcal C_n = \{e_1,\ldots,e_n\}$ es la base canónica de $\K^n$ y  sea  $T: \K^n \to \K^m$ la única transformación lineal tal que 
			\begin{equation*}
			T(e_j)  = (a_{1j},\, \ldots\,,a_{mj} ), \quad j=1,\ldots,n
			\end{equation*}	
			Entonces, 
			\begin{equation*}
			T(x_1,\ldots,x_n) = (a_{11}x_1+\cdots + a_{1n}x_n,\, \ldots\,,a_{m1}x_1+\cdots + a_{mn}x_n ).
			\end{equation*}
			es la transformación lineal resultante.
		\end{ejemplo}
		
		\begin{ejemplo}
			Los vectores
			\begin{align*}
				v_1 &= (1,2)\\
				v_2 &= (3,4)
			\end{align*}
			son linealmente independientes y, por tanto, forman una base de $\R^2$. De acuerdo con el teorema \ref{th-tl-definida-en-base}, existe una única transformación lineal de $\R^2$ en $\R^2$ tal que
			\begin{align*}
				T(v_1) &= (3,2, 1) \\
				T(v_2) &= (6, 5,4).
			\end{align*}
			Para poder describir $T$ respecto a las coordenadas canónicas debemos calcular $T(e_1)$ y $T(e_2)$,  ahora bien,
			\begin{align*}
			(1,0) &= c_{1}(1,2) + c_{2}(3,4)\\
			(0,1) &= c_{3}(1,2) + c_{4}(3,4)
			\end{align*}
			y resolviendo este sistema de cuatro ecuaciones con cuatro incógnitas obtenemos
			\begin{equation*}
			\begin{array}{rcrcr}
			(1,0) &=& -2(1,2) &+ &(3,4)\\
			(0,1) &=& \displaystyle\frac32(1,2) &- &\displaystyle\frac12(3,4)
			\end{array}
			\end{equation*}
			Luego, 
				\begin{align*}
			T(1,0) &= -2T(1,2)+ T(3,4) = -2(3,2, 1)+(6, 5,4) = (0,1,2)\\
			T(0,1) &= \displaystyle\frac32T(1,2) - \displaystyle\frac12T(3,4) = \displaystyle\frac32(3,2,1) - \displaystyle\frac12(6, 5,4)= (\displaystyle\frac32,\displaystyle\frac12,-\displaystyle\frac12)
			\end{align*}
			Entonces
				\begin{equation*}
			T(x_1,x_2) = x_1(0,1,2) + x_2		 (\displaystyle\frac32,\displaystyle\frac12,-\displaystyle\frac12)
			= (\displaystyle\frac32x_2,x_1+\displaystyle\frac12x_2,2x_1-\displaystyle\frac12x_2)
			\end{equation*}
		\end{ejemplo}
		
		\end{section}
	
	
		\begin{section}{Núcleo e imagen de una transformación lineal}
		
		\begin{definicion}
			Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal.  Definimos
			\begin{align*}
				\img(T) &:= \{w \in W:\text{existe $v \in V$, tal que } T(v)=w\} = \{T(v): v \in V \}, \\
				\nuc(T) &:= \{v \in V: T(v)=0 \}. 
			\end{align*}
			A $\img(T)$ lo llamamos la \textit{imagen}\index{imagen de una trasnformación lineal} de $T$ y a $ \nuc(T)$ el \textit{núcleo}\index{núcleo  de una transformación lineal} de $T$. 
		\end{definicion}
		
		\begin{teorema}
			Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal; entonces $\img(T) \subset W$ y $\nuc(T) \subset V$ son subespacios vectoriales.
		\end{teorema}
		\begin{proof}
			$\img(T) \ne \emptyset$, pues $0 = T(0) \in \img(T)$. 
			
			Si $T(v_1),T(v_2) \in \img(T)$ y $\lambda \in \K$,  entonces $T(v_1) + T(v_2) = T(v_1+v_2) \in \img(T)$ y $\lambda T(v_1) = T(\lambda v_1) \in \img(T)$.
			
			
			$\nuc(T) \ne \emptyset$ pues $T(0) =0$ y por lo tanto $0 \in \nuc(T)$.
			
			Si $v,w \in V$ tales que $T(v) =0$ y $T(w)=0$,  entonces, $T(v+w)= T(v)+T(w) =0$. por lo tanto $v+w \in \nuc(T)$. Si  $\lambda \in \K$,  entonces $T(\lambda v) = \lambda T(v) = \lambda.0 =0$, luego  $\lambda v \in \nuc(T)$.
		\end{proof}

	
		\begin{definicion}
			Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal. Supongamos que $V$ es de dimensión finita.
			\begin{enumerate}
\item El \textit{rango}\index{rango de una transformación lineal} de $T$ es la dimensión de la imagen de $T$.
\item La \textit{nulidad}\index{nulidad de una transformación lineal} de $T$ es la dimensión del núcleo  de $T$.
			\end{enumerate}
			
		\end{definicion}
				
		\begin{ejemplo}
			Sea $T: \R^3 \to \R$, definida
			$$
			T(x,y,z) = x +2y +3z.
			$$
			Encontrar una base del núcleo y de la imagen.
			\begin{proof}[Solución]
				Es claro que como $T$ no es 0, la imagen es todo $\R$ (y por lo tanto cualquier $r\in \R, r \ne 0$ es base de la imagen). 
				
				Con respecto al núcleo,  debemos encontrar una base del subespacio
				$$
				\nuc(T) = \{(x,y,z): x+2y+3z =0\}.
				$$
				Como $x+2y+3z =0 \Leftrightarrow x = -2y-3z$, luego, 
				\begin{equation}\label{forma-parametrica-2}
					\nuc(T) =  \{(-2s-3t,s,t): s,t \in \R\}.
				\end{equation}
				Ahora bien, $(-2s-3t,s,t) = s(-2,1,0)+t(-3,0,1)$, por lo tanto 
				\begin{equation*}
				\nuc(T) =  <(-2,1,0),(-3,0,1)>,
				\end{equation*}
				y  como $(-2,1,0),(-3,0,1)$ son LI, tenemos que forman una base del núcleo. 
			\end{proof}
		
		La expresión (\ref{forma-parametrica-2}),  que depende de dos parámetros ($s$ y $t$) que son independientes entre ellos, es llamada la \textit{descripción paramétrica} del núcleo	
		\end{ejemplo}
		
		\begin{ejemplo}
		Sea $T: \R^3 \to \R^4$, definida
		$$
		T(x,y,z) = (x +y ,\,x +2y +z,\,3y +3z,\,2x +4y +2z).
		$$
		\begin{enumerate}
			\item Describir $\nuc(T)$  en forma paramétrica y dar una base.
			\item Describir $\img(T)$  en forma paramétrica y  dar una base. 
		\end{enumerate}
		\begin{proof}[Solución] 
		Observemos que 
		\begin{align*}
		\nuc(T) &= \{(x,y,z)\in \R^3: (x +y ,\,x +2y +z,\,3y +3z,\,2x +4y +2z) =(0,0,0,0)\}.
		\end{align*}
		Por  lo tanto, describir el núcleo de $T$ se reduce a encontrar las soluciones de una sistema de ecuaciones lineales homogéneo, y la matriz asociada a este sistema es.   
		\begin{equation*}
		A = \begin{bmatrix}
		1&1&0\\1&2&1\\0&3&3\\2&4&2
		\end{bmatrix}
		\end{equation*}
		
		Por  otro lado,
		\begin{align*}
		\img(T) &= \{(y_1,y_2,y_3,y_4)\in \R^4: (x +y ,\,x +2y +z,\,3y +3z,\,2x +4y +2z) =(y_1,y_2,y_3,y_4)\}\\
		&=  \{(y_1,y_2,y_3,y_4)\in \R^4: A\begin{bmatrix}x\\y\\z	\end{bmatrix}  =
		\begin{bmatrix}y_1\\y_2\\y_3\\y_4\end{bmatrix}\}.
		\end{align*}
		
		
		Por lo tanto, y haciendo abuso de notación,  
		\begin{align*}
		\nuc(T) &= \{v=(x,y,z):   A.{v}=0\}\\
		\img(T) &= \{y= (y_1,y_2,y_3,y_4): \text{ tal que } \exists v \in \R^3, A.{v} = {y}  \}
		\end{align*}
		En  ambos casos, la solución depende de resolver el sistema de ecuaciones cuya matriz asociada es $A$:
		\begin{equation*}
		\left[\begin{array}{rrr|r}1&1&0&y_1\\1&2&1&y_2\\0&3&3&y_3\\2&4&2&y_4 \end{array}\right]
		\underset{F_4-2F_1}{\stackrel{F_2 -F_1}{\longrightarrow}} 
		\left[\begin{array}{rrr|r}1&1&0&y_1\\0&1&1&-y_1+y_2\\0&3&3&y_3\\0&2&2&-2y_1 +y_4 \end{array}\right]
		\underset{F_4-2F_2}{\underset{F_3-3F_2}{\stackrel{F_1 -F_2}{\longrightarrow}} }
		\left[\begin{array}{rrr|r}1&0&-1&2y_1-y_2\\0&1&1&-y_1+y_2\\0&0&0&3y_1-3y_2+y_3\\0&0&0&-2y_2 +y_4 \end{array}\right],
		\end{equation*}
		es decir
		\begin{equation}\label{eq-gen}
		T(x,y,z) = (y_1,y_2,y_3,y_4) \quad\Leftrightarrow \quad
		\begin{array}{rl}
		x -z &= 2y_1-y_2\\ 
		y +z &= -y_1+y_2\\
		0&=3y_1-3y_2+y_3 \\
		0&= -2y_2 +y_4
		\end{array}
		\end{equation}
		Si hacemos $y_1 = y_2 = y_3 = y_4 = 0$, entonces las soluciones del sistema describen el núcleo de $T$, es decir
		$$
		\nuc(T) = \{(x,y,z):x-z=0, y+z =0 \} = \{(s,-s,s):s \in \R \},
		$$
		que es la forma paramétrica. Una base del núcleo de $T$  es $(1,-1,1)$. 
		
		
		Con respecto a la imagen de $T$ tenemos
		\begin{equation*}
		\text{$(y_1,y_2,y_3,y_4) \in \img(T)$} \; \Leftrightarrow \;
		\begin{array}{l}
		\text{$\exists\, x,y,z \in \R$ tal que\, $2y_1-y_2 = x-z$\, y \,$-y_1+y_2 = y+z$; y} \\
		\text{$0=3y_1-3y_2+y_3$\; y\; $0= -2y_2 +y_4$.}
		\end{array}
		\end{equation*}
		Como variando los valores de $x, y,z$ es posible obtener, para cualesquiera valores de  $y_1,y_2,y_3,y_4$,\; $2y_1-y_2 = x-z$ \;y \;$-y_1+y_2 = y+z$, tenemos
		\begin{equation}\label{eq-gen-2}
		\text{$(y_1,y_2,y_3,y_4) \in \img(T)$} \quad \Leftrightarrow \quad \text{$0=3y_1-3y_2+y_3$\; y\; $0= -2y_2 +y_4$.}
		\end{equation}
		Luego, 
		$$
		\img(T) =  \{(y_1,y_2,y_3,y_4): \text{ tal que $0=3y_1-3y_2+y_3$ y $0= -2y_2 +y_4$} \}.
		$$
		Ahora bien, $$0= -2y_2 +y_4 \Leftrightarrow 2y_2 = y_4 \Leftrightarrow\; y_2 = \frac12 y_4.$$
		Por  otro lado 
		$$0=3y_1-3y_2+y_3 \Leftrightarrow 3y_1 =3y_2-y_3 \Leftrightarrow y_1 =y_2-\frac13y_3 \Leftrightarrow\; y_1 =\frac12y_4-\frac13y_3$$ 
		
		Es decir, cambiando $y-3$ por $s$ e $y_4$ por $t$:
		$$
		\img(T) =  \{(-\frac13 s + \frac12 t, \frac 12 t, s,t): s,t \in \R \}.
		$$
		Como $(-\frac13 s + \frac12 t, \frac 12 t, s,t) = s(-\frac13,0,1,0) + t(\frac12,\frac12,0,1)$ el conjunto  $\{(-\frac13,0,1,0),(\frac12,\frac12,0,1) \}$ es una base de $\img(T)$.
		
		\end{proof}

		\end{ejemplo}

		\vskip .5cm
		
		He aquí uno de los resultados más importantes del álgebra lineal.
		
		\begin{teorema}\label{rang+nul=dimV}
			 	Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal. Supóngase que $V$ es de dimensión finita. Entonces 
			 	$$
			 	rango (T) + nulidad (T) = \dim V.
			 	$$
		\end{teorema}
		\begin{proof} Sean 
			\begin{align*}
				n &= \dim V \\
				k &= \dim(\nuc T) = nulidad(T).  
			\end{align*}
			Entonces debemos probar  que 
			$$
			n-k = \dim(\img T) = rango(T).
			$$
		 	Sea $\{v_1,\ldots,v_k \}$ una base de $\nuc T$. Existen vectores $\{v_{k+1},\ldots,v_n \}$, en $V$ tales que $\{v_1,\ldots,v_n \}$ es una base de $V$. Para probar el teorema, demostraremos que 
		 	$\{Tv_{k+1},\ldots,Tv_n \}$ es una base para la imagen de $T$. 
		 	
		 	 \textit{(1) $\{Tv_{k+1},\ldots,Tv_n \}$ genera la imagen de $T$.} 
		 	 
		 	 Si $w \in \img(T)$,  entonces existe $v \in V$ tal que $T(v)=w$, como $\{v_{1},\ldots,v_n \}$ es base de $V$,  existen $\lambda_1,\ldots,\lambda_{n} \in \K$,  tal que $v=  \lambda_1 v_1+ \cdots + \lambda_n v_n$, por lo tanto 
		 	\begin{align*}
		 		w &= T(v) \\ 
		 		&=  \lambda_1T( v_1)+ \cdots + \lambda_kT( v_k)+ \lambda_{k+1}T( v_{k+1}) +\cdots+ \lambda_nT( v_n) \\
		 		&=  0+ \cdots + 0+ \lambda_{k+1}T( v_{k+1}) +\cdots+ \lambda_nT( v_n) \\
		 		&=  \lambda_{k+1}T( v_{k+1}) +\cdots+ \lambda_nT( v_n).
		 	\end{align*}
		 	Por  lo tanto, $\{Tv_{k+1},\ldots,Tv_n \}$ genera la imagen de $T$. 
		 	
		 	\textit{(2) $\{Tv_{k+1},\ldots,Tv_n \}$ es un conjunto linealmente independiente.}
		 	
		 	Para ver que  $\{Tv_{k+1},\ldots,Tv_n \}$ es linealmente independiente, supóngase que se tienen escalares $\mu_i$ tales que
		 	$$
		 	\sum_{i=k+1}^n \mu_i Tv_i = 0,
		 	$$
		 	luego
		 	$$
		 	0 = \sum_{i=k+1}^n \mu_i Tv_i =   T(\sum_{i=k+1}^n\mu_iv_i).  
		 	$$
		 	Por lo tanto $v =\sum_{i=k+1}^n \mu_iv_i\in \nuc(T)$. Como  $\{v_1,\ldots,v_k \}$ es una base de $\nuc T$,  existen escalares $\lambda_i$ tales que
		 	$$
		 	v =  \sum_{i=1}^k \lambda_i v_i,
		 	$$
		 	es decir
		 	$$
		 	\sum_{j=k+1}^n \mu_jv_j =  \sum_{i=1}^k \lambda_i v_i.
		 	$$
		 	Luego
		 	\begin{align*}
		 			0 &= \sum_{i=1}^k \lambda_i v_i - (\sum_{j=k+1}^n \mu_jv_j) \\
		 			&= \lambda_1 v_1 + \cdots +\lambda_k v_k - \mu_{k+1}v_{k+1} -\cdots-\mu_nv_n.
		 	\end{align*}
		 	Como $\{v_1,\ldots,v_n \}$ es una base, y por lo tanto un conjunto LI,  tenemos que  $0=\lambda_1=\cdots=\lambda_k=\mu_{k+1}=\cdots=\mu_n$, y  en particular $0=\mu_{k+1}=\cdots=\mu_n$. Por lo tanto $\{Tv_{k+1},\ldots,Tv_n \}$ es un conjunto linealmente independiente.		  
		\end{proof}
		
	
		
		Sea $A$ una matriz $m \times n$ con coeficientes  en $\K$. El  \textit{rango fila}\index{rango  fila} de $A$ es la dimensión del subespacio de $\K^n$ generado por las filas de $A$, es decir la dimensión del espacio fila de $A$. El \textit{rango columna}\index{rango  columna} de $A$  es es la dimensión del subespacio de $\K^m$ generado por las columna de $A$. Un  consecuencia importante del teorema \ref{rang+nul=dimV} es le siguiente resultado. 
		
		\begin{teorema}
			Si $A$ es una matriz $m \times n$ con coeficientes  en $\K$, entonces
			$$
			\text{\textit{rango fila }} (A) = \text{\textit{rango  columna }} (A).
			$$
		\end{teorema}
		\begin{proof}
			Sea $T$ la transformación lineal
			\begin{equation*}
				\begin{array}{lllll}
				&T: &\K^{n \times 1} &\to &\K^{m \times 1} \\
				&&X &\mapsto &AX.
				\end{array}
			\end{equation*}
			Observar que
			$$
			\nuc(T) = \{X \in \K^{n \times 1}: AX=0 \}.
			$$
			Es decir $\nuc(T)$  es el subespacio de soluciones del sistema homogéneo $AX=0$. Ahora bien, si $k = \text{\textit{rango fila }} (A)$, ya hemos dicho (capítulo \ref{chap-esp-vect}, sección \ref{sec-dimensiones-de-subespacios}) que la dimensión del subespacio de soluciones del sistema homogéneo $AX=0$ es $n-k$. Luego
			\begin{equation}\label{rangofila=n-nulT}
				\text{\textit{rango fila }} (A) = \dim V - \text{\textit{nulidad}}(T). 
			\end{equation}
			
			Por otro lado 
			$$
			\img(T) = \{AX: X \in \K^{n \times 1} \}.
			$$
			Ahora bien, 
			$$
			AX = 
			\begin{bmatrix} a_{11} x_1 +\cdots a_{1n} x_n \\ \vdots \\ a_{m1} x_1 +\cdots a_{mn} x_n  \end{bmatrix}
			= 
			x_1\begin{bmatrix} a_{11} \\ \vdots \\ a_{m1} \end{bmatrix} + \cdots +
			x_n\begin{bmatrix}  a_{1n}  \\ \vdots \\ a_{mn}  \end{bmatrix}
			$$
			Es decir, que la imagen de $T$ es el espacio generado por las columnas de $A$. Por tanto,
			$$
			\text{\textit{ rango}}(T) =\text{\textit{ rango  columna }}(A).
			$$
			Por  el  teorema \ref{rang+nul=dimV}
			$$
			\text{\textit{ rango}}(T) = \dim V - \text{\textit{nulidad}}(T),
			$$
			y por lo tanto
			\begin{equation}\label{rangocolumna=n-nulT}
			\text{\textit{rango columna }} (A) = \dim V - \text{\textit{nulidad}}(T). 
			\end{equation}
			
			Obviamente, las igualdades (\ref{rangofila=n-nulT}) y (\ref{rangocolumna=n-nulT}) implican 
			$$
		\text{\textit{rango fila }} (A) = \text{\textit{rango  columna }} (A).
		$$
		\end{proof}
	
		\begin{definicion}
				Si $A$ es una matriz $m \times n$ con coeficientes  en $\K$,  entonces el \textit{rango} de $A$ es el rango fila de $A$ (que es igual al rango columna).
		\end{definicion}
		
		\end{section}
	
		\begin{section}{Isomorfismos de espacios vectoriales}
		
		\begin{definicion}
			Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal.
			\begin{enumerate}
				\item $T$  es \textit{epimorfismo}\index{epimorfismo} si $T$ es suryectiva, es decir si $\img(T) = W$.
				\item $T$ es \textit{monomorfismo}\index{monomorfismo} si $T$ es inyectiva (o 1-1),  es decir si dados $v_1,v_2 \in V$ tales que $T(v_1) = T(v_2)$,  entonces $v_1 = v_2$.
			\end{enumerate}  
		\end{definicion}
	
		\begin{obs}
			$T$  es epimorfismo si y sólo si 
			$$
			\text{$T$ es lineal y }\forall\, w \in W, \; \exists v \in V \text{ tal que }T(v)=w.
			$$
			Esto se deduce inmediatamente de la definiciones de función suryectiva y de $\img(T)$
			
			$T$ es monomorfismo si y sólo si 
			$$
				\text{$T$ es lineal y }\forall\, v_1,v_2  \in V: \; v_1 \ne v_2 \Rightarrow T(v_1) \not= T(v_2).
			$$ 
			Esto se obtiene aplicando el contrarrecíproco a la definición de función inyectiva.
		\end{obs}	
		
		
			
		\begin{proposicion}\label{inyectiva-sii-nuT=0}
			Sea $T:V \to W$ una transformación lineal. Entonces $T$ es monomorfismo si y sólo si $\nuc(T) =0$.
		\end{proposicion}	
		\begin{proof} 
			
			\
			
			($\Rightarrow$) Debemos ver que  $\nuc(T)=0$,  es decir que si $T(v)=0$,  entonces  $v=0$. Ahora bien,  si $T(v) = 0$, como  $T(0)=0$, tenemos que $T(v)  = T(0)$, y como $T$ es inyectiva, implica que $v =0$.
			
			($\Leftarrow$) Sean  $v_1,v_2 \in V$ tal que $T(v_1)=T(v_2)$. Entonces 
			$$
			0 = T(v_1)- T(v_2) = T(v_1 -v_2).
			$$
			Por  lo tanto, $v_1 -v_2 \in \nuc(T)$. Por hipótesis, tenemos que $v_1 -v_2 =0$,  es decir $v_1 = v_2$.
		\end{proof}
		
		
		\begin{obs} Sea $T: V \to W$ transformación lineal, 
			\begin{enumerate}
				\item $T$  es {epimorfismo} si y sólo  si $\img(T) = W$ si  y solo si $rango(T) = \dim W$.
				\item $T$ es {monomorfismo} si y sólo  si $\nuc(T) = 0$ si y sólo si $nulidad(T) =0$.
			\end{enumerate}  
		\end{obs}		
		
		\begin{proposicion}\label{prop-T-mono-sii-li-2-li} Sea $T:V \to W$ transformación lineal. Entonces,
			\begin{enumerate}
				\item $T$ es monomorfismo si y sólo si $T$ de un conjunto LI  es  LI.
				\item $T$ es epimorfismo si y sólo si $T$ de un conjunto de generadores de $V$ es un conjunto de generadores de $W$.
			\end{enumerate}
		\end{proposicion}
		\begin{proof}
			
			\
			
			
			(1) ($\Rightarrow$) Sea $\{v_1,\ldots,v_n \}$ un conjunto LI en $V$ y sean  $\lambda_1,\ldots,\lambda_n \in \K$ tales que
			$$
			\lambda_1T(v_1) +\cdots+ \lambda_{n}T(v_n) =0,
			$$
			entonces
			$$
			0 = T(\lambda_1v_1+\cdots + \lambda_{n}v_n).
			$$
			Como $T$  es inyectiva, por proposición \ref{inyectiva-sii-nuT=0}, 
			$$
			\lambda_1v_1+\cdots + \lambda_{n}v_n =0,
			$$
			lo cual implica que $\lambda_1,\ldots,\lambda_n$ son todos nulos. Por lo tanto,  $T(v_1),\ldots,T(v_n)$ son LI.
			
			
			(1) ($\Leftarrow$) Sea $v \in V$ tal que $T(v)=0$. Veremos que eso implica que $v =0$.  Ahora bien, sea  $\{v_1,\ldots,v_n \}$ una base de $V$,  entonces existen $\lambda_1,\ldots,\lambda_n \in \K$ tales que
			$$
			v = \lambda_1v_1+\cdots + \lambda_{n}v_n,
			$$
			por lo tanto
			$$
			0 = T(v) = T(\lambda_1v_1+\cdots + \lambda_{n}v_n) = \lambda_1T(v_1) +\cdots+ \lambda_{n}T(v_n).
			$$
			Como $\{v_1,\ldots,v_n \}$ es LI, por hipótesis, $\{T(v_1),\ldots,T(v_n)\}$ es LI y, por lo tanto, $\lambda_1,\ldots,\lambda_n$ son todos nulos. Luego $v=0$. Es decir probamos  que el núcleo de $T$ es 0, luego por proposición \ref{inyectiva-sii-nuT=0}, $T$ es monomorfismo. 

		(1) ($\Leftarrow$/\textit{alternativa}) Sea $v \in V$ tal que $T(v)=0$.	Si $v\neq 0$, entonces $\{v\}$ es un conjunto LI en $V$. Luego, $\{T(v)\}$ es un conjunto LI en $W$ y por lo tanto $T(v)\neq 0$. Así, si $T(v)=0$ entonces $v=0$ y por lo tanto $T$ es un monomorfismo.
			  
			(2) ($\Rightarrow$) Sea   $\{v_1,\ldots,v_n \}$ un conjunto de generadores de $V$ y sea  $w \in W$. Como $T$  es epimorfismo, existe $v \in V$ tal que $T(v)=w$. Ahora bien, 
			$$
			v = \lambda_1v_1+\cdots + \lambda_{n}v_n,\, \text{ para algún $\lambda_1,\ldots,\lambda_n \in \K$,}
			$$
			por lo tanto,
			$$
			w =T(v) = T(\lambda_1v_1+\cdots + \lambda_{n}v_n) = \lambda_1T(v_1) +\cdots+ \lambda_{n}T(v_n).
			$$ 
			Es decir,  cualquier $w \in W$ se puede escribir como combinación lineal de los  $T(v_1),\ldots,T(v_n)$ y, por lo tanto,  generan $W$.
			
			(2) ($\Leftarrow$) Sea $\{v_1,\ldots,v_n \}$ una base de $V$, por hipótesis $T(v_1),\ldots,T(v_n)$ generan $W$,  es decir dado cualquier $w \in W$,   existen $\lambda_1,\ldots,\lambda_n \in \K$ tales que
			$$
			w = \lambda_1T(v_1)+\cdots + \lambda_{n}T(v_n),
			$$
			y por lo tanto $w = T(v)$,  con 
			$$
			v = \lambda_1v_1+\cdots + \lambda_{n}v_n.
			$$
		\end{proof}
		
			

		\begin{definicion}
			Si $V$ y $W$ son espacios vectoriales sobre el cuerpo $\K$, toda transformación	lineal $T:V \to W$ suryectiva e inyectiva, se dice \textit{isomorfismo}\index{isomorfismo} de $V$ sobre $W$. Si existe un isomorfismo de $V$ sobre $W$, se dice que $V$ es \textit{isomorfo} a $W$ y se denota $V \cong W$.
		\end{definicion}
		
		
		
		Obsérvese que $V$ es trivialmente isomorfo a $V$, ya que el operador identidad es un isomorfismo de $V$ sobre $V$. 
			
		Recordemos que si una función $f: X \to Y$ es suryectiva e inyectiva, es decir biyectiva, existe su inversa, la cual también es biyectiva. La inversa se denota $f^{-1}: Y \to X$ y viene definida por
		$$
		f^{-1}(y) = x \Leftrightarrow f(x) =y.
		$$ 
		
		\begin{teorema}
			Sea $T:V \to W$ un isomorfismo. Entonces $T^{-1}: W \to V$ es lineal y, por lo tanto, también es un isomorfismo.
		\end{teorema}
		\begin{proof}
			\
			
			(1) Sean $w_1, w_2 \in W$ y sean $v_1 = T^{-1}(w_1) $, $v_2 = T^{-1}(w_2)$. Por lo tanto $T(v_1) = w_1$ y $T(v_2) = w_2$. Ahora bien,
			\begin{multline*}
				T^{-1}(w_1+w_2) = 	T^{-1}(T(v_1)+T(v_2))  = 	T^{-1}(T(v_1+v_2)) = \\ =(T^{-1}\circ T)(v_1+v_2) = v_1+v_2 = T^{-1}(w_1)+ T^{-1}(w_2). 
			\end{multline*}    
			
			(2) Sean $w \in W$ y $\lambda  \in \K$. Sea $v = T^{-1}(w)$, entonces
			\begin{equation*}
			T^{-1}(\lambda w) = 	T^{-1}(\lambda T(v))  = 	T^{-1}(T(\lambda v)) = \\ =(T^{-1}\circ T)(\lambda v) = \lambda v = \lambda  T^{-1}(w). 
			\end{equation*}
				\end{proof}
		
		
		\begin{ejercicio} 
			Sean $V$, $W$ y $Z$ espacios vectoriales sobre el cuerpo $\K$ y sean $T:V \to W$, $S:W \to Z$ isomorfismos. Entonces, 
			\begin{enumerate}
				\item $S\circ T:V \to Z$  también es un isomorfismo y
				\item 	$(S\circ T)^{-1} = T^{-1}\circ S^{-1}$.
			\end{enumerate}
		\end{ejercicio}
		
		
		Como ya se ha dicho, $V$ es isomorfo a $V$ vía la identidad. Por  el teorema anterior, si $V$ es isomorfo a $W$,  entonces $W$ es isomorfo a $V$. Por  el ejercicio anterior, si $V$ es isomorfo a $W$ y $W$ es isomorfo a $Z$, entonces $V$ es isomorfo a $Z$. En resumen, el isomorfismo es una relación de equivalencia sobre la clase de espacios vectoriales. Si existe un isomorfismo de $V$ sobre $W$, se dirá a veces que \textit{$V$ y $W$ son isomorfos}, en vez de que $V$ es isomorfo a $W$. Ello no será motivo de confusión porque $V$ es isomorfo a $W$, si, y solo si, $W$ es isomorfo a $V$.	
			
		
			
			
		\begin{teorema}
			Sean $V,W$ espacios vectoriales de dimensión  finita sobre $\K$ tal que $\dim V = \dim W$. Sea $T: V \to W$ transformación lineal. Entonces,  son equivalentes:
			\begin{enumerate}[label=(\alph*),ref=\alph*]
				\item\label{a-dimV=dimW} $T$ es un  isomorfismo.
				\item\label{b-dimV=dimW} $T$ es monomorfismo.
				\item\label{c-dimV=dimW} $T$ es epimorfismo.
				\item\label{d-dimV=dimW} Si $\{v_1,\ldots,v_n \}$ es una base de $V$,  entonces $\{T(v_1),\ldots,T(v_n) \}$ es una base de $W$.
			\end{enumerate}
		\end{teorema}
		\begin{proof} Sea $n = \dim V = \dim W$.
			
			
			(\ref{a-dimV=dimW}) $\Rightarrow$ (\ref{b-dimV=dimW}). Como $T$ es isomorfismo,  es biyectiva y por lo tanto inyectiva.
			
			(\ref{b-dimV=dimW}) $\Rightarrow$ (\ref{c-dimV=dimW}). $T$ monomorfismo,  entonces $nulidad(T) = 0$ (proposición \ref{inyectiva-sii-nuT=0}). Luego, como  $rango(T) +nulidad(T) = \dim V$,  tenemos que $rango(T) = \dim V$. Como $\dim V = \dim W$, tenemos que $\dim \img(T) = \dim W$ y por lo tanto $\img(T) = \dim W$. En  consecuencia, $T$ es suryectiva.
			
			(\ref{c-dimV=dimW}) $\Rightarrow$ (\ref{a-dimV=dimW}). $T$ es suryectiva, entonces $rango(T) = n$, luego  $nulidad(T) = 0$, por lo tanto $\nuc(T)=0$ y en consecuencia $T$ es inyectiva. Como $T$ es suryectiva e inyectiva  es un isomorfismo. 
			
			\vskip .3cm
			
			Hasta aquí probamos que (\ref{a-dimV=dimW}), (\ref{b-dimV=dimW}) y (\ref{c-dimV=dimW}) son equivalentes, luego si probamos que (\ref{a-dimV=dimW}), (\ref{b-dimV=dimW}) o (\ref{c-dimV=dimW}) $\Rightarrow$ (\ref{d-dimV=dimW}) y que (\ref{d-dimV=dimW}) $\Rightarrow$ (\ref{a-dimV=dimW}), (\ref{b-dimV=dimW}) o (\ref{c-dimV=dimW}), estaría probado el teorema. 
			
			\vskip .3cm			
			
			(\ref{a-dimV=dimW}) $\Rightarrow$ (\ref{d-dimV=dimW}). Sea $\{v_1,\ldots,v_n \}$  una base de $V$,  entonces $\{v_1,\ldots,v_n \}$ es LI y  genera $V$. Por proposición \ref{prop-T-mono-sii-li-2-li}, tenemos que $\{T(v_1),\ldots,T(v_n) \}$ es LI y  genera $W$, por lo tanto $\{T(v_1),\ldots,T(v_n) \}$ es una base de $W$.
			
			(\ref{d-dimV=dimW}) $\Rightarrow$ (\ref{a-dimV=dimW}). Como $T$ de una base es una base,  entonces $T$  de un conjunto LI es un conjunto LI y $T$ de un conjunto de generadores de $V$  es un conjunto de generadores de $W$. Por lo tanto, por proposición \ref{prop-T-mono-sii-li-2-li}, $T$ es monomorfismo y epimorfismo, luego $T$ es un isomorfismo. 	
		\end{proof}
	
		\begin{corolario}
			Sean $V,W$ espacios vectoriales de dimensión  finita sobre $\K$ tal que $\dim V = \dim W$. Entonces $V$ y $W$ son  isomorfos. 
		\end{corolario}
		\begin{proof}
			Sea $\{v_1,\ldots,v_n \}$ es una base de $V$ y $\{w_1,\ldots,w_n \}$ es una base de $W$. Poe teorema \ref{th-tl-definida-en-base} existe una única transformación lineal $T:V \to W$ tal que
			\begin{equation*}
			T(v_i) = w_i, \qquad i=1,\ldots, n.
			\end{equation*}
			Por  el teorema anterior, $T$  es un isomorfismo.
		\end{proof}
	
	
	\begin{ejemplo} $\K_n[x] = \{a_0+a_1x+\cdots+a_{n-1}x^{n-1}: a_0,a_1, \ldots,a_{n-1} \in \K \}$  es isomorfo a $\K^n$, esto es  consecuencia inmediata del corolario anterior, pues ambos tienen dimensión $n$. Explícitamente, $1,x,\ldots,x^{n-1}$  es base de $\K_n[x]$ y sea $e_1,\ldots,e_n$ la base canónica  de $\K^n$,  entonces un isomorfismo de $\K_n[x]$ a $\K^n$ viene dado por la única transformación lineal $T:\K_n[x] \to\K^n$ tal que
		$$
		T(x^i) = e_{i+1},\qquad i=0,\ldots, n-1.
		$$   
		
	\end{ejemplo}

	\begin{ejemplo} $M_{m \times n}(\K)$  es isomorfo a $\K^{mn}$. El isomorfismo viene dado por $T: M_{m \times n}(\K) \to \K^{mn}$ tal que
		$$
		T(E_{ij}) = e_{(i-1)n+j}, \qquad i=1,\ldots, m,\; j=1,\ldots, n.
		$$
	Por  ejemplo, en el caso  $2 \times 2$,
	\begin{equation*}
	\begin{array}{llll}
	\begin{bmatrix} 1&0\\0&0\end{bmatrix} &\mapsto (1,0,0,0) \qquad&
	\begin{bmatrix} 0&1\\0&0\end{bmatrix} &\mapsto (0,1,0,0) \\
	&&&\\
	\begin{bmatrix} 0&0\\1&0\end{bmatrix} &\mapsto (0,0,1,0) &
	\begin{bmatrix} 0&0\\0&0\end{bmatrix} &\mapsto (0,0,0,1).
	\end{array}
	\end{equation*}	 
		
	\end{ejemplo}
	
		\end{section}
	
		\begin{section}{Álgebra de las transformaciones lineales}
			En el estudio de las transformaciones lineales de $V$ en $W$ es de fundamental importancia que el conjunto de estas transformaciones hereda una estructura natural de espacio vectorial. El conjunto de las transformaciones lineales de un espacio $V$ en sí mismo tiene incluso una estructura algebraica mayor, pues la composición ordinaria de funciones da una ``multiplicación'' de tales transformaciones. 
			
			
			Observemos primero que si $X$ conjunto y $W$ espacio vectorial sobre el cuerpo $\K$,  entonces
			$$
			F(X,W) := \{f:X\to W\},
			$$ 
			es decir el conjunto de funciones de $X$ en $W$ es un espacio vectorial sobre $\K$ con la suma y el producto por escalares definido:
			\begin{equation*}
			\begin{array}{rlcl}
			(f+g)(x) &= f(x)+ g(x),\qquad &f,g& \in F(X,W),\; x \in X\\
			(\lambda f)(x) &= \lambda f(x), & f& \in F(X,W),\; x \in X, \; \lambda \in \K.
			\end{array}
			\end{equation*}
			La demostración de esto es sencilla y se basa en el hecho que $W$  es un espacio vectorial. 
			
			\begin{teorema}
				Sean $V$ y $W$ espacios vectoriales sobre el cuerpo $\K$ Sean $T,S : V \to W$ transformaciones y $\mu \in \K$. Entonces, $T + S$ y $\mu T$ son transformaciones lineales de $V$ en $W$.
			\end{teorema}
			\begin{proof}
				Sean $v,v' \in V$ y $\lambda \in \K$, entonces
				\begin{equation*}
				\begin{array}{rlll}
					(T + S)(\lambda v + v') &= T(\lambda v + v') + S(\lambda v + v')&\qquad&\text{(definición de $T+S$)} \\
					&= \lambda T(v) + T(v') + \lambda S(v) + S(v')& &\text{($T$ y $S$ lineales)}\\
					&= \lambda (T(v) +S(v)) + T(v') + S(v')&&\text{}\\
					&= \lambda ((T+S)(v)) + (T + S) (v')& &\text{(definición de $T+S$)}\\
					&= \lambda(T + S)(v) +(T + S) (v')&&\text{(definición de $\lambda(T + S)$)}.
				\end{array}
				\end{equation*}
				que dice que $T + U$ es una transformación lineal. En forma análoga, si $\mu \in \K$, 
				\begin{equation*}
				\begin{array}{rlll}
				(\mu T)(\lambda v + v') &= \mu T(\lambda v + v')&\qquad&\text{(definición de $\mu T$)} \\
				&= \mu \lambda T(v) + \mu T(v') &\qquad&\text{($T$ lineal)}\\
				&=  \lambda \mu T(v) + \mu T(v')&\qquad&\text{}\\
				&= \lambda (\mu T)(v) + (\mu T)(v') &\qquad&\text{(definición de $\mu T$)}.
				\end{array}
				\end{equation*}
				que dice que $\mu T$ es una transformación lineal.
			\end{proof}
			
		\begin{corolario}
			Sean $V$ y $W$ espacios vectoriales sobre el cuerpo $\K$. Entonces, el conjunto de transformaciones lineales de $V$ en $W$ es un subespacio vectorial  de $F(V,W)$. 
		\end{corolario} 	
			
		Se denotará  $L(V,W)$ al espacio vectorial de las transformaciones lineales de $V$ en $W$.
		
		\begin{teorema}\label{th-6-hoffman}
			Sean $V$, $W$ y $Z$ espacios vectoriales sobre el cuerpo $\K$.  Sean $T: V \to W$ y $U: W \to Z$ 
			transformaciones lineales. Entonces la función compuesta $U\circ T$ definida por $(U\circ T)(v) = U(T(v))$ es una transformación lineal de $V$ en $Z$.
		\end{teorema} 
		\begin{proof} Sean $v, v' \in V$ y $\lambda \in \K$, entonces
			\begin{equation*}
			\begin{array}{rlll}
				(U\circ T)(\lambda v + v') &= U(T(\lambda v + v'))&\qquad&\text{(definición de composición)} \\
				 &= U(\lambda T( v) + T(v'))&\qquad&\text{($T$ lineal)} \\
				 &= \lambda U(T( v)) + U(T(v'))&\qquad&\text{($U$ lineal)} \\
				 &= \lambda (U\circ T)( v) + (U\circ T)(v')&\qquad&\text{(definición de composición)}. \\
			\end{array}
			\end{equation*}
			
		\end{proof}
		
		Para simplificar, a veces  denotaremos la composición por yuxtaposición,  es decir $$U\circ T = U T.$$
		
		
		En lo que sigue debemos interesarnos principalmente en transformaciones lineales de un espacio vectorial en sí mismo. Como se tendrá a menudo que 	escribir ``$T$ es una transformación lineal de $V$ en $V$'', se dirá más bien: ``$T$ es un operador lineal sobre $V$''. 
		
		\begin{definicion}
			Si $V$ es un espacio vectorial sobre el cuerpo $\K$, un \textit{operador lineal}\index{operador lineal} sobre $V$ es una transformación lineal de $V$ en $V$.
		\end{definicion}
		
		Cuando  en el teorema \ref{th-6-hoffman}, consideramos $V = W = Z$, tenemos que $U$ y $T$ son opera-
		dores lineales en el espacio $V$, y por lo tanto la composición $UT$ es también un operador lineal sobre $V$. Así, el espacio $L(V, V)$ tiene una ``multiplicación'' definida por composición. En este caso el operador $TU$ también está definido, y debe observarse que en general $UT \not= TU$, es decir, $UT - TU \not= 0$. Se ha de advertir de manera especial que si $T$ es un operador lineal sobre $V$, entonces se puede componer $T$ con $T$. Se usará para ello la notación $T^2 = TT$, y en general $T^n = T \cdots T$ ($n$ veces) para $n = 1, 2, 3, \ldots$\,  Si $T \ne 0$, se define $T^0 = I_V$, el operador identidad.
			
		\begin{lema}
			Sea $V$ un espacio vectorial sobre el cuerpo $\K$; sean $U$, $T$ y $S$ operadores lineales sobre $V$ y sea $\lambda$ un elemento de $\K$. Denotemos $I_V$ el operador identidad. Entonces
			\begin{enumerate}
				\item $U = I_VU = UI_V$,
				\item $U(T+S) = UT + US$, $(T+S)U = TU + SU$,
				\item $\lambda (UT) = (\lambda U)T = U (\lambda T)$.
			\end{enumerate}¡
		\end{lema}
		\begin{proof}
			(1) es trivial. 
			
			Demostraremos $U(T+S) = UT + US$ de (2) y todo lo demás se dejará como ejercicio.
			Sea $ v \in V$, entonces
			\begin{equation*}
			\begin{array}{rlll}
			U(T+S)(v) &= U((T+S)(v))&\qquad&\text{(definición de composición)} \\
			&= U(T(v)+S(v))&\qquad&\text{(definicion  de $T+S$)} \\
			&= U(T(v))+U(S(v))&\qquad&\text{($U$ lineal)} \\
			&= UT(v)+US(v)&\qquad&\text{(definición de composición)}.
			\end{array}
			\end{equation*}  
		\end{proof}	
	
	El contenido de este lema, y algunos otros resultados  sobre composición de funciones de un conjunto en si mismo (como ser la asociatividad), dicen que el espacio vectorial $L(V, V)$, junto con la operación de composición, es lo que se conoce tomo una  álgebra asociativa sobre $\K$,  con identidad (ver \href{https://es.wikipedia.org/wiki/Álgebra\_asociativa}{https://es.wikipedia.org/wiki/Álgebra\_asociativa}). 
	
	

		\end{section}
	
		\begin{section}{Matriz de una transformación lineal}
			Sea $V$ un espacio vectorial de dimensión $n$ sobre el cuerpo $\K$, y sea $W$ un espacio vectorial de dimensión $m$ sobre $\K$. Sea $\mathcal B = \{v_1,\ldots,v_n\}$ una base ordenada de $V$, y $\mathcal B' = \{w_1,\ldots,w_n\}$ una base ordenada de $W$. Si $T$ es cualquier 		transformación lineal de $V$ en $W$, entonces $T$ está determinada por su efecto sobre los vectores $v_j$, puesto que todo vector de $V$ es combinación lineal de ellos. Cada uno de los $n$ vectores $Tv_j$ se expresa de manera única 	como combinación lineal
			\begin{equation}\label{matriz-de-T-1}
				Tv_j = \sum_{i=1}^{m} a_{ij} w_i
			\end{equation}
			de los $w_i$. Los escalares $a_{1j},\ldots,a_{mj}$ son las coordenadas de $Tv_j$ en la base or	denada $\mathcal B'$. Por consiguiente, la transformación $T$ está determinada por los
			$mn$ escalares $a_{ij}$ mediante la expresión (\ref{matriz-de-T-1}). 
			
			\begin{definicion}
				La matriz $m \times n$, $A$, definida por $[A]_{ij} = a_{ij}$, donde $a_{ij}$  es como en  (\ref{matriz-de-T-1}), se llama \textit{matriz de $T$ respecto a las bases ordenadas $\mathcal B$ y $\mathcal B'$;}\index{matriz de una transformación lineal} y se denota 
				$$
				[T]_{\mathcal B \mathcal B'} = A .
				$$.
			\end{definicion}
			
			\begin{ejemplo}
				Sea $T: \R^3 \to \R^4$ definida
				$$
				T(x,y,z) = (2x+y, 3y, x+4z,z).
				$$
				Sean  $B = \{e_1,e_2,e_3\}$ la base canónica de $\R^3$ y  $B' = \{e_1,e_2,e_3,e_4\}$ la base canónica de $\R^4$. Entonces
				\begin{equation*}
					\begin{array}{ccccrcrcrcr}
					T(e_1) & = & (2,0,1,0) & = &  2e_1& + & 0.e_2& + &   e_3& + & 0.e_4  \\
					T(e_2) & = & (1,3,0,0) & = &   e_1& + &  3e_2& + & 0.e_3& + & 0.e_4  \\
					T(e_3) & = & (0,0,4,1) & = & 0.e_1& + & 0.e_2& + &  4e_3& + &   e_4
					\end{array}
				\end{equation*}
				Por  lo tanto
				\begin{equation*}
						[T]_{\mathcal B \mathcal B'} =\begin{bmatrix} 2&1&0 \\0&3&0 \\1&0&4 \\0&0&1
					\end{bmatrix}.
				\end{equation*}
				Observar que si escribimos los vectores en coordenadas con respecto  a las bases canónicas,  tenemos que 
				\begin{equation*}
				\begin{bmatrix} 2&1&0 \\0&3&0 \\1&0&4 \\0&0&1
				\end{bmatrix}
				\begin{bmatrix} x \\ y \\z\end{bmatrix} = 
				\begin{bmatrix} 2x+y\\ 3y\\ x+4z\\z\end{bmatrix}
				\end{equation*}
				o más formalmente
				\begin{equation*}
					[T]_{\mathcal B \mathcal B'} [v]_{\mathcal B} = [T(v)]_{\mathcal B'}.
				\end{equation*}
			\end{ejemplo}
		
		
			\begin{definicion}
				Sa $A$ matriz $m \times n$,  el \textit{operador lineal asociado a $A$} es $A: \R^n \to \R^m$  definido $A(X) = AX$. Es decir
				\begin{equation}
					A(x_1,\ldots,x_n):= \begin{bmatrix} a_{11} &\cdots &a_{1n} \\ \vdots &\ddots& \vdots \\a_{m1} &\cdots &a_{mn}
					\end{bmatrix}\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}.
				\end{equation}
			\end{definicion}	
		
			\begin{ejemplo}
					Sa $A$ matriz $m \times n$, entonces  el operador lineal asociado a $A$ en las bases canónicas es $A$. Es decir  si $\mathcal B$ y $\mathcal B'$ son las bases canónicas de $\R^n$ y $\R^m$,  respectivamente, entonces 
					\begin{equation*}
						[A]_{\mathcal B\mathcal B'} =A.
					\end{equation*}
			\end{ejemplo}	
			
			\begin{proposicion} 	Sea $V$  y $W$ un espacios vectoriales de dimensión $n$ y $m$ respectivamente y sea $T: V \to W$ una transformación lineal. Sea $\mathcal B = \{v_1,\ldots,v_n\}$ una base ordenada de $V$, y $\mathcal B' = \{w_1,\ldots,w_n\}$ una base ordenada de $W$. Entonces
			\begin{equation}\label{matriz-de-tl-2}
			[T]_{\mathcal B \mathcal B'} [v]_{\mathcal B} = [T(v)]_{\mathcal B'}, \quad \forall \,v \in V.
			\end{equation}	
			\end{proposicion}
		\begin{proof}
			Si 
			\begin{equation*}
			Tv_j = \sum_{i=1}^{m} a_{ij} w_i
			\end{equation*}
			entonces $[T]_{ij} = a_{ij}$. Sea $v \in $,  entonces $v = x_1v_1+\cdots+x_n v_n$ con $x_i \in \K$, por lo tanto 
			$$
			[v]_{\mathcal B} = \begin{bmatrix} x_1\\ \vdots\\x_n\end{bmatrix}.
			$$
			Ahora bien,
			\begin{multline*}
				T(v) = T(\sum_{j=1}^{n}x_jv_j) = \sum_{j=1}^{n}x_jT(v_j) =  \sum_{j=1}^{n}\sum_{i=1}^{m} a_{ij} w_i = \sum_{i=1}^{m} (\sum_{j=1}^{n}a_{ij}) w_i =\\
				= ( \sum_{j=1}^{n}a_{1j})w_1+(\sum_{j=1}^{n}a_{2j})w_2+\cdots+(\sum_{j=1}^{n}a_{mj})w_m
			\end{multline*}
			y, por lo tanto,
			\begin{equation}\label{eq-45}
			[T(v)]_{\mathcal B'} = \begin{bmatrix}  \sum_{j=1}^{n}a_{1j}\\\sum_{j=1}^{n}a_{2j}\\ \vdots\\\sum_{j=1}^{n}a_{mj}\end{bmatrix}.
			\end{equation}
			Por otro lado, 
			\begin{equation}\label{eq-46}
			[T]_{\mathcal B \mathcal B'} [v]_{\mathcal B} = 
			\begin{bmatrix} a_{11} &a_{12}&\cdots & a_{1n}
			\\a_{21} &a_{22}&\cdots & a_{2n}\\ \vdots &\vdots&& \vdots \\ a_{m1} &a_{m2}& \cdots & a_{mn}\end{bmatrix}
			\begin{bmatrix} x_1\\x_2\\ \vdots\\x_n\end{bmatrix}
			=
			\begin{bmatrix}  \sum_{j=1}^{n}a_{1j}\\\sum_{j=1}^{n}a_{2j}\\ \vdots\\\sum_{j=1}^{n}a_{mj}\end{bmatrix}.
			\end{equation}
			De las ecuaciones (\ref{eq-45}) y (\ref{eq-46}) se deduce la formula (\ref{matriz-de-tl-2}).
		\end{proof}
		
		\begin{teorema}
			Sea $V$  y $W$ un espacios vectoriales de dimensión $n$ y $m$ respectivamente y $\mathcal B = \{v_1,\ldots,v_n\}$ y $\mathcal B' = \{w_1,\ldots,w_m\}$ dos bases  ordenadas de $V$ y $W$ respectivamente. Entonces 
			$$
			\kappa: L(V,W) \to M_{m \times n}(\K)
			$$
			definida
			$$
			T \mapsto [T]_{\mathcal B \mathcal B'},
			$$
			es un isomorfismos de espacios vectoriales. 
		\end{teorema}
		\begin{proof}[Demostracion ($*$)]
			Primero probaremos que $\kappa$  es lineal y luego que tiene inversa.
			
			Sean $T,T' \in L(V,W)$ y $\lambda \in \K$, veamos que $\kappa(\lambda T+ T') = \lambda \kappa(T)+ \kappa(T')$,  es decir
			\begin{equation}\label{kappa-lineal}
				[\lambda T+ T']_{\mathcal B \mathcal B'} = \lambda[T]_{\mathcal B \mathcal B'}+ [T']_{\mathcal B \mathcal B'}.
			\end{equation}
			Para $1 \le j \le n$, sean 
			\begin{equation*}
				T(v_j) = \sum_{i=1}^m a_{ij} w_i\qquad \text{ y } \qquad T'(v_j) = \sum_{i=1}^m a'_{ij} w_i,
			\end{equation*}
			es  decir
			\begin{equation*}
			[T]_{\mathcal B \mathcal B'} =	[a_{ij}] \qquad\text{ y } \qquad [T']_{\mathcal B \mathcal B'} =	[a'_{ij}],
			\end{equation*}
			entonces
			\begin{align*}
				(\lambda T+ T')(v_j) &= \lambda T(v_j)+ T'(v_j) \\
				&=\lambda \sum_{i=1}^m a_{ij} w_i+ \sum_{i=1}^m a'_{ij} w_i \\
				&= \sum_{i=1}^m (\lambda a_{ij}+ a'_{ij}) w_i,
			\end{align*}
			por lo tanto
			\begin{equation*}
				[\lambda T+ T']_{\mathcal B \mathcal B'} = [\lambda a_{ij}+ a'_{ij}] =
				\lambda[T]_{\mathcal B \mathcal B'}+ [T']_{\mathcal B \mathcal B'}
			\end{equation*}
			y hemos probado (\ref{kappa-lineal}) y, en consecuencia, $\kappa$  es lineal.
			
			\vskip .3cm
			
			Definamos ahora la inversa de $\kappa$: sea $A = [a_{ij}]$ matriz $m \times n$ y sea $T: V \to W$ la única transformación lineal que satisface, para $1 \le j \le n$, que
			$$
			T(v_j) = \sum_{i=1}^m a_{ij} w_i.
			$$
			Es claro que esta aplicación tiene dominio en $M_{m \times n}(\K)$ y su imagen está contenida en  $L(V,W)$. Más aún,  es muy sencillo comprobar que es la aplicación inversa a $\kappa$.
			
		\end{proof}
	
		
		
		\begin{teorema}\label{th-5.5}
			Sean $V$, $W$ y $Z$ espacios vectoriales de dimensión finita sobre el cuerpo $\K$; sean $T: V \to W$ y $U: W \to Z$  transformaciones lineales.  Si $\mathcal B$, $\mathcal B'$ y $\mathcal B''$ son bases ordenadas de los espacios $V$, $W$ y $Z$, respectivamente, entonces
			\begin{equation}
				[UT]_{\mathcal B\mathcal B''} = 	[U]_{\mathcal B'\mathcal B''} 	[T]_{\mathcal B\mathcal B'}.  
			\end{equation} 
		\end{teorema}
		\begin{proof}
			Sean
			\begin{equation*}
				 \mathcal B = \{v_1,\ldots,v_n\},\quad\mathcal B' = \{w_1,\ldots,w_m\},\quad  \mathcal B''= \{z_1,\ldots,z_l\}
			\end{equation*}
			y
			\begin{equation*}
			T(v_j) = \sum_{i=1}^m a_{ij} w_i, \; 1 \le j \le n; \qquad U(w_i) = \sum_{k=1}^l b_{ki} z_k, \; 1 \le i \le m.
			\end{equation*}
			Es decir 
			$$
			[T]_{\mathcal B\mathcal B'} = [a_{ij}]\qquad \text{y} \qquad [U]_{\mathcal B'\mathcal B''} = [b_{ij}]. 
			$$
			Entonces
			\begin{align*}
				(UT)(v_j) &= U(\sum_{i=1}^m a_{ij} w_i) \\
				&=  \sum_{i=1}^m a_{ij} U(w_i) \\
				&=  \sum_{i=1}^m a_{ij} \sum_{k=1}^l b_{ki} z_k \\
				&= \sum_{k=1}^l(\sum_{i=1}^m  b_{ki}a_{ij})  z_k.
			\end{align*}
			Luego el coeficiente $kj$ de la matriz $[UT]_{\mathcal B\mathcal B''}$ es $\sum_{i=1}^m  b_{ki}a_{ij}$ que es igual a la fila $k$ de $[U]_{\mathcal B'\mathcal B''}$ por la columna $j$ de  $[T]_{\mathcal B\mathcal B'}$,  en símbolos,  si $A= 	[T]_{\mathcal B\mathcal B'}$, $B = 	[U]_{\mathcal B'\mathcal B''}$ y $C = 	[UT]_{\mathcal B\mathcal B''}$, entonces
			\begin{equation*}
				[C]_{kj} = \sum_{i=1}^m  b_{ki}a_{ij} = F_k(B)C_j(A) = [BA]_{kj}.
			\end{equation*}
		\end{proof}	
			
		\begin{corolario}\label{cor-5.6} Sean $V$ espacio vectorial de dimensión finita, $\mathcal B = \{v_1,\ldots,v_n\}$ base ordenada de $V$ y $T,U: V \to V$ operadores lineales. Entonces
			\begin{enumerate}
				\item\label{itm-cor-cambio-1} $[UT]_{\mathcal B} = [U]_{\mathcal B} [T]_{\mathcal B}$.
				\item\label{itm-cor-cambio-2} Si $I: V \to V$  es el operador identidad, entonces $[I]_{\mathcal B} =Id$,  donde $Id$  es la matriz identidad $n \times n$.
				\item\label{itm-cor-cambio-3} Si $T$  es inversible,  entonces $[T]_{\mathcal B}$  es una matriz inversible y  $$[T^{-1}]_{\mathcal B} = [T]_{\mathcal B}^{-1}.$$
			\end{enumerate}
		\end{corolario}
		\begin{proof}
			(\ref{itm-cor-cambio-1}). Es inmediato del teorema anterior tomado $\mathcal B' =\mathcal B'' = \mathcal B$. 
			
			(\ref{itm-cor-cambio-2}). $I(v_i) = v_i$ y por lo tanto
			\begin{equation*}
				[I]_{\mathcal B} = \begin{bmatrix} 1&0&\cdots&0 \\0&1&\cdots&0\\\vdots&&&\vdots\\0&0&\cdots&1 	\end{bmatrix} = Id.
			\end{equation*}
			
			
			(\ref{itm-cor-cambio-3}). $I = T T^{-1}$, luego
			\begin{equation*}
				Id = [I]_{\mathcal B} =  [T T^{-1}]_{\mathcal B} =  [T]_{\mathcal B} [T^{-1}]_{\mathcal B}.
			\end{equation*}
			 Análogamente, $I = T^{-1} T$, luego
			\begin{equation*}
			Id = [I]_{\mathcal B} =  [T^{-1} T]_{\mathcal B} =  [T^{-1}]_{\mathcal B} [T]_{\mathcal B}. 
			\end{equation*}
			Por  lo tanto $[T]_{\mathcal B}^{-1} = [T^{-1}]_{\mathcal B}$.
		\end{proof}	
	
	
			
		\begin{teorema}[Matriz de cambio de base]
			Sea $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y sean
			$$
			\mathcal B = \{v_1,\ldots,v_n \}, \qquad \mathcal B' = \{v'_1,\ldots,v'_n \}
			$$
			bases ordenadas de $V$. Sea $T$ es un operador lineal sobre V. Entonces existe $P$ una  matriz inversible  $n \times n$ tal que 
			\begin{equation*}
			[T]_{\mathcal B'} = P^{-1}[T]_{\mathcal B}  P.
			\end{equation*}
			Más aún, $P = [I]_{\mathcal B' \mathcal B}$,  donde $I$ al  operador  identidad de $V$.
		\end{teorema}
		\begin{proof} Tenemos que $T = I T$ y $T =T  I$, luego 
			\begin{align*}
				[T]_{\mathcal B'\mathcal B'} &=  [I T]_{\mathcal B'\mathcal B'}& \qquad& \\
				&= [I]_{\mathcal B \mathcal B'} [T]_{\mathcal B' \mathcal B}& &(\text{teorema \ref{th-5.5}})  \\
				&= [I]_{\mathcal B \mathcal B'} [T I]_{\mathcal B' \mathcal B}& &\\
				&= [I]_{\mathcal B \mathcal B'} [T]_{\mathcal B\mathcal B} [I]_{\mathcal B' \mathcal B}& &(\text{teorema \ref{th-5.5}}).
			\end{align*} 
			Es decir
			\begin{equation}\label{eq-cambio de base}
				[T]_{\mathcal B'} = [I]_{\mathcal B \mathcal B'} [T]_{\mathcal B} [I]_{\mathcal B' \mathcal B}.
			\end{equation}
			Falta probar que $[I]_{\mathcal B \mathcal B'}$  es el inverso de  $[I]_{\mathcal B' \mathcal B}$. Ahora bien,
			\begin{align*}
				[I]_{\mathcal B \mathcal B'} [I]_{\mathcal B' \mathcal B}&= [I]_{\mathcal B \mathcal B} = Id,\\
				[I]_{\mathcal B' \mathcal B} [I]_{\mathcal B \mathcal B'}&= [I]_{\mathcal B' \mathcal B'} = Id.
			\end{align*}
		\end{proof}
	
	
		La matriz $P$ del teorema anterior es llamada la \textit{matriz de cambio de base}. \index{matriz de cambio de base}La fórmula (\ref{eq-cambio de base}) es importante por si misma y debemos recordarla.
	\vskip .3cm
	
		El teorema anterior nos permite definir el determinante de un operador lineal.	Sea $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y $T$ un operador lineal sobre $V$. Sean  $\mathcal B$, $\mathcal B'$ 
		bases ordenadas de $V$, entonces $[T]_{\mathcal B'} = P^{-1}[T]_{\mathcal B}P$, para $P$ una matriz inversible. Por lo tanto, 
		\begin{equation*}
			\det([T]_{\mathcal B'}) = \det(P^{-1}[T]_{\mathcal B}  P) =  \det([T]_{\mathcal B}  PP^{-1}) =  \det([T]_{\mathcal B}). 
		\end{equation*}
		Es decir,  el determinante de la matriz de $T$ en cualquier base siempre es igual.
		
		\begin{definicion} 
				Sea $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y $T$ un operador lineal sobre $V$. El \textit{determinante de $T$}\index{determinante de una transformación lineal} es el determinante de la matriz de $T$ en alguna base de $V$.  
		\end{definicion}
		\end{section}
	
	
	
	
	
	
	
		\begin{section}{Autovalores y autovectores}
		Sea $V$ espacio vectorial de dimensión finita. Un operador lineal en $V$ es \textit{diagonalizable}\index{matriz diagonalizable}  si existe una base ordenada $\mathcal B= \{v_1,\ldots,v_n\}$ de $V$ y $\lambda_1,\ldots,\lambda_n \in \K$ tal que 
		\begin{equation}\label{eq-auto-49}
			T(v_i) = \lambda_i v_i,\qquad 1\le i \le n. 
		\end{equation}
		En  general, los operadores diagonalizables permiten hacer cálculos sobre ellos en forma sencilla, por ejemplo el núcleo del  operador definido por (\ref{eq-auto-49}) es $\nuc(T)=\langle v_i: \lambda_i =0 \rangle$ y  su imagen es $\img(T)=\langle v_i: \lambda_i \not=0 \rangle$ (vermos la demostración de estos resultado más adelante). 
		Otra propiedad importante de los operadores diagonalizables es que la matriz de la transformación lineal en una base adecuada es diagonal (de allí viene el nombre de diagonalizable). En  el caso del  operador definido por (\ref{eq-auto-49}) tenemos que
		$$
		[T]_{\mathcal B} = 
		\begin{bmatrix}
		\lambda_1&0&0&\cdots&0 \\
		0&\lambda_2&0&\cdots&0\\
		0&0&\lambda_3&\cdots&0\\
		\vdots&\vdots&\vdots&&\vdots\\
		0&0&0&\cdots&\lambda_n
		\end{bmatrix}.
		$$
		
		
		No todo operador lineal es diagonalizable y no es inmediato, ni sencillo, de la definición de un operador lineal decidir si es diagonalizable o no. En esta sección veremos herramientas para estudiar un operador lineal $T$ y su posible diagonalización. La ecuación (\ref{eq-auto-49}) sugiere se estudien los vectores que son transformados por $T$ en múltiplos de sí mismos.
		
		\begin{definicion}
			Sea $V$ un espacio vectorial sobre el cuerpo $\K$ y sea $T$ un operador lineal sobre $V$. Un \textit{valor propio}\index{valor propio} o \textit{autovalor}\index{autovalor} de $T$ es un escalar $\lambda$ de $\K$ tal que existe un vector no nulo $v \in V$ con $T(v) = \lambda v$. Si $\lambda$ es un autovalor de $T$, entonces
			\begin{enumerate}
				\item  cualquier  $v \in V$ tal que $T(v) = \lambda v$  se llama un \textit{vector propio}\index{vector propio} o  \textit{autovector}\index{autovector} de $T$ asociado al valor propio $\lambda$;
				\item la colección de todos los $v \in V$ tal que $T(v) = \lambda v$  se llama \textit{espacio propio}\index{espacio propio} o \textit{autoespacio}\index{autoespacio} 	asociado a $\lambda$.
			\end{enumerate}
			
			Los valores propios se llaman también a menudo raíces características, eigenvalores. valores característicos o valores espectrales. Nosotros usaremos, preferentemente, ``autovalores''.
			
			Sea ahora $\lambda \in \K$, definimos
			$$
			V_\lambda := \{v \in V: Tv = \lambda v \}.
			$$
			Observar que $V_\lambda \ne 0$ si y sólo si $\lambda$ es autovalor. 
		\end{definicion}
		
		
		\begin{teorema}
			Sea $V$ un espacio vectorial y sea $T:V \to V$ una aplicación lineal. Sea $\lambda \in \K$ entonces, $V_\lambda$  es subespacio de $V$.
		\end{teorema}
		\begin{proof}
			Sean $v_1,v_2 \in V$ tales que $Tv_1 = \lambda v_1$ y $Tv_2 = \lambda v_2$. Entonces
			$$
			T(v_1+v_2) = T(v_1)+ T(v_2) = \lambda v_1 + \lambda v_2 = \lambda (v_1 + v_2),
			$$
			es decir si  $v_1,v_2 \in V_\lambda$, probamos que $v_1+v_2 \in V_\lambda$. 
			
			Sea ahora $c \in F$, entonces $T(cv_i) = cT(v_1) = c\lambda v_1 = \lambda (cv_1)$. Por lo tanto, si  $v_1\in V_\lambda$ y $c \in F$, probamos que $cv_1 \in V_\lambda$.
			
			Esto termina de probar el teorema.
		\end{proof}

		
		
		\begin{teorema}
			Sea $V$ espacio vectorial y sea $T: V \to V$ una aplicación lineal.  Sean $v_1,\ldots,v_m$ autovectores de $T$, con autovalores $\lambda_1,\ldots,\lambda_m$ respectivamente. Suponga que estos  autovalores son distintos entre si, esto es, $\lambda_i \ne \lambda_j$ si $i \ne j$. Entonces $v_1,\ldots,v_m$ son linealmente independientes.
		\end{teorema}
		\begin{proof}
			Hagamos la demostración por inducción sobre $m$.
			
			\textit{Caso base.} Si $m=1$, no hay nada que demostrar puesto que un vector no nulo el LI.
			
			\medskip
			
			\textit{Paso inductivo.} Supongamos que el enunciado es verdadero para el caso $m-1$ con $m>1$, (hipótesis inductiva o HI), y probemos entonces que esto implica que es cierto para $m$. Debemos ver  que si 
			\begin{equation}
			c_1v_1+	c_2v_2+ \cdots c_mv_m = 0 \tag{$*$}
			\end{equation}
			entonces $c_1 = \cdots c_m = 0$.
			Multipliquemos $(*)$ por $\lambda_1$, obtenemos:
			\begin{equation}
			c_1\lambda_1v_1+ c_2\lambda_1v_2+\cdots c_m\lambda_1v_m = 0. \tag{$**$}
			\end{equation}
			También apliquemos $T$ a $(*)$ y obtenemos
			\begin{equation}
			c_1\lambda_1v_1+ c_2\lambda_2v_2+\cdots c_m\lambda_mv_m = 0. \tag{$***$}
			\end{equation}
			Ahora a $(**)$ le restamos $(***)$ y obtenemos:
			\begin{equation}
			c_2(\lambda_1 -\lambda_2)v_2+\cdots c_m(\lambda_1 -\lambda_m)v_m = 0. 	 
			\end{equation}
			Como, por hipótesis inductiva, $v_2,\ldots,v_m$ son LI, tenemos que $c_i(\lambda_1 -\lambda_i)=0$ para $i\ge 2$. Como $\lambda_1 -\lambda_i \ne 0$ para $i\ge 2$, obtenemos que $c_i = 0$ para $i\ge 2$. Por $(*)$ eso implica que $c_1=0$ y por lo tanto $c_i=0$ para todo $i$.
		\end{proof}
		
		\begin{corolario}\label{cor-aut-li}
			Sea $V$ espacio vectorial de dimensión $n$ y sea $T: V \to V$ una aplicación lineal que tiene $n$
			autovectores $v_1,\ldots, v_n$ cuyos autovalores $\lambda_1,\ldots,\lambda_n$ son distintos entre
			si. Entonces $\{v_1,\ldots, v_n\}$ es una base de $V$.
		\end{corolario}
		
		Recordemos que si $T$  es una transformación lineal, el determinante de $T$  se define como el determinante de la matriz de la transformación lineal en una base dada y que este determinante no depende de la base.    
		
		\begin{definicion}
			Sea $A \in M_n(\K)$   el \textit{polinomio característico}\index{polinomio característico} de $A$ es $\chi_A(x) = \det(A-xI)$, donde $x$ es una indeterminada. 
			
			Sea $V$ espacio vectorial de dimensión finita y sea $T: V \to V$ lineal, el  \textit{polinomio característico de $T$} es $\chi_T(x) = \det(T-x Id)$.
		\end{definicion}
	
			
		En general,  si $A = [a_{ij}]$ matriz $n \times n$, tenemos que
		\begin{equation}
		\chi_A(x) = \det(A- x\,Id) = \det
		\begin{bmatrix}
		a_{11}-x&a_{12}&\cdots&a_{1n}\\
		a_{21}&a_{22}-x&\cdots&a_{2n}\\
		\vdots&\vdots&\ddots&\vdots\\
		a_{n1}&a_{n2}&\cdots&a_{nn}-x\\
		\end{bmatrix}
		\end{equation} 
		y el polinomio característico de $A$ es un polinomio  de grado $n$,  más precisamente  
		$$
		\chi_A(x) = (-1)^n x^n + a_{n-1}x^{n-1}+ \cdots + a_1x + a_0.
		$$ 
		Esto se puede demostrar fácilmente por inducción. 
		
		\begin{ejemplo}
			Sea $T: \R^2 \to \R^2$ y su matriz en la base canónica es
			\begin{equation*}
				A = \begin{bmatrix}
					a&b\\c&d
				\end{bmatrix},
			\end{equation*}
		entonces
		\begin{equation*}
				 \det \begin{bmatrix}
				a -x & b \\ c &d-x
				\end{bmatrix} = 
				(a-x)(d-x) - bc = x^2 -(a+d)x + (ad -bc).
		\end{equation*}
		Es decir,
		$$
		\chi_T(x) = x^2 -(a+d)x + (ad -bc).
		$$ 
		\end{ejemplo}

	
	
		
		\begin{ejemplo} \label{ej-autovectores}
			Sea
			$$ 
			A=\begin{bmatrix}10&-10&6\\8& -8& 6\\-5& 5& -3\end{bmatrix},
			$$
			entonces el  polinomio característico de $A$ es
			$$
			\det \begin{bmatrix}10-x&-10&6\\8& -8-x& 6\\-5& 5& -3-x\end{bmatrix} =- x^3  - x^2 + 6 x .
			$$
			Es posible factorizar esta expresión y obtenemos
			$$
			\chi_A(x) = -x (x-2)(x+3).
			$$
		\end{ejemplo}
		
	\begin{comment}
		\begin{ejemplo}
		Sea
		$$ 
		A=\begin{bmatrix}1&2&1\\ 6&-1&0\\ -1&-2&-1\end{bmatrix},
		$$
		entonces el  polinomio característico de $A$ es
		$$
		\det \begin{bmatrix}1-x&2&1\\ 6&-1-x&0\\ -1&-2&-1-x\end{bmatrix} = -x^3 - x^2 + 12 x.
		$$
		Es posible factorizar esta expresión y obtenemos
		$$
		\chi_A(x) = -x(x-3)(x+4).
		$$
		\end{ejemplo}
	\end{comment}	
		
	

		
		\begin{proposicion}\label{autovalores}
			Sea $V$ espacio vectorial de dimensión $n$ y sea $T: V \to V$ lineal. Entonces $\lambda\in \K$ es autovalor si y sólo si $\lambda$ es raíz del polinomio característico.  
		\end{proposicion}
		\begin{proof}${}^{}$
			
			($\Rightarrow$) Si $\lambda$ es autovalor, entonces existe $v \in V$, no nulo, tal que $Tv = \lambda v$, luego 
			$$
			0 = Tv -\lambda v = Tv - \lambda Id \, v=  (T - \lambda Id)v.
			$$
			Por lo tanto, $T - \lambda Id$ no es inversible, lo cual implica que $0 = \det(T-\lambda Id) = \chi_T(\lambda)$. Es decir, $\lambda$ es raíz del polinomio característico. 
			
			($\Leftarrow$) Si $\lambda$ es raíz del polinomio característico, es decir si $0 = \chi_T(\lambda) = \det(T-\lambda Id)$, entonces $T-\lambda Id$ no es una matriz inversible, por lo tanto  su núcleo es no trivial. Es decir existe $v \in V$ tal que $(T-\lambda Id)v =0$, luego $Tv =\lambda v$, por lo tanto $v$ es autovector con autovalor $\lambda$.   
		\end{proof}
		
		Repetimos ahora algunos conceptos ya expresados al comienzo de la sección. 
		
		\begin{definicion}
			Sea $V$ espacio vectorial de dimensión finita y sea $T: V \to V$ lineal. Diremos que \textit{$T$ es diagonalizable} si existe una base de $V$ de autovectores de $T$. 
		\end{definicion}	
		
		En el caso que $T$ sea una transformación lineal diagonalizable y $\mathcal{B} = \{v_1,\ldots,v_n \}$ sea una base de autovectores con autovalores $\lambda_1,\ldots,\lambda_n$, entonces
		$$
		T(v_i) = \lambda_i v_i, \qquad 1 \le i \le n,
		$$ 
		y, por lo tanto, la matriz de $T$ en  la base $\mathcal{B}$ es diagonal, más precisamente
		$$
		[T]_\mathcal{B} = \begin{bmatrix}
		\lambda_1 &0 & \cdots & 0 \\
		0 & \lambda_2 & \cdots &0 \\
		\vdots & &\ddots & \vdots \\
		0 & 0 & \ldots &\lambda_n
		\end{bmatrix}
		$$
		
		\begin{ejemplo} Consideremos la transformación lineal de $\R^3$ en $\R^3$  definida por la matriz $A$ del ejemplo \ref{ej-autovectores},  es decir (con abuso de notación incluido)
			\begin{equation*}
				\begin{bmatrix}10&-10&6\\8& -8& 6\\-5& 5& -3\end{bmatrix}
				\begin{bmatrix} x\\y\\z \end{bmatrix} =
				\begin{bmatrix} 10x-10y+6z\\8x -8y +6z \\-5x+5y-3z\end{bmatrix}.
			\end{equation*}
		Ya vimos que  el  polinomio característico de esta aplicación es 
		$$
		\chi_A(x) = -x (x-2)(x+3).
		$$
		Luego, por 	proposición \ref{autovalores}, los autovalores de $A$ son $0$, $2$ y $-3$. Debido al corolario \ref{cor-aut-li} existe una base de autovectores de $A$. Veamos cuales son. Si $\lambda$ autovalor de $A$, para encontrar los autovectores con autovalor $\lambda$  debemos resolver la ecuación $Av -\lambda v=0 $,  en este caso sería
		\begin{equation*}
		\begin{bmatrix}10- \lambda &-10&6\\8& -8- \lambda & 6\\-5& 5& -3- \lambda \end{bmatrix}
		\begin{bmatrix} x\\y\\z \end{bmatrix} =
		\begin{bmatrix} 0\\0 \\0\end{bmatrix},
		\end{equation*}  
		para $\lambda =0, 2,-3$. Resolviendo estos tres sistemas, obtenemos que 
		\begin{equation*}
			V_0 = \{(y,y,0): y \in \R \},\quad V_2 = \{(-2z,-z,z): z \in \R \},\quad V_{-3} = \{(-2z,-2z,z): z \in \R \}. 
		\end{equation*}
		Por lo tanto, $\{(1,1,0), (-2,-1,1), (-2,-2,1)\}$ es una base de autovectores de la transformación lineal. 
		\end{ejemplo}
		
		\begin{proposicion} Sea $V$ espacio vectorial de dimensión $n$ y sea $T: V \to V$ lineal tal que tiene una base de autovectores $\mathcal{B} = \{v_1,\ldots,v_n \}$  con autovalores $\lambda_1,\ldots,\lambda_n$. Entonces $\nuc(T)=\langle v_i: \lambda_i =0 \rangle$ e  $\img(T)=\langle v_i: \lambda_i \not=0 \rangle$.			
		\end{proposicion}
		\begin{proof} Reordenemos la base de tal forma que  $\lambda_i =0$ para $1 \le i \le k$ y $\lambda_i \ne 0$ para $k < i \le n$. 
		Todo $v \in V$ se escribe en términos de la base como 
		$$
		v = x_1v_1 + \cdots+ x_k v_k+ x_{k+1} v_{k+1}+\cdots+ x_n v_n,\quad (x_i \in \K),
		$$
		y entonces
		\begin{equation}\label{eq-av-01}
			T(v) =  \lambda_{k+1}x_{k+1} v_{k+1}+\cdots+ \lambda_nx_n v_n.
		\end{equation}
		Luego, $T(v) =0$ si y sólo si $x_{k+1} = \cdots = x_n=0$, y esto se cumple si y solo si $v =  x_1v_1 + \cdots+ x_k v_k$,  es decir $v \in \langle v_i: \lambda_i =0 \rangle$. 
		También es claro por la ecuación (\ref{eq-av-01}) que 
		\begin{align*}
			\img(T) &= \{\lambda_{k+1}x_{k+1} v_{k+1}+\cdots+ \lambda_nx_n v_n: x_i \in \K \} \\
			&=\{\mu_{k+1} v_{k+1}+\cdots+ \mu_n v_n: \mu_i \in \K \}\\
			&= \langle v_i: \lambda_i \not=0 \rangle.
		\end{align*}
		\end{proof}
		
		\begin{ejemplo}
			Sea $T:\R^2 \longrightarrow \R^2$ el operador definido por $T(x,y)=(y,x)$. Probar que $T$ es diagonalizable y encontrar una base de autovectores. 
		\end{ejemplo}
		\begin{proof}
			Por la proposición \ref{autovalores}, los autovalores de $T$  son las raíces del polinomio característico,  es decir las raíces de 
			$$
			\det\left[\begin{matrix}
			-x& 1 \\ 1 & -x
			\end{matrix} \right]= x^2 -1  =(x -1)(x +1). 
			$$
			Luego los autovalores son 1 y $-1$. Para hallar un autovector con autovalor 1 debemos resolver la ecuación $T(x,y) = (x,y)$. Ahora bien, 
			$$
			(x,y) = T(x,y) = (y,x),
			$$
			luego $x =y$ y claramente $(1,1)$ es autovector con autovalor 1.
			
			Por otro lado $T(x,y) = -(x,y)$, implica  que $(y,x) = -(x,y)$, es decir $y = -x$ y claramente podemos elegir $(1,-1)$ como autovector con autovalor $-1$.
			
			Luego $\mathcal{B} = \{(1,1),(1,-1) \}$  es una base de $\R^2$ de autovectores de  $T$.
		\end{proof}
		
		\vskip .3cm
		
		No todas la matrices son diagonalizables,  como veremos en el ejemplo a continuación. 
		
		
		\begin{ejemplo}
			Sea $T:\R^2 \longrightarrow \R^2$ el operador definido por $T(x,y)=(2x-y,x+4y)$. Probar que $T$ tiene un \'unico autovalor $\lambda$ cuyo autoespacio $V_\lambda=\{v\in \R^2: Tv=\lambda v\}$ es de dimensi\'on 1.
		\end{ejemplo}
		\begin{proof} La matriz de  $T$ en la base canónica es 
			$$
			A = \begin{bmatrix}
			2 & -1 \\ 1 & 4
			\end{bmatrix}. 
			$$
			Por la proposición \ref{autovalores}, los autovalores de $T$  son las raíces del polinomio característico,  es decir las raíces de 
			$$
			\det\left[\begin{matrix}
			2-x & -1 \\ 1 & 4-x
			\end{matrix} \right] = (2-x)(4-x)+1 = x^2 -6x + 9 = 
			(x -3)^2. 
			$$
			Es decir el  único autovalor posible es 3.
			
			Debemos ver para que valores  $(x,y)\in \R^2$ se satisface la ecuación
			$$
			T(x,y) = 3(x,y).
			$$
			tiene solución. Esta ecuación es equivalente a 
			\begin{equation*}
				\begin{array}{rcll}
				(2x-y,x+4y)&=&(3 x, 3 y)\quad &\Rightarrow \\
				2x-y = 3 x&,& x+4y = 3y \quad &\Rightarrow \\
				-y =  x&,& x = -y \quad &\Rightarrow \\
				y &=&  -x \quad &
				\end{array}
			\end{equation*}
			Luego $V_3 = \{(x,-x): x \in \R \}$ que es de dimensión 1.  	
			
		\end{proof}
	
		\begin{proposicion}
			 Sea $T$ un operador lineal diagonalizable sobre un espacio vectorial $V$ de dimensión finita. Sean $\lambda_1,\ldots,\lambda_k$ los autovalores distintos de $T$. Entonces,  el  polinomio característico de $T$ es
			 	$$
			 	\chi_T(x) =(-1)^n(x -\lambda_1)^{d_1}\ldots(x -\lambda_k)^{d_k}
			 	$$
			 	con
			 	$$
			 	d_i =  \dim V_{\lambda_i},
			 	$$
			 	para  $i=1, \ldots, k$.
		\end{proposicion}
		\begin{proof}[Demostración ($*$)]
			$T$ es un operador lineal diagonalizable y $\lambda_1,\ldots,\lambda_k$ los valores propios distintos de $T$. Entonces existe una base ordenada $\mathcal B$ con respecto a la cual $T$ está representado por una matriz diagonal; es decir, los elementos de la diagonal son los escalares $\lambda_j$ cada uno de los cuales se repite un cierto número de veces. Más específicamente, si
			 $v_{j1},\ldots,v_{jd_j}$ son los vectores en $\mathcal{B}$ con autovalor $\lambda_j$ ($1 \le j \le k$),  reordenamos la base de tal forma que primero estén los autovectores con autovalor $\lambda_1$, a continuación los de autovalor $\lambda_2$, etc.: 
			\begin{equation*}
				\mathcal{B} = \{v_{11},\ldots,v_{1d_1},\ldots,v_{k1},\ldots,v_{kd_k}\}. 
			\end{equation*}
			Ahora bien,  si $v \in V$,  entonces 
			\begin{align*}
			v &=x_1v_{11}+\cdots+x_{d_1}v_{1d_1}+\cdots+x_nv_{n1}+\cdots+x_{d_n}v_{nd_n} \\
			&= v_1 + v_2 +\cdots + v_k
			\end{align*}
			con $v_i = x_iv_{i1}+\cdots+x_{d_i}v_{id_i} \in V_{\lambda_i}$. Luego
			\begin{equation}\label{eq-desc-vectores-propios}
				T(v)  =\lambda_1v_1+\lambda_2v_2\cdots+\lambda_kv_k
			\end{equation} 
			
			Veamos que $V_{\lambda_i} = <v_{i1},\ldots,v_{id_i}>$ para $1 \le i \le k$. Es claro que  $<v_{i1},\ldots,v_{id_i}> \subset V_{\lambda_i}$. Probemos  ahora que,  $V_{\lambda_i} \subset <v_{i1},\ldots,v_{id_i}>$: si  $v \in V_{\lambda_i}$,  entonces $T(v)$ es como en (\ref{eq-desc-vectores-propios}) y, por lo tanto, si $v_j \ne 0$ para $j\not=i$ entonces $T(v) \ne \lambda_j v$, lo que contradice la hipótesis. Es decir $v = v_i \in <v_{i1},\ldots,v_{id_i}>$. Hemos probado que    $V_{\lambda_i} = <v_{i1},\ldots,v_{id_i}>$ y como $v_{i1},\ldots,v_{id_i}$ son LI, entonces $\dim V_{\lambda_i} = d_i$.
			
			Por otro lado, la matriz de $T$ en la base $\mathcal{B}$ tiene la forma
			\begin{equation*}
				\begin{bmatrix}
				\lambda_1 I_1 &0&\cdots&0 \\0&\lambda_2 I_2&\cdots&0 \\\vdots&\vdots&&\vdots \\0&0&\cdots&\lambda_n I_n 
				\end{bmatrix}
			\end{equation*}
			donde $I_j$ es la matriz identidad $d_j \times d_j$. Luego,  el polinomio característico de $T$ es el producto
			\begin{align*}
				(\lambda_1-x)^{d_1}\ldots(\lambda_k-z)^{d_k} = (-1)^n(x -\lambda_1)^{d_1}\ldots(x -\lambda_k)^{d_k}.
			\end{align*}
		\end{proof}
	
		\begin{ejemplo}
			Sea $T$ un operador lineal sobre $\R^3$  representado en la base ordenada canónica por la matriz
			\begin{equation*}
				A = 
				\begin{bmatrix}
				5 &-6 &-6\\ -1& 4& 2\\3 &-6& -4
				\end{bmatrix}.
			\end{equation*}
			El  polinomio característico de $A$ es
			\begin{equation*}
				\chi_A(x) = \det \begin{bmatrix}
				5-x &-6 &-6\\ -1& 4-x& 2\\3 &-6& -4-x
				\end{bmatrix} 
				= -x^3 + 5 x^2 - 8 x + 4 = -(x-2)^2(x-1).
			\end{equation*}
			¿Cuáles son las dimensiones de los espacios de los vectores propios asociados
			con los dos valores propios? Se deben resolver las ecuaciones asociadas a las matrices
			\begin{equation*}
			A - 2I = 
			\begin{bmatrix}
			3 &-6 &-6\\ -1& 2& 2\\3 &-6& -6
			\end{bmatrix} 
			\end{equation*}
			y 
			\begin{equation*}
			A -I= 
			\begin{bmatrix}
			4 &-6 &-6\\ -1& 3& 2\\3 &-6& -5
			\end{bmatrix}.
			\end{equation*}
			Las soluciones de estos sistemas son los autoespacios de autovalor 2 y 1 respectivamente. En  el primer caso, 
			\begin{equation*}
			\begin{bmatrix} 3 &-6 &-6\\ -1& 2& 2\\3 &-6& -6 \end{bmatrix}
			\underset{F_3+3F_2}{\stackrel{F_1+3 F_2}{\longrightarrow}} 
			\begin{bmatrix} 0 &0 &0\\ -1& 2& 2\\0 &0& 0 \end{bmatrix}.
			\end{equation*}
			Luego,  la solución del sistema asociado a $A-2I$ es 
			$$
			V_2 = \{(2y+2z,y,z): y,z \in \R\} = <(2,1,0),(2,0,1)>
			$$
			cuya dimensión es 2. 
			
			Por otro lado, 
			\begin{equation*}
			\begin{bmatrix}4 &-6 &-6\\ -1& 3& 2\\3 &-6& -5 	\end{bmatrix}
			\underset{F_3+3F_2}{\stackrel{F_1+4 F_2}{\longrightarrow}} 
			\begin{bmatrix}0 &6 &2\\ -1& 3& 2\\0 &3& 1 	\end{bmatrix}
			\underset{F_2-F_3}{\stackrel{F_1-2 F_3}{\longrightarrow}}
			\begin{bmatrix}0 &0 &0\\ -1& 0& 1\\0 &3& 1 	\end{bmatrix}.
			\end{equation*}
			Luego,  la solución del sistema asociado a  $A-I$ es 
			$$
			V_1 = \{(z,-\frac13z,z): z \in \R\} = <(1,-\frac13,1)>.
			$$
			
			Entonces, una base de autovectores de $T$ podría ser
			$$
			\mathcal{B} = \{(2,1,0),(2,0,1),(1,-\frac13,1) \}
			$$
			y en esa base la matriz de la transformación lineal es
			\begin{equation*}
			[T]_{\mathcal{B}} = \begin{bmatrix}2 &0 &0\\ 0& 2& 0\\0 &0& 1 	\end{bmatrix}.
			\end{equation*}
		\end{ejemplo}
	
		
		\end{section}
	
	
		
	
		
	\end{chapter}

	\begin{chapter}{Espacios de producto  interno}\label{chap-esp-prod-int}
	
	\begin{comment}
	

	
	\begin{section}{Suma directa de subespacios y proyecciones}
		
		En esta sección se define la descomposición de un espacio vectorial como suma directa de dos subespacios. Se muestra que esa descomposición equivale a a definir un operador idempotente en el espacio, al cual llamaremos \textit{proyección}. Finalmente, se establece la conexión entre proyecciones e involuciones.
		
		\medskip
		
		Si  $V_1$, $V_2$, $W$  dos subespacios del espacio vectorial $V$,  entonces sabemos que 
		\begin{equation*}
		V_1 + V_2 = \{v_1+v_2: v_1 \in V_1, v_2 \in V_2\}, \qquad V_1 \cap V_2 = \{v: v \in V_1 \text{ y } v \in V_2 \}
		\end{equation*}
		son subespacios vectoriales. 
		
		\begin{definicion} Sean $V_1$, $V_2$, $W$  subespacios vectoriales del espacio vectorial $V$, entonces 
			\begin{equation*}
			W = V_1 \oplus V_2
			\end{equation*}
			es la \textit{suma directa}\index{suma directa} de $V_1$ y $V_2$ si $V_1 + V_2 = W$ y $V_1 \cap V_2 = 0$. 
		\end{definicion}
		
		\medskip
		
		\begin{proposicion}
			Sea $V$  espacio vectorial y $V_1$, $V_2$ subespacios vectoriales de $V$. Entonces, $V = V_1 \oplus V_2$ si y sólo si para todo vector $v \in V$ existe únicos  $v_1\in V_1, v_2 \in V_2$ tal que $v = v_1 + v_2$. 
		\end{proposicion}
		\begin{proof} {\,${}^{}$}
			\begin{enumerate}
				\item[$(\Rightarrow)$] Sea $v \in V$,  como $V = V_1 + V_2$, existen $v_1\in V_1, v_2 \in V_2$ tal que $v = v_1 + v_2$. Veamos que $v_1$ y $v_2$ son  únicos. Sean $v'_1\in V_1, v'_2 \in V_2$ tal que $v = v'_1 + v'_2$. Por lo tanto $v_1 + v_2 =  v'_1 + v'_2$. Haciendo pasajes de término obtenemos 
				$$
				v_1 -v_1' = v_2' - v_2.
				$$
				Sea  $v_0=v_1 -v_1' = v_2' - v_2$. Ahora bien, $v_1 -v_1' \in V_1$, por lo tanto $v_0= v_1 -v_1'  \in V_1$. Análogamente,  $v_2' -v_2 \in V_2$, por lo tanto $v_0= v_2' -v_2  \in V_2$. Es decir, $v_0 \in V_1 \cap V_2 = 0$, luego $v_0 =0$, por lo tanto $v_1 = v'_1$ y $v_2 = v'_2$. 
				\item[$(\Leftarrow)$] Es claro que $V = V_1 + V_2$. Probemos que $V_1 \cap V_2 = 0$. Sea $v \in V_1 \cap V_2$. Por hipótesis, existe únicos $v_1\in V_1, v_2 \in V_2$ tal que $v = v_1 + v_2$.
				Podemos escribir  entonces
				$$
				\begin{array}{rcllll}
				v &=& v_1 + v_2,& \quad &v_1\in V_1,& v_2 \in V_2 \\
				v &=& v + 0,& \quad &v\in V_1, &0 \in V_2 \\
				v &=& 0 + v& \quad &0\in V_1, &v \in V_2. 
				\end{array}
				$$
				Por la unicidad, resulta que $v_1 = v= 0$ y $v_2 = 0 =v$, es decir $v =0$.
			\end{enumerate}	
			
		\end{proof}
		\medskip 
		
		\begin{proposicion} Sea $V$  espacio vectorial de dimensión finita y sean $V_1$, $V_2$ dos subespacios de  $V$ tal que $V = V_1 \oplus V_2$. Sea $\mathcal{B}_1$ base de $V_1$ y $\mathcal{B}_2$ base de $V_2$, entonces $\mathcal{B} = \mathcal{B}_1 \cup \mathcal{B}_2$ es base de $V$.
		\end{proposicion}
		\begin{proof}
			Sea $\mathcal{B}_1 = \{u_1,\ldots,u_r \}$ y $\mathcal{B}_2 = \{u_{r+1},\ldots,u_{r+s} \}$, debemos ver entonces que el conjunto  $\mathcal{B} = \{u_{1},\ldots,u_{r+s} \}$  genera todo el espacio y es LI.
			
			- Sea $v \in V$, como $V_1 + V_2 = V$, existen $v_1 \in V_1$ y  $v_2 \in V_2$ tales que $v = v_1 + v_2$. Como  $\mathcal{B}_1$ es base de $V_1$, tenemos que $v_1 = a_1u_1+\cdots+a_ru_r$, análogamente
			$v_2 = a_{r+1}u_{r+1}+\cdots+a_{r+s}u_{r+s}$ y por lo tanto $v = a_{1}u_{1}+\cdots+a_{r+s}u_{r+s}$. Es decir  $\mathcal{B}$ genera $V$.
			
			- Si $a_1u_1+\cdots+a_ru_r + a_{r+1}u_{r+1}+\cdots+a_{r+s}u_{r+s} = 0$, entonces 
			$$a_1u_1+\cdots+a_ru_r = - a_{r+1}u_{r+1}-\cdots-a_{r+s}u_{r+s}.$$ Ahora bien, el termino de la izquierda en la última igualdad pertenece a $V_1$, mientras que el de a derecha pertenece a $V_2$. Como $V_1 \cap V_2 = 0$, tenemos que
			$$a_1u_1+\cdots+a_ru_r = 0 = - a_{r+1}u_{r+1}-\cdots-a_{r+s}u_{r+s}.$$
			Como $\mathcal{B}_1$ es base de $V_1$, $a_1 = \cdots = a_r = 0$ y como $\mathcal{B}_1$ es base de $V_1$, $a_{r+1} = \cdots = a_{r+s} =0$. Es decir $\mathcal{B}$ es LI.	
		\end{proof}
		
		\begin{corolario}
			Sea $V$  espacio vectorial de dimensión finita y sean $V_1$, $V_2$ dos subespacios de  $V$ tal que $V = V_1 \oplus V_2$.  Entonces $\dim(V) = \dim(V_1) + \dim(V_2)$.
		\end{corolario}
		\begin{proof}
			Sea $\mathcal{B}_1 = \{u_1,\ldots,u_r \}$ base de $V_1$ y $\mathcal{B}_2 = \{u_{r+1},\ldots,u_{r+s} \}$ base de $V_2$. Por la proposición anterior, $\mathcal{B} = \{u_{1},\ldots,u_{r+s} \}$ es base de $V$. Luego   $\dim(V) = r +s = \dim(V_1) + \dim(V_2)$.
		\end{proof}
		
		Se puede generalizar la noción de suma directa a varios subespacios. 
		
		\begin{definicion} Sean $V_1,\ldots,V_k$, $W$  subespacios vectoriales de un espacio vectorial $V$, entonces 
			$$
			W = V_1 \oplus V_2 \oplus \cdots \oplus V_k 
			$$ 
			si $V_1 + V_2 + \cdots +V_k= W$ y $V_j \cap (\sum_{i \ne j}V_i) = 0$. En  este caso  diremos que  $W$ es \textit{suma directa} de $V_1,\ldots,V_k$.
		\end{definicion}
		
		Esta definición se reduce  a la de suma directa de dos subespacios cuando $k=2$.
		
		Observar que si definimos $W_j = \sum_{i \ne j}V_i$ ($j=1,\ldots,k$) entonces, 
		$$W = V_1 \oplus V_2 \oplus \cdots \oplus V_k \;\text{ si y sólo si }\;W = V_j \oplus W_j\text{ \; }(j=1,\ldots,k).
		$$
		
		\begin{definicion} Sea $W$  un subespacio vectorial de un espacio vectorial $V$. Entonces un complemento de $W$ es un subespacio $U$ de $V$ tal que $V=W\oplus U$. 
		\end{definicion}
		
		\begin{proposicion} Sea $V$  espacio vectorial de dimensión finita y sea $W$ un subespacio de  $V$. Sean $\mathcal{B}_W$ una base de $W$ y $\mathcal{B}_V$ una base de $V$ tal que $\mathcal{B}_W\subset \mathcal{B}_V$. 
			
			Sea 
			\[
			\mathcal{B}'=\mathcal{B}_V -\mathcal{B}_W=\{b\in\mathcal{B}_V \text{ tales que } b\notin\mathcal{B}_W \}.
			\]  
			Entonces $U=\langle 	\mathcal{B}'\rangle$ es un complemento de $W$ y $\mathcal{B}'$ es una base de $U$.
		\end{proposicion}
		\begin{proof}
			Sea $\mathcal{B}_W = \{u_1,\ldots,u_r \}$ y $\mathcal{B}_V = \{u_1,\ldots,u_r, u_{r+1},\ldots,u_{r+s}\}$. Así, 	$\mathcal{B}'=\{u_{r+1},\ldots,u_{r+s} \}$. Como este conjunto es LI, entonces es una base del espacio $U=\langle 	\mathcal{B}'\rangle$ que genera. Por otro lado, como $\mathcal{B}_V$ es base de $V$, entonces todo vector $v\in V$ puede escribirse como
			\[
			v=a_1u_1+\cdots+a_ru_r + a_{r+1}u_{r+1}+\cdots+a_{r+s}u_{r+s},
			\]
			para algunos $a_1,\dots a_{r+s}\in \K$.				
			Ahora, definimos $v_W=a_1u_1+\cdots+a_ru_r$ y $v_U=a_{r+1}u_{r+1}+\cdots+a_{r+s}u_{r+s}$, de manera tal que  $v_W\in W$, $v_U\in U$ y $v=v_W+v_U$.
			
			Finalmente, si $v\in W\cap U$, entonces existen $a_1,\dots a_{r+s}\in \K$ tales que 
			\[
			v=a_1u_1+\cdots+a_ru_r = a_{r+1}u_{r+1}+\cdots+a_{r+s}u_{r+s}.
			\]
			Pero esto determina que $\mathcal{B}_V$ es LI (es una base). Luego, $W\cap U=\{0\}$ y por lo tanto $V=W\oplus U$.
		\end{proof}
		
		La noción de suma directa está ligada a la noción de proyección.
		
		\begin{definicion} Sea $V = V_1 \oplus V_2$. Definimos el operador lineal $P: V \to V$ por  $$P(v+u) = v,$$ con $u \in V_1$, $v \in V_2$. En este caso, diremos que $P$ es la \emph{proyección a $V_1$ paralela a $V_2$}.
		\end{definicion}
		
		Observar que $P$ está bien definida y que $P_{|V_1} = Id_{|V_1}$,  $P_{|V_2} = 0$. 
		
		\medskip 
		
		\begin{proposicion}\label{matriz-proy}
			Sea $P: V \to V$ una proyección, entonces existe una base $\mathcal{B}$ tal que 
			$$
			[P]_{\mathcal{B}} = diag(1,\ldots,1,0,\ldots,0).
			$$
			(una  matriz diagonal con 1's  y a continuación 0's).
		\end{proposicion}
		\begin{proof}
			Sean $V_1,V_2$ subespacios de $V$ tal que   $P$ es la {proyección a $V_1$ paralela a $V_2$}. Sea $\{v_1,\ldots,v_m\}$ una base de $V_1$ y completamos con $\{v_{m+1},\ldots,v_n\}$, vectores en $V_2$, a una base de $V$. Sea $\mathcal{B}= \{v_1,\ldots,v_n\}$. Como  $P_{|V_1} = Id_{|V_1}$ y  $P_{|V_2} = 0$, es claro que $
			[P]_{\mathcal{B}} = diag(1,\ldots,1,0,\ldots,0)
			$, donde la cantidad de 1's es  $m$  y la cantidad de 0's es $n-m$.
		\end{proof}
		
		\medskip
		
		\begin{definicion} Sea $V$ espacio vectorial de dimensión finita. $P:V \to V$ una aplicación lineal. Diremos que $P$ es \emph{idempotente} si $P\circ P = P$. Denotemos $P \circ P = P^2$.
		\end{definicion}
		
		
		
		
		\begin{proposicion}\label{proy->idem}
			Sea $P: V \to V$ una proyección a  $V_1$ paralela a  $V_2$, entonces $P^2 = P$.
		\end{proposicion}
		\begin{proof}
			Como $P$ proyecta a  $V_1$ paralela a  $V_2$, entonces para $v_1 \in V_1, v_2 \in V_2$ tenemos  $P(v_1+v_2) = v_1$, por lo tanto  $P^2(v_1+v_2) = P(v_1) = v_1= P(v_1+v_2)$.
		\end{proof}
		
		\begin{teorema}\label{idem->proy}
			Sea $P: V \to V$ un operador lineal. Si $P^2 = P$  entonces
			$$
			V = \nu(P) \oplus \im(P).
			$$
			Además, $P$ es la proyección  a $\im(P)$ paralela a $\nu(P)$.
		\end{teorema}
		\begin{proof} Veamos primero que $\nu(P) \cap \im(P)= 0$. Sea $v \in \nu(P) \cap \im(P)$. Como $v \in \im(P)$, entonces $v = P(w)$, luego   $P(v) = P^2(w) =P(w) = v$. Ahora bien, como $v\in  \nu(P)$, entonces $P(v)=0$. Es decir, si  $v \in\nu(P) \cap \im(P)$, entonces  $v= P(v)=0$. 
			
			Observar que $v = (v -P(v)) + P(v)$ y que $v -P(v)\in \nu(P)$ y $P(v) \in \im(P)$. Luego $v \in \nu(P) + \im(P)$. Como $v \in V$ es arbitrario,  tenemos que $V= \nu(P) + \im(P)$
		\end{proof}
		
		
		\medskip 
		
		
		
		\begin{teorema} 
			Sea $V$ espacio vectorial de dimensión finita y $P:V \to V$ una aplicación lineal. Entonces, $P$ es una proyección a $V_1$ paralela a  $V_2$ si y sólo si $P^2 = P$ y $V_1= \im(P)$, $V_2 = \nu(P)$. 
		\end{teorema}
		\begin{proof} ($\Rightarrow$) es proposición \ref{proy->idem}. ($\Leftarrow$) es teorema \ref{idem->proy}.
		\end{proof}
		
		\begin{definicion} Sea $V$ espacio vectorial de dimensión finita. $S:V \to V$ una aplicación lineal. Diremos que $S$ es un \emph{involución} si $S^2 = I$.
		\end{definicion}
		
		\medskip 
		
		\begin{teorema}\label{th-involucion}
			Sea $S: V \to V$ una involución. Entonces los conjuntos
			$$
			V_1 =\{v\in V:S(v)=v\}, \qquad V_{-1} =\{v\in V:S(v)=-v\}
			$$ 
			son subespacios vectoriales y 
			$$
			V = V_1 \oplus V_{-1}.
			$$
		\end{teorema}
		\begin{proof}[Idea de la demostración] Primero ver que $V_i$ es subespacio ($i=1,-1$). Luego observar que 
			$$
			w = \frac{w + S(w)}{2}+\frac{w - S(w)}{2}
			$$
			y  que $\displaystyle{\frac{w \pm S(w)}{2}} \in V_{\pm 1}$.
		\end{proof}
		
	\end{section}

		\end{comment}
	
	
	\begin{section}{Producto interno}
		Los axiomas de espacio vectorial no son suficientes para hacer frente a ciertas nociones geométricas  como el ángulo, la perpendicularidad, longitud, distancia, etc. Con la  introducción del producto interno podremos hacer las definiciones correspondientes y trabajar con los conceptos previamente mencionados.
		
		\medskip 
		
		En  el  capítulo \ref{chap-vectores} hemos visto el producto escalar en $\R^n$ y que este producto escalar  cumplía cuatro propiedades básicas,  que llamamos \ref{prop-P1}, \ref{prop-P2}, \ref{prop-P3} y \ref{prop-P4}. Estas propiedades son las que se utilizarán para generalizar el producto escalar a cualquier espacio vectorial y resultados  muy intuitivos e importantes, como ser el teorema de Pitágoras y la desigualdad triangular, se  podrán probar con el uso de estas propiedades.  
		
		\medskip
		
			\begin{definicion} Sea $V$ un espacio vectorial. Un \emph{producto interno} es  una aplicación 
			\begin{equation*}
				\la\;|,\;\ra: V \times V \to \R
			\end{equation*}
			que satisface las propiedades enumeradas a continuación.
			\begin{enumerate}[label=\textbf{P\arabic*.},ref=P\arabic*]
				\item\label{prop-P1-pi}	Para $v,w \in V$, 
				\begin{equation*}
					\langle v , w \rangle = \langle w , v \rangle.
				\end{equation*}
				\item\label{prop-P2-pi} Para $v,w,u \in V$,
				\begin{equation*}  
				\langle v , w + u \rangle =\langle v , w \rangle + \langle v , u \rangle = \langle w +u , v \rangle.
				\end{equation*}
				\item\label{prop-P3-pi} Si $v,w \in V$ y $\lambda$ es un número, entonces 
				\begin{equation*}
				\langle \lambda v , w \rangle = \lambda \langle v , w \rangle \quad \text{ y } \quad  \langle v , \lambda w \rangle = \lambda \langle v , w \rangle.
				\end{equation*}
				\item\label{prop-P4-pi} Si $v=0$ es el vector cero, entonces $\langle v , v \rangle =0$,  de lo contrario
				\begin{equation*}
				\langle v , v \rangle >0
				\end{equation*}
			\end{enumerate}
		\end{definicion}
		
		Debido a que  $\langle  ,  \rangle$ satisface \ref{prop-P2-pi} y \ref{prop-P3-pi}, decimos que  $\langle  ,  \rangle$ es una \textit{2-forma bilineal}.  Debido a \ref{prop-P1-pi} se dice que la forma bilineal es \textit{simétrica} y por \ref{prop-P4-pi} que es \textit{positiva.} Es decir,  un producto interno es una 2-forma bilineal,  simétrica y positiva.   

		
		Veremos luego  que hay una infinidad de productos internos en un espacio vectorial dado. Cuando hablamos de un \emph{espacio vectorial con producto interno}, nos estamos refiriendo a un espacio vectorial con un producto interno particular. 
		
		
		\medskip 
		
		\begin{ejemplo} Consideremos la función $d:\R^2 \times \R^2 \to \R$,  definida de la siguiente manera: si  $v,w \in \R^2$, $d(v,w) = \det[\,v\;w\,]$,  donde a $v$ y $w$ los miramos como vectores columna. Por las propiedades del determinante es claro  que $d$  es una forma bilineal (cumple \ref{prop-P2-pi} y \ref{prop-P3-pi}). Sin embargo,  no es simétrica,  si no lo que se llama \textit{antisimétrica},  debido a que $d(w,v) = det[w\, v] = -det[v\,w] =-d(v,w)$. Tampoco es positivo.   
		\end{ejemplo}
		
		\medskip 
		
		\begin{ejemplo}\label{prodescalar}  Hemos visto que el  producto escalar en $\R^n$ es un producto interno y este será el ejemplo que tendremos siempre presente para lo que sigue. Al  producto escalar en $\R^n$ se lo denomina el \emph{producto interno canónico} de $\R^n$. 
		\end{ejemplo}
		
		\medskip 
	
				
		\begin{observacion}
			Sea $V$  espacio vectorial  de dimensión finita con $\dim(V)=n$ y $\la \;|\;\ra$ un producto interno en $V$. Sea $v_1,v_2,\ldots,v_n$ es una base de $V$. Dados $v,w \in V$, tenemos que $v = \sum_i a_i v_i$ y $w = \sum_i b_i v_i$ y
			$$
			\begin{array}{rlr}
			\la v|w \ra &=  \la  \sum_i a_i v_i|\sum_j b_j v_j\ra & \\
			&=  \sum_i a_i  \la  v_i|\sum_j b_j v_j\ra&\quad \text{(linealidad en la primera variable)} \\
			&=  \sum_i a_i \sum_j b_j \la  v_i| v_j\ra&\quad \text{(linealidad en la segunda variable)} \\
			&= \sum_{i,j} a_i b_j \la  v_i| v_j\ra.&
			\end{array}
			$$
			Luego conociendo $ \la  v_i| v_j\ra$ para $1 \le i,j \le n$ podemos averiguar el producto escalar de dos vectores $v = \sum_i a_i v_i$ y $w = \sum_i b_i v_i$ con la fórmula
			\begin{equation}
			\la v|w \ra = \sum_{i=1}^n\sum_{j=1}^n a_i b_j \la  v_i| v_j\ra
			\end{equation}
			
		\end{observacion}
		
		\medskip
		
		\begin{definicion} Sea $V$ espacio vectorial y $\la\;|\;\ra$ un producto interno en $V$. Dado $v \in V$ la \emph{norma} de $v$ es
			$$
			||v|| = \sqrt{\la v,v\ra}.
			$$
			Diremos que un vector $v$ es \emph{unitario} si $||v|| = 1$.
		\end{definicion}
		
		\medskip 
		

		
		
		\begin{proposicion} Sea $V$ espacio  vectorial con  producto interno $\la\;|\;\ra$. Entonces, 
			\begin{enumerate}
				\item\label{prop-pi-1} Si $v \in V$, $\la0|v\ra = \la v|0\ra = 0$.
				\item\label{prop-pi-2} Si $c \in \R$, y $v \in V$, tenemos $||cv|| = |c|||v||$.
				\item\label{prop-pi-3} $||u+v||^2 = ||u||^2 + ||v||^2 + 2\la u,v\ra$. 
			\end{enumerate}
		\end{proposicion}
		\begin{proof}
			
			\ref{prop-pi-1} $\la0|v\ra = \la 0\cdot0|0\ra = 0 \cdot \la0|v\ra  =0$.
			
			\ref{prop-pi-2}  $||cv|| =  \sqrt{\la cv,cv\ra} = \sqrt{c^2\la v,v\ra} =\sqrt{c^2}\sqrt{\la v,v\ra}=|c|||v||$.
			
			\ref{prop-pi-3} 
			\begin{align*}
				||u+v||^2 &= \la u+v,u+v\ra\\
				&= \la u,u+v\ra+ \la v,u+v\ra  \\
				&= \la u,u\ra+ \la u,v\ra+\la v,u\ra+ \la v,v\ra  \\
				&= \la u,u\ra+ 2\la u,v\ra+ \la v,v\ra  \\
				&= ||u||^2 + ||v||^2 + 2\la u,v\ra.
			\end{align*}
			
			
		\end{proof}
		
		\medskip
		
	
		En dimensión finita todos los ejemplos son ``parecidos''  al producto escalar en $\R^n$. En dimensión infinita podemos encontrar productos internos muy diversos y el siguiente ejemplo muestra uno de ellos.   
		
		\begin{ejemplo} 
			Sea $E = C^0([a,b])$ el espacio vectorial cuyos elementos son las funciones continuas $f: [a, b] \to\R$. Se puede definir un producto interno en $E$ de la siguiente manera: sean $f,g \in  C^0([a,b])$,  entonces
			\begin{equation*}
				\la f, g \ra = \int_a^b f(x)g(x) dx.
			\end{equation*}
			En este caso, la función norma es
			\begin{equation*}
			||f|| = \sqrt{\int_a^b f(x)^2 dx}.
			\end{equation*}
			Usando las propiedades de la integral es sencillo ver que $\la , \ra $ es una 2-forma, bilineal y simétrica. Por propiedades de las funciones continuas se demuestra que además la 2-forma es positiva. 
			
			Este producto interno se utiliza en el estudio de series de Fourier.
			 
		\end{ejemplo}
		
	
		
		
		\begin{definicion} Se $V$ un espacio vectorial con producto interno $\la\; ,\; \ra$. Dos vectores $v,w \in V$ se dicen \emph{ortogonales} o \emph{perpendiculares} si $\la v,w \ra = 0$. En  ese caso podemos usar la notación $v\perp w$. Sea $X \subset V$, diremos que $X$ es un \emph{conjunto ortogonal} si $v\perp w$ para $v,w \in X$, $v\not= w$. Diremos que $X$ es un \emph{conjunto ortonormal} si $X$ es ortogonal y todos los vectores de $X$ son  unitarios (es decir $||v|| =1$ para $v \in X$).
		\end{definicion}
		
		\begin{obs}
			Hemos visto que en $\R^n$ si dos vectores son perpendiculares, entonces su producto escalar es 0 y viceversa. Luego perpendicular es el mismo concepto que ortogonal. 
		\end{obs}
		
		\begin{proposicion}\label{ortogonal->ortonormal}
			Sea $V$ un espacio vectorial con producto interno $\la\; ,\; \ra$ y $$X = \{v_1,\ldots,v_r \} \subset V$$ un conjunto ortogonal. Sea 
			\begin{equation*}
				X' =  \left\{\frac{v_1}{||v_1||},\ldots,\frac{v_r}{||v_r||} \right\}.
			\end{equation*}
			Entonces $X'$ es  un conjunto ortonormal de $V$. 
		\end{proposicion}
		\begin{proof}
			Para demostrar esto debemos ver que dos vectores distintos de $X'$ son ortogonales y que cada vector de $X'$ es de norma 1.
			
			Sea $i \ne j$, entonces
			\begin{equation*}
			\la \frac{v_i}{||v_i||} | \frac{v_j}{||v_j||}\ra = \frac{1}{||v_i||||v_j||} \la v_i | v_j\ra = 0.
			\end{equation*}
			Por otro lado,
			\begin{equation*}
			\la \frac{v_i}{||v_i||} | \frac{v_i}{||v_i||}\ra = \frac{1}{||v_i||^2} \la v_i | v_i\ra=  \frac{1}{||v_i||^2}||v_i||^2 =1.
			\end{equation*}
		\end{proof}	
			
			
			
		
		
		\medskip
		
		\begin{teorema}\label{th-ortogonal-implica-li} Se $V$ un espacio vectorial de dimensión finita con producto interno $\la\; ,\; \ra$. Sea $X \subset V$ un conjunto  ortogonal. Entonces $X$ es LI. 
		\end{teorema}
		\begin{proof} Sea $X =\{v_1,\ldots,v_r \}$ y sea $a_1,\ldots,a_r$ en $F$ tales que  $\sum_{i=1}^r a_iv_i =0$. Entonces,  dado $j$ con $1 \le j \le r$,  tenemos 
			$$
			0=\la\sum_{i=1}^r a_iv_i ,v_j \ra = \sum_{i=1}^r a_i\la v_i ,v_j \ra = a_j\la v_j ,v_j \ra = a_j||v_j||^2.
			$$
			Como $X$  es un conjunto ortogonal,  $||v_j|| >0$, luego $a_j =0$ para cualquier $j$. Es decir hemos probado que todos los coeficientes de la suma son cero y por lo tanto $X$  es LI.
		\end{proof}
		
		
		
		\begin{proposicion}
			${}^{}$
			\begin{enumerate}
				\item Teorema de Pitágoras: si $u\perp v$, entonces $||u+v||^2 = ||u||^2 + ||v||^2$.
				\item Ley del Paralelogramo: $||u+v||^2+ ||u-v||^2 = 2||u||^2 + 2||v||^2$.
			\end{enumerate}
		\end{proposicion}
		\begin{proof} Ambas demostraciones se hacen desarrollando las fórmulas y usando las propiedades del producto interno.
			
			(1) 
			\begin{equation*}
			||u+v||^2 = \la u+v, u+v\ra = \la u, u+v\ra+\la v, u+v\ra = \la u, u\ra+\la u, v\ra+\la v, u\ra+\la v, v\ra.
			\end{equation*} 
			Las igualdades de arriba se deben a la bilinealidad del producto interno. 
			Ahora bien,  como $u\perp v$,  tenemos que $0 = \la u, v\ra=\la v, u\ra$, luego
			\begin{equation*}
			||u+v||^2 = \la u, u\ra+ \la v, v\ra =  ||u||^2 + ||v||^2.
			\end{equation*} 
			
			(2) 
			\begin{align*}
			||u+v||^2+ ||u-v||^2 &= \la u+v, u+v\ra + \la u-v, u-v\ra \\
			&= \la u, u\ra+2\la u, v\ra+\la v, v\ra + \la u, u\ra-2\la u, v\ra+\la v, v\ra \\
			&=  2||u||^2 + 2||v||^2.
			\end{align*}
		\end{proof}
		
		\begin{definicion} Sea $V$ un espacio vectorial con producto interno $\la\; ,\; \ra$. Si $X \subset V$ es ortogonal (ortonormal) y es base diremos que $X$ es una \emph{base ortogonal} (resp. \emph{base ortonormal}) o diremos que $X$ es \textit{BO} (resp. \textit{BON}).
		\end{definicion}
		
		\begin{ejemplo} 
			\begin{enumerate}
				\item La base canónica de $\R^n$ es ortonormal. 
				\item Si $u=(1,1)$, $v=(1,-1)$, entonces $u,v$ es una base ortogonal.  
			\end{enumerate}
		\end{ejemplo}
		
		\begin{proposicion} Sea $V$ un espacio vectorial de dimensión finita con producto interno $\la\; ,\; \ra$ y sea $ X = \{v_1,\ldots,v_n\}$ una base ortogonal, entonces 
			$$X' = \left\{\frac{v_1}{||v_1||},\ldots,\frac{v_n}{||v_n||} \right\}
			$$
			es una base ortonormal.
		\end{proposicion}
		\begin{proof}
			Hemos probado en la proposición \ref{ortogonal->ortonormal} que $X'$ es un conjunto ortonormal.  Por  teorema \ref{th-ortogonal-implica-li}. $X'$ es un conjunto LI.   Veamos ahora que es $X'$ genera a $V$. 
			
			Sea $v \in V$, como $X$  es base de  $V$, en particular genera a $V$, luego existen $a_i \in R$, tal que  $v = \sum_i a_i v_i$. Luego
			$$
			v = \sum_i a_i v_i = \sum_i a_i \frac{||v_i||}{||v_i||} v_i = \sum_i (a_i ||v_i||) \frac{v_i}{||v_i||}.
			$$
			Luego $X'$ es un conjunto de generadores de $V$. 
		\end{proof}
		\begin{ejemplo} 
			\begin{enumerate}
				\item Si $u=(1,1)$, $v=(1,-1)$, entonces $||u||=||v|| = \sqrt{2}$ y $(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}),(\frac{1}{\sqrt{2}},\frac{-1}{\sqrt{2}})$ es una base ortonormal.  
			\end{enumerate}
		\end{ejemplo}
		
		\medskip
		
		\begin{observacion}\label{obs-proyeccion-ort}
			No es difícil ver en un dibujo que $\operatorname{pr}_u(v) := \frac{\la u|v\ra}{\la u|u\ra}u$ es la proyección de $v$ en $u$ y que   $(v - \operatorname{pr}(v)) \perp u$. Es decir,   los vectores
			\begin{equation*}
					u, \quad  v - \operatorname{pr}(v) = v - \frac{\la u|v\ra}{\la u|u\ra}u,
			\end{equation*}
			son ortogonales.
			\begin{figure}[h]
				falta
				\caption{Proyección de $v$ en $u$ cuando $||v||=1$.}\label{fig-proyeccion-ort}
			\end{figure}
		
		
			Esto,  además de la interpretación geométrica, lo podemos demostrar algebraicamente:
			\begin{equation*}
			\la v - \frac{\la u,v\ra}{\la u,u\ra} u , u \ra =
			\la v , u \ra  -\frac{\la u,v\ra}{\la u,u\ra}\la u , u \ra =  \la v , u \ra -  \la u , v \ra =0.
			\end{equation*}
			
		\end{observacion}
		
		\begin{proposicion}[Desigualdad de Schwarz] Sea   $V$ un espacio vectorial con producto interno y $u, v\in V$. Entonces 
			$$|\la u, v \ra| \le ||u||||v||.$$
		\end{proposicion}
		\begin{proof}[Demostración ($*$)]
			Sea $c = \displaystyle\frac{\la u,v\ra}{\la u,u\ra} = \frac{\la u,v\ra}{||u||^2}$, entonces, por la observación \ref{obs-proyeccion-ort}, tenemos que  $ v-cu$ es ortogonal a $u$. Ahora bien, 
			\begin{equation*}
				v = (v - cu) + cu   
			\end{equation*}
			y  $(v - cu) \perp cu$. Por Pitágoras
			\begin{align*}
			||v||^2 &= ||v-cu||^2 +||cu||^2  \\
			&=||v-cu||^2 +|c|^2||u||^2.
			\end{align*}
			Como $||v-cu||^2 \ge 0$, tenemos que $|c|^2||u||^2 \le 	||v||^2$ y sacando raíces cuadradas obtenemos
			$$
			|c|||u|| \le ||v|| \Rightarrow \frac{|\la u,v\ra|}{||u||^2}||u|| \le ||v|| \Rightarrow \frac{|\la u,v\ra|}{||u||}\le ||v|| \Rightarrow |\la u,v\ra|\le ||v||||u||.
			$$
		\end{proof}
		
		\begin{teorema}[Desigualdad triangular] Sea   $V$ un espacio vectorial con producto interno y $u, v\in V$. Entonces 
			$$||u + v|| \le ||u|| + ||v||	$$
		\end{teorema}
		\begin{proof}
			Probar 
			$$
			||u + v||^2 \le (||u|| + ||v||)^2
			$$
			desarrollando  el lado izquierdo de la desigualdad  como $\la u+v, u+v \ra$ y el lado derecho por el cálculo del binomio al cuadrado. Luego usar la desigualdad de Schwarz.
		\end{proof}
		
		
		\medskip
		
		Ahora vamos a demostrar que hay bases ortonormales en cualquier espacio vectorial de dimensión finita  con un producto interno.
		
		Más precisamente, vamos a mostrar el proceso de ortonormalización de Gram-Schmidt, que consta de  un algoritmo que permite pasar de una base cualquiera $\{v_1,\ldots, v_n\}$ a una base ortonormal $\{u_1,\ldots, u_n\}$, con la importante propiedad de que, para $m$ con  $1 \le m \le n$, el subespacio generado por los vectores $\{u_1,\ldots, u_m\}$ es el mismo que el subespacio generado por los vectores $\{v_1,\ldots, v_m\}$.
		
		Dada la base $\{v_1,\ldots, v_n\}$ obtendremos primero una base ortogonal $\{w_1,\ldots, w_n\}$ y luego haciendo $u_i = w_i/||w_i||$ obtenemos la base ortonormal $\{u_1,\ldots, u_n\}$.
		
		\medskip 
		
		La idea del proceso es sencillo para dos vectores $v_1,v_2$ no nulos y no proporcionales vimos en la observación \ref{obs-proyeccion-ort} que  los vectores
		$$
		w_1 = v_1, \quad w_2 = v_2 - \operatorname{pr}_{v_1}(v_2) = v_2 - \frac{\la v_1,v_2\ra}{\la v_1,v_1\ra}v_1
		$$
		son ortogonales. 
		Ahora bien,  $v_1 = w_1$ y $v_2  = \frac{\la v_1,v_2\ra}{\la v_1,v_1\ra}w_1 + w_2$, luego  $w_1,w_2$ generan el mismo subespacio que $v_1,v_2$. Concluyendo, dados  $v_1,v_2$ dos vectores LI, $w_1,w_2$ son dos vectores ortogonales que generan el mismo subespacio. Para $n >2$ la idea es similar. 
		
		\medskip 
		
		
		
		
		
		\begin{proposicion}[Proceso de ortogonalización de Gram-Schmidt]
			Sea $V$ espacio vectorial de dimensión finita con producto interno $\la\;,\;\ra$ y $\{v_1,\ldots,v_n\}$ una base de $V$. Entonces existe una base ortogonal $\{w_1,\ldots, w_n\}$ tal que el subespacio generado por los vectores $\{w_1,\ldots, w_m\}$ es el mismo que el subespacio generado por $\{v_1,\ldots, v_m\}$ ($1\le m \le n$). Explícitamente, la base es
			\begin{align}
			w_1 &= v_1, \tag{1} \\
			w_2 &= v_2 - \frac{\la v_2,w_1\ra}{\la w_1,w_1\ra}w_1, \tag{2}\\
			w_3 &= v_3 - \frac{\la v_3,w_1\ra}{\la w_1,w_1\ra}w_1- \frac{\la v_3,w_2\ra}{\la w_2,w_2\ra}w_2,\tag{3} \\
			\vdots &\quad \vdots \notag \\
			w_n &= v_n - \frac{\la v_n,w_1\ra}{\la w_1,w_1\ra}w_1- \frac{\la v_n,w_2\ra}{\la w_2,w_2\ra}w_2 - \cdots - \frac{\la v_n,w_{n-1}\ra}{\la w_{n-1},w_{n-1}\ra}w_{n-1}.\tag{$n$}	
			\end{align}
			En forma más breve, para $1 \le i \le n$, 
			\begin{equation}
			w_i = v_i - \sum_{j = 1}^{i-1} \frac{\la v_i,w_{j}\ra}{\la w_{j},w_{j}\ra}w_{j} \tag{$i$}
			\end{equation}
		\end{proposicion} 
		\begin{proof}[Demostración ($*$)]
			Haremos la demostración por inducción sobre $n$. 
			
			Para $n= 1$ el resultado es trivial.
			
			Supongamos que el resultado valga para $k-1>0$, es decir  $\{w_1,\ldots, w_{k-1}\}$ es ortogonal y 
			$\operatorname{span}(w_1,\ldots, w_{k-1}) = \operatorname{span}(v_1,\ldots, v_{k-1})$. Probemos el resultado para $k$.  Si  $i < k$, 
			\begin{align*}
			\la w_k, w_i \ra &= \la  v_k - \sum_{j = 1}^{k-1} \frac{\la v_k,w_{j}\ra}{\la w_{j},w_{j}\ra}w_{j} , w_i \ra 
			= \la v_k, w_i\ra -  \sum_{j = 1}^{k-1} \frac{\la v_k,w_{j}\ra}{\la w_{j},w_{j}\ra}\la w_{j} , w_i \ra \\
			&=  \la v_k, w_i\ra -  \la v_k, w_i\ra = 0.
			\end{align*}
			Es decir $	\la w_k, w_i \ra =0$ para todo $i < k$. Por consiguiente,  $\{w_1,\ldots, w_{k}\}$ es ortogonal.
			
			Demostremos ahora que $\operatorname{span}\{w_1,\ldots, w_{m}\} = \operatorname{span}\{v_1,\ldots, v_{m}\}$ para $1 \le m \le n$. 
			
			$\operatorname{span}\{w_1,\ldots, w_{m}\} \subset \operatorname{span}\{v_1,\ldots, v_{m}\}$: por la fórmula ($i$) es claro que $w_m$  es combinación lineal de $v_m$ y $w_1,\ldots, w_{m-1}$. Por hipótesis inductiva, los $w_1,\ldots, w_{m-1}$ son combinación lineal de  los $v_1,\ldots, v_{m-1}$,  luego los $w_1,\ldots, w_{m}$ son combinación lineal de los  $v_1,\ldots, v_{m}$.
			
			$\operatorname{span}\{v_1,\ldots, v_{m}\} \subset \operatorname{span}\{w_1,\ldots, w_{m}\}$: Como
			$$
			v_k = w_k + \sum_{j = 1}^{k-1} \frac{\la v_k,w_{j}\ra}{\la w_{j},w_{j}\ra}w_{j},
			$$
			tenemos que 	$\operatorname{span}\{v_1,\ldots, v_{m}\} \subset \operatorname{span}\{w_1,\ldots, w_{m}\}$.
		\end{proof}
		
		
		\medskip
		
		\begin{obs} 
			Sea $V$ espacio vectorial de dimensión finita con producto interno $\la\;,\;\ra$ y $\{v_1,\ldots,v_n\}$ una base de $V$. Entonces, por Gram-Schmidt existe una base ortogonal $\{w_1,\ldots, w_n\}$ de $V$. Sea $u_i = w_i/||w_i||$, entonces  $\{u_1,\ldots, u_n\}$ es una base de $V$ donde los vectores son ortogonales y de norma 1.
		\end{obs}
		
		\medskip
		
		\begin{definicion}  Sea $V$ espacio vectorial con producto interno $\la \;,\;\ra$ y sean $U, W$ subespacios de $V$. Diremos que \textit{$U$ es ortogonal a $W$} y denotaremos $U \perp W$ si  para todo $u \in U$ y para todo $w \in W$ tenemos que $ \la u,w\ra =0$. 
			
			Si $X$ es subconjunto de $V$,  definimos 
			$$
			X^\perp := \{u \in V: \la u,x\ra = 0, \forall\, x \in X\} = \{u \in V: \la u,X\ra = 0\}.
			$$ 
		\end{definicion}
		
		\begin{proposicion}
			Sea $V$ espacio vectorial con producto interno $\la \;,\;\ra$ y $X \subset V$. Entonces $X^\perp$  es un subespacio de $V$.
		\end{proposicion}
		\begin{proof}
			Debemos probar que si $u,v \in X^\perp$ y $c \in \R$,  entonces $cu+v \in X^\perp$,  es decir que para todo $x \in X$,  se cumple que  $\la cu+v,x\ra=0$. Ahora bien,
			$$
			\la cu+v,x\ra= c\la u,x\ra +\la v,x\ra= 0.
			$$
		\end{proof}
		
		\begin{definicion}
			Sea $V$ espacio vectorial con producto interno $\la \;,\;\ra$ y sea $X$ subconjunto de $V$. Diremos que $X^\perp$ es el  \textit{subespacio  ortogonal a $X$ en $V$}.   
		\end{definicion}
		
		
	
	\end{section}

\begin{comment}

\begin{section}{La adjunta de una transformación lineal} Mostraremos en esta sección como el producto interno nos permite asociar a cada transformación lineal $T: V \to W$ una nueva transformación  lineal $T^*: W \to V$ llamada la adjunta de $T$. 
	
	Repasemos algunas definiciones.  
	
	\begin{definicion} 
		Sea $V$ un espacio vectorial  sobre $F$ cuerpo. Entonces el \textit{espacio dual de $V$}, es
		$$
		V^* = \{\lambda: V \to F: \text{$\lambda$ es $F$-lineal} \}.
		$$  
	\end{definicion}
	
	\begin{obs} Sea $V$ un espacio vectorial  sobre $F$ . Las siguientes son propiedades elementales del espacio dual. 
		\begin{enumerate}
			\item $V^*$ es un espacio vectorial. 
			\item Si $e_1,\ldots,e_n$ es una base de $V$, entonces $V^*$ también tiene una base de $n$ elementos: $f_1,\ldots,f_n$, donde 
			$f_i$ se define sobre la base de $V$ de la siguiente manera $f_i(e_i) =1$ y $f_i(e_j)=0$ si $i\ne j$; y entonces extendemos $f_i$ a $V$ por linealidad. La base $f_1,\ldots,f_n$ es llamada la \textit{la base dual} de $e_1,\ldots,e_n$.
			\item En particular, por el item anterior, tenemos que $\dim(V) = \dim(V^*)$. 
		\end{enumerate}
	\end{obs}	
	
	\medskip 
	
	En lo que resta de la sección trabajaremos con $\R$-espacios vectoriales.
	
	\medskip
	
	\begin{proposicion} \label{intida}
		Sea $V$ espacio vectorial de dimensión finita con producto interno $\la \;,\;\ra$ y sea $v \in V$. Entonces la aplicación 
		$$
		\begin{matrix}
		v^*: &V &\to& \R\quad &\text{ definida } \\
		&w&\mapsto &  \la w,v\ra
		\end{matrix}	
		$$
		es lineal. Es decir $v^* \in V^*$. 
	\end{proposicion}
	\begin{proof}
		El resultado se obtiene directamente por la bilinealidad del producto interno. Es decir, como el producto interno es lineal en la primera entrada, entonces la aplicación $v^* = \la \;,v\ra$ es lineal. Simbólicamente
		$$
		v^*( ru+w)= \la ru+w,v\ra = r\la u,v\ra + \la w,v\ra = rv^*(u)+v^*(w),
		$$
		para $r \in \R$, $u,w,\in V$.
	\end{proof}
	
	Veamos que la recíproca es cierta
	
	\begin{proposicion}\label{intvuelta}
		Sea $V$ espacio vectorial de dimensión finita con producto interno $\la \;,\;\ra$ y sea $\lambda \in V^*$. Entonces existe un único $v \in $V tal que 
		$$\lambda(w) = \la w,v\ra,$$
		para $w \in V$.
	\end{proposicion}
	\begin{proof}
		Sea $e_1,\ldots,e_n$ una BON de $V$ y  sea $a_i = \lambda(e_i)$. Definimos $v = \sum a_ie_i$. Entonces, dado cualquier $w\in V$, si $w = \sum b_ie_i$, tenemos que 
		$$
		\lambda(w) = \lambda(\sum_i b_ie_i) = \sum_i b_i\lambda(e_i) =  \sum_i b_ia_i.
		$$ 
		Por otro lado
		$$
		\la w,v\ra = \la \sum_i b_ie_i,\sum_j a_je_j\ra  = \sum_i\sum_j b_ia_j\la e_i,e_j\ra  = \sum_i b_ia_i.
		$$
		Es decir $\lambda(w) = \la w,v\ra$ para cualquier $w \in V$. Por lo tanto $\lambda =v^*$.
		
		\medskip
		
		\textit{Unicidad.} Supongamos que existe $v'\in V$ tal que $\lambda(w) = \la w,v'\ra$ para todo $w \in V$. Por lo tanto $0= \lambda(w) - \lambda(w) =  \la w,v\ra -  \la w,v'\ra =  \la w,v-v'\ra$. En particular $\la v-v',v-v'\ra =0$, lo cual implica que $v-v'=0$ o, equivalentemente, $v = v'$.
	\end{proof}
	
	\begin{obs}\label{obs45}
		La proposición anterior nos permite definir $\phi: V^* \to V$, de la siguiente forma: sea $\lambda \in V^*$, entonces $\phi(\lambda) =v$, donde $v$ es el único elemento de $V$ tal que  $\lambda(w) = \la w,v\ra.$
	\end{obs}
	
	\begin{teorema}\label{isodual}
		Sea $V$ espacio vectorial de dimensión finita con producto interno $\la \;,\;\ra$. Entonces la aplicación 
		$$
		\begin{matrix}
		V &\to& V^* &\text{  } \\
		v&\mapsto &  v^* &
		\end{matrix}	
		$$
		es un isomorfismo de espacios vectoriales. 
	\end{teorema}
	\begin{proof}
		Primero debemos ver que la aplicación es lineal: Sea $r \in \R$ y $v,w \in V$, debemos verificar que 
		$$
		(rv + w)^* =  rv^* +  w^*.
		$$
		Ahora bien, esto es cierto pues, debido a la linealidad del producto interno en el segundo factor, para todo $u \in V$ se cumple que
		$$
		(rv + w)^*(u) = \la u,rv + w\ra =  r\la u,v\ra +  \la u,w\ra = rv^*(u) +  w^*(u).
		$$
		
		
		\medskip
		
		Ahora mostraremos que la aplicación  $v\mapsto   v^*$ es un isomorfismo mostrando que  es inyectiva, luego se deduce lo requerido por dimensión.
		
		Recordemos que una aplicación lineal  es inyectiva si y sólo si su nucleo es $0$. Ahora bien, sea $v \in V$ tal que  $v^* =0$. Luego, $0= v^*(w) = \la w,v\ra$ para todo $w \in V$, en particular para $w=v$. Es decir, $\la v,v\ra =0$. Por positividad del producto interno, tenemos que  esto sólo se puede cumplir si $v=0$. Por lo tanto la aplicación $v \mapsto v^*$ es inyectiva  y como $\dim(V) = \dim(V^*)$, tenemos que es un isomorfismo.    
	\end{proof}
	
	\begin{obs}
		No es difícil  comprobar que $v \mapsto v^*$ y $\phi$ (ver proposición \ref{intvuelta} y observación  \ref{obs45}) son una inversa de la otra. 
	\end{obs}
	
	
	\begin{teorema}
		Sean $V, W$ espacios vectoriales de dimensión finita y con producto interno $\la \;,\;\ra$ y $\la \;,\;\ra$ respectivamente (se denotan igual). Sea $T: V \to W$ lineal, entonces existe una única $T^*: W \to V$ que cumple
		\begin{equation}\label{adjunta}
		\la Tv,w\ra = \la v,T^*w\ra,
		\end{equation}  
		para $v\in V$, $w \in W$.
	\end{teorema}
	\begin{proof}
		Sea $w \in W$ y definimos $\lambda_w :V \to \R$ por $\lambda_w(v) = \la Tv,w\ra$. Por linealidad de $T$ y del producto interno en el primer factor, claramente $\lambda_w \in V^*$. Entonces por teorema \ref{isodual} existe un único $u \in V$ tal que $u^* = \lambda_w$, es decir   
		$$
		\la v,u\ra= \la Tv,w\ra
		$$
		para todo $v \in V$. Definamos $T^*w = u$ y entonces se satisface la ecuación (\ref{adjunta}). 
		
		\medskip
		
		Faltaría comprobar que es lineal y es única. Se deja como ejercicio. 
	\end{proof}
	
	
	
	\begin{obs} El caso más interesante, y que pasaremos a estudiar ahora, es cuando $T: V \to V$, es decir cuando el espacio de llegada y  de partida es el mismo, y por lo tanto también $T^*: V \to V$. 
	\end{obs}
	
	
	\begin{ejemplo}\label{ejemplo4.10}
		Sea $T: \R^3 \to \R^3$ la transformación lineal $T(x,y,z) = (3x +y, 2x -y+ 3z, x)$. Calcular $T^*$ y la matriz de $T$ y $T^*$ en la base canónica.  
	\end{ejemplo}
	\begin{proof}[Solución 1] 
		La observación principal para hacer el cálculo de $T^*$ es que dada cualquier transformación lineal $S$, tenemos que 
		$$
		\la e_i, S(v)\ra  = t_i \Leftrightarrow S(v) = (t_1,\ldots,t_n). 
		$$ 
		Aplicado a este caso,
		\begin{align*}
		\la e_1, T^*(x,y,z)\ra  &= \la  T(e_1),(x,y,z)\ra =    \la (3,2,1),(x,y,z)\ra = 3x+2y+z \\
		\la e_2, T^*(x,y,z)\ra  &= \la  T(e_2),(x,y,z)\ra =    \la (1,-1,0),(x,y,z)\ra = x-y \\
		\la e_3, T^*(x,y,z)\ra  &= \la  T(e_3),(x,y,z)\ra =    \la (0,3,0),(x,y,z)\ra = 3y. \\
		\end{align*}
		Por lo tanto
		$$
		T^*(x,y,z) = (3x+2y+z,x-y,3y).
		$$
		Por lo tanto, la matriz de $T$ en la base canónica es 
		$$
		\left[\begin{matrix}
		3&1&0\\2&-1&3\\1&0&0.
		\end{matrix}
		\right]
		$$
		y la matriz de $T^*$ en la base canónica es
		$$
		\left[\begin{matrix}
		3&2&1\\1&-1&0\\0&3&0.
		\end{matrix}
		\right]
		$$
	\end{proof}
	
	
	
	\begin{definicion}
		Sea $n \in \mathbb{N}$ y $A = [a_{ij}]$ matriz $n \times n$. Entonces la \textit{matriz transpuesta} se define como
		$$A^\t = [a_{ji}].$$  
		Diremos que una matriz $A$ es \textit{simétrica} si $A^\t = A$. 
	\end{definicion}
	
	Observemos que en el ejemplo anterior la matriz de la adjunta es la transpuesta de la matriz de la transformación original. Veremos ahora, que este es un resultado general. 
	
	\begin{teorema}\label{matrizadjunta}
		Sea $V$ espacio vectorial de dimensión finita con producto interno $\la \;,\;\ra$ y sea  $\mathcal U = \{u_1, \ldots , u_n\}$ una BON de $V$. Sea $T: V \to V$ una transformación lineal y $A$ la matriz de $T$ en  la base $\mathcal U$,  es decir $[T]_{\mathcal U} = A$.
		
		Entonces, $[T^*]_{\mathcal U} = A^\t$, es decir, la matriz de $T^*$ en la base $\mathcal U$ es la transpuesta de $A$. 
	\end{teorema}
	\begin{proof}
		Observemos  que como $T(u_j) = \sum_i a_{ij}u_i$, entonces 
		$$
		\la Tu_j,u_i\ra = a_{ij}.
		$$
		Luego 
		$$
		a_{ij} = \la Tu_j,u_i\ra = \la u_j,T^*u_i\ra.
		$$
		Es decir  que $T^*(u_i) = \sum_j a_{ij}$. Es decir $[T^*]_{\mathcal U} = A^\t$.
	\end{proof}
	
	
	\begin{ejemplo} Resolveremos nuevamente, en forma más sencilla, el ejemplo  \ref{ejemplo4.10}.
	\end{ejemplo}
	\begin{proof}[Solución 2]
		Como $T(x,y,z) = (3x +y, 2x -y+ 3z, x)$, la matriz de $T$ en la base canónica es 
		$$
		\left[\begin{matrix}
		3&1&0\\2&-1&3\\1&0&0.
		\end{matrix}
		\right].
		$$
		Por lo tanto, por teorema  \ref{matrizadjunta}, la matriz de $T^*$ en la base canónica es
		$$
		\left[\begin{matrix}
		3&2&1\\1&-1&0\\0&3&0.
		\end{matrix}
		\right].
		$$
		Luego $T^*(x,y,z) = (3x+2y+z,x-y,3y)$.
	\end{proof}
	
	
	
	\begin{proposicion}\label{adjprop}
		Sean $V$ espacio vectorial de dimensión finita con producto interno $\la \;,\;\ra$ y $T,S:V \to V$ transformaciones lineales. Entonces
		\begin{enumerate}
			\item\label{adj0} $Id^* = Id$.
			\item\label{adj1} Si $c \in \R$, entonces $(cR)^* = cR^*$.
			\item\label{adj2} $(R+S)^* = R^* + S^*$.
			\item\label{adj3} $(RS)^* = S^*R^*$.
			\item\label{adj4} $R^{**} = R$.
		\end{enumerate} 
	\end{proposicion}
	\begin{proof}
		(\ref{adj0}). Es trivial.
		
		\vskip .3cm
		
		(\ref{adj1}). Por definición de adjunta $(cR)^*$ es la única transformación lineal tal que
		$$
		\la cR(v),w\ra = \la v,(cR)^*(w)\ra, \quad \forall v,w \in V.
		$$
		Ahora bien
		$$
		\la v,cR^*(w)\ra = c\la v,R^*(w)\ra = c\la R(v),w\ra =  \la cR(v),w\ra. 
		$$
		Es decir $(cR)^* = cR^*$.
		
		\vskip .3cm
		
		(\ref{adj2}). Como en el caso anterior, debemos demostrar que  $R^* + S^*$ es la única transformación lineal tal que
		$$ 
		\la (R+S)(v),w\ra = \la v,(R^* + S^*)(w)\ra, \quad \forall v,w \in V.
		$$
		Ahora bien,
		\begin{align*}
		\la v,(R^* + S^*)(w)\ra &= \la v,R^*(w) + S^*(w)\ra =  \la v,R^*(w) \ra+\la v, S^*(w)\ra \\
		&=  \la R(v),w \ra+\la S(v), w\ra =  \la R(v) + S(v), w\ra =  \la (R+S)(v), w\ra .
		\end{align*}
		
		\vskip .3cm
		
		(\ref{adj3}). 
		\begin{align*}
		\la v,(S^*R^*)(w)\ra &= \la v, S^*(R^*(w))\ra =  \la S(v),R^*(w) \ra \\
		&=  \la R(S(v)),w \ra =  \la (RS)(v), w\ra .
		\end{align*}
		Por lo tanto  $(RS)^* = S^*R^*$.
		
		\vskip .3cm
		
		(\ref{adj4}). Por definición de adjunta de $R^*$, tenemos que $ (R^*)^*=R^{**}$  es la única transformación lineal tal que 
		$$
		\la R^*(v), w\ra = \la v, R^{**}(w)\ra  , \; \forall\, v,w \in V.
		$$ 
		Ahora bien, por la definición de adjunta de $R$ sabemos que 
		$$
		\la R^*(v), w\ra = \la v, R(w)\ra , \; \forall\, v,w \in V.
		$$ 
		Luego $R = R^{**}$.
		
	\end{proof}
	
	
	\begin{teorema}\label{rel-adj-ort}
		Sea $T: V \to W$ una transformación lineal entre espacios vectoriales de dimensión finita con producto interno. Entonces,
		\begin{enumerate}
			\item $\nu(T^*) = \im(T)^\perp$,
			\item $\im(T^*) = \nu(T)^\perp$,
			\item $\nu(T) = \im(T^*)^\perp$,
			\item $\im(T) = \nu(T^*)^\perp$.
		\end{enumerate}
	\end{teorema}
	\begin{proof}
		Probaremos la primera afirmación, pues las otras se deducen fácilmente del hecho que $T^{**} = T$ y $U^{\perp\perp} = U$. Ahora bien,
		\begin{align*}
		w \in  \nu(T^*) &\Leftrightarrow T^*(w) =0 \Leftrightarrow \la v, T^*(w) \ra = 0,\; \forall\, v\in V \Leftrightarrow \\
		&\Leftrightarrow \la T(v), w \ra = 0,\; \forall\, v\in V\Leftrightarrow  w \in \im(T)^\perp.
		\end{align*}
		
		
	\end{proof}
\end{section}



\end{comment}

\begin{section}{La adjunta de una transformación lineal} Mostraremos en esta sección como el producto interno nos permite asociar a cada transformación lineal $T: V \to W$ una nueva transformación  lineal $T^*: W \to V$ llamada la adjunta de $T$. 
	
	
	
	\begin{teorema}
		Sean $V, W$ espacios vectoriales de dimensión finita y con producto interno $\la \;,\;\ra$ y $\la \;,\;\ra$ respectivamente (se denotan igual). Sea $T: V \to W$ lineal, entonces existe una única $T^*: W \to V$ que cumple
		\begin{equation}\label{eq-adjunta}
		\la Tv,w\ra = \la v,T^*w\ra,
		\end{equation}  
		para $v\in V$, $w \in W$ (el producto de la izquierda es en $W$ y el de la derecha en $V$).
	\end{teorema}
	\begin{proof}
	Sea $\{v_1,\ldots,v_n\}$ una BON de $V$ y $\{w_1,\ldots,w_m\}$ una BON de $W$, observemos que la coordenada $j$ (en $V$) de $T^*w_i$ debe cumplir
	\begin{equation}\label{eq-transp}
		\la T^*w_i,v_j\ra = 	\la w_i,Tv_j\ra.
	\end{equation}
	Por lo tanto,  definimos
	\begin{equation*}
		T^*(w_i) = \sum_{j=1}^{n} \la w_i,Tv_j\ra v_j,
	\end{equation*}
	y extendemos linealmente a una transformación lineal $T^*: W \to V$. Claramente $T^*$   está bien definida y es lineal (por definición).  La unicidad está garantizada por la ecuación (\ref{eq-transp}).  
	
	Finalmente,  debemos comprobar que se verifica la ecuación (\ref{eq-adjunta}): sean  $w \in W$ y $v \in V$, entonces $w = \sum_{i=1}^{m} \la w_i,w\ra w_i$ y $v = \sum_{j=1}^{n} \la v_j,v\ra v_j$. Reemplazando en  la ecuación (\ref{eq-adjunta}) $w$ y $v$ por su desarrollo en las bases se obtiene la igualdad. Para el lector curioso, a continuación desarrollamos la demostración:
	\begin{align*}
		\la v,T^*w\ra &= 	\la v, T^*(\sum_{i=1}^{m} \la w_i,w\ra w_i) \ra\\
		 &= \la v, \sum_{i=1}^{m} \la w_i,w\ra T^*(w_i) \ra\\
		 &=	\sum_{i=1}^{m} \la w_i,w\ra\la v,  T^*(w_i) \ra\\
		 &=\sum_{i=1}^{m} \la w_i,w\ra\la v,  \sum_{j=1}^{n} \la w_i,Tv_j\ra v_j\ra\\
		 &=\sum_{i=1}^{m} \la w_i,w\ra \sum_{j=1}^{n} \la w_i,Tv_j\ra\la v,  v_j\ra.
	\end{align*}
	Por otro lado, como $v = \sum_{j=1}^{n} \la v_j,v\ra v_j$, entonces
	\begin{equation*}
		\sum_{j=1}^{n} \la w_i,Tv_j\ra\la v,  v_j\ra = \sum_{j=1}^{n} \la w_i,\la v,  v_j\ra Tv_j\ra = \la w_i, T (\sum_{j=1}^{n}\la v,  v_j\ra v_j)\ra = \la w_i, T v\ra, 
	\end{equation*}
	por lo tanto 
	\begin{equation*}
		\la v,T^*w\ra = \sum_{i=1}^{m} \la w_i,w\ra \la w_i, T v\ra =  \la\sum_{i=1}^{m} \la w_i,w\ra w_i, T v\ra = \la w, T v\ra
	\end{equation*}
	\end{proof}
	
	
	
	\begin{obs} El caso más interesante, y que pasaremos a estudiar ahora, es cuando $T: V \to V$, es decir cuando el espacio de llegada y  de partida es el mismo, y por lo tanto también $T^*: V \to V$. 
	\end{obs}
	
	
	\begin{ejemplo}\label{ejemplo4.10}
		Sea $T: \R^3 \to \R^3$ la transformación lineal $T(x,y,z) = (3x +y, 2x -y+ 3z, x)$. Calcular $T^*$ y la matriz de $T$ y $T^*$ en la base canónica.  
	\end{ejemplo}
	\begin{proof}[Solución 1] 
		La observación principal para hacer el cálculo de $T^*$ es que dada cualquier transformación lineal $S$, tenemos que 
		$$
		\la e_i, S(v)\ra  = t_i \Leftrightarrow S(v) = (t_1,\ldots,t_n). 
		$$ 
		Aplicado a este caso,
		\begin{align*}
		\la e_1, T^*(x,y,z)\ra  &= \la  T(e_1),(x,y,z)\ra =    \la (3,2,1),(x,y,z)\ra = 3x+2y+z \\
		\la e_2, T^*(x,y,z)\ra  &= \la  T(e_2),(x,y,z)\ra =    \la (1,-1,0),(x,y,z)\ra = x-y \\
		\la e_3, T^*(x,y,z)\ra  &= \la  T(e_3),(x,y,z)\ra =    \la (0,3,0),(x,y,z)\ra = 3y. \\
		\end{align*}
		Por lo tanto
		$$
		T^*(x,y,z) = (3x+2y+z,x-y,3y).
		$$
		La matriz de $T$ en la base canónica es 
		$$
		\left[\begin{matrix}
		3&1&0\\2&-1&3\\1&0&0.
		\end{matrix}
		\right]
		$$
		y la matriz de $T^*$ en la base canónica es
		$$
		\left[\begin{matrix}
		3&2&1\\1&-1&0\\0&3&0.
		\end{matrix}
		\right]
		$$
	\end{proof}
	
	
	Observemos que en el ejemplo anterior la matriz de la adjunta es la transpuesta de la matriz de la transformación original. Veremos ahora, que este es un resultado general. 
	
	\begin{teorema}\label{matrizadjunta}
		Sea $V$ espacio vectorial de dimensión finita con producto interno $\la \;,\;\ra$ y sea  $\mathcal U = \{u_1, \ldots , u_n\}$ una BON de $V$. Sea $T: V \to V$ una transformación lineal y $A$ la matriz de $T$ en  la base $\mathcal U$,  es decir $[T]_{\mathcal U} = A$.
		
		Entonces, $[T^*]_{\mathcal U} = A^\t$, es decir, la matriz de $T^*$ en la base $\mathcal U$ es la transpuesta de $A$. 
	\end{teorema}
	\begin{proof}
		Observemos  que como $T(u_j) = \sum_i a_{ij}u_i$, entonces 
		$$
		\la Tu_j,u_i\ra = a_{ij}.
		$$
		Luego 
		$$
		a_{ij} = \la Tu_j,u_i\ra = \la u_j,T^*u_i\ra.
		$$
		Es decir  que $T^*(u_i) = \sum_j a_{ij}$. Es decir $[T^*]_{\mathcal U} = A^\t$.
	\end{proof}
	
	

	
	\begin{ejemplo} Resolveremos nuevamente, en forma más sencilla, el ejemplo  \ref{ejemplo4.10}.
	\end{ejemplo}
	\begin{proof}[Solución 2]
		Como $T(x,y,z) = (3x +y, 2x -y+ 3z, x)$, la matriz de $T$ en la base canónica es 
		$$
		\left[\begin{matrix}
		3&1&0\\2&-1&3\\1&0&0.
		\end{matrix}
		\right].
		$$
		Por lo tanto, por teorema  \ref{matrizadjunta}, la matriz de $T^*$ en la base canónica es
		$$
		\left[\begin{matrix}
		3&2&1\\1&-1&0\\0&3&0.
		\end{matrix}
		\right].
		$$
		Luego $T^*(x,y,z) = (3x+2y+z,x-y,3y)$.
	\end{proof}
	
	\begin{comment}
		

	
	\begin{proposicion}\label{adjprop}
		Sean $V$ espacio vectorial de dimensión finita con producto interno $\la \;,\;\ra$ y $T,S:V \to V$ transformaciones lineales. Entonces
		\begin{enumerate}
			\item\label{adj0} $Id^* = Id$.
			\item\label{adj1} Si $c \in \R$, entonces $(cR)^* = cR^*$.
			\item\label{adj2} $(R+S)^* = R^* + S^*$.
			\item\label{adj3} $(RS)^* = S^*R^*$.
			\item\label{adj4} $R^{**} = R$.
		\end{enumerate} 
	\end{proposicion}
	\begin{proof}
		(\ref{adj0}). Es trivial.
		
		\vskip .3cm
		
		(\ref{adj1}). Por definición de adjunta $(cR)^*$ es la única transformación lineal tal que
		$$
		\la cR(v),w\ra = \la v,(cR)^*(w)\ra, \quad \forall v,w \in V.
		$$
		Ahora bien
		$$
		\la v,cR^*(w)\ra = c\la v,R^*(w)\ra = c\la R(v),w\ra =  \la cR(v),w\ra. 
		$$
		Es decir $(cR)^* = cR^*$.
		
		\vskip .3cm
		
		(\ref{adj2}). Como en el caso anterior, debemos demostrar que  $R^* + S^*$ es la única transformación lineal tal que
		$$ 
		\la (R+S)(v),w\ra = \la v,(R^* + S^*)(w)\ra, \quad \forall v,w \in V.
		$$
		Ahora bien,
		\begin{align*}
		\la v,(R^* + S^*)(w)\ra &= \la v,R^*(w) + S^*(w)\ra =  \la v,R^*(w) \ra+\la v, S^*(w)\ra \\
		&=  \la R(v),w \ra+\la S(v), w\ra =  \la R(v) + S(v), w\ra =  \la (R+S)(v), w\ra .
		\end{align*}
		
		\vskip .3cm
		
		(\ref{adj3}). 
		\begin{align*}
		\la v,(S^*R^*)(w)\ra &= \la v, S^*(R^*(w))\ra =  \la S(v),R^*(w) \ra \\
		&=  \la R(S(v)),w \ra =  \la (RS)(v), w\ra .
		\end{align*}
		Por lo tanto  $(RS)^* = S^*R^*$.
		
		\vskip .3cm
		
		(\ref{adj4}). Por definición de adjunta de $R^*$, tenemos que $ (R^*)^*=R^{**}$  es la única transformación lineal tal que 
		$$
		\la R^*(v), w\ra = \la v, R^{**}(w)\ra  , \; \forall\, v,w \in V.
		$$ 
		Ahora bien, por la definición de adjunta de $R$ sabemos que 
		$$
		\la R^*(v), w\ra = \la v, R(w)\ra , \; \forall\, v,w \in V.
		$$ 
		Luego $R = R^{**}$.
		
	\end{proof}
	
	
	\begin{teorema}[$*$]\label{rel-adj-ort}
		Sea $T: V \to W$ una transformación lineal entre espacios vectoriales de dimensión finita con producto interno. Entonces,
		\begin{enumerate}
			\item $\nu(T^*) = \im(T)^\perp$,
			\item $\im(T^*) = \nu(T)^\perp$,
			\item $\nu(T) = \im(T^*)^\perp$,
			\item $\im(T) = \nu(T^*)^\perp$.
		\end{enumerate}
	\end{teorema}
	\begin{proof}
		Probaremos la primera afirmación, pues las otras se deducen fácilmente del hecho que $T^{**} = T$ y $U^{\perp\perp} = U$. Ahora bien,
		\begin{align*}
		w \in  \nu(T^*) &\Leftrightarrow T^*(w) =0 \Leftrightarrow \la v, T^*(w) \ra = 0,\; \forall\, v\in V \Leftrightarrow \\
		&\Leftrightarrow \la T(v), w \ra = 0,\; \forall\, v\in V\Leftrightarrow  w \in \im(T)^\perp.
		\end{align*}
		
		
	\end{proof}
	
	
		\end{comment}
\end{section}

\begin{section}{Diagonalización de matrices simétricas}
	
	\begin{definicion}
		Sea $V$ un espacio vectorial de dimensión finita con producto interno y $T: V \to V$ una transformación lineal. Diremos que $T$ es una \textit{transformación lineal autoadjunta} si $T^* = T$. En  ese caso, también suele decirse que $T$ es un \textit{operador lineal autoadjunto}.
	\end{definicion}
	
	
	Del  teorema \ref{matrizadjunta} (y un  poco más) se deduce el siguiente resultado.
	
	\begin{proposicion}\label{prop-autoadjunta-impl-sim}
		Sea $V$ un espacio vectorial de dimensión finita con producto interno y $T: V \to V$ una transformación lineal. Entonces $T$ es un operador lineal autoadjunto si y sólo si para cualquier $\mathcal{U}$ BON de $V$, la matriz de $T$ en la base $\mathcal{U}$  es simétrica.
	\end{proposicion}
	\begin{proof}
		($\Rightarrow$) Por teorema \ref{matrizadjunta}, si $A$ es la matriz de $T$, entonces  la  matriz de $T^*$ es $A^\t$. Como $T=T^*$, entonces $A = A^\t$.
		
		\vskip .1cm
		($\Leftarrow$) Por hipótesis, $[T]_\mathcal{U} =[T]_\mathcal{U}^\t $. Pero por el teorema \ref{matrizadjunta}, tenemos que  $[T^*]_\mathcal{U} =[T]_\mathcal{U}^\t$. Por lo tanto $[T^*]_\mathcal{U} = [T]_\mathcal{U}$, lo cual implica que  $T= T^*$.
	\end{proof}
	
	Este proposición nos  permite reducir los espacios de producto interno al  espacio  vectorial $\R^n$ con el producto interno canónico y al los operadores autoadjuntos a matrices simétricas. 
	
	
	\vskip .3cm
	
	
	Observar que en $\R^n$ el producto escalar es 
	\begin{equation*}
		\la (x_1,\ldots,x_n),(y_1,\ldots,y_n)  \ra = \sum_i x_iy_i = \begin{bmatrix} x_1& \cdots &x_n\end{bmatrix}\begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}.
	\end{equation*}
	Es decir si usamos la convención que un vector en $\R^n$  se escribe como una matriz columna (de $n$ filas y una columna),  tenemos que dados $x,y \in \R^n$, 
	\begin{equation*}
		\la x,y \ra = x^\t y.
	\end{equation*}
	Si $A$ es una matriz $n \times n$ simétrica, entonces notar que, sin usar la proposición \ref{prop-autoadjunta-impl-sim}, podemos demostrar directamente que es autoadjunta:
	\begin{equation*}
		\la Ax,y \ra = (Ax)^\t y = x^\t A^\t y  = x^\t A y = \la x,Ay \ra 
	\end{equation*}
	
	
	\begin{proposicion}\label{wperpinv}
		Sea $A$ matriz simétrica $n \times n$ y $W$ un subespacio de $\R^n$ invariante por $A$, entonces $W^\perp$ es invariante por $A$.
	\end{proposicion}
	\begin{proof}
		Sea $u \in W^\perp$,  debemos ver que $Au \in W^\perp$,  es decir que $(Au)^\t w =0$ para todo $w \in W$. Ahora bien,
		\begin{equation*}
			(Au)^\t w = u^\t A^\t w =u^\t A w = u^\t (A w) =0.  
		\end{equation*}
		La útima igualdad es válida pues $u \in W^\perp$ y  como $w \in W$, por hipótesis $Aw \in W$.
	\end{proof}
	
	
	
 	\begin{comment}
 		\begin{proposicion} Sean $A$ y $B$ dos matrices simétricas. Entonces, $AB$  es simétrica si y sólo  si $A$ y $B$ conmutan. 
 		\end{proposicion}
 		\begin{proof}
 		($\Rightarrow$) Como $AB$ es simétrica, tenemos que $AB = (AB)^\t$. Por proposición \ref{prop-matriz-transpuesta} tenemos que  $(AB)^\t = B^\t A^\t$, y  como $A,B$ son simétricas, $B^\t A^\t = BA$. 
 		Reconstruyendo las igualdades tenemos
 		$$
 		AB = (AB)^\t = B^\t A^\t = BA,
 		$$
 		es decir, $A$ y $B$ conmutan.
 		
 		\vskip .3cm
 		
 		($\Leftarrow$)  
 		$$
 		(AB)^\t = B^\t A^\t = BA=AB.
 		$$
 		\end{proof}
 		
 		
 		\begin{ejemplo}
 		Sean 
 		\begin{equation*}
 		A = \begin{bmatrix} 1&0\\0&2 \end{bmatrix} \quad \text{ y } \quad B = \begin{bmatrix}0&1\\1&0\end{bmatrix}.
 		\end{equation*}
 		Tanto $A$ como $B$ son matrices simétricas. Sin embargo, 
 		\begin{align*}
 		AB = \begin{bmatrix}0&1\\2&0\end{bmatrix} 
 		\end{align*}
 		no es simétrica. Esto ocurre, pues $A$ y $B$ no conmutan. 
 		\end{ejemplo}
 		
 	\end{comment}
	
	
	
	\vskip 0.5 cm
	Veremos ahora que una matriz simétrica es diagonalizable, es decir que hay una base de autovectores del operador asociado a la matriz o, equivalentemente, existe matriz $P$ invertible tal que $P^{-1}AP$ es diagonal. 
	
	\vskip .3cm
	
	Usaremos el siguiente resultado sin demostración.
	
	\begin{teorema}[Teorema fundamental del álgebra]
		Todo polinomio no constante con coeficientes complejos tiene al menos una raíz compleja. Es decir si 
		\begin{equation*}
			\text{$p(x) = a_nx^n+ a_{n-1}x^{n-1} +\cdots+a_0$, con $a_i \in \mathbb{C}$,  $a_n\ne 0$ y $n\ge 1$,}
		\end{equation*}
		entonces existe $\alpha \in \mathbb{C}$ tal que $p(\alpha)=0$.
	\end{teorema}

	Pese a llamarse ``Teorema fundamental del álgebra'', este resultado  no suele demostrarse en los cursos de álgebra, pues su demostración requiere del uso de análisis matemático.
	
	Si $\alpha$ es raíz de $p$, un polinomio de grado $n$, por  el teorema del resto,  $p(x) = (x-\alpha)p_1(x)$, con  $p_1$ un polinomio de grado $n-1$.  Aplicando inductivamente este procedimiento, podemos deducir:
	
	\begin{corolario} Si $p$ es un polinomio de de grado $n\ge 1$ con coeficientes en $\C$,  entonces
		\begin{equation*}
			p(x)= c(x-\alpha_1)(x-\alpha_2)\ldots(x-\alpha_n),
		\end{equation*}
		con $c,\alpha_i \in \C$.
	\end{corolario}
	
	
	
	
	\begin{obs}\label{5.9} 	Recordemos que si $a+bi \in \mathbb{C}$, $a$ es la \textit{parte real} y $b$ es la \textit{parte imaginaria}. El conjugado $a+b_i$ es $\overline{a +bi} = a-bi$. La conjugación cumple que $\overline{\overline{z}} = z$, $\overline{z +w} = \overline{z}+\overline{w}$ y $\overline{z w} = \overline{z}\,\overline{w}$ ($z,w \in \mathbb{C}$). Recordemos también que $z\overline{z} = |z|^2$.
		
		Si $x \in \mathbb{C}^n$,  entonces cada coordenada de $x$ es un número complejo,  es decir $x_i = a_i + ib_i$, con $a_i,b_i \in \R$. Luego si $v = (a_1,\ldots,a_n)$ y $w = (b_1,\ldots,b_n)$, tenemos que  $x = v + wi$  con  $v,w \in \R^n$.  En  este caso, diremos que $v$  es la parte real de $x$ y $w$ la parte imaginaria. También podemos extender la conjugación a $\mathbb{C}^n$ y  $\mathbb{C}^{n \times m}$ coordenada a coordenada y entonces no es difícil verificar que si $A, B \in \mathbb{C}^{n \times m}$
		\begin{equation*}\overline{\overline{A}} = A,\qquad \overline{A+B} = \overline{A}+\overline{B},
		\end{equation*}
		y que si $A\in \mathbb{C}^{n \times m}$, $ B \in \mathbb{C}^{m \times k}$, $\alpha \in \mathbb{C}$, entonces $\overline{\alpha A} =\overline{\alpha}\, \overline{A}$ y además
		\begin{equation*}\overline{\alpha A B} =\overline{\alpha}\, \overline{A}\,\overline{B} = \overline{A}\,\overline{\alpha B}.
		\end{equation*} 
		Notar también que si  $z = (z_1,\ldots,z_n)$, 
		\begin{equation*}z^\t \, \overline{z} = [ z_1 \, \ldots \, z_n ]
		{\begin{bmatrix} \overline{z_1} \\\vdots \\\overline{z_n} \end{bmatrix}} = |z_1|^2+\cdots+|z_n|^2,
		\end{equation*}   
		que es $>0$ si el vector no es nulo. Denotaremos la expresión de arriba como $||z||^2$.
		
	\end{obs}
	
	\begin{teorema}\label{existeauto}
		Sea $A$ matriz simétrica $n \times n$, con coeficientes reales. Entonces existe $\lambda \in \R$ autovalor de $A$.   
	\end{teorema}
	\begin{proof} Extendamos $A$  a una transformación lineal de $A: \mathbb{C}^n \to \mathbb{C}^n$ de manera natural (con el producto de matrices). Sea $\chi_A$ el polinomio característico de $A$. Por el teorema fundamental  del álgebra, existe $\lambda \in \mathbb{C}$ tal que $\chi_A(\lambda)=0$. Luego existe $x \in \mathbb{C}^n$, no nulo,  tal que $Ax = \lambda x$.  Luego
		\begin{equation*}
			\lambda ||x||^2 = \lambda x^\t\overline{x} = (\lambda x)^\t\overline{x} =
			(A x)^\t\overline{x} = x^\t A\overline{x} = x^\t\overline{Ax} =x^\t\overline{\lambda x} = \overline{\lambda} x^\t\overline{x} = \overline{\lambda} ||x||^2 .
		\end{equation*}
		Por lo tanto, $\lambda = \overline{\lambda}$, lo cual nos dice que ${\lambda} \in \R$.  Es decir, tenemos un vector $x \in \C^n$ y $\lambda \in \R$, tal que $Ax=\lambda x$. Si  $x = v +iw$ con $v,w \in \R^n$, entonces 
		\begin{equation*}
		\lambda v + i \lambda w = \lambda x = Ax = Av + i Aw.
		\end{equation*}
		Como $A$ es una matriz real $Av,Aw \in \R^n$ y como $\lambda \in \R$, tenemos que $Av = \lambda v$ y $Aw = \lambda w$, luego hay al menos un autovector en $\R^n$ con autovalor $\lambda \in \R$. 
	\end{proof}
	
	\vskip .3cm
	
	
		
	\begin{teorema}[Teorema espectral] Sea $A$ matriz simétrica $n \times n$. Entonces existe $\mathcal{U} = \{u_1,\ldots,u_n\}$ una BON de $\R^n$ de autovectores de $A$.
	\end{teorema}
	\begin{proof}		
		Se hará por inducción en $n= \dim(V)$.
		
		Si $n=1$ es trivial. 
		
		Supongamos que vale  para $n-1$ con $n >1$, y  probaremos el resultado para $n$. Por le teorema  \ref{existeauto} existe $\lambda \in \R$ y $x \in \R^n$ tal que $Ax = \lambda x$. Si $u_n = x/||x||$, $u_n$ tiene norma 1 y  cumple también que $Au_n = \lambda u_n$. Sea $W = span\{u_n\}$. Entonces por  corolario \ref{wperpinv}, $W^\perp$ es invariante por $A$. Podemos considerar entonces a  $A$ como una transformación lineal de $W^\perp$ a $W^\perp$. Como $\dim(W^\perp) = n-1$, por hipótesis inductiva existe   $\{u_1,\ldots,u_{n-1}\}$ una BON de $W^\perp$ de autovectores de $A: W^\perp \to W^\perp$. Es claro entonces que $\mathcal{U} = \{u_1,\ldots,u_n\}$  una BON de $\R^n$ de autovectores de $A$. 
	\end{proof}

	\begin{corolario}
		 Sea $A$ matriz simétrica $n \times n$, entonces $A$ es diagonalizable.
	\end{corolario}


	\begin{ejemplo} Encontremos autovalores y autovectores de  la matriz
		\begin{equation*}
			A = \begin{bmatrix} 2&-1&0\\-1&2&-1\\0&-1&2 \end{bmatrix}.
		\end{equation*}
		Como es una matriz simétrica sabemos que es diagonalizable, es decir tiene una base de autovectores. 
		El polinomio característicos es
		\begin{equation*}
			\chi_A(x) = \det  \begin{bmatrix} 2-x&-1&0\\-1&2-x&-1\\0&-1&2-x \end{bmatrix} =	-x^3 + 6 x^2 - 10 x + 4.
		\end{equation*}
		Ahora bien, las raices de $-x^3 + 6 x^2 - 10 x + 4 $  son 
		\begin{align*}
			\lambda_1 &= 2 + \sqrt 2 \\
			\lambda_2 &= 2 \\
			\lambda_3 &= 2 - \sqrt 2
		\end{align*}
		Para averiguar los autovectores debemos plantear las ecuaciones $Ax = \lambda_ix$,  que resultan en los siguiente sistema de ecuaciones
		\begin{align*}
		\begin{split}
		2x_1 - x_2  &= \lambda_i x_1 \\
		-x_1 + 2x_2 -x_3 &=\lambda_ix_2 \\
		-x_2 + 2x_3 &= \lambda_ix_3,  
		\end{split}
		\end{align*}
		($i=1,2,3$),  o equivalentemente,
		\begin{align*}
		\begin{split}
		(2 -\lambda_i)x_1+ x_2  &= 0 \\
		-x_1 +(2-\lambda_i)x_2  -x_3 &=0 \\
		-x_2 + (2-\lambda_i)x_3 &= 0,  
		\end{split}
		\end{align*}
		($i=1,2,3$).
		En el caso de $\lambda_1= 2 + \sqrt 2$,  resulta
		\begin{align*}
		\begin{split}
		-\sqrt 2x_1+ x_2  &= 0 \\
		-x_1 -\sqrt 2x_2  -x_3 &=0 \\
		-x_2 -\sqrt 2x_3 &= 0,  
		\end{split}
		\end{align*}
		cuya solución es $\lambda(1, -\sqrt2, 1)$. Si continuamos resolviendo los sistemas de ecuaciones, podemos encontrar la siguiente base de autovectores:
		\begin{align*}
		v_1 &= (1, -\sqrt2, 1) \\		
		v_2 &= (-1, 0, 1) \\		
		v_3 &= (1, \sqrt2, 1).
		\end{align*}
		
	\end{ejemplo}
	
	\end{section}

\end{chapter}



\appendix
\setcounter{chapter}{0}
\renewcommand{\thechapter}{\Alph{chapter}}

\begin{chapter}{Números complejos}\label{chap-num-compl}
	
	\begin{section}{Cuerpos}
		
		En el cuatrimestre pasado se ha visto el concepto de cuerpo, del cual haremos un repaso (ver también  \href{https://es.wikipedia.org/wiki/Cuerpo\_(matemáticas)}{https://es.wikipedia.org/wiki/Cuerpo\_(matemáticas)}).
		
		
		\begin{definicion}
			Un  conjunto $\K$ es un \textit{cuerpo}\index{cuerpo} si es un anillo de división conmutativo, es decir, un anillo conmutativo con unidad en el que todo elemento distinto de cero es invertible respecto del producto. Por tanto,  un cuerpo es un conjunto $\K$ en el que se han definido dos operaciones, '$+$' y '$\cdot$', llamadas \textit{adición} y \textit{multiplicación} respectivamente, que cumplen las siguientes propiedades. En la siguiente lista de axiomas $a$, $b$, $c$ denotan elementos arbitrarios de $\K$, y $0$ y $1$ denotan elementos especiales de $\K$ que cumplen las propiedades especificadas más abajo.
			\begin{enumerate}
				\item[{\bf I1.}] $a+b$ y $a\cdot b$ pertenecen a ${\K}$.
				\item[{\bf I2.}] {\em Conmutatividad.}\, $a+b = b+a$; $ab=ba$. 
				\item[{\bf I3.}] {\em Asociatividad.}\, $(a+b)+c = a+(b+c)$;\; $(a\cdot b)\cdot c = a\cdot (b\cdot c)$. 
				\item[{\bf I4.}] {\em Existencia de elemento neutro.}\, Existen números $0$, $1 \in \K$ con $0\not=1$ tal que $a+0=a$; $a\cdot 1=a$. 
				\item[{\bf I5.}] {\em Distributividad.}\, $a\cdot (b+c)=a\cdot b+a\cdot c$. 
				\item[{\bf I6.}] {\em Existencia del inverso aditivo.}\, Por cada $a$ en ${\K}$ existe un único  $-a$ en ${\K}$ tal que $a+(-a)=0$. 
				\item[{\bf I7.}] {\em Existencia de inverso multiplicativo.}\, Si $a$ es distinto de 0, existe un único elemento $a^{-1} \in \K$  tal que $a\cdot a^{-1}=1$. 
			\end{enumerate}
		\end{definicion}
		
		Muchas veces denotaremos el producto yuxtaponiendo los elementos,  es decir $ab := a\cdot b$, para $a,b \in \K$. Debido a la ley de asociatividad para la suma (axioma {\bf I3}) $(a+b)+c$ es igual a $a+(b+c)$ y por lo tanto podemos eliminar los paréntesis sin ambigüedad. Es decir, denotamos
		$$
		a+b+c := (a+b)+c = a+(b+c).
		$$
		De forma análoga, usaremos la notación
		$$
		abc = (ab)c = a(bc).
		$$
		Debido a la ley de conmutatividad (axioma {\bf I2}), es claro que del axioma {\bf I4} se deduce que  $0+a=a+0=a$ y $1a = a1=a$. Análogamante,  por  {\bf I2} e  {\bf I6} obtenemos que  $-a+a =   
		a+(-a)=0$, y por {\bf I6} que  $a a^{-1} = a^{-1}a=1$.
		
		
		
		Todos los axiomas corresponden a propiedades familiares de los cuerpos que ya conocemos,  como ser el cuerpo de los números reales, denotado $\R$ y el cuerpo de los números racionales (fracciones),  denotado $\Q$. De ellas pueden deducirse la mayoría de las reglas comunes a los cuerpos. Por ejemplo, podemos {\it definir} la operación de sustracción diciendo que $a-b$ es lo mismo que $a+(-b)$; y deducir las reglas elementales por ejemplo,
		\begin{equation*}
		a-(-b) = a+b, \qquad -(-a) = a.
		\end{equation*}	
		También podemos deducir
		\begin{equation*}
		(ab)^{-1} = a^{-1}b^{-1}
		\end{equation*}
		con tal que $a$ y $b$ sean diferentes de cero. Otras reglas útiles incluyen	
		\begin{equation*}
		-a = (-1)a 
		\end{equation*}					
		y más generalmente
		\begin{equation*}
		- (ab) = (-a) b = a  (-b),
		\end{equation*}	
		y  también 
		\begin{equation*}
		ab = (-a) (-b),
		\end{equation*}
		así como
		\begin{equation*}
		a\cdot 0 = 0,
		\end{equation*}
		todas reglas familiares de la aritmética elemental.
	\end{section}
	
	
	\begin{subsection}{Un cuerpo finito}
		A modo de ejemplo, y para entrenar la intuición de que un cuerpo no necesariamente tiene un número infinito de elementos, consideremos el conjunto con dos elementos $\F_2=\{0,1\}$. Definimos la suma $+\colon\F_2\times \F_2\to \F_2$ mediante la regla
		\begin{align*}
		0+0&=0, & 0+1&=1, & 1+0&=1, & 1+1&=0
		\end{align*}
		y el producto $\cdot \colon\F_2\times \F_2\to \F_2$ como 
		\begin{align*}
		0\cdot 0&=0, & 0\cdot 1&=0, & 1\cdot 0&=0, & 1\cdot 1&=1.
		\end{align*}
		Dejamos como ejercicio para el lector comprobar que estas operaciones así definidas satisfacen los axiomas {\bf I1.} a {\bf I7.} y por lo tanto $\F_2$ es un cuerpo, con dos elementos.
		
		\begin{obs}
			El lector suspicaz reconocerá en estas operaciones a la suma y el producto definidos en el conjunto $\Z_2=\{\overline{0},\overline{1}\}$ de congruencias módulo 2  definido en Álgebra 1/Matemática Discreta I. En efecto, resultados desarrollados en ese curso permiten demostrar que los conjuntos $\Z_p$, con $p$ primo, son ejemplos de cuerpos, en este caso con $p$ elementos.
		\end{obs}
		
	\end{subsection}
	
	
	\begin{section}{Números complejos}
		La ecuacion polinómica $x^2 + 1 =0$ (¿cuál es el número que elevado  al cuadrado y adicionado 1 da 0?) no tiene solución dentro del cuerpo de los números reales,  pues todos sabemos que  $x^2 \ge 0$ para todo $x \in \R$ y por lo tanto $x^2 + 1 >0$ $\forall\; x \in \R$. Podemos extender $\R$ a otro cuerpo,  de tal forma que \textit{toda} ecuación polinómica con coeficentes en $\R$ tenga solución. 
		
		\begin{definicion} 
			Los \textit{números complejos}\index{números complejos} es el conjunto $\C$  de los pares ordendados $(a,b)$,  denotados $a+ib$, con $a, b$  en $\R$, con las operaciones '$+$' y '$\cdot$', definidas
			\begin{align}
			(a+ib)+ (c+id) &:= (a+c) + i(c+d), \label{sumacompleja} \\
			(a+ib) \cdot (c+id) &:= (ac -bd) + i(ad+bc). \label{productocomplejo}
			\end{align}
			Al número complejo $i = 0 + i\cdot 1$ lo llamamos el \textit{imaginario puro}.  Si $z= a + ib$  es un número complejo,  diremos que $a$ es la \textit{parte real} de $z$ y  la denotamos $a = \operatorname{Re} z$. Por otro lado,  $b$ es la \textit{parte  imaginaria} de $z$ que es denotada $b = \operatorname{Im} z$.
		\end{definicion}
		
		Es claro  que $z=a+ib$ es igual a $w = c+id$ si coinciden su parte real e imaginaria, es decir
		\begin{equation*}
		a+ bi = c+ di\quad \Leftrightarrow\quad a=c \;\wedge\; b = d.
		\end{equation*}
		
		Podemos  ver a $\R$ contenido en $\C$,  con la correspondencia $a \to a + i \cdot 0$ y  observamos que si  nos restringimos a $\R$, tenemos las reglas de adición y  multiplicación usuales.  
		
		La definición de la suma de dos números complejos no debería sorprendernos, pues es la suma ``coordenada a coordenada''. La definición del producto se basa en que deseamos que $i^2 = -1$ y que el producto sea distributivo. 	
		
		Primero, comprobemos que  $i^2 = -1$. Esto es debido a que
		\begin{equation*}
		i^2 = (0 + i\cdot 1)(0 + i\cdot 1) = (0\cdot 0 - 1 \cdot 1) + i(0\cdot 1 + 1 \cdot 0) = -1,
		\end{equation*} 
		y por lo tanto $i^2 + 1 = -1+1 = 0$,  es decir $i$  es solución de la ecuación polinómica $x^2 + 1 =0$.  
		
		
		Sean $0 = 0 + i\cdot 0, 1 = 1 + i\cdot 0 \in \C$,  es fácil comprobar que son los elementos neutros de la suma y el producto,  respectivamente. Por otro lado, si $z = a + ib$,  entonces $-z = -a -ib$ es el opuesto aditivo de $z$. 
		El inverso multiplicativo es un poco más complicado. Primero observemos que dado $a+ib \in \C$,
		\begin{equation*}
		(a+ ib)(a-ib) = aa -b(-b) = a^2 + b^2 \in \R. 
		\end{equation*}
		Supongamos que $a+ib\ne0$,  encontremos  a partir  de las reglas de adición y multiplicación la inversa de $z$. Sea $c+id$ tal que $(a+ib)(c+id)=1$, luego
		\begin{equation*}
		c + id = \frac{1}{a+ib} = \frac{1}{a+ib}\,\frac{a-ib}{a-ib} = \frac{a-ib}{(a+ib)(a-ib)} = 
		\frac{a-ib}{ a^2 + b^2} = \frac{a}{ a^2 + b^2} - i\frac{b}{ a^2 + b^2}
		\end{equation*}  
		(observar que como $a+ib\ne0$,  entonces $a^2 + b^2 >0$.)
		
		Usando lo anterior,  y un poco más de trabajo, obtenemos
		
		\begin{proposicion}
			Sean $0 = 0 + i\cdot 0, 1 = 1 + i\cdot 0\in \C$. Entonces, $\C$ con las operaciones '$+$' y '$\cdot$', definidas en (\ref{sumacompleja}) y (\ref{productocomplejo}),  respectivamente, es un cuerpo con elementos neutros $0$ y $1$, y
			\begin{align*}
			-(a+ib) &= -a -ib \\
			(a+ib)^{-1} &= 	\frac{a-ib}{ a^2 + b^2}, \qquad \text{para $a+ib \ne 0$}.
			\end{align*}
		\end{proposicion}
		\begin{proof}
			Ejercicio.
		\end{proof}
		
		\vskip .3cm
		
		Hemos definido los números complejos como pares ordenados y como tales es posible representarlos en el plano $\R \times \R$:
		
		
		
		\begin{figure}[h]
			\begin{tikzpicture}
			\draw[->] (-4.0,0) -- (4.0,0) node[right] {}; % eje x
			\draw[->] (0,-3) -- (0,3) node[above] {}; % eje y
			\draw[fill] (2.5,1.5) circle [radius=0.05];
			\node [right] at (2.5,1.5) {$a+ ib$};
			\node [below] at (2.5,-3pt) {$a$};
			\node [left] at (-3pt,1.5) {$b$};
			\draw (2.5,-3pt) -- (2.5,3pt);
			\draw (-3pt, 1.5) -- (3pt, 1.5);
			\draw [dashed] (0,1.5) -- (2.5,1.5);
			\draw [dashed] (2.5,0) -- (2.5,1.5);
			\end{tikzpicture}
			
			
			\caption{Representación gráfica de los números complejos. }
		\end{figure}
		
		
		\begin{figure}[h]
			\begin{tikzpicture}
			\draw[->] (-4.0,0) -- (4.0,0) node[right] {}; % eje x
			\draw[->] (0,-3) -- (0,3) node[above] {}; % eje y
			\foreach \x in {-4,...,-1}
			\draw (\x,3pt) -- (\x,-3pt)
			node[anchor=north] {\x};
			\foreach \x in {1,...,4}
			\draw (\x,3pt) -- (\x,-3pt)
			node[anchor=north] {\x};
			\foreach \y in {-3,...,-1}
			\draw (3pt,\y) -- (-3pt,\y) 
			node[anchor=east] {\y}; 
			\foreach \y in {1,...,3}
			\draw (3pt,\y) -- (-3pt,\y) 
			node[anchor=east] {\y}; 
			\draw[fill] (2,1) circle [radius=0.05];
			\node [right] at (2,1) {$2+ i$};
			\draw [dashed] (0,1) -- (2,1);
			\draw [dashed] (2,0) -- (2,1);
			\draw[fill] (-1,2.5) circle [radius=0.05];
			\node [left] at (-1,2.5) {$-1+i \,2.5$};
			\draw [dashed] (0,2.5) -- (-1,2.5);
			\draw [dashed] (-1,0) -- (-1,2.5);
			\draw[fill] (-2.5,-2.5) circle [radius=0.05];
			\node [below] at (-2.5,-2.5) {$-2.5-i\,2.5$};
			\draw [dashed] (0,-2.5) -- (-2.5,-2.5);
			\draw [dashed] (-2.5,0) -- (-2.5,-2.5);
			\end{tikzpicture}
			
			
			\caption{Ejemplos de la representación gráfica de los números complejos. }
		\end{figure}
		
		\vskip .3cm
		
		Por el teorema de Pitágoras, la distancia del  número complejo $a+ib$ al $0$ es $\sqrt{a^2+b^2}$.
		
		\begin{definicion} Sea $z = a + ib \in \C$. El \textit{módulo} de $z$ es
			\begin{equation*}
			|z| = \sqrt{a^2+b^2}.
			\end{equation*}
			El  \textit{conjugado} de $z$ es
			\begin{equation*}
			\bar z = a-ib.
			\end{equation*}
		\end{definicion} 
		
		\vskip .5cm 
		
		
		\begin{ejemplo}
			$|4+3i| = \sqrt{4^2+3^2} = \sqrt{25} =5$, $\overline{4+3i} = 4-3i$.
		\end{ejemplo}
		
		\vskip .5cm 
		
		\begin{proposicion} Sean $z$ y $w$ números complejos.
			\begin{enumerate}
				\item $z\bar{z} = |z|^2$.
				\item Si $z \ne 0$, $z^{-1} = \displaystyle\frac{\overline{z}}{|z|^2}$.
				\item  $\overline{z+w} = \overline{z} + \overline{w}$.
				\item  $\overline{zw} = \overline{z}\;  \overline{w}$.
			\end{enumerate}
			\begin{proof}
				Son comprobaciones rutinarias. Para ejemplificar, hagamos la demostración  de (4).
				
				Si $z = a + bi$ y $w = c +di$, entonces $(a+bi) (c+di) = (ac -bd) + (ad+bc)i$. Por lo tanto,
				\begin{equation*}
				\overline{zw} = (ac -bd) - (ad+bc)i.
				\end{equation*} 
				Como $\overline{z} = a - bi$ y $\overline{w} = c -di$,
				\begin{equation*}
				\overline{z}\;  \overline{w} = (ac -(-b)(-d)) + (a(-d)+b(-c) )i = (ac -bd) - (ad+bc)i.
				\end{equation*} 
				Por lo tanto $	\overline{zw} = \overline{z}\;  \overline{w}$.
			\end{proof}	
		\end{proposicion}
		
		
		\begin{ejercicio}
			Determinar el número complejo $2- 3i + \displaystyle\frac{i}{1-i}$.
		\end{ejercicio}
		\begin{proof}[Solución] 
			El  ejercicio nos pide que escribamos el número en el formato $a + bi$. En general, para eliminar un cociente donde el divisor tiene parte imaginaria no nula, multiplicamos arriba y abajo por el conjugado del divisor, como $z\overline{z} \in \R$, obtenemos un divisor real. En  el ejemplo: 
			\begin{align*}
			2+ 3i + \frac{i}{1-i} &= 2+3i +\frac{i}{1-i}\times\frac{1+i}{1+i}  = 2+3i +\frac{i(1+i)}{(1-i)(1+i)}\\ 
			&=  2+3i +\frac{i-1}{2} = 2+3i +\frac{i}{2}-\frac{1}{2} =\frac{3}{2}+i\frac{7}{2}
			\end{align*}
		\end{proof}
		
		\vskip .3cm
		\textbf{Un poco de trigonometría}. Recordemos que dado un punto $p=(x,y)$ en el plano, la recta que une el origen con $p$ determina un ángulo $\theta$ con el eje $x$ y entonces 
		\begin{equation*}
		x = r\sin(\theta) , \qquad y =  r\cos(\theta)  
		\end{equation*}  
		donde $r$ es la longitud del segmento determinado por $(0,0)$ y $(x,y)$. En  el  lenguaje  de los números complejos, si $z = a +bi$ y $\theta$  el ángulo determinado por $z$ y  el eje horizontal, entonces 
		\begin{equation*}
		a = |z|\sin(\theta) , \qquad b =   |z|\cos(\theta), 
		\end{equation*}  
		es decir
		\begin{equation}\label{forma-polar}
		z = |z|(\cos(\theta)+i\sin(\theta)).
		\end{equation}
		Si $z \in \C$, la fórmula (\ref{forma-polar}) e llamada la \textit{forma polar}\index{forma polar} de $z$ y $\theta$ es llamado  el  \textit{argumento de $z$}. 
		
		\vskip .3cm 
		
		
		\textbf{Notación exponencial.} Otra notación para representar a los números complejos es la \textit{notación exponencial}, \index{notación exponencial}en la cual se denota
		\begin{equation*}
		e^{i\theta} := \cos(\theta) + i\sin(\theta).
		\end{equation*}
		Por lo tanto si $z \in \C$ y $\theta$  es el argumento de  $z$,
		\begin{equation*}
		z = r e^{i\theta} 
		\end{equation*} 
		donde  $r = |z|$. No  perder de vista,  que la notación exponencial no es más que una notación (por ahora). 
		
		\begin{proposicion}
			Sean $z_1 = r_1 e^{i\theta_1}$, $z_2 = r_2 e^{i\theta_2}$,  entonces
			$$
			z_1 z_2 =  r_1r_2 \,e^{i(\theta_1+ \theta_2)}.
			$$
		\end{proposicion}
		\begin{proof}
			$z_1 = r_1(\cos(\theta_1)+i\sin(\theta_1))$, $z_2 = r_2(\cos(\theta_2)+i\sin(\theta_2))$, luego
			\begin{align*}
			z_1z_2 &= r_1r_2(\cos(\theta_1)+i\sin(\theta_1))(\cos(\theta_2)+i\sin(\theta_2)) \\
			&= r_1r_2(\cos(\theta_1)\cos(\theta_2)+i\cos(\theta_1)\sin(\theta_2)+i\sin(\theta_1)\cos(\theta_2)+i^2\sin(\theta_1)\sin(\theta_2)) \\
			&= r_1r_2((\cos(\theta_1)\cos(\theta_2)-\sin(\theta_1)\sin(\theta_2))+i(\sin(\theta_1)\cos(\theta_2) +\cos(\theta_1)\sin(\theta_2))) \\
			&\overset{(*)}= r_1r_2(\cos(\theta_1+\theta_2) + i\sin(\theta_1+\theta_2)) =  r_1r_2e^{i(\theta_1+ \theta_2)}.
			\end{align*}
			La igualdad ($*$) se debe a las tradicionales fórmulas trigonométrica del coseno y  seno de la suma de ángulos.
		\end{proof}
		
		
		
		
	\end{section}	
	
	
\end{chapter}


	\begin{chapter}{Determinante}\label{apend.Determinante}
		En  el apéndice se harán las demostraciones de los resultados correspondientes a la sección de determinantes (sección  \ref{seccion-determinate}).
		
		\begin{section}{Determinantes}
			
		Lo primero que veremos será la demostración del  teorema \ref{det-prop-fundamentales}. Los tres resultados de ese teorema los demostraremos en forma separada: serán los teoremas \ref{det-prop-mult-esc}, \ref{det-prop-perm-filas} y \ref{det-prop-fila-mas-cfila}.
		
		
		\begin{teorema} \label{det-prop-mult-esc}
			Sea $A  \in M_n(\K)$ y sea $c \in \K$ y $B$ la matriz que se obtiene de $A$ multiplicando la fila $r$ por $c$, es decir $A  \stackrel{cF_r}{\longrightarrow} B$, entonces $\det B = c \det A$.
		\end{teorema}
		\begin{proof}
			Si multiplicamos la fila $r$ por $c$ obtenemos
			\begin{equation*}
			B = \begin{bmatrix}
			a_{11}&a_{12}&\cdots&a_{1n}\\
			\vdots&&\ddots&\vdots \\
			ca_{r1}&ca_{r2}&\cdots&ca_{rn} \\
			\vdots&&\ddots&\vdots \\
			a_{n1}&&\cdots&a_{nn}
			\end{bmatrix}.
			\end{equation*}
			Observemos que al hacer el desarrollo por la primera columna obtenemos
			\begin{equation*}
			|B| = \sum_{i=1}^{r-1}  a_{i1}C^B_{1i} + ca_{r1}C^B_{r1} + \sum_{i=r+1}^{n}  a_{i1}C^B_{1i}.
			\end{equation*}
			Ahora bien, si $i \ne r$, la matriz $B(i|1)$ es la matriz   $A(i|1)$ con una fila multiplicada por $c$, luego $|B(i|1)| = c|A(i|1)|$ y, en consecuencia $C^B_{i1} = c\,C^A_{i1}$. Además, $B(r|1) = A(r|1)$, luego  $C^B_{r1} = C^A_{r1}$. Por lo tanto, reemplazando en la ecuación anterior $C^B_{i1}$ por $c\,C^A_{i1}$ si $i\ne r$ y $C^B_{r1}$ por $C^A_{r1}$, obtenemos
			\begin{equation*}
			|B| = \sum_{i=1}^{r-1}  a_{i1}c\,C^A_{1i} + ca_{r1}C^A_{r1} + \sum_{i=r+1}^{n}  a_{i1}cC^A_{1i} = c|A|.
			\end{equation*}
		\end{proof}
		

		
		
		\begin{teorema} \label{det-prop-perm-filas}
			Sea $A  \in M_n(\K)$ y sean $1 \le r,s \le n$.
			Sea $B$ la matriz que se obtiene de $A$ permutando la fila $r$ con la fila $s$, es decir  $A  \stackrel{F_r \leftrightarrow F_s}{\longrightarrow} B$, entonces $\det B = -\det A$.
		\end{teorema}
		\begin{proof}
			Primero probaremos el teorema bajo el supuesto de que la fila $1$ es permutada con la fila $k$, para $k > 1$. Esto será suficiente para probar el teorema, puesto que intercambiar las filas $k$ y $k_ 0$ es equivalente a realizar tres permutaciones de filas: primero intercambiamos las filas $1$ y $k$, luego las filas $1$ y $k_{0}$, y finalmente intercambiando las filas $1$ y $k$. Cada permutación cambia el signo del determinante y al ser tres permutaciones,  el intercambio de la fila $k$ con la fila $k_0$ cambia el signo.
			
			
			La prueba es por inducción en $n$. El caso base $n = 1$ es completamente trivial.
			(O, si lo prefiere, puede tomar $n = 2$ como el caso base, y el teorema es
			fácilmente probado usando la fórmula para el determinante de una matriz $2 \times 2$).
			Las definiciones de los determinantes de $A$ y $B$ son:
			\begin{equation*}
			\det(A) = \sum_{i=1}^{n}  a_{i1}C^A_{i1} \quad \text{ y } \quad \det(B) = \sum_{i=1}^{n}  b_{i1}C^B_{i1}.
			\end{equation*}
			
			
			Supongamos primero que $i \ne 1, k$. En este caso, está claro que $A(i|1)$ y $B(i|1)$ son iguales, excepto que dos filas se intercambian. Por lo tanto, por hipótesis inductiva $C^A_{i1} = -C^B_{i1}$. Ya que también $a_{i1} = b_{i1}$, tenemos entonces que
			\begin{equation}\label{det-perm-01}
			a_{i1}C^A_{i1} = - b_{i1}C^B_{i1}, \quad \text{ para } i\ne 1,k.
			\end{equation}
			
			Queda por considerar los términos $i = 1$ y $i = k$. Nosotros afirmamos que
			\begin{equation}\label{det-perm-02}
			-	a_{k1}C^A_{k1} =  b_{11}C^B_{11}  \quad \text{ y } \quad - a_{11}C^A_{11} = b_{k1}C^B_{k1}. 
			\end{equation}
			Si probamos esto, entonces 
			\begin{align*}
			\det(A) &= \sum_{i=1}^{n}  a_{i1}C^A_{i1} \\
			&= a_{11}C^A_{11} +  \sum_{i=2}^{k-1}  a_{i1}C^A_{i1} + a_{k1}C^A_{k1} + \sum_{i=k+1}^{n}  a_{i1}C^A_{i1}\qquad \text{por (\ref{det-perm-01}) y (\ref{det-perm-02})} \\
			&= -b_{k1}C^B_{k1} - \sum_{i=2}^{k-1}  b_{i1}C^B_{i1} - b_{11}C^B_{11} - \sum_{i=k+1}^{n}  b_{i1}C^B_{i1}  \\
			&= - \sum_{i=1}^{n}  b_{i1}C^B_{i1} =- \det(B).
			\end{align*}
			Luego el teorema está probado. Por lo tanto debemos probar (\ref{det-perm-02}). Por simetría, basta probar la primera identidad de (\ref{det-perm-02}),  es decir  que $	a_{k1}C^A_{k1} = - b_{11}C^B_{11}$. 
			
			Para esto, primero debemos observar que $a_{k1} = b_{11}$, por lo tanto sólo hace falta probar que $-C^A_{k1} = C^B_{11}$. En segundo lugar, debemos tener  en cuenta que $B(1|1)$ se obtiene de $A(k|1)$ reordenando las filas $1,2,\ldots, k -1$  de $A(k|1)$ en el orden $2,3, \ldots, k-1,1$. Este reordenamiento puede hacerse permutando la fila $1$ con la fila $2$, luego permutando esa fila con la fila $3$, etc., terminando con una permutación con la fila $k-1$. Esto es un total de $k - 2$  permutaciones de fila. Asi que, por hipótesis inductiva,
			\begin{equation*}
			\det(B(1|1)) = (-1)^{k-2}\det(A(k|1)) = (-1)^{k}\det(A(k|1)) = - (-1)^{k+1}\det(A(k|1)), 
			\end{equation*}
			es decir $C^B_{11} = -C^A_{k1}$. Esto completa la demostración del teorema.
		\end{proof}
		
	\begin{observacion}
		Recordar que del resultado anterior se deduce fácilmente que si una matriz tiene dos filas iguales entonces su determinante es 0. Esto se debía a que, intercambiando las dos filas iguales obtenemos la misma matriz, pero calculando el determinante con el teorema anterior vemos que cambia de signo y el único número en $\K$ que es igual a su opuesto es el 0. 
	\end{observacion}
		
		\begin{lema} Sean $A,B,C$ matrices $n \times n$ tal que
			\begin{equation*}
			A= \begin{bmatrix}
			a_{11}&a_{12}&\cdots&a_{1n}\\
			\vdots&\vdots&&\vdots \\
			a_{r1}&a_{r2}&\cdots&a_{rn} \\
			\vdots&\vdots&&\vdots \\
			a_{n1}&a_{n2}&\cdots&a_{nn}
			\end{bmatrix}, 
			\quad B=  \begin{bmatrix}
			a_{11}&a_{12}&\cdots&a_{1n}\\
			\vdots&\vdots&&\vdots \\
			b_{r1}&b_{r2}&\cdots&b_{rn} \\
			\vdots&\vdots&&\vdots \\
			a_{n1}&a_{n2}&\cdots&a_{nn}
			\end{bmatrix}
			\end{equation*}	
			y
			\begin{equation*}
			C= \begin{bmatrix}
			a_{11}&a_{12}&\cdots&a_{1n}\\
			\vdots&\vdots&&\vdots \\
			a_{r1}+b_{r1}&a_{r2}+b_{r2}&\cdots&a_{rn}+b_{rn} \\
			\vdots&\vdots&&\vdots \\
			a_{n1}&a_{n2}&\cdots&a_{nn}
			\end{bmatrix}.
			\end{equation*}
			Es decir $B$ es igual a $A$ pero con la fila $r$ cambiada y $C$ es como $A$ y $B$ excepto en la fila $r$ donde cada coeficiente el la suma del de $A$ y $B$ correspondiente. Entonces $\det(C) = \det(A)+ \det(B)$.
		\end{lema}
		\begin{proof} Se hará por inducción en $n$. Para $n=1$, del resultado se reduce a probar que $\det[a+b] = \det[a]+ \det[b]$, lo cual es trivial, pues el determinante en matrices $1 \times 1$  es la identidad. 
			
			Primero consideremos el caso $r = 1$. En  este caso tenemos que  $A(1|1) =B(1|1) = C(1|1)$, pues en la única fila que difieren las matrices es en la primera. Además, si $i>1$,  $A(i|1)$,  $B(i|1)$ y $C(i|1)$ son iguales, excepto que difieren en la primera fila donde los coeficientes de $C(i|1)$ son la suma de los de  $A(i|1)$ y  $B(i|1)$,  entonces, por hipótesis inductiva, 
			$\det C(i|1) = \det A(i|1) + \det B(i|1)$. Concluyendo, tenemos que
			\begin{align*}
			\det A(1|1) &=\det B(1|1) =\det  C(1|1),&& \\ \det C(i|1) &= \det A(i|1) + \det B(i|1), \qquad i>1&&,
			\end{align*}
			lo cual implica que 
			\begin{align*}
			C^C_{11}&= C^A_{11} = C^B_{11} ,&& \\ C^C_{i1} &= C^A_{i1} + C^B_{i1}, \qquad i>1&&.
			\end{align*}
			Luego
			\begin{align*}
			\det C &= (a_{11}+b_{11}) C^C_{11} + \sum_{i=2}^{n} a _{i1}C^C_{i1} \\
			&= a_{11} C^C_{11} + b_{11}  C^C_{11}+ \sum_{i=2}^{n} a _{i1}( C^A_{i1} + C^B_{i1}) \\
			&= a_{11} C^A_{11} + b_{11}  C^B_{11}+ \sum_{i=2}^{n} a _{i1}( C^A_{i1} + C^B_{i1}) \\
			&= a_{11} C^A_{11} +\sum_{i=2}^{n} a _{i1}C^A_{i1} + b_{11}  C^B_{11}+ \sum_{i=2}^{n} a _{i1} C^B_{i1}\\
			&= \det A + \det B.
			\end{align*}
			
			El caso $r >1$ se demuestra de manera similar o,  si se prefiere, puede usarse el teorema \ref{det-prop-perm-filas}, observando que la permutación entre la fila $1$ y la fila $r$ cambia el signo del determinante.  
		\end{proof}
	
		\begin{teorema}\label{det-prop-fila-mas-cfila}
			Sea $A  \in M_n(\K)$. Sea $c \in \K$ y $B$ la matriz que se obtiene de $A$ sumando a la fila $r$ la fila $s$ multiplicada por $c$, es decir  $A  \stackrel{F_r + cF_s}{\longrightarrow} B$, entonces $\det B = \det A$.
		\end{teorema}
		\begin{proof}
			$A$ y $B$ difieren solo en la fila $r$, donde los coeficientes de $B$ son los los de $A$ más $c$ por los de la fila $s$. Luego si 
			\begin{equation*}
			A = \begin{bmatrix} F_1 \\ \vdots \\ F_s \\\vdots \\ F_r \\ \vdots \\ F_n \end{bmatrix}, \qquad 
			B = \begin{bmatrix} F_1 \\ \vdots \\ F_s \\\vdots \\ F_r + cF_s \\ \vdots \\ F_n \end{bmatrix},\qquad 
			A'= \begin{bmatrix} F_1 \\ \vdots \\ F_s \\\vdots \\ cF_s \\ \vdots \\ F_n \end{bmatrix},
			\end{equation*}
			el lema anterior nos dice que 
			\begin{equation}
			\det B = \det A + \det A'.
			\end{equation}
			Ahora bien, por teorema \ref{det-prop-mult-esc}, 
			$$
			\det A' = c \left| \begin{matrix} F_1 \\ \vdots \\ F_s \\\vdots \\ F_s \\ \vdots \\ F_n \end{matrix} \right|,
			$$
			y este último determinante es cero, debido a que la matriz tiene dos filas iguales. Luego,  $\det B = \det A$. 
		\end{proof}
		
		 \vskip .3cm
		 
		 A continuación veremos que el determinante del producto de matrices es el producto de los determinantes de las matrices. 
		 
		 \begin{teorema}\label{det-elem-por-mtrx} Sea $A  \in M_n(\K)$ y $E$ una matriz elemental $n \times n$. Entonces
		 	\begin{equation}\label{det-prod-elem-matr}
		 	\det (EA) = \det E \det A.
		 	\end{equation}  
		 \end{teorema}
		 \begin{proof}
		 	(1) Si $c \not=0$, y $E$ es la matriz elemental que se obtiene de multiplicar por  $c$ la fila $r$ de $I_n$, sabemos, por corolario \ref{det-mtrx-elem}, que tiene determinante igual a $c$. Por otro lado, el teorema \ref{th-mrtx-elem} nos dice que $EA$ es la matriz que se obtiene a partir de $A$ multiplicando la fila $r$ por $c$, y entonces, por teorema \ref{det-prop-fundamentales}, $\det(EA) = c \det A = \det E \det A$.
		 	
		 	(2) Si $E$ es la matriz elemental que se obtiene de sumar a la fila $r$ de $I_n$  la fila $s$ multiplicada por $c$, entonces $\det E =1$. Por  otro lado  $\det(EA) = \det(A)$, por lo tanto  $\det(EA) = \det(E)\det(A)$.
		 	
		 	(3) Finalmente, si $E$ es la matriz elemental que se obtiene de intercambiar la fila $r$ por la fila $s$ de $I_n$, entonces $\det E=-1$. Por  otro lado  $\det(EA) = -\det(A)$, por lo tanto  $\det(EA) = \det(E)\det(A)$.
		 \end{proof}
		 
		 \begin{corolario}\label{coro-det-prod-elem-mtrx}
		 	Sea $A  \in M_n(\K)$ y $E_1,\ldots,E_k$ matrices elementales $n \times n$. Entonces
		 	\begin{equation*}
		 	\det (E_kE_{k-1}\ldots E_1A) = \det (E_k) \det (E_{k-1})\ldots \det (E_1) \det (A).
		 	\end{equation*}  
		 \end{corolario}
		 \begin{proof}
		 	Por  la aplicación reiterada del teorema \ref{det-elem-por-mtrx} tenemos, 
		 	\begin{equation*}
		 	\begin{matrix*}[l]
		 	\det (E_kE_{k-1}\ldots E_1A) &= \det (E_k) \det(E_{k-1}\ldots E_1 A) \\
		 	&= \det( E_k) \det(E_{k-1})\det(E_{k-2}\ldots E_1A) \\
		 	&\qquad\quad \vdots \\
		 	& = \det( E_k) \det(E_{k-1})\det(E_{k-2})\ldots \det(E_1) \det(A).
		 	\end{matrix*}		
		 	\end{equation*}
		 \end{proof}
		 
		 
		 \begin{obs}\label{mtrx-elem-por-merf}
		 	Sea $A  \in M_n(\K)$,  entonces $A$ es equivalente por filas a una matriz $R$ que es  MERF, por lo tanto $R$ es equivalente por filas a $A$. Es decir, existen $E_1,\ldots,E_k$ matrices elementales, tal que
		 	\begin{equation}
		 	A = E_kE_{k-1}\ldots E_1R.
		 	\end{equation}
		 	Ya vimos,  en el  teorema \ref{mtrx-inv-equiv} que si $A$ inversible $R= I_n$ y por lo tanto  $A$ es equivalente por filas a un producto de  matrices elementales. Si $A$ no es inversible, entonces $R$ no es la identidad y por lo tanto tiene filas nulas. 
		 \end{obs}
		
		 
		 
		 \begin{teorema}  Sean $A,B \in M_n(\K)$,  entonces
		 	\begin{enumerate}
		 		\item $\det (A B) = \det(A)\det(B)$. 
		 		\item $A$ inversible si y solo si  $\det(A)\ne0$.
		 	\end{enumerate}
		 \end{teorema}
		 \begin{proof} 	
		 		Sea $A = E_kE_{k-1}\ldots E_1R$ con $E_i$  elemental y $R$ una MERF.
		 	
		 	(1) Como $AB = E_kE_{k-1}\ldots E_1RB$, por corolario \ref{coro-det-prod-elem-mtrx}  tenemos que  $\det(AB) = \det( E_k) \ldots \det(E_1)\det(RB)$.
		 	
		 	
		 	Separaremos la demostración en dos casos: (a) $A$ es inversible y  (b) $A$ no es inversible. 
		 	
		 	(a) Si $A$  es inversible,  entonces $R=I_n$ y  $A = E_k\ldots E_1$, por lo tanto 
		 	\begin{equation*}
		 	\det(AB) = \det(E_k) \ldots \det(E_1)\det(B) = \det(A)\det(B).
		 	\end{equation*}
		 	
		 	(b) Si $A$ no es inversible, entonces $R$ no es la identidad, por lo tanto tiene la última fila nula y por lo tanto $\det(R)=0$, y en consecuencia $\det(A)=0$. Como $R$ tiene la última fila nula, no es difícil ver que entonces $RB$ tiene la última fila nula y por lo tanto $\det(RB)=0$. Luego 
		 	$$
		 	\det(AB) = \det( E_k) \ldots \det(E_1)\det(RB) =0.
		 	$$
		 	Como  $\det(A)=0$, tenemos también 
		 	$$
		 	\det(A)\det(B) =0.
		 	$$ 
		 	
		 	\vskip .3cm
		 	
		 	(2) ($\Rightarrow$) Como $A$ inversible, $R = I_n$ y $\det A = \det( E_k) 	 \ldots \det(E_1)$ es no nulo, pues las matrices elementales tiene determinante no nulo. 
		 	
		 	(2)($\Leftarrow$)  Si $\det(A) \ne 0$, entonces $\det(R) \ne 0$, luego $R = I_n$ y $A$ es producto de matrices elementales, por lo tanto inversible. 
		 \end{proof}
		 
		 Haremos ahora la demostración del teorema \ref{det-a-trans}. 
		 
		 \begin{teorema}
		 	Sea $E$ matriz elemental, entonces $E^\t$ es matriz elemental del mismo tipo y $\det(E) = \det(E^\t)$. 
		 \end{teorema}
		 \begin{proof}
		 	Si $c \not=0$ y $E$ es la matriz elemental que se obtiene de multiplicar por  $c$ la fila $r$ de $I_n$, es claro que $E^t = E$ y por lo tanto 	$\det(E) = \det(E^\t)$.
		 	
		 	Si $E$ es la matriz elemental que se obtiene de sumar a la fila $r$ de $I_n$ la fila $s$ multiplicada por $c \in \K$,  entonces  $E^\t$  es la matriz elemental que se obtiene de sumar a la fila $s$ de $I_n$ la fila $r$ multiplicada por $c$. Luego,   $\det(E) = \det(E^\t) = 1$.
		 	
		 	Finalmente, si $E$ es la  matriz elemental que se obtiene de intercambiar la fila $r$ por la fila $s$ de $I_n$,entonces $E^\t = E$ y por lo tanto $\det(E) = \det(E^\t)$.
		 \end{proof}
		 

		 
		 \begin{teorema}%\label{det-a-trans} 
		 	Sea $A \in M_n(\K)$,  entonces 
		 	$\det(A) = \det(A^\t)$
		 \end{teorema}
		 \begin{proof}
		 	Si $A$ es inversible, entonces  $A = E_kE_{k-1}\ldots E_1$ con $E_i$ elemental, por lo tanto $\det(A) = \det( E_k)\det(E_{k-1})\ldots \det(E_1)$. Luego,
		 	\begin{equation*}
		 	\det(A^\t) = \det(E_1^\t \ldots E_k^\t) = \det(E_1^\t) \ldots \det(E_k^\t) = 
		 	\det(E_1) \ldots \det(E_k) = \det(A).
		 	\end{equation*}
		 	Si $A$ no es inversible, entonces $A^\t$ no es inversible y en ese caso $\det(A) = \det(A^\t)=0$.
		 \end{proof}
		 
		 \vskip .3cm
		 
		 Finalmente,  demostremos el teorema \ref{th-expancion-cofactores}.
	
		 \begin{teorema} El determinante de una matriz $A$ de orden $n \times n$ puede ser calculado por la expansión de los cofactores en  cualquier columna o cualquier fila. Más específicamente, 
		 	\begin{enumerate}
		 		\item si usamos la expansión por la $j$-ésima columna, $1 \le j \le n$, tenemos
		 		\begin{align*}
		 		\det A &= \sum_{i=1}^{n} a_{ij} C_{ij} \\
		 		& = a_{1j}C_{1j}+a_{2j}C_{2j}+\cdots+a_{nj}C_{nj}.
		 		\end{align*} 
		 		\item si usamos la expansión por la $i$-ésima fila, $1 \le i \le n$, tenemos
		 		\begin{align*}
		 		\det A &= \sum_{j=1}^{n} a_{ij} C_{ij} \\
		 		& = a_{i1}C_{i1}+a_{i2}C_{i2}+\cdots+a_{in}C_{in};
		 		\end{align*} 
		 	\end{enumerate}
		 \end{teorema}
		 \begin{proof} (1) Primero hagamos la demostración para $j=2$,  es decir para el desarrollo por la segunda columna.  Escribamos $A$ en función de sus columnas,  es decir 
		 	$$
		 	A = \begin{bmatrix}C_1& C_2&C_3 &\cdots& C_n\end{bmatrix},
		 	$$
		 	donde $C_k$  es la columna $k$ de $A$.
		 	Sea $B=[b_{ij}]$ la matriz definida por
		 	$$
		 	B = \begin{bmatrix} C_2 &C_1 &C_3 &\cdots &C_n\end{bmatrix}.
		 	$$
		 	Entonces, $\det(B) = -\det(A)$. Por otro lado, por la definición de determinante, 
		 	\begin{align*}
		 	\det(B) &=   \sum_{i=1}^{n} b_{i1} C^B_{i1} \\
		 	&= \sum_{i=1}^{n} b_{i1} (-1)^{i+1}B(i|1) \\
		 	& = \sum_{i=1}^{n} a_{i2} (-1)^{i+1}B(i|1).
		 	\end{align*} 
		 	Ahora bien, es claro que $B(i|1) = A(i|2)$, por lo tanto 
		 	$$
		 	\det(B)  = \sum_{i=1}^{n} a_{i2} (-1)^{i+1}A(i|2) = - \sum_{i=1}^{n} a_{i2} C_{i2}.
		 	$$
		 	Es decir, $\det(A) = -\det(B)= \sum_{i=1}^{n} a_{i2} C_{i2}$.
		 	
		 	El caso $j>2$  se demuestra de forma similar: si $B$ es la matriz
		 	$$
		 	B = \begin{bmatrix} C_j &C_1 &C_2 &\cdots& C_{j-1}&C_{j+1}&\cdots &C_n\end{bmatrix}.
		 	$$        
		 	entonces $\det(B)=(-1)^{j-1}\det(A)$, pues son necesarios $j-1$ permutaciones para recuperar la matriz $A$ (es decir, llevar la columna $j$ a su lugar).
		 	%que se obtiene de $A$ permutando la columna $1$ por la columna $j$,  entonces $\det(B) = -\det(A)$. 
		 	Como $B(i|1) = A(i|j)$,  desarrollando por la primera columna el determinante de $B$ obtenemos el resultado.  
		 	
		 	
		 	(2) Observemos primero que $A^\t(j|i) = A(i|j)^\t$, por lo tanto, si calculamos $\det(A^\t)$ por desarrollo por columna $i$, obtenemos 
		 	\begin{align*}
		 	\det A = \det(A^\t)&=\sum_{j=1}^{n} [A^\t]_{ji} (-1)^{i+j}\det (A^\t(j|i)) \\
		 	&=  \sum_{j=1}^{n} a_{ij} (-1)^{i+j}\det (A(i|j)^\t) \\
		 	&= \sum_{j=1}^{n} a_{ij} (-1)^{i+j}\det (A(i|j)).
		 	\end{align*}
		 	
		 	
		 \end{proof}
		 
		 
		\end{section}
	
		 
		 
		 \begin{section}{Regla de Cramer} Veremos ahora que la inversa de una matriz inversible se puede escribir en términos de determinantes de algunas matrices relacionadas.  
		 	
		 	\begin{teorema} \label{th-cof-01}
		 		Sea $A$ matriz $n \times n$, entonces
		 		\begin{align*}
		 		\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix} A &= 
		 		\begin{bmatrix} 0 & \cdots& 0 & \det A & 0 &\cdots  & 0 \end{bmatrix}. \\
		 		&\hspace{3cm} \uparrow i
		 		\end{align*}
		 		Es decir, la matriz fila formada por los cofactores correspondientes a la columna  $i$ multiplicada por la matriz $A$ es igual a la matriz fila con valor $\det A$ en la posición $i$ y 0 en las otras posiciones.	 
		 	\end{teorema}
		 	\begin{proof}
		 		Si $C_j$ denota la matriz formada por la columna $j$ de $A$ debemos probar que
		 		$$
		 		\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix} C_j = \sum_{k=1}^{n} a_{kj}C_{ki} = \left\{ \begin{matrix*}[l] \det(A) &\text{si $j = i$}  \\ 0 &\text{si $j \ne i$.}\end{matrix*}\right.
		 		$$ 
		 		Ahora bien,
		 		\begin{align*}
		 		\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix} C_i = \sum_{j=1}^{n} C_{ji}a_{ji},
		 		\end{align*}
		 		y esto último no es más que el cálculo del determinante por desarrollo de la  columna $i$,  es decir,  es igual a $\det(A)$.
		 		
		 		Para ver el caso $i \ne j$, primero observemos que si 
		 		\begin{align*}
		 		B &= \begin{bmatrix} C_1 & C_2&\cdots &C_j& \cdots &C_j & \cdots &C_{n-1}& C_{n}\end{bmatrix}, \\
		 		&\hspace{3.2cm} \uparrow i \hspace{1.2cm} \uparrow j
		 		\end{align*}
		 		es decir, $B$ es la matriz $A$ donde reemplazamos la columna $i$ por la columna $j$, entonces como $B$ tiene dos columnas iguales, $\det(B) =0$. Por lo tanto, si calculamos el determinante de $B$ por el desarrollo en la columna $i$, obtenemos
		 		\begin{equation} \label{eq-cof}
		 		0 = \det(B) = \sum_{k=1}^{n} a_{kj}C_{ki}.
		 		\end{equation}
		 		Por otro lado, 
		 		\begin{align*}
		 		\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix} C_j = \sum_{k=1}^{n} C_{ki}a_{kj},
		 		\end{align*}
		 		luego, por la ecuación (\ref{eq-cof}) tenemos que 
		 		$$
		 		\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix} C_j =0
		 		$$
		 		si $i \ne j$.
		 		
		 	\end{proof}
		 	
		 	\begin{definicion} Sea $A$ matriz $n \times n$, la \textit{matriz de cofactores}\index{matriz de cofactores} es la matriz cuyo coeficiente $ij$ vale $C_{ij}$. La matriz de cofactores de $A$  se denota $\operatorname{cof}(A)$. La \textit{matriz adjunta de A} es $\operatorname{adj}(A) = \operatorname{cof}(A)^\t$.
		 	\end{definicion}
		 	
		 	\begin{teorema} Sea $A$ matriz $n \times n$,  entonces
		 		\begin{equation*}
		 		\operatorname{adj}(A).A = \det(A)I_n.
		 		\end{equation*}
		 	\end{teorema}
		 	\begin{proof}
		 		Observar que la fila $i$ de $\operatorname{adj}(A)$ es $\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix}$. Por lo tanto, la fila $i$ de $\operatorname{adj}(A).A$ es
		 		$$
		 		\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix} A,
		 		$$ 
		 		que por el teorema \ref{th-cof-01} es una matriz fila con el valor $\det A$ en la posición $i$ y todos los demás coeficientes iguales a 0. Luego
		 		$$
		 		\operatorname{adj}(A).A = 
		 		\begin{bmatrix} C_{11} & C_{21} & \cdots & C_{n1}\\
		 		C_{12} & C_{22} & \cdots & C_{n2} \\
		 		\vdots & \vdots & \ddots & \vdots \\
		 		C_{1n} & C_{2n} & \cdots & C_{nn}\end{bmatrix}. A = 
		 		\begin{bmatrix}\det A & 0 & \cdots & 0\\
		 		0 & \det A & \cdots & 0 \\
		 		\vdots & \vdots & \ddots & \vdots \\
		 		0 & 0 & \cdots & \det A\end{bmatrix} = \det(A) I_n
		 		$$
		 	\end{proof}
		 	
		 	\begin{corolario}\label{cor-inv-x-det}
		 		Si $A$  es inversible,  entonces 
		 		\begin{equation*}
		 		A^{-1} = \frac{1}{\det A} \operatorname{adj}A.
		 		\end{equation*}
		 	\end{corolario}
		 	\begin{proof}
		 		\begin{equation*}
		 		\frac{1}{\det A} \operatorname{adj}A.A = \frac{1}{\det A}\det A I_n = I_n.
		 		\end{equation*}
		 	\end{proof}
		 	
		 	
		 	
		 	\begin{teorema}[Regla de Cramer]\index{regla de Cramer} Sea $AX=Y$ un sistema de ecuaciones tal que $A \in M_n(\K)$ es inversible. Entonces, el sistema tiene una única solución $(x_1,\ldots,,x_n)$ con 
		 		\begin{equation*}
		 		x_j = \frac{\det A_j}{\det A},\qquad j=1,\ldots,n,
		 		\end{equation*}
		 		donde $A_j$ es la matriz $n \times n$ que se obtiene de $A$ remplazando la columna $j$ de
		 		$A$ por $Y$.
		 	\end{teorema}
		 	\begin{proof}
		 		Haremos la demostración para matrices $3 \times 3$. La demostración en el caso general es completamente análoga. 
		 		
		 		Como $A$  es inversible, existe $A^{-1}$ y multiplicamos la ecuación a izquierda por $A^{-1}$ y obtenemos que $A^{-1}A X = A^{-1}Y$,  es decir $X = A^{-1}Y$ y esta es la única solución. Luego
		 		\begin{align*}
		 		A^{-1}Y & = \frac{1}{\det A}
		 		\begin{bmatrix} C_{11} & C_{21} & C_{31}\\
		 		C_{12} & C_{22} &  C_{32} \\
		 		C_{13} & C_{23} & C_{33}\end{bmatrix}
		 		\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix} \\
		 		&= \frac{1}{\det A}\begin{bmatrix} y_1C_{11}+  y_2C_{21} + y_3C_{31}\\
		 		y_1C_{12}+  y_2C_{22}+   y_3C_{32} \\
		 		y_1C_{13}+  y_2C_{23}+  y_3C_{33}\end{bmatrix} \tag{$*$}
		 		\end{align*}
		 		Ahora bien,  $y_1C_{11}+  y_2C_{21}+   y_3C_{31}$ es el cálculo de determinante por desarrollo de la primera columna de la matriz 
		 		$$
		 		\begin{bmatrix}
		 		y_1 & a_{12} & a_{13} \\y_2 & a_{22} & a_{23} \\y_1 & a_{32} & a_{33} 
		 		\end{bmatrix},
		 		$$
		 		y,  de forma análoga, el segundo y tercer coeficiente de la matriz ($*$) son el determinante de las matrices $3 \times 3$ que se obtienen de $A$ remplazando la columna $2$ y $3$, respectivamente, de
		 		$A$ por $Y$. Es decir
		 		\begin{align*}
		 		\begin{bmatrix} x_1\\
		 		x_2 \\
		 		x_3\end{bmatrix} = A^{-1}Y & = \frac{1}{\det A}\begin{bmatrix} \det A_1\\
		 		\det A_2 \\
		 		\det A_3 \end{bmatrix} = \begin{bmatrix} \frac{\det A_1}{\det A}\\
		 		\frac{\det A_2}{\det A} \\
		 		\frac{\det A_3}{\det A}\end{bmatrix},
		 		\end{align*}
		 		luego $x_j = \displaystyle\frac{\det A_j}{\det A}$ para $j =1,2,3$.
		 	\end{proof}
		 	
		 	\begin{ejemplo}
		 		Resolvamos usando la regla de Cramer el siguiente sistema:
		 		\begin{align*}
		 		x_1 + x_2 - x_3 &= 6 \\
		 		3x_1 -2 x_2 + x_3 &= -5 \\ 
		 		x_1 +3 x_2 - 2x_3 &= 14.
		 		\end{align*}
		 		La matriz asociada al sistema es 
		 		$$
		 		A= \begin{bmatrix}
		 		1 &1  &-1 \\
		 		3 &-2  &1 \\ 
		 		1 &3 &-2
		 		\end{bmatrix}.
		 		$$
		 		Luego 
		 		$$
		 		A_1 = \begin{bmatrix}
		 		6 &1  &-1 \\
		 		-5 &-2  &1 \\ 
		 		14 &3 &-2
		 		\end{bmatrix}, \qquad
		 		A_2 = \begin{bmatrix}
		 		1 &6  &-1 \\
		 		3 &-5  &1 \\ 
		 		1 &14 &-2
		 		\end{bmatrix}, \qquad
		 		A_2 = \begin{bmatrix}
		 		1 &1  &6 \\
		 		3 &-2  &-5 \\ 
		 		1 &3 &14
		 		\end{bmatrix}, \qquad
		 		$$
		 		y
		 		$$
		 		\det A = -3,\qquad\det A_1 = -3,\qquad \det A_2 = -9,\qquad \det A_3=6.
		 		$$
		 		Por lo tanto,
		 		\begin{align*}
		 		x_1 &= \frac{\det A_1}{\det A} = \frac{-3}{-3} = 1 \\
		 		x_2 &= \frac{\det A_2}{\det A} = \frac{-9}{-3} = 3\\
		 		x_3 &= \frac{\det A_3}{\det A} = \frac{6}{-3} = -2.
		 		\end{align*} 
		 	\end{ejemplo}
		 	
		 	\vskip .5cm
		 	
		 	\begin{observacion}
		 		En  general, la regla de Cramer no es utilizada para resolver sistemas  de ecuaciones. El  método de Gauss, además de ser mucho más rápido, tiene el beneficio de ser un método que abarca más casos y nos da más información que la regla de Cramer. Por ejemplo,  el método de Gauss también funciona para matrices no inversibles, mientras que la regla de Cramer no. Más aún, el método de Gauss se puede utilizar para resolver sistemas con matrices no cuadradas (es decir, sistemas de ecuaciones con números no iguales de variables y ecuaciones), mientras que la regla de Cramer no.
		 		
		 		Con respecto a la velocidad  de resolución, para matrices $n \times n$ el método de Gauss requiere ``alrededor de $n^3$ operaciones'', mientras que la regla de Cramer requiere ``alrededor de $n^4$ operaciones'', haciendo el primer método $n$ veces más rápido que el segundo. En  lenguaje técnico, el método de Gauss tiene complejidad $O(n^3)$ y la regla de Cramer complejidad $O(n^4)$ (ver
		 		\href{ https://es.wikipedia.org/wiki/Eficiencia\_Algorítmica}{ https://es.wikipedia.org/wiki/Eficiencia\_Algorítmica}) 
		 	\end{observacion}
		 	
		 \end{section}
	\end{chapter}
	

\printindex




\end{document}
%configuration={"latex_command":"latexmk -pdf -f -g -bibtex -synctex=1 -interaction=nonstopmode 'teoricoAlgII.tex'"}}