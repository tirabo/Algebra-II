      
    \begin{chapter}{Transformaciones lineales}\label{chap-trans-lin}
        Las transformaciones lineales son las funciones con las que trabajaremos en álgebra lineal. Se trata de funciones entre espacios vectoriales que son compatibles con la estructura,  es decir con la suma y el producto por escalares.
        
        
        \begin{section}{Transformaciones lineales}\label{seccion-transformaciones-lineales}
            \begin{definicion}
                Sean $V$ y $W$ dos espacios vectoriales sobre el cuerpo $\K$. Una 				\textit{transformación lineal}\index{transformación lineal} de $V$ en $W$ es una función $T:V \to W$  tal que
                \begin{enumerate}
                    \item $T(v+v') = T(v)+ T(v')$, para $v,v' \in V$,
                    \item $T(\lambda v) = \lambda T(v)$, para $v \in V$, $\lambda \in \K$.
                \end{enumerate}
            \end{definicion}
        
            \begin{obs*}
                $T:V \to W$ es transformación lineal si y sólo si
                \begin{enumerate}
                    \item[({a})]  $T(\lambda v+v') = \lambda T(v)+ T(v')$, para $v,v' \in V$, $\lambda \in \K$.
                \end{enumerate}
            Algunas veces usaremos esto último para comprobar si una aplicación de $V$ en $W$ es una transformación lineal. 
            \end{obs*}
            
            \begin{ejemplo*}
                Si $V$ es cualquier espacio vectorial, la transformación identidad $\Id$, definida por $\Id v = v$ ($v \in V$), es una transformación lineal de $V$ en $V$. La transformación cero $0$, definida por $0v = 0$, es una transformación lineal de $V$ en $V$.
            \end{ejemplo*}
        
            \begin{ejemplo*}
                Sea $T : \K^3 \to \K^2$ definida por
                $$
                T(x_1,x_2,x_3) = (2x_1 - x_3, -x_1+3x_2+x_3).
                $$
                Entonces, $T$  es una transformación lineal. La demostración la veremos en la observación que sigue a este ejemplo. 
                
            Observar que si 
            $$
             A = \begin{bmatrix}
             2&0&-1 \\ -1&3&1
             \end{bmatrix},
            $$
            entonces
            $$
            \begin{bmatrix}
            2&0&-1 \\ -1&3&1
            \end{bmatrix} 
            \begin{bmatrix}
            x_1\\x_2\\x_3
            \end{bmatrix} =
            \begin{bmatrix}
            2x_1 - x_3 \\ -x_1+3x_2+x_3
            \end{bmatrix}.
            $$
            Es decir,  si $\mathcal C_n$  es la báse canónica de $\K^n$ y $[x]_{\mathcal C_3}$ es la matriz de $x$ en la base canónica,  entonces 
            $$
            A.[x]_{\mathcal C_3} = [T(x)]_{\mathcal C_2}.
            $$ 
            \end{ejemplo*}
        
            \begin{obs}\label{obs-tl-1.5} 	Sea $T: \K^n \to \K^m$. En  general si $T(x_1,\ldots,x_n)$ en cada coordenada tiene una combinación lineal de los $x_1,\ldots,x_n$,  entonces $T$ es una transformación lineal. Mas precisamente, si $T$ está definida por
                \begin{align*}
                T(x_1,\ldots,x_n) &= (a_{11}x_1+\cdots + a_{1n}x_n,\, \ldots\,,a_{m1}x_1+\cdots + a_{mn}x_n )\\
                &=(\sum_{j=1}^n a_{1j} x_j,\ldots,\sum_{j=1}^n a_{mj} x_j),
                \end{align*}
                con $a_{ij} \in \K$, entonces $T$  es lineal. 
            \end{obs}
            \begin{proof} Se puede hacer directamente como ejercicio. También se demuestra más adelante en la observación \ref{obs-nu-im}. 
            \begin{comment}
                Sean $(x_1,\cdots,x_n), (y_1,\cdots,y_n) \in \K^n$ y $\lambda \in \K$,  entonces
                \begin{align*}
                T(\lambda(x_1,\cdots,x_n)+ (y_1,\cdots,y_n)) &= T(\lambda x_1+y_1,\ldots,\lambda x_n+y_n) \\
                &= (\sum_{j=1}^n a_{1j} (\lambda x_j+y_j),\ldots,\sum_{j=1}^n a_{mj} (\lambda x_j+y_j)) \\
                &= (\lambda\sum_{j=1}^n a_{1j} x_j+\sum_{j=1}^n a_{1j} y_j,\ldots,\lambda\sum_{j=1}^n a_{mj} x_j+\sum_{j=1}^n a_{mj} y_j) \\
                &= \lambda(\sum_{j=1}^n a_{1j} x_j,\ldots,\sum_{j=1}^n a_{mj} x_j) +(\sum_{j=1}^n a_{1j} y_j,\ldots,\sum_{j=1}^n a_{mj} y_j) \\
                & =\lambda T(x_1,\cdots,x_n)+ T(y_1,\cdots,y_n).
                \end{align*}
            \end{comment}
            \end{proof}
            
            
            \begin{ejemplo}[\textsc{Transformaciones de $\R^2$ en $\R^2$}] Las rotaciones y reflexiones en $\R^2$ son transformaciones lineales. Sea $\theta \in \R$ tal que  $0 \le \theta \le 2\pi$,  definimos
                \begin{equation*}
                    \begin{array}{llll}
                    R_\theta:&\R^2 &\to &\R^2 \\
                    &(x,y) &\mapsto & (x \cos\theta - y \sin\theta, y\cos\theta+ x\sin\theta)
                    \end{array}
                \end{equation*}
                Observemos que si escribimos el vector $(x,y)$  en coordenadas polares,  es decir  si 
                $$
                (x,y)= r(\cos\alpha,\sin\alpha),\quad r> 0, \; 0 \le \alpha < 2\pi, 
                $$
                entonces
                \begin{align*}
                    R_\theta(x,y) &= R_\theta(r\cos\alpha,r\sin\alpha) \\
                    &= (r\cos\alpha \cos\theta - r\sin\alpha \sin\theta, r\sin\alpha\cos\theta+ r\cos\alpha\sin\theta) \\
                    &= (r\cos(\alpha+\theta) , r\sin(\alpha+\theta)) \\
                    &= r(\cos(\alpha+\theta) , \sin(\alpha+\theta)).
                \end{align*}
                Por lo tanto $R_\theta(x,y)$ es el vector $(x,y)$ rotado $\theta$ grados en sentido antihorario y en consecuencia  $R_\theta$ es denominada la \textit{rotación antihoraria en $\theta$ radianes}. No es difícil verificar que $R_\theta$ es una transformación lineal. 
                
                
                    \begin{figure}[h]
                    	\centering	
                    \begin{tikzpicture}[scale=5]
                    \draw[->] (-0.1,0)  -- (1.1,0) node(xline)[right]	{$x$};
                    \draw[->] (0,-0.1) -- (0,1.1) node(yline)[above] {$y$};
                    \draw[->] (0,0) -- (0.5,0.866) node(yline)[above] {$R_{\theta}(v)$};
                    \draw[->] (0.9238,0.3826) arc[start angle=22.5, end angle=60, radius=1 ];
                    \draw[->] (0,0) -- (0.9238,0.3826) node(yline)[right] {$v$};
                    \draw[->] (0.7,0) arc[start angle=0, end angle=22.5, radius=0.7 ];
                    \draw[->] (0.9238/2,0.3826/2) arc[start angle=22.5, end angle=60, radius=0.5 ];
                    \node [right] at (0.3,0.3) {$\theta$};
                    \node [right] at (0.59,0.12) {$\alpha$};
                    \draw (1,1pt) -- (1,-1pt);
                    \node [right] at (0.95,-0.07) {$r$};
                    % Lines
                    \end{tikzpicture}
                    \caption{Rotación  $\theta$ grados. }
                    \label{rotacion-theta}
                \end{figure}
                
                
                
                Otras transformaciones lineales importantes de $\R^2$ en $\R^2$ son 
                $$
                S_h(x,y) = (x,-y) \quad \text{ y } \quad S_v(x,y) = (-x,y).
                $$
                La primera es la reflexión en el eje $x$ y la segunda la reflexión en el eje $y$. Las siguientes afirmaciones se comprueban algebraicamente en forma sencilla, pero nos podemos convencer de ellas por su interpretación geométrica:
                \begin{equation*}
                \begin{array}{ll}
                R_\theta \circ R_\varphi = R_{\theta +\varphi}, \quad &\text{(rotar $\varphi$ y $\theta$ $=$  rotar $\theta+\varphi$)} \\
                R_{\pi/2} \circ S_h \circ R_{-\pi/2} = S_v&
                \end{array}
                \end{equation*}
                
            \end{ejemplo}
    
        
            \begin{ejemplo*}
            Sea $V = \R[x]$ el espacio vectorial de los polinomios con coeficientes reales. Definimos $D:V \to V$, por
            $$
            D(P)(x) = P'(x),\quad x \in \R. 
            $$
            Observemos primero que la derivada de un polinomio es un polinomio, pues 
            $$
            (a_nx^n+ a_{n-1}x^{n-1}+\cdots + a_1 x + a_0)' = na_nx^{n-1}+ (n-1)a_{n-1}x^{n-2}+\cdots + a_1.
            $$
            Además  $D$  es lineal, pues $(f+g)' = f' + g'$ y $(\lambda f)' = \lambda f'$, para$f,g$ funciones derivables y $\lambda \in \R$.
            \end{ejemplo*} 
        
        \begin{obs*}	Sean $V$ y $W$ dos espacios vectoriales sobre el cuerpo $\K$ y 	 $T:V \to W$			un transformación lineal. Entonces $T(0) =0$
        \end{obs*}
        \begin{proof} $T(0) = T(0+0) = T(0) + T(0)$, por lo tanto 
            \begin{align*}
                -T(0) + T(0) = -T(0) + T(0)+T(0) \;\Rightarrow\; 0 = 0 +T(0) \;\Rightarrow\; 0= T(0).
            \end{align*}
        \end{proof}
        
        \begin{obs*}
            Las transformaciones lineales preservan  combinaciones lineales, es decir si $T:V \to W$ es una transformación lineal, $ v_1,\ldots,v_k \in V$ y $\lambda_1, \ldots+ \lambda_k \in \K$,  entonces
            $$
            T(\lambda_1 v_1 + \cdots+ \lambda_k v_k) = \lambda_1 T(v_1) + \cdots+ \lambda_k T(v_k).
            $$
            Observar que el caso $k=2$ se demuestra de la siguiente manera
            $$
            T(\lambda_1 v_1 +  \lambda_2 v_2) =T(\lambda_1 v_1) + T(\lambda_2 v_2) =\lambda_1 T(v_1) + \lambda_2T( v_2).
            $$
            El caso general se demuestra por inducción. 
        
        \end{obs*}
        
        
        \begin{teorema}\label{th-tl-definida-en-base}
            Sean $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y $\{v_1,\ldots,v_n\}$  una base ordenada de $V$. Sean $W$ un espacio vectorial sobre el mismo cuerpo y $\{w_1,\ldots,w_n\}$, vectores cualesquiera de $W$. Entonces existe una única transformación  lineal $T$ de $V$ en $W$ tal que
            \begin{equation*}
            T(v_j) = w_j, \quad j=1,\ldots,n.
            \end{equation*}
        \end{teorema}
            \begin{proof}
                Recordemos que si $v \in V$,  existen únicos $a_1,\ldots,a_n \in \K$ (las coordenadas de $v$) tal que $$v = a_1v_1 + \cdots+a_n v_n.$$  Luego para este vector $v$  definimos
                \begin{equation*}
                    T(v) = a_1w_1 + \cdots+a_n w_n.
                \end{equation*}
                Entonces, $T$ es una correspondencia bien definida que asocia a cada vector $v$ 
                de $V$ un vector $T(v)$ de $W$. De la definición queda claro que $T(v_j) = w_j$ para cada $j$. Para ver que $T$ es lineal, sea
                \begin{equation*}
                    w = b_1v_1 + \cdots+b_n v_n,
                \end{equation*}
                y sea  $\lambda \in \K$. Ahora
                \begin{align*}
                    \lambda v+w &= \lambda(a_1v_1 + \cdots+a_n v_n) + b_1v_1 + \cdots+b_n v_n \\
                    &= (\lambda a_1+b_1)v_1 + \cdots+(\lambda a_n+b_n)v_n
                \end{align*}
                con lo que, por definición
                \begin{equation*}
                    T(\lambda v+w) =(\lambda a_1+b_1)w_1 + \cdots+(\lambda a_n+b_n)w_n. 
                \end{equation*}
                Por otra parte
                \begin{align*}
                \lambda  T(v) + T(w) &= \lambda (a_1w_1 + \cdots+a_n w_n)+b_1w_1 + \cdots+b_n w_n	 \\
                 &=(\lambda a_1+b_1)w_1 + \cdots+(\lambda a_n+b_n)w_n ,			
                \end{align*}
                y así
                \begin{equation*}
                    T(\lambda v+w) = 	\lambda  T(v) + T(w).
                \end{equation*}
                
                Finalmente,  debemos probar la unicidad de $T$. Sea $S: V \to W$ transformación lineal tal que $S(v_j) = w_j$ para $1 \le j \le n$. Entonces,  si $v \in V$ un vector arbitrario, $v = \sum_i a_i v_i$ y
                \begin{equation*}
                    S(v) = S(\sum_i a_i v_i)\sum_i a_i S( v_i) = \sum_i a_iw_i = 
                     \sum_i a_i T( v_i) =  T(\sum_i a_i v_i) = T(v)
                \end{equation*}
            \end{proof}
        
        El teorema \ref{th-tl-definida-en-base} es muy elemental, pero por su importancia ha sido presentado
        detalladamente. 
        
            \begin{ejemplo*} Usando el teorema \ref{th-tl-definida-en-base}, podemos demostrar la observación \ref{obs-tl-1.5} de la siguiente manera: sea  $\mathcal C_n = \{e_1,\ldots,e_n\}$ es la base canónica de $\K^n$ y  sea  $T: \K^n \to \K^m$ la única transformación lineal tal que 
            \begin{equation*}
            T(e_j)  = (a_{1j},\, \ldots\,,a_{mj} ), \quad j=1,\ldots,n
            \end{equation*}	
            Entonces, 
            \begin{equation*}
            T(x_1,\ldots,x_n) = (a_{11}x_1+\cdots + a_{1n}x_n,\, \ldots\,,a_{m1}x_1+\cdots + a_{mn}x_n ).
            \end{equation*}
            es la transformación lineal resultante.
        \end{ejemplo*}
        
        \begin{ejemplo*}
            Los vectores
            \begin{align*}
                v_1 &= (1,2)\\
                v_2 &= (3,4)
            \end{align*}
            son linealmente independientes y, por tanto, forman una base de $\R^2$. De acuerdo con el teorema \ref{th-tl-definida-en-base}, existe una única transformación lineal de $\R^2$ en $\R^2$ tal que
            \begin{align*}
                T(v_1) &= (3,2, 1) \\
                T(v_2) &= (6, 5,4).
            \end{align*}
            Para poder describir $T$ respecto a las coordenadas canónicas debemos calcular $T(e_1)$ y $T(e_2)$,  ahora bien,
            \begin{align*}
            (1,0) &= c_{1}(1,2) + c_{2}(3,4)\\
            (0,1) &= c_{3}(1,2) + c_{4}(3,4)
            \end{align*}
            y resolviendo este sistema de cuatro ecuaciones con cuatro incógnitas obtenemos
            \begin{equation*}
            \begin{array}{rcrcr}
            (1,0) &=& -2(1,2) &+ &(3,4)\\
            (0,1) &=& \displaystyle\frac32(1,2) &- &\displaystyle\frac12(3,4)
            \end{array}
            \end{equation*}
            Luego, 
                \begin{align*}
            T(1,0) &= -2T(1,2)+ T(3,4) = -2(3,2, 1)+(6, 5,4) = (0,1,2)\\
            T(0,1) &= \displaystyle\frac32T(1,2) - \displaystyle\frac12T(3,4) = \displaystyle\frac32(3,2,1) - \displaystyle\frac12(6, 5,4)= (\displaystyle\frac32,\displaystyle\frac12,-\displaystyle\frac12)
            \end{align*}
            Entonces
                \begin{equation*}
            T(x_1,x_2) = x_1(0,1,2) + x_2		 (\displaystyle\frac32,\displaystyle\frac12,-\displaystyle\frac12)
            = (\displaystyle\frac32x_2,x_1+\displaystyle\frac12x_2,2x_1-\displaystyle\frac12x_2)
            \end{equation*}
        \end{ejemplo*}
        
        \end{section}
    
    
        \begin{section}{N\'ucleo e imagen de una transformaci\'on lineal}\label{seccion-nucleo-e-imagen-de-una-tl}
        
        \begin{definicion}
            Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal.  Definimos
            \begin{align*}
                \img(T) &:= \{w \in W:\text{existe $v \in V$, tal que } T(v)=w\} = \{T(v): v \in V \}, \\
                \nuc(T) &:= \{v \in V: T(v)=0 \}. 
            \end{align*}
            A $\img(T)$ lo llamamos la \textit{imagen}\index{imagen de una trasnformación lineal} de $T$ y a $ \nuc(T)$ el \textit{núcleo}\index{núcleo  de una transformación lineal} de $T$. 
        \end{definicion}
        
        \begin{teorema}
            Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal; entonces $\img(T) \subset W$ y $\nuc(T) \subset V$ son subespacios vectoriales.
        \end{teorema}
        \begin{proof}
            $\img(T) \ne \emptyset$, pues $0 = T(0) \in \img(T)$. 
            
            Si $T(v_1),T(v_2) \in \img(T)$ y $\lambda \in \K$,  entonces $T(v_1) + T(v_2) = T(v_1+v_2) \in \img(T)$ y $\lambda T(v_1) = T(\lambda v_1) \in \img(T)$.
            
            
            $\nuc(T) \ne \emptyset$ pues $T(0) =0$ y por lo tanto $0 \in \nuc(T)$.
            
            Si $v,w \in V$ tales que $T(v) =0$ y $T(w)=0$,  entonces, $T(v+w)= T(v)+T(w) =0$. por lo tanto $v+w \in \nuc(T)$. Si  $\lambda \in \K$,  entonces $T(\lambda v) = \lambda T(v) = \lambda.0 =0$, luego  $\lambda v \in \nuc(T)$.
        \end{proof}

    
        \begin{definicion}
            Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal. Supongamos que $V$ es de dimensión finita.
            \begin{enumerate}
\item El \textit{rango}\index{rango de una transformación lineal} de $T$ es la dimensión de la imagen de $T$.
\item La \textit{nulidad}\index{nulidad de una transformación lineal} de $T$ es la dimensión del núcleo  de $T$.
            \end{enumerate}
            
        \end{definicion}
                
        \begin{ejemplo*}
            Sea $T: \R^3 \to \R$, definida
            $$
            T(x,y,z) = x +2y +3z.
            $$
            Encontrar una base del núcleo y de la imagen.
            \begin{proof}[Solución]
                Es claro que como $T$ no es 0, la imagen es todo $\R$ (y por lo tanto cualquier $r\in \R, r \ne 0$ es base de la imagen). 
                
                Con respecto al núcleo,  debemos encontrar una base del subespacio
                $$
                \nuc(T) = \{(x,y,z): x+2y+3z =0\}.
                $$
                Como $x+2y+3z =0 \Leftrightarrow x = -2y-3z$, luego, 
                \begin{equation}\label{forma-parametrica-2}
                    \nuc(T) =  \{(-2s-3t,s,t): s,t \in \R\}.
                \end{equation}
                Ahora bien, $(-2s-3t,s,t) = s(-2,1,0)+t(-3,0,1)$, por lo tanto 
                \begin{equation*}
                \nuc(T) =  <(-2,1,0),(-3,0,1)>,
                \end{equation*}
                y  como $(-2,1,0),(-3,0,1)$ son LI, tenemos que forman una base del núcleo. 
            \end{proof}
        
        La expresión (\ref{forma-parametrica-2}),  que depende de dos parámetros ($s$ y $t$) que son independientes entre ellos, es llamada la \textit{descripción paramétrica} del núcleo	
        \end{ejemplo*}

        Todas las transformaciones lineales entre $\R^n$ y $\R^m$ son de la forma ``multiplicar por una matriz''. Más aún,  toda transformación lineal entre espacios vectoriales de dimensión finita se puede expresar de esta forma. Así que analizaremos un poco más en detalle este tipo de transformaciones.
\begin{observacion}
Sea $A\in\R^{m\times n}$ y consideramos la función $T$:
\begin{align*}
\begin{array}{rccc}
    T : &\R^n &\to &\R^m \\
        &v &\mapsto &Av.
\end{array}
\end{align*}
Entonces $T$ es una transformación lineal. 
\end{observacion}
\begin{proof}
    Debemos ver que $T$ respeta suma y producto por escalares.
Sean $v_1,v_2\in\R^n$ y $\lambda\in\R$ entonces
\begin{align*}
T(v_1+\lambda v_2)=A(v_1+\lambda v_2)=
Av_1+\lambda Av_2=T(v_1)+\lambda T(v_2)
\end{align*}
\end{proof}

\begin{definicion}
    Sea $A\in\R^{m\times n}$ y sea $T$ la transformación lineal 
    \begin{align*}
        \begin{array}{rccc}
            T : &\R^n &\to &\R^m \\
                &v &\mapsto &Av.
        \end{array}
        \end{align*}
        Diremos que $T$  es \textit{la transformación lineal asociada a $A$} o  \textit{la transformación lineal inducida por $A$}. Muchas veces denotaremos a esta transformación lineal con el mismo símbolo que la matriz, es decir, en este caso con $A$.  
\end{definicion}



\begin{ejemplo*}
    Consideremos la matriz $A=\begin{bmatrix}   1&1&1\\   2&2&2 \end{bmatrix}$.
    

    
    Entonces si $v =(x,y,z)$, 
    \begin{align*}
    A(v) &= \begin{bmatrix} 1&1&1\\2&2&2 \end{bmatrix} \begin{bmatrix}  x\\y\\z \end{bmatrix}
    = \begin{bmatrix}  x+y+z\\2x+2y+2z \end{bmatrix}           
    \end{align*}
    

    
    En particular,
    $(1,-1,0)\in\nu(A)$ pues $A(1,-1,0)=0$ y
    \begin{align*}
    A(1,0,0)&=(1,2)\in\im(A)\\
    A(0,1,\pi)&=(1+\pi,2+2\pi)\in\im(A)
    \end{align*}
\end{ejemplo*}


\begin{observacion}\label{obs-nu-im} 	Sea $T: \K^n \to \K^m$ definida por
    \begin{align*}
    T(x_1,\ldots,x_n) &= (a_{11}x_1+\cdots + a_{1n}x_n,\, \ldots\,,a_{m1}x_1+\cdots + a_{mn}x_n )
    \end{align*}
    con $a_{ij} \in \K$, entonces 
    \begin{equation*}
        T(x) = \begin{bmatrix*}
            a_{11}& a_{12} & \cdots &a_{1n} \\
            a_{21}& a_{22} & \cdots &a_{2n} \\
            \vdots&\vdots&\ddots&\vdots \\
            a_{m1}& a_{m2} & \cdots &a_{mn}
        \end{bmatrix*}
        \begin{bmatrix}
            x_1 \\ x_2 \\ \vdots \\ x_n
        \end{bmatrix}
    \end{equation*} 

    Es decir,  $T$ es la transformación lineal inducida por la matriz $A = [a_{ij}]$. 
 
    Esto, en particular,  demuestra la observación  \ref{obs-tl-1.5}.
\end{observacion}



\begin{proposicion}

    Sea $A\in\R^{m\times n}$ y $T:\R^n\longrightarrow\R^m$ la transformación lineal asociada. Entonces
    \begin{itemize}
     \item El núcleo de $T$ es el conjunto de soluciones del sistema homogéneo $AX=0$
     
     \item La imagen de $T$ es el conjunto de los $b\in\R^m$ para los cuales el sistema $AX=b$ tiene solución
    \end{itemize}
    \end{proposicion}
    \begin{proof}
        
   
    Se demuestra fácilmente escribiendo las definiciones de los respectivos subconjuntos.
    \begin{align*}
    v\in\nu(T)\Leftrightarrow Av=0\Leftrightarrow v\,\mbox{ es solución de $AX=0$.}
    \end{align*}
    \begin{align*}
    b\in\im(T)&\Leftrightarrow\exists v\in\R^n\,\mbox{ tal que } Av=b 
    \Leftrightarrow\,\mbox{$AX=b$ tienen solución}.
    \end{align*}
\end{proof}
    
        
\begin{ejemplo*}
        Sea $T: \R^3 \to \R^4$, definida
        $$
        T(x,y,z) = (x +y ,\,x +2y +z,\,3y +3z,\,2x +4y +2z).
        $$
        \begin{enumerate}
            \item Describir $\nuc(T)$  en forma paramétrica y dar una base.
            \item Describir $\img(T)$  en forma paramétrica y  dar una base. 
        \end{enumerate}
        \end{ejemplo*}
\begin{proof}[Solución]
    La matriz asociada a esta transformación lineal  es 
    \begin{equation*}
        A = \begin{bmatrix}
        1&1&0\\1&2&1\\0&3&3\\2&4&2
        \end{bmatrix}
    \end{equation*}

    Debemos encontrar la descripción paramétrica de
    \begin{align*}
        \nuc(T) &= \{v=(x,y,z):   A.{v}=0\}\\
        \img(T) &= \{y= (y_1,y_2,y_3,y_4): \text{ tal que } \exists v \in \R^3, A.{v} = {y}  \}
        \end{align*}

    En  ambos casos, la solución depende de resolver el sistema de ecuaciones cuya matriz asociada es $A$:
    \begin{align*}
    \left[\begin{array}{@{}*{3}{c}|c@{}}1&1&0&y_1\\1&2&1&y_2\\0&3&3&y_3\\2&4&2&y_4 \end{array}\right]
    &\underset{F_4-2F_1}{\stackrel{F_2 -F_1}{\longrightarrow}} 
    \left[\begin{array}{@{}*{3}{c}|c@{}}1&1&0&y_1\\0&1&1&-y_1+y_2\\0&3&3&y_3\\0&2&2&-2y_1 +y_4 \end{array}\right]\\
    &\underset{F_4-2F_2}{\underset{F_3-3F_2}{\stackrel{F_1 -F_2}{\longrightarrow}} } 
    \left[\begin{array}{@{}*{3}{c}|c@{}}1&0&-1&2y_1-y_2\\0&1&1&-y_1+y_2\\0&0&0&3y_1-3y_2+y_3\\0&0&0&-2y_2 +y_4 \end{array}\right].
    \end{align*}
Luego, 
    \begin{equation*}\label{eq-gen}
    T(x,y,z) = (y_1,y_2,y_3,y_4) \quad\Leftrightarrow \quad
    \left\{\begin{array}{rl}
    x -z &= 2y_1-y_2\\ 
    y +z &= -y_1+y_2\\
    0&=3y_1-3y_2+y_3 \\
    0&= -2y_2 +y_4
    \end{array}\right.\tag{*}
    \end{equation*}
    Si hacemos $y_1 = y_2 = y_3 = y_4 = 0$, entonces las soluciones del sistema describen el núcleo de $T$, es decir
    \begin{align*}
    \nuc(T) &= \{(x,y,z):x-z=0, y+z =0 \} = \{(s,-s,s):s \in \R \} \\
    &= \{s(1,-1,1):s \in \R \}
    \end{align*}
    que es la forma paramétrica del $\nu(T)$. Una base del núcleo de $T$  es $\{(1,-1,1)\}$. 


    En  el sistema (*) las dos primeras ecuaciones no imponen ninguna restricción sobre los $y_i$ (por ejemplo si hacemos $z=0$ resulta $x = 2y_1-y_2$,  $y= -y_1+y_2$). Claramente, las últimas dos ecuaciones sí establecen condiciones sobre los $y_i$ y resulta entonces que   
    \begin{align*}
        \img(T) &=  \{(y_1,y_2,y_3,y_4): \text{ tal que $0=3y_1-3y_2+y_3$ y $0= -2y_2 +y_4$} \} 
    \end{align*}
    
    
    Resolviendo este sistema, obtenemos
    \begin{align*}
        \img(T) &=  \{(-\frac13 s + \frac12 t, \frac 12 t, s,t): s,t \in \R \}\\
        &=  \{s(-\frac13,0,1,0)+t(\frac12,\frac12,0,1): s,t \in \R \}
    \end{align*}
    que es la descripción paramétrica $\img(T)$.  Es claro que  $\{(-\frac13,0,1,0),(\frac12,\frac12,0,1) \}$ es una base de $\img(T)$.
\end{proof}

        He aquí uno de los resultados más importantes del álgebra lineal.
        
        \begin{teorema}\label{rang+nul=dimV}
                 Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal. Suponga que $V$ es de dimensión finita. Entonces 
                 $$
                 rango (T) + nulidad (T) = \dim V.
                 $$
        \end{teorema}
        \begin{proof} Sean 
            \begin{align*}
                n &= \dim V \\
                k &= \dim(\nuc T) = nulidad(T).  
            \end{align*}
            Entonces debemos probar  que 
            $$
            n-k = \dim(\img T) = rango(T).
            $$
             Sea $\{v_1,\ldots,v_k \}$ una base de $\nuc T$. Existen vectores $\{v_{k+1},\ldots,v_n \}$, en $V$ tales que $\{v_1,\ldots,v_n \}$ es una base de $V$. Para probar el teorema, demostraremos que 
             $\{Tv_{k+1},\ldots,Tv_n \}$ es una base para la imagen de $T$. 
             
              \textit{(1) $\{Tv_{k+1},\ldots,Tv_n \}$ genera la imagen de $T$.} 
              
              Si $w \in \img(T)$,  entonces existe $v \in V$ tal que $T(v)=w$, como $\{v_{1},\ldots,v_n \}$ es base de $V$,  existen $\lambda_1,\ldots,\lambda_{n} \in \K$,  tal que $v=  \lambda_1 v_1+ \cdots + \lambda_n v_n$, por lo tanto 
             \begin{align*}
                 w &= T(v) \\ 
                 &=  \lambda_1T( v_1)+ \cdots + \lambda_kT( v_k)+ \lambda_{k+1}T( v_{k+1}) +\cdots+ \lambda_nT( v_n) \\
                 &=  0+ \cdots + 0+ \lambda_{k+1}T( v_{k+1}) +\cdots+ \lambda_nT( v_n) \\
                 &=  \lambda_{k+1}T( v_{k+1}) +\cdots+ \lambda_nT( v_n).
             \end{align*}
             Por  lo tanto, $\{Tv_{k+1},\ldots,Tv_n \}$ genera la imagen de $T$. 
             
             \textit{(2) $\{Tv_{k+1},\ldots,Tv_n \}$ es un conjunto linealmente independiente.}
             
             Para ver que  $\{Tv_{k+1},\ldots,Tv_n \}$ es linealmente independiente, suponga que se tienen escalares $\mu_i$ tales que
             $$
             \sum_{i=k+1}^n \mu_i Tv_i = 0,
             $$
             luego
             $$
             0 = \sum_{i=k+1}^n \mu_i Tv_i =   T(\sum_{i=k+1}^n\mu_iv_i).  
             $$
             Por lo tanto $v =\sum_{i=k+1}^n \mu_iv_i\in \nuc(T)$. Como  $\{v_1,\ldots,v_k \}$ es una base de $\nuc T$,  existen escalares $\lambda_i$ tales que
             $$
             v =  \sum_{i=1}^k \lambda_i v_i,
             $$
             es decir
             $$
             \sum_{j=k+1}^n \mu_jv_j =  \sum_{i=1}^k \lambda_i v_i.
             $$
             Luego
             \begin{align*}
                     0 &= \sum_{i=1}^k \lambda_i v_i - (\sum_{j=k+1}^n \mu_jv_j) \\
                     &= \lambda_1 v_1 + \cdots +\lambda_k v_k - \mu_{k+1}v_{k+1} -\cdots-\mu_nv_n.
             \end{align*}
             Como $\{v_1,\ldots,v_n \}$ es una base, y por lo tanto un conjunto LI,  tenemos que  $0=\lambda_1=\cdots=\lambda_k=\mu_{k+1}=\cdots=\mu_n$, y  en particular $0=\mu_{k+1}=\cdots=\mu_n$. Por lo tanto $\{Tv_{k+1},\ldots,Tv_n \}$ es un conjunto linealmente independiente.		  
        \end{proof}
        
    
        
        Sea $A$ una matriz $m \times n$ con coeficientes  en $\K$. El  \textit{rango fila}\index{rango  fila} de $A$ es la dimensión del subespacio de $\K^n$ generado por las filas de $A$, es decir la dimensión del espacio fila de $A$. El \textit{rango columna}\index{rango  columna} de $A$  es es la dimensión del subespacio de $\K^m$ generado por las columna de $A$. Un  consecuencia importante del teorema \ref{rang+nul=dimV} es le siguiente resultado. 
        
        \begin{teorema}
            Si $A$ es una matriz $m \times n$ con coeficientes  en $\K$, entonces
            $$
            \text{\textit{rango fila }} (A) = \text{\textit{rango  columna }} (A).
            $$
        \end{teorema}
        \begin{proof}
            Sea $T$ la transformación lineal
            \begin{equation*}
                \begin{array}{lllll}
                &T: &\K^{n \times 1} &\to &\K^{m \times 1} \\
                &&X &\mapsto &AX.
                \end{array}
            \end{equation*}
            Observar que
            $$
            \nuc(T) = \{X \in \K^{n \times 1}: AX=0 \}.
            $$
            Es decir $\nuc(T)$  es el subespacio de soluciones del sistema homogéneo $AX=0$. Ahora bien, si $k = \text{\textit{rango fila }} (A)$, ya hemos dicho (capítulo \ref{chap-esp-vect}, sección \ref{seccion-dimensiones-de-subespacios}) que la dimensión del subespacio de soluciones del sistema homogéneo $AX=0$ es $n-k$. Luego
            \begin{equation}\label{rangofila=n-nulT}
                \text{\textit{rango fila }} (A) = \dim V - \text{\textit{nulidad}}(T). 
            \end{equation}
            
            Por otro lado 
            $$
            \img(T) = \{AX: X \in \K^{n \times 1} \}.
            $$
            Ahora bien, 
            $$
            AX = 
            \begin{bmatrix} a_{11} x_1 +\cdots a_{1n} x_n \\ \vdots \\ a_{m1} x_1 +\cdots a_{mn} x_n  \end{bmatrix}
            = 
            x_1\begin{bmatrix} a_{11} \\ \vdots \\ a_{m1} \end{bmatrix} + \cdots +
            x_n\begin{bmatrix}  a_{1n}  \\ \vdots \\ a_{mn}  \end{bmatrix}
            $$
            Es decir, que la imagen de $T$ es el espacio generado por las columnas de $A$. Por tanto,
            $$
            \text{\textit{ rango}}(T) =\text{\textit{ rango  columna }}(A).
            $$
            Por  el  teorema \ref{rang+nul=dimV}
            $$
            \text{\textit{ rango}}(T) = \dim V - \text{\textit{nulidad}}(T),
            $$
            y por lo tanto
            \begin{equation}\label{rangocolumna=n-nulT}
            \text{\textit{rango columna }} (A) = \dim V - \text{\textit{nulidad}}(T). 
            \end{equation}
            
            Obviamente, las igualdades (\ref{rangofila=n-nulT}) y (\ref{rangocolumna=n-nulT}) implican 
            $$
        \text{\textit{rango fila }} (A) = \text{\textit{rango  columna }} (A).
        $$
        \end{proof}
    
        \begin{definicion}
                Si $A$ es una matriz $m \times n$ con coeficientes  en $\K$,  entonces el \textit{rango}\index{rango de una matriz}\index{matriz!rango} de $A$ es el rango fila de $A$ (que es igual al rango columna).
        \end{definicion}
        
        \end{section}
    
        \begin{section}{Isomorfismos de espacios vectoriales}\label{seccion-isomorfismos-de-ev}
        
        \begin{definicion}
            Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal.
            \begin{enumerate}
                \item $T$  es \textit{epimorfismo}\index{epimorfismo} si $T$ es suryectiva, es decir si $\img(T) = W$.
                \item $T$ es \textit{monomorfismo}\index{monomorfismo} si $T$ es inyectiva (o 1-1),  es decir si dados $v_1,v_2 \in V$ tales que $T(v_1) = T(v_2)$,  entonces $v_1 = v_2$.
            \end{enumerate}  
        \end{definicion}
    
        \begin{obs*}
            $T$  es epimorfismo si y sólo si 
            $$
            \text{$T$ es lineal y }\forall\, w \in W, \; \exists v \in V \text{ tal que }T(v)=w.
            $$
            Esto se deduce inmediatamente de la definiciones de función suryectiva y de $\img(T)$
            
            $T$ es monomorfismo si y sólo si 
            $$
                \text{$T$ es lineal y }\forall\, v_1,v_2  \in V: \; v_1 \ne v_2 \Rightarrow T(v_1) \not= T(v_2).
            $$ 
            Esto se obtiene aplicando el contrarrecíproco a la definición de función inyectiva.
        \end{obs*}	
        
        
            
        \begin{proposicion}\label{inyectiva-sii-nuT=0}
            Sea $T:V \to W$ una transformación lineal. Entonces $T$ es monomorfismo si y sólo si $\nuc(T) =0$.
        \end{proposicion}	
        \begin{proof} 
            
            ($\Rightarrow$) Debemos ver que  $\nuc(T)=0$,  es decir que si $T(v)=0$,  entonces  $v=0$. Ahora bien,  si $T(v) = 0$, como  $T(0)=0$, tenemos que $T(v)  = T(0)$, y como $T$ es inyectiva, implica que $v =0$.
            
            ($\Leftarrow$) Sean  $v_1,v_2 \in V$ tal que $T(v_1)=T(v_2)$. Entonces 
            $$
            0 = T(v_1)- T(v_2) = T(v_1 -v_2).
            $$
            Por  lo tanto, $v_1 -v_2 \in \nuc(T)$. Por hipótesis, tenemos que $v_1 -v_2 =0$,  es decir $v_1 = v_2$.
        \end{proof}
        
        
        \begin{obs*} Sea $T: V \to W$ transformación lineal, 
            \begin{enumerate}
                \item $T$  es {epimorfismo} si y sólo  si $\img(T) = W$ si  y solo si $rango(T) = \dim W$.
                \item $T$ es {monomorfismo} si y sólo  si $\nuc(T) = 0$ si y sólo si $nulidad(T) =0$.
            \end{enumerate}  
        \end{obs*}		
        
        \begin{proposicion}\label{prop-T-mono-sii-li-2-li} Sea $T:V \to W$ transformación lineal. Entonces,
            \begin{enumerate}
                \item\label{itm-T-mono-sii} $T$ es monomorfismo si y sólo si $T$ de un conjunto LI  es  LI.
                \item\label{itm-T-epi-sii} $T$ es epimorfismo si y sólo si $T$ de un conjunto de generadores de $V$ es un conjunto de generadores de $W$.
            \end{enumerate}
        \end{proposicion}
        \begin{proof} Haremos la demostración para el caso de dimensión finita,  pero en el caso general la demostración es similar. 
            
            
            \ref{itm-T-mono-sii} ($\Rightarrow$) Sea $\{v_1,\ldots,v_n \}$ un conjunto LI en $V$ y sean  $\lambda_1,\ldots,\lambda_n \in \K$ tales que
            $$
            \lambda_1T(v_1) +\cdots+ \lambda_{n}T(v_n) =0,
            $$
            entonces
            $$
            0 = T(\lambda_1v_1+\cdots + \lambda_{n}v_n).
            $$
            Como $T$  es inyectiva, por proposición \ref{inyectiva-sii-nuT=0}, 
            $$
            \lambda_1v_1+\cdots + \lambda_{n}v_n =0,
            $$
            lo cual implica que $\lambda_1,\ldots,\lambda_n$ son todos nulos. Por lo tanto,  $T(v_1),\ldots,T(v_n)$ son LI.
            
            
           \ref{itm-T-mono-sii} ($\Leftarrow$) Sea $v \in V$ tal que $T(v)=0$. Veremos que eso implica que $v =0$.  Ahora bien, sea  $\{v_1,\ldots,v_n \}$ una base de $V$,  entonces existen $\lambda_1,\ldots,\lambda_n \in \K$ tales que
            $$
            v = \lambda_1v_1+\cdots + \lambda_{n}v_n,
            $$
            por lo tanto
            $$
            0 = T(v) = T(\lambda_1v_1+\cdots + \lambda_{n}v_n) = \lambda_1T(v_1) +\cdots+ \lambda_{n}T(v_n).
            $$
            Como $\{v_1,\ldots,v_n \}$ es LI, por hipótesis, $\{T(v_1),\ldots,T(v_n)\}$ es LI y, por lo tanto, $\lambda_1,\ldots,\lambda_n$ son todos nulos. Luego $v=0$. Es decir probamos  que el núcleo de $T$ es 0, luego por proposición \ref{inyectiva-sii-nuT=0}, $T$ es monomorfismo. 

        \ref{itm-T-mono-sii} ($\Leftarrow$ \textit{alternativa}) Sea $v \in V$ tal que $T(v)=0$.	Si $v\neq 0$, entonces $\{v\}$ es un conjunto LI en $V$. Luego, $\{T(v)\}$ es un conjunto LI en $W$ y por lo tanto $T(v)\neq 0$. Así, si $T(v)=0$ entonces $v=0$ y por lo tanto $T$ es un monomorfismo.
              
            \ref{itm-T-epi-sii} ($\Rightarrow$) Sea   $\{v_1,\ldots,v_n \}$ un conjunto de generadores de $V$ y sea  $w \in W$. Como $T$  es epimorfismo, existe $v \in V$ tal que $T(v)=w$. Ahora bien, 
            $$
            v = \lambda_1v_1+\cdots + \lambda_{n}v_n,\, \text{ para algún $\lambda_1,\ldots,\lambda_n \in \K$,}
            $$
            por lo tanto,
            $$
            w =T(v) = T(\lambda_1v_1+\cdots + \lambda_{n}v_n) = \lambda_1T(v_1) +\cdots+ \lambda_{n}T(v_n).
            $$ 
            Es decir,  cualquier $w \in W$ se puede escribir como combinación lineal de los  $T(v_1),\ldots,T(v_n)$ y, por lo tanto,  generan $W$.
            
             \ref{itm-T-epi-sii} ($\Leftarrow$) Sea $\{v_1,\ldots,v_n \}$ una base de $V$, por hipótesis $T(v_1),\ldots,T(v_n)$ generan $W$,  es decir dado cualquier $w \in W$,   existen $\lambda_1,\ldots,\lambda_n \in \K$ tales que
            $$
            w = \lambda_1T(v_1)+\cdots + \lambda_{n}T(v_n),
            $$
            y por lo tanto $w = T(v)$,  con 
            $$
            v = \lambda_1v_1+\cdots + \lambda_{n}v_n.
            $$
        \end{proof}
        
            

        \begin{definicion}
            Si $V$ y $W$ son espacios vectoriales sobre el cuerpo $\K$, toda transformación	lineal $T:V \to W$ suryectiva e inyectiva, se dice \textit{isomorfismo}\index{isomorfismo} de $V$ sobre $W$. Si existe un isomorfismo de $V$ sobre $W$, se dice que $V$ es \textit{isomorfo} a $W$ y se denota $V \cong W$.
        \end{definicion}
        
        
        
        Observe que $V$ es trivialmente isomorfo a $V$, ya que el operador identidad es un isomorfismo de $V$ sobre $V$. 

        \begin{proposicion}
            Sea $T:V \to W$ transformación lineal. Entonces $T$ es un isomorfismo si y solo si $T$ de una base de $V$ es una base de $W$.
        \end{proposicion}
        \begin{proof} 

         ($\Rightarrow$) 
           Sea $ \mathcal B$ base de $V$.  Como $T$ es isomorfismo, $T$ es mono y epi, luego por proposición \ref{prop-T-mono-sii-li-2-li}, $T(\mathcal B)$ es LI y genera $W$,  es decir,  es base de $W$.

           ($\Leftarrow$) Sea $ \mathcal B$ base de $V$ y $T:V \to W$ transformación lineal tal que $T(\mathcal B)$ es base. Por lo tanto, manda un conjunto LI a un conjunto LI y un conjunto de generadores de $V$ a un conjunto de generadores de $W$. Por proposición \ref{prop-T-mono-sii-li-2-li}, $T$ es mono  y epi, por lo tanto $T$ es un isomorfismo.
        \end{proof}

        \begin{corolario}
            Sean $V$ y $W$ dos  $\K$-espacios vectoriales de dimensión finita tal que $V$ es isomorfo a $W$. Entonces $\dim(V) = \dim(W)$.
        \end{corolario}
        \begin{proof}
        Como $V$  es isomorfo  a $W$,  existe un isomorfismo $T: V \to W$. Por la proposición anterior si $v_1,\ldots,v_n$ es base de $V$,  entonces $T(v_1),\ldots,T(v_n)$ es base de $W$. Por lo tanto, $\dim(V) = n = \dim(W)$.
        \end{proof}


        Recordemos que si una función $f: X \to Y$ es suryectiva e inyectiva, es decir biyectiva, existe su inversa, la cual también es biyectiva. La inversa se denota $f^{-1}: Y \to X$ y viene definida por
        $$
        f^{-1}(y) = x \Leftrightarrow f(x) =y.
        $$ 
        
        \begin{teorema}
            Sea $T:V \to W$ un isomorfismo. Entonces $T^{-1}: W \to V$ es lineal y, por lo tanto, también es un isomorfismo.
        \end{teorema}
        \begin{proof}
            \
            
            Sean $w_1, w_2 \in W$, probemos que $ T^{-1}(w_1+w_2) =  T^{-1}(w_1)+ T^{-1}(w_2)$. 
            
            Sean $v_1 = T^{-1}(w_1) $, $v_2 = T^{-1}(w_2)$. Por lo tanto $T(v_1) = w_1$ y $T(v_2) = w_2$. Ahora bien,
            \begin{multline*}
                T^{-1}(w_1+w_2) = 	T^{-1}(T(v_1)+T(v_2))  = 	T^{-1}(T(v_1+v_2)) = \\ =(T^{-1}\circ T)(v_1+v_2) = v_1+v_2 = T^{-1}(w_1)+ T^{-1}(w_2). 
            \end{multline*}    
            
            
            Sean $w \in W$ y $\lambda  \in \K$, probemos que $T^{-1}(\lambda w) =\lambda  T^{-1}(w)$. 
            
            Sea $v = T^{-1}(w)$, entonces
            \begin{equation*}
            T^{-1}(\lambda w) = 	T^{-1}(\lambda T(v))  = 	T^{-1}(T(\lambda v)) = (T^{-1}\circ T)(\lambda v) = \lambda v = \lambda  T^{-1}(w). 
            \end{equation*}
                \end{proof}
        
        



        \begin{ejercicio*} 
            Sean $V$, $W$ y $Z$ espacios vectoriales sobre el cuerpo $\K$ y sean $T:V \to W$, $S:W \to Z$ isomorfismos. Entonces, 
            \begin{enumerate}
                \item $S\circ T:V \to Z$  también es un isomorfismo y
                \item 	$(S\circ T)^{-1} = T^{-1}\circ S^{-1}$.
            \end{enumerate}
        \end{ejercicio*}
        
        


        Como ya se ha dicho, $V$ es isomorfo a $V$ vía la identidad. Por  el teorema anterior, si $V$ es isomorfo a $W$,  entonces $W$ es isomorfo a $V$. Por  el ejercicio anterior, si $V$ es isomorfo a $W$ y $W$ es isomorfo a $Z$, entonces $V$ es isomorfo a $Z$. En resumen, el isomorfismo es una relación de equivalencia sobre la clase de espacios vectoriales. Si existe un isomorfismo de $V$ sobre $W$, se dirá a veces que \textit{$V$ y $W$ son isomorfos}, en vez de que $V$ es isomorfo a $W$. Ello no será motivo de confusión porque $V$ es isomorfo a $W$, si, y solo si, $W$ es isomorfo a $V$.	
            
        
   
            
            
        \begin{teorema}
            Sean $V,W$ espacios vectoriales de dimensión  finita sobre $\K$ tal que $\dim V = \dim W$. Sea $T: V \to W$ transformación lineal. Entonces,  son equivalentes:
            \begin{enumerate}[label=(\alph*),ref=\alph*]
                \item\label{a-dimV=dimW} $T$ es un  isomorfismo.
                \item\label{b-dimV=dimW} $T$ es monomorfismo.
                \item\label{c-dimV=dimW} $T$ es epimorfismo.
                \item\label{d-dimV=dimW} Si $\{v_1,\ldots,v_n \}$ es una base de $V$,  entonces $\{T(v_1),\ldots,T(v_n) \}$ es una base de $W$.
            \end{enumerate}
        \end{teorema}
        \begin{proof}[Demostración (*)] Sea $n = \dim V = \dim W$.
            
            
            (\ref{a-dimV=dimW}) $\Rightarrow$ (\ref{b-dimV=dimW}). Como $T$ es isomorfismo,  es biyectiva y por lo tanto inyectiva.
            
            (\ref{b-dimV=dimW}) $\Rightarrow$ (\ref{c-dimV=dimW}). $T$ monomorfismo,  entonces $nulidad(T) = 0$ (proposición \ref{inyectiva-sii-nuT=0}). Luego, como  $rango(T) +nulidad(T) = \dim V$,  tenemos que $rango(T) = \dim V$. Como $\dim V = \dim W$, tenemos que $\dim \img(T) = \dim W$ y por lo tanto $\img(T) = \dim W$. En  consecuencia, $T$ es suryectiva.
            
            (\ref{c-dimV=dimW}) $\Rightarrow$ (\ref{a-dimV=dimW}). $T$ es suryectiva, entonces $rango(T) = n$, luego  $nulidad(T) = 0$, por lo tanto $\nuc(T)=0$ y en consecuencia $T$ es inyectiva. Como $T$ es suryectiva e inyectiva  es un isomorfismo. 
            
            
            
            Hasta aquí probamos que (\ref{a-dimV=dimW}), (\ref{b-dimV=dimW}) y (\ref{c-dimV=dimW}) son equivalentes, luego si probamos que (\ref{a-dimV=dimW}), (\ref{b-dimV=dimW}) o (\ref{c-dimV=dimW}) $\Rightarrow$ (\ref{d-dimV=dimW}) y que (\ref{d-dimV=dimW}) $\Rightarrow$ (\ref{a-dimV=dimW}), (\ref{b-dimV=dimW}) o (\ref{c-dimV=dimW}), estaría probado el teorema. 
            
          		
            
            (\ref{a-dimV=dimW}) $\Rightarrow$ (\ref{d-dimV=dimW}). Sea $\{v_1,\ldots,v_n \}$  una base de $V$,  entonces $\{v_1,\ldots,v_n \}$ es LI y  genera $V$. Por proposición \ref{prop-T-mono-sii-li-2-li}, tenemos que $\{T(v_1),\ldots,T(v_n) \}$ es LI y  genera $W$, por lo tanto $\{T(v_1),\ldots,T(v_n) \}$ es una base de $W$.
            
            (\ref{d-dimV=dimW}) $\Rightarrow$ (\ref{a-dimV=dimW}). Como $T$ de una base es una base,  entonces $T$  de un conjunto LI es un conjunto LI y $T$ de un conjunto de generadores de $V$  es un conjunto de generadores de $W$. Por lo tanto, por proposición \ref{prop-T-mono-sii-li-2-li}, $T$ es monomorfismo y epimorfismo, luego $T$ es un isomorfismo. 	
        \end{proof}
    
        \begin{corolario}
            Sean $V,W$ espacios vectoriales de dimensión  finita sobre $\K$ tal que $\dim V = \dim W$. Entonces $V$ y $W$ son  isomorfos. 
        \end{corolario}
        \begin{proof}
            Sea $\{v_1,\ldots,v_n \}$ es una base de $V$ y $\{w_1,\ldots,w_n \}$ es una base de $W$. Poe teorema \ref{th-tl-definida-en-base} existe una única transformación lineal $T:V \to W$ tal que
            \begin{equation*}
            T(v_i) = w_i, \qquad i=1,\ldots, n.
            \end{equation*}
            Por  el teorema anterior, $T$  es un isomorfismo.
        \end{proof}
    
    
    \begin{ejemplo*} $\K_n[x] = \{a_0+a_1x+\cdots+a_{n-1}x^{n-1}: a_0,a_1, \ldots,a_{n-1} \in \K \}$  es isomorfo a $\K^n$, esto es  consecuencia inmediata del corolario anterior, pues ambos tienen dimensión $n$. Explícitamente, $1,x,\ldots,x^{n-1}$  es base de $\K_n[x]$ y sea $e_1,\ldots,e_n$ la base canónica  de $\K^n$,  entonces un isomorfismo de $\K_n[x]$ a $\K^n$ viene dado por la única transformación lineal $T:\K_n[x] \to\K^n$ tal que
        $$
        T(x^i) = e_{i+1},\qquad i=0,\ldots, n-1.
        $$   
        
    \end{ejemplo*}

    \begin{ejemplo*} $M_{m \times n}(\K)$  es isomorfo a $\K^{mn}$. El isomorfismo viene dado por $T: M_{m \times n}(\K) \to \K^{mn}$ tal que
        $$
        T(E_{ij}) = e_{(i-1)n+j}, \qquad i=1,\ldots, m,\; j=1,\ldots, n.
        $$
    Por  ejemplo, en el caso  $2 \times 2$,
    \begin{equation*}
    \begin{array}{llll}
    \begin{bmatrix} 1&0\\0&0\end{bmatrix} &\mapsto (1,0,0,0) \qquad&
    \begin{bmatrix} 0&1\\0&0\end{bmatrix} &\mapsto (0,1,0,0) \\
    &&&\\
    \begin{bmatrix} 0&0\\1&0\end{bmatrix} &\mapsto (0,0,1,0) &
    \begin{bmatrix} 0&0\\0&0\end{bmatrix} &\mapsto (0,0,0,1).
    \end{array}
    \end{equation*}	 
        
    \end{ejemplo*}
    
        \end{section}
    
        \begin{section}{\'Algebra de las transformaciones lineales (*)}\label{seccion-algebra-de-las-transformaciones-lineales}
            En el estudio de las transformaciones lineales de $V$ en $W$ es de fundamental importancia que el conjunto de estas transformaciones hereda una estructura natural de espacio vectorial. El conjunto de las transformaciones lineales de un espacio $V$ en sí mismo tiene incluso una estructura algebraica mayor, pues la composición ordinaria de funciones da una ``multiplicación'' de tales transformaciones. 
            
            
            Observemos primero que si $X$ conjunto y $W$ espacio vectorial sobre el cuerpo $\K$,  entonces
            $$
            F(X,W) := \{f:X\to W\},
            $$ 
            es decir el conjunto de funciones de $X$ en $W$ es un espacio vectorial sobre $\K$ con la suma y el producto por escalares definido:
            \begin{equation*}
            \begin{array}{rlcl}
            (f+g)(x) &= f(x)+ g(x),\qquad &f,g& \in F(X,W),\; x \in X\\
            (\lambda f)(x) &= \lambda f(x), & f& \in F(X,W),\; x \in X, \; \lambda \in \K.
            \end{array}
            \end{equation*}
            La demostración de esto es sencilla y se basa en el hecho que $W$  es un espacio vectorial. 
            
            \begin{teorema}
                Sean $V$ y $W$ espacios vectoriales sobre el cuerpo $\K$ Sean $T,S : V \to W$ transformaciones y $\mu \in \K$. Entonces, $T + S$ y $\mu T$ son transformaciones lineales de $V$ en $W$.
            \end{teorema}
            \begin{proof}
                Sean $v,v' \in V$ y $\lambda \in \K$, entonces
                \begin{equation*}
                \begin{array}{rlll}
                    (T + S)(\lambda v + v') &= T(\lambda v + v') + S(\lambda v + v')&\qquad&\text{(definición de $T+S$)} \\
                    &= \lambda T(v) + T(v') + \lambda S(v) + S(v')& &\text{($T$ y $S$ lineales)}\\
                    &= \lambda (T(v) +S(v)) + T(v') + S(v')&&\text{}\\
                    &= \lambda ((T+S)(v)) + (T + S) (v')& &\text{(definición de $T+S$)}\\
                    &= \lambda(T + S)(v) +(T + S) (v')&&\text{(definición de $\lambda(T + S)$)}.
                \end{array}
                \end{equation*}
                que dice que $T + U$ es una transformación lineal. En forma análoga, si $\mu \in \K$, 
                \begin{equation*}
                \begin{array}{rlll}
                (\mu T)(\lambda v + v') &= \mu T(\lambda v + v')&\qquad&\text{(definición de $\mu T$)} \\
                &= \mu \lambda T(v) + \mu T(v') &\qquad&\text{($T$ lineal)}\\
                &=  \lambda \mu T(v) + \mu T(v')&\qquad&\text{}\\
                &= \lambda (\mu T)(v) + (\mu T)(v') &\qquad&\text{(definición de $\mu T$)}.
                \end{array}
                \end{equation*}
                que dice que $\mu T$ es una transformación lineal.
            \end{proof}
            
        \begin{corolario}
            Sean $V$ y $W$ espacios vectoriales sobre el cuerpo $\K$. Entonces, el conjunto de transformaciones lineales de $V$ en $W$ es un subespacio vectorial  de $F(V,W)$. 
        \end{corolario} 	
            
        Se denotará  $L(V,W)$ al espacio vectorial de las transformaciones lineales de $V$ en $W$.
        
        \begin{teorema}\label{th-6-hoffman}
            Sean $V$, $W$ y $Z$ espacios vectoriales sobre el cuerpo $\K$.  Sean $T: V \to W$ y $U: W \to Z$ 
            transformaciones lineales. Entonces la función compuesta $U\circ T$ definida por $(U\circ T)(v) = U(T(v))$ es una transformación lineal de $V$ en $Z$.
        \end{teorema} 
        \begin{proof} Sean $v, v' \in V$ y $\lambda \in \K$, entonces
            \begin{equation*}
            \begin{array}{rlll}
                (U\circ T)(\lambda v + v') &= U(T(\lambda v + v'))&\qquad&\text{(definición de composición)} \\
                 &= U(\lambda T( v) + T(v'))&\qquad&\text{($T$ lineal)} \\
                 &= \lambda U(T( v)) + U(T(v'))&\qquad&\text{($U$ lineal)} \\
                 &= \lambda (U\circ T)( v) + (U\circ T)(v')&\qquad&\text{(definición de composición)}. \\
            \end{array}
            \end{equation*}
            
        \end{proof}
        
        Para simplificar, a veces  denotaremos la composición por yuxtaposición,  es decir $$U\circ T = U T.$$
        
        
        En lo que sigue debemos interesarnos principalmente en transformaciones lineales de un espacio vectorial en sí mismo. Como se tendrá a menudo que 	escribir ``$T$ es una transformación lineal de $V$ en $V$'', se dirá más bien: ``$T$ es un operador lineal sobre $V$''. 
        
        \begin{definicion}
            Si $V$ es un espacio vectorial sobre el cuerpo $\K$, un \textit{operador lineal}\index{operador lineal} sobre $V$ es una transformación lineal de $V$ en $V$.
        \end{definicion}
        
        Cuando  en el teorema \ref{th-6-hoffman}, consideramos $V = W = Z$, tenemos que $U$ y $T$ son opera-
        dores lineales en el espacio $V$, y por lo tanto la composición $UT$ es también un operador lineal sobre $V$. Así, el espacio $L(V, V)$ tiene una ``multiplicación'' definida por composición. En este caso el operador $TU$ también está definido, y debe observarse que en general $UT \not= TU$, es decir, $UT - TU \not= 0$. Se ha de advertir de manera especial que si $T$ es un operador lineal sobre $V$, entonces se puede componer $T$ con $T$. Se usará para ello la notación $T^2 = TT$, y en general $T^n = T \cdots T$ ($n$ veces) para $n = 1, 2, 3, \ldots$\,  Si $T \ne 0$, se define $T^0 = \Id_V$, el operador identidad.
            
        \begin{lema}
            Sea $V$ un espacio vectorial sobre el cuerpo $\K$; sean $U$, $T$ y $S$ operadores lineales sobre $V$ y sea $\lambda$ un elemento de $\K$. Denotemos $\Id_V$ el operador identidad. Entonces
            \begin{enumerate}
                \item\label{itm-op-1} $U = \Id_VU = U\Id_V$,
                \item\label{itm-op-2} $U(T+S) = UT + US$, $(T+S)U = TU + SU$,
                \item\label{itm-op-3} $\lambda (UT) = (\lambda U)T = U (\lambda T)$.
            \end{enumerate}
        \end{lema}
        \begin{proof}
            \ref{itm-op-1} es trivial. 
            
            Demostraremos $U(T+S) = UT + US$ de \ref{itm-op-2} y todo lo demás se dejará como ejercicio.
            Sea $ v \in V$, entonces
            \begin{align*}
            U(T+S)(v) &= U((T+S)(v))&\qquad&\text{(definición de composición)} \\
            &= U(T(v)+S(v))&\qquad&\text{(definición  de $T+S$)} \\
            &= U(T(v))+U(S(v))&\qquad&\text{($U$ lineal)} \\
            &= UT(v)+US(v)&\qquad&\text{(definición de composición)}.
            \end{align*}  
        \end{proof}	
    
    El contenido de este lema, y algunos otros resultados  sobre composición de funciones de un conjunto en si mismo (como ser la asociatividad), dicen que el espacio vectorial $L(V, V)$, junto con la operación de composición, es lo que se conoce tomo una  álgebra asociativa sobre $\K$,  con identidad (ver \href{https://es.wikipedia.org/wiki/Álgebra\_asociativa}{https://es.wikipedia.org/wiki/Álgebra\_asociativa}). 
    
    

        \end{section}




        


    \begin{section}{Coordenadas}\label{seccion-coordenadas}
        Una de las características útiles de una base $\mathcal B$ en un espacio vectorial  $V$ de dimensión $n$ es que permite introducir coordenadas en $V$ en forma análoga a las ``coordenadas naturales'', $x_i$, de un vector $v = (x_l,\ldots, x_n)$ en el espacio $\K^n$. En este esquema, las coordenadas de un vector $v$ en $V$, respecto de la base $\mathcal B$, serán los escalares que sirven para expresar $v$ como combinación lineal de los vectores de la base. En  el caso  de la base canónica $e_1,\ldots,e_n$ de $\K^n$ tenemos
        $$
        v = (x_1,\ldots,x_n) = \sum_{i=1}^{n} x_ie_i.
        $$
        por lo tanto $x_i$  es la coordenada $i$-ésima de $v$ respecto a la base canónica. 
        
        En  forma análoga veremos que si $v_1,\ldots,v_n$  es una base de $V$,  entonces existe una única forma de  escribir 
        $$
        v =  \sum_{i=1}^{n} x_iv_i,
        $$ 
        y los valores  $x_i$  serán las \textit{coordenadas de $v$}\index{coordenadas de un vector} en la base dada. 
        
        \begin{definicion}
            Si $V$ es un espacio vectorial de dimensión finita, una \textit{base ordenada}\index{base ordenada} de $V$ es una sucesión finita de vectores linealmente independiente y que genera $V$.
        \end{definicion}
        
        
        La diferencia entre la definición de ``base'' y la de ``base ordenada'',  es que en la última es  importante el orden de los vectores de la base. Si la sucesión $v_1,\ldots,v_n$ es una base ordenada de $V$, entonces el conjunto $\{v_1,\ldots,v_n\}$ es una base de $V$. La base ordenada es el conjunto, juntamente con el orden dado. Se incurrirá en un pequeño abuso de notación y se escribirá
        $$
        \mathcal{B} = \{v_1,\ldots,v_n\}
        $$
        diciendo que $\mathcal{B}$ es una base ordenada de $V$.
        
        \begin{proposicion}
            Sea $V$  espacio vectorial de dimensión finita y sea $\mathcal{B} = \{v_1,\ldots,v_n\}$ una base ordenada de $V$. Entonces, para cada $v \in V$,  existen únicos $x_1,\ldots,x_n \in \K$ tales que $$v =   x_1v_1 + \cdots +x_nv_n.$$
        \end{proposicion}
        \begin{proof}
            Como $v_1,\ldots,v_n$  generan $V$,  es claro que existen $x_1,\ldots,x_n \in \K$ tales que $v =   x_1v_1 + \cdots +x_nv_n$. Sean $y_1,\ldots,y_n \in \K$ tales que $v =   y_1v_1 + \cdots +y_nv_n$. Veremos que $x_i = y_i$ para $1 \le i \le n$.
            
            Como $v =  \sum_{i=1}^{n} x_iv_i$ y $v =  \sum_{i=1}^{n} y_iv_i$,  restando miembro a miembro obtenemos 
            $$
            0 =   \sum_{i=1}^{n} (x_i-y_i)v_i.
            $$
            Ahora bien,  $v_1,\ldots,v_n$ son  LI, por lo tanto todos los coeficientes de la ecuación anterior son nulos, es decir $x_i-y_i=0$ para $1 \le i \le n$ y entonces $x_i = y_i$ para $1 \le i \le n$.
        \end{proof}
    
    La proposición anterior permite, dada una base ordenada,  asociar a cada vector una $n$-tupla que serán la coordenadas del vector en esa base.
    
    \begin{definicion}
    sea $V$  espacio vectorial de dimensión finita y sea $\mathcal{B} = \{v_1,\ldots,v_n\}$ una base ordenada de $V$, si $v \in V$ y $$v =   x_1v_1 + \cdots +x_nv_n,$$  entonces \textit{$x_i$ es la coordenada $i$-ésima de $v$} y denotamos
    $$
    [v]_\mathcal{B} = (x_1,\ldots,x_n).
    $$
    También nos será útil describir a $v$ como una matriz $n \times 1$ y en ese caso hablaremos de \textit{la matriz de $v$  en la base  $\mathcal{B}$}:
    $$
    [v]_\mathcal{B} = \begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix}.
    $$
    (Usamos la misma notación).
    \end{definicion}



    \begin{ejemplo*}
        Sea $\mathcal B = \{(1,-1),(2,3)\}$ base ordenada de $\R^2$. Encontrar las coordenadas  de $(1,0)$ y $(0,1)$ en la base $\mathcal B$.
    \end{ejemplo*}
    \begin{proof}[Solución] Debemos encontrar $x_1, x_2 \in \R$ tal que 
        $$
        (1,0) = x_1(1,-1)+ x_2(2,3).
        $$
        Es decir 
        \begin{align*}
            x_1+ 2x_2 &= 1\\
            -x_1 + 3x_2 &= 0.
        \end{align*}
        Resolviendo el sistema de ecuaciones obtenemos $x_1 = \frac35$ y $x_2 = \frac15$,  es decir
        $$
        (1,0) =\; \frac35(1,-1)+ \frac15(2,3)\quad \text{o equivalentemente} \quad (1,0) = (\;\frac35,\frac15)_{\mathcal B}.
        $$ 
        De forma análoga podemos ver que
        $$
        (0,1) = -\frac25(1,-1)+ \frac15(2,3)\quad \text{o equivalentemente} \quad (0,1) = (-\frac25,\frac15)_{\mathcal B}.
        $$
    \end{proof}
    
    \begin{proposicion}\label{vectorbase->lineal}
        Sea $\mathcal{B}=\{v_1,\ldots,v_n\}$ una base ordenada de $V$ un $\K$-espacio vectorial. Entonces
        \begin{enumerate}
            \item\label{itm-coor-1} $[v + w]_\mathcal{B} = [v]_\mathcal{B} +[w]_\mathcal{B}$, para $v,w \in V$,
            \item\label{itm-coor-2} $[\lambda v]_\mathcal{B} = \lambda[v]_\mathcal{B}$, para $\lambda \in \K$ y $v \in V$.
        \end{enumerate}
    \end{proposicion} 
    \begin{proof}
        \ref{itm-coor-1} Si $v = x_1v_1 + \cdots +x_nv_n$ y $w = y_1v_1 + \cdots +y_nv_n$, entonces 
        $$
        v + w = (x_1+y_1)v_1 + \cdots +(x_n+y_n)v_n,
        $$
        luego
        $$
        [v + w]_\mathcal{B} = \begin{bmatrix}x_1+y_1 \\ \vdots \\ x_n+y_n\end{bmatrix}
        = \begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix}+\begin{bmatrix}y_1 \\ \vdots \\ y_n\end{bmatrix} = [v]_\mathcal{B} +[w]_\mathcal{B}.
        $$
        
        \ref{itm-coor-2} Si $v = x_1v_1 + \cdots +x_nv_n$ y $\ \in \K$, entonces 
        $$
        \lambda v = (\lambda x_1)v_1 + \cdots +(\lambda x_n)v_n,
        $$
        luego
        $$
        [\lambda v ]_\mathcal{B} = \begin{bmatrix}\lambda x_1 \\ \vdots \\ \lambda x_n\end{bmatrix}
        = \lambda \begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix} = \lambda [v]_\mathcal{B}.
        $$
    \end{proof}
    
    \begin{comment}
    
    \begin{teorema}
            Sea $V$  espacio vectorial de dimensión $n$  sobre el cuerpo $\K$, 	y sean $	\mathcal{B} = \{v_1,\ldots,v_n\}$ y $\mathcal{B'} = \{v'_1,\ldots,v'_n\}$ bases ordenadas de $V$. Entonces existe una única matriz $P$ de orden $n \times n$, invertible, tal que para todo $v \in V$
            \begin{enumerate}
                \item\label{itm-coor-sum} $[v]_\mathcal{B} = P[v]_\mathcal{B'}$ y
                \item\label{itm-coor-prod} $[v]_\mathcal{B'} = P^{-1}[v]_\mathcal{B}$.
            \end{enumerate} 
            Las columnas de $P$ están dadas por
            $$
            C_j = [v'_j]_\mathcal{B},\qquad 1 \le j \le n,
            $$
            es decir, los coeficientes de la columna $j$ de $P$ son las coordenadas de $v'_j$ en la base $\mathcal{B}$. 
    \end{teorema}
    \begin{proof} \ref{itm-coor-sum} Tenemos
        \begin{align*}
            v &=   x_1v_1 + \cdots +x_nv_n, \\
            v &=  x'_1v'_1 + \cdots +x'_nv'_n.
        \end{align*}
         También podemos escribir cada $v'_j$ en coordenadas respecto a $\mathcal B$:
         $$
         v'_j =  p_{1j}v_1 + \cdots +p_{nj}v_n, \qquad  1 \le j \le n.
         $$ 
        Luego, 
        \begin{align*}
             \sum_{i=1}^{n}x_iv_i &=  \sum_{j=1}^{n}x'_jv'_j \\
            &=  \sum_{j=1}^{n}x'_j(\sum_{i=1}^{n}p_{ij}v_i) \\
            &=  \sum_{i=1}^{n}\sum_{j=1}^{n}x'_jp_{ij}v_i \\
            &=  \sum_{i=1}^{n}(\sum_{j=1}^{n}p_{ij}x'_j)v_i.			
        \end{align*}
        Es decir, $x_i = \sum_{j=1}^{n}p_{ij}x'_j$. Por lo tanto, si $P = [p_{ij}]$ tenemos que
        $$
        \begin{bmatrix}x_1 \\ x_2\\ \vdots \\ x_n\end{bmatrix} = 
        \begin{bmatrix}p_{11} &p_{12} & \ldots & p_{1n} \\p_{21} &p_{22} & \ldots & p_{2n} \\ \vdots &\vdots && \vdots\\ p_{n1} &p_{n2} & \ldots & p_{nn}\end{bmatrix}
         \begin{bmatrix}x'_1 \\ x'_2\\ \vdots \\ x'_n\end{bmatrix}.
        $$
        
        Veamos que $P$  es invertible: sea $(x_1,\ldots,x_n)$ solución del sistema de ecuaciones $PX =0$. Eso implica que si $v =\sum x_i v'_i$,  entonces $P[v]_{\mathcal B'}=0$.     Eso quiere decir que el vector  escrito en la base $\mathcal B$ tiene coordenadas $0$,  es decir $v = \sum 0. v_i =0$ y entonces $v = \sum 0. v'_i$ por la unicidad de escritura. Por lo tanto, la única solución del sistema $PX=0$ es 0 y en consecuencia $P$  es invertible.
        
        \ref{itm-coor-prod}  Como $P$ es invertible, multiplicamos por $P^{-1}$  a izquierda la ecuación $[v]_\mathcal{B} = P[v]_\mathcal{B'}$ y obtenemos el resultado.
    \end{proof}

    Observemos que, por simetría,  los coeficientes de la columna $j$ de $P^{-1}$ son las coordenadas de $v_j$ en la base $\mathcal{B'}$. 
    
    \begin{definicion} 
        Sea $V$  espacio vectorial de dimensión $n$  sobre el cuerpo $\K$, 	y sean $\mathcal{B} = \{v_1,\ldots,v_n\}$ y $\mathcal{B'} = \{v'_1,\ldots,v'_n\}$ bases ordenadas de $V$. Entonces la matriz
        $$
        P = \begin{bmatrix}
         C_1 &C_2 &\cdots &C_n
        \end{bmatrix}
        $$
        con 
        $$
            C_j = [v'_j]_\mathcal{B},\qquad 1 \le j \le n,
        $$
        es la matriz de \textit{cambio de coordenadas}\index{cambio de coordenadas} de la base $\mathcal{B'}$  a la base $\mathcal{B}$.
    \end{definicion}
    
    
    \begin{ejemplo*}
         $\mathcal{B} = \{v_1,v_2,v_3 \}=\{(1,0,-1),(1,1,1),(1,0,0) \}$ en $\R^3$.
        \begin{enumerate}
            \item\label{itm-ejem-coor-1} Probar que $\mathcal{B}$ es una base de $\R^3$, y 
            \item\label{itm-ejem-coor-2} encontrar las coordenadas del vector  $(2,3,5)$ respecto a la base  $\mathcal{B}$. 
        \end{enumerate}
    \end{ejemplo*}
\begin{proof}[Solución]
    \ref{itm-ejem-coor-1} Sea $A$ la matriz cuyas filas son los vectores de $\mathcal{B}$ :
    $$
    A = \begin{bmatrix} 1&0&-1\\1&1&1\\1&0&0	\end{bmatrix}.
    $$
    Entonces,  calculando el determinante  por desarrollo de la última fila, obtenemos
    $$
    \det A = 1.(-1)^{3+1}. 
    \left| \begin{matrix} 0&-1\\1&1	\end{matrix} \right| = 1.1.1 =1.
    $$ 
    Por  lo tanto $A$ es invertible y en consecuencia el espacio fila de $A$ es $\R^3$. Como $\mathcal{B}$ es un conjunto de 3  generadores de  $\R^3$, $\mathcal{B}$ es base.
    
    \ref{itm-ejem-coor-2} Si $x_1,x_2,x_3$  son las coordenadas de $(2,3,5)$ respecto a la base  $\mathcal{B}$, entonces
    $$
    (2,3,5) = x_1(1,0,-1)+x_2(1,1,1)+x_3(1,0,0)
    $$
    y esto es un sistema de tres ecuaciones y tres incógnitas.  Resolviendo este sistema encontramos la solución buscada. 
    
    Sin embargo,  nos interesa conocer una solución que sirva para cualquier vector, no solo para $(2,3,5)$, es decir buscamos la matriz $P = [p_{ij}]$ de cambio de coordenadas de la base canónica a la base $\mathcal B$. En particular,  esta matriz cumple 
    $$
    \begin{bmatrix} x_1\\x_2\\x_3 \end{bmatrix} = 
    \begin{bmatrix}p_{11} &p_{12}& p_{13} \\p_{21} &p_{22} & p_{23}\\ p_{31} &p_{32}& p_{33}\end{bmatrix}
    \begin{bmatrix} 2\\3\\5 \end{bmatrix}.
    $$
    con $(x_1,x_2,x_3)_{\mathcal B} = (2,3,5)$.
    
    
    Ahora bien, sabemos que $P^{-1}$ es la matriz cuya columna $j$ es la matriz de $v_j$ en la base canónica (fácil de calcular) y luego calculamos su inversa que es $P$.
    
    Por lo dicho en el párrafo anterior, $P^{-1}$  es la matriz cuya  columna $1$ es $(1,0,-1)$,  la columna $2$ es $(1,1,1)$ y la columna $3$ es $(1,0,0)$, luego 
    $$
    P^{-1} = \begin{bmatrix} 1&1&1\\0&1&0\\-1&1&0	\end{bmatrix}.
    $$ 
    Encontremos su inversa,
        \begin{multline*} 
    \left[\begin{array}{rrr|rrr}	1&1&1&1&0&0\\ 0&1&0&0&1&0\\ -1&1&0&0&0&1 \end{array}\right]
    \stackrel{F_3+ F_1}{\longrightarrow}
    \left[\begin{array}{rrr|rrr}	1&1&1&1&0&0\\ 0&1&0&0&1&0\\ 0&2&1&1&0&1 \end{array}\right]
    \underset{F_3-2F_2}{\stackrel{F_1 - F_2}{\longrightarrow}}\\
    \longrightarrow 
    \left[\begin{array}{rrr|rrr}	1&0&1&1&-1&0\\ 0&1&0&0&1&0\\ 0&0&1&1&-2&1 \end{array}\right]
    \stackrel{F_1 - F_3}{\longrightarrow}
    \left[\begin{array}{rrr|rrr}	1&0&0&0&1&-1\\ 0&1&0&0&1&0\\ 0&0&1&1&-2&1 \end{array}\right]
    .
    \end{multline*}
    Entonces 
    $$
    P = \left[\begin{array}{rrr}	0&1&-1\\ 0&1&0\\ 1&-2&1 \end{array}\right]
    $$
    y 
    $$
    \left[\begin{array}{rrr}	0&1&-1\\ 0&1&0\\ 1&-2&1 \end{array}\right]
    \left[\begin{array}{r} 2\\3\\5 \end{array}\right] = 
    \left[\begin{array}{r} -2\\3\\1 \end{array}\right].
    $$
    Por  lo tanto las coordenadas de   $(2,3,5)$ respecto a la base  $\mathcal{B}$ son $(-2,3,1)_{\mathcal B}$ o,  equivalentemente, 
    $$
    (2,3,5) = -2(1,0,-1)+3(1,1,1)+(1,0,0).
    $$
    
      
\end{proof}
\end{comment}
    \end{section}



    
        \begin{section}{Matriz de una transformaci\'on lineal}\label{seccion-matriz-de-una-tl}
            Sea $V$ un espacio vectorial de dimensión $n$ sobre el cuerpo $\K$, y sea $W$ un espacio vectorial de dimensión $m$ sobre $\K$. Sea $\mathcal B = \{v_1,\ldots,v_n\}$ una base ordenada de $V$, y $\mathcal B' = \{w_1,\ldots,w_m\}$ una base ordenada de $W$. Si $T$ es cualquier 		transformación lineal de $V$ en $W$, entonces $T$ está determinada por su efecto sobre los vectores $v_j$, puesto que todo vector de $V$ es combinación lineal de ellos. Cada uno de los $n$ vectores $Tv_j$ se expresa de manera única 	como combinación lineal
            \begin{equation}\label{matriz-de-T-1}
                Tv_j = \sum_{i=1}^{m} a_{ij} w_i
            \end{equation}
            de los $w_i$. Los escalares $a_{1j},\ldots,a_{mj}$ son las coordenadas de $Tv_j$ en la base ordenada $\mathcal B'$. Por consiguiente, la transformación $T$ está determinada por los
            $mn$ escalares $a_{ij}$ mediante la expresión (\ref{matriz-de-T-1}). 
            
            \begin{definicion} Sean $V$ y $W$ espacios vectoriales de dimensión finita con bases ordenadas $\mathcal B = \{v_1,\ldots,v_n\}$  y $\mathcal B' = \{w_1,\ldots,w_m\}$, respectivamente. Sea $T: V \to W$ una transformación lineal tal que 
                \begin{equation*}\label{matriz-de-T-1}
                    Tv_j = \sum_{i=1}^{m} a_{ij} w_i.
                \end{equation*}
                A  $A$  La matriz $m \times n$  definida por $[A]_{ij} = a_{ij}$ se la  denomina \textit{la matriz de $T$ respecto a las bases ordenadas $\mathcal B$ y $\mathcal B'$;}\index{matriz!de una transformación lineal} y se la denota 
                $$
                [T]_{\mathcal B \mathcal B'} = A .
                $$.
            \end{definicion}
            
            \begin{ejemplo*}
                Sea $T: \R^3 \to \R^4$ definida
                $$
                T(x,y,z) = (2x+y, 3y, x+4z,z).
                $$
                Sean  $B = \{e_1,e_2,e_3\}$ la base canónica de $\R^3$ y  $B' = \{e_1,e_2,e_3,e_4\}$ la base canónica de $\R^4$. Entonces
                \begin{equation*}
                    \begin{array}{ccccrcrcrcr}
                    T(e_1) & = & (2,0,1,0) & = &  2e_1& + & 0.e_2& + &   e_3& + & 0.e_4  \\
                    T(e_2) & = & (1,3,0,0) & = &   e_1& + &  3e_2& + & 0.e_3& + & 0.e_4  \\
                    T(e_3) & = & (0,0,4,1) & = & 0.e_1& + & 0.e_2& + &  4e_3& + &   e_4
                    \end{array}
                \end{equation*}
                Por  lo tanto
                \begin{equation*}
                        [T]_{\mathcal B \mathcal B'} =\begin{bmatrix} 2&1&0 \\0&3&0 \\1&0&4 \\0&0&1
                    \end{bmatrix}.
                \end{equation*}
                Observar que si escribimos los vectores en coordenadas con respecto  a las bases canónicas,  tenemos que 
                \begin{equation*}
                \begin{bmatrix} 2&1&0 \\0&3&0 \\1&0&4 \\0&0&1
                \end{bmatrix}
                \begin{bmatrix} x \\ y \\z\end{bmatrix} = 
                \begin{bmatrix} 2x+y\\ 3y\\ x+4z\\z\end{bmatrix}
                \end{equation*}
                o más formalmente
                \begin{equation*}
                    [T]_{\mathcal B \mathcal B'} [v]_{\mathcal B} = [T(v)]_{\mathcal B'}.
                \end{equation*}
            \end{ejemplo*}
        
        
            \begin{observacion*}
                Recordemos que si  $A = [a_{ij}]$ matriz $m \times n$, el operador lineal asociado a $A$ si se define por 
                \begin{align*}
                    \begin{array}{rccc}
                        T : &\R^n &\to &\R^m \\
                            &v &\mapsto &Av.
                    \end{array}.
                \end{align*}
                Es decir
                \begin{equation}
                    T(x_1,\ldots,x_n):= \begin{bmatrix} a_{11} &\cdots &a_{1n} \\ \vdots &\ddots& \vdots \\a_{m1} &\cdots &a_{mn}
                    \end{bmatrix}\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}.
                \end{equation}
         	
        
                Sean  $\mathcal C_n$ y $\mathcal C_m$ las bases canónicas de $\R^n$ y $\R^m$,  respectivamente, entonces 
                    \begin{equation*}
                        [T]_{\mathcal C_n\mathcal C_m} =A.
                    \end{equation*}
                \end{observacion*}
    
            \begin{proposicion}\label{prop-matriz-de-tv} 	Sea $V$  y $W$ un espacios vectoriales de dimensión $n$ y $m$ respectivamente y sea $T: V \to W$ una transformación lineal. Sea $\mathcal B = \{v_1,\ldots,v_n\}$ una base ordenada de $V$, y $\mathcal B' = \{w_1,\ldots,w_n\}$ una base ordenada de $W$. Entonces
            \begin{equation}\label{matriz-de-tl-2}
            [T]_{\mathcal B \mathcal B'} [v]_{\mathcal B} = [T(v)]_{\mathcal B'}, \quad \forall \,v \in V.
            \end{equation}	
            \end{proposicion}
        \begin{proof}
            Si 
            \begin{equation*}
            Tv_j = \sum_{i=1}^{m} a_{ij} w_i
            \end{equation*}
            entonces $[T]_{ij} = a_{ij}$. Sea $v \in $,  entonces $v = x_1v_1+\cdots+x_n v_n$ con $x_i \in \K$, por lo tanto 
            $$
            [v]_{\mathcal B} = \begin{bmatrix} x_1\\ \vdots\\x_n\end{bmatrix}.
            $$
            Ahora bien,
            \begin{multline*}
                T(v) = T(\sum_{j=1}^{n}x_jv_j) = \sum_{j=1}^{n}x_jT(v_j) =  \sum_{j=1}^{n}x_j\sum_{i=1}^{m} a_{ij} w_i = \sum_{i=1}^{m} (\sum_{j=1}^{n}x_ja_{ij}) w_i =\\
                = ( \sum_{j=1}^{n}x_ja_{1j})w_1+(\sum_{j=1}^{n}x_ja_{2j})w_2+\cdots+(\sum_{j=1}^{n}x_ja_{mj})w_m
            \end{multline*}
            y, por lo tanto,
            \begin{equation}\label{eq-45}
            [T(v)]_{\mathcal B'} = \begin{bmatrix}  \sum_{j=1}^{n}x_ja_{1j}\\\sum_{j=1}^{n}x_ja_{2j}\\ \vdots\\\sum_{j=1}^{n}x_ja_{mj}\end{bmatrix}.
            \end{equation}
            Por otro lado, 
            \begin{equation}\label{eq-46}
            [T]_{\mathcal B \mathcal B'} [v]_{\mathcal B} = 
            \begin{bmatrix} a_{11} &a_{12}&\cdots & a_{1n}
            \\a_{21} &a_{22}&\cdots & a_{2n}\\ \vdots &\vdots&& \vdots \\ a_{m1} &a_{m2}& \cdots & a_{mn}\end{bmatrix}
            \begin{bmatrix} x_1\\x_2\\ \vdots\\x_n\end{bmatrix}
            =
            \begin{bmatrix}  \sum_{j=1}^{n}a_{1j}x_j\\\sum_{j=1}^{n}a_{2j}x_j\\ \vdots\\\sum_{j=1}^{n}a_{mj}x_j\end{bmatrix}.
            \end{equation}
            De las ecuaciones (\ref{eq-45}) y (\ref{eq-46}) se deduce la formula (\ref{matriz-de-tl-2}).
        \end{proof}

        \begin{corolario}\label{cor-cambio-de-base}
            Sea $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$, sean $\mathcal B$, $\mathcal B'$  bases ordenadas de $V$. Entonces 
            \begin{equation*}
                [v]_{\mathcal B} = [\Id]_{\mathcal B' \mathcal B}\, [v]_{\mathcal B'}, \quad \forall v \in V.
            \end{equation*}
        \end{corolario}
        \begin{proof}
            Por la proposición \ref{prop-matriz-de-tv} tenemos que 
            $$
            [\Id]_{\mathcal B' \mathcal B}  [v]_{\mathcal B'} = [\Id (v)]_{\mathcal B} = [v]_{\mathcal B}.
            $$
        \end{proof}

        \begin{definicion}
            Sea $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y sean $\mathcal B$ y $\mathcal B'$ bases ordenadas de $V$. La matriz $P =[\Id]_{\mathcal B' \mathcal B}$  es llamada la \textit{matriz de cambio de base} \index{matriz!de cambio de base} de la base $\mathcal B'$  a la base $\mathcal B$. 
        \end{definicion}

        La matriz de cambio de base nos permite calcular los cambios de  coordenadas: dadas dos bases ordenadas $\mathcal B$ y $\mathcal B'$,  y dadas las coordenadas de $v$ en la base  $\mathcal B'$ es decir la matriz columna $[v]_{\mathcal B'}$, tenemos, por corolario \ref{cor-cambio-de-base}, que
        \begin{equation*}
            [v]_{\mathcal B} =P\, [v]_{\mathcal B'}, \quad \forall v \in V.
        \end{equation*}
        
        \begin{teorema}[*]
            Sea $V$  y $W$ un espacios vectoriales de dimensión $n$ y $m$ respectivamente y $\mathcal B = \{v_1,\ldots,v_n\}$ y $\mathcal B' = \{w_1,\ldots,w_m\}$ dos bases  ordenadas de $V$ y $W$ respectivamente. Entonces 
            $$
            \kappa: L(V,W) \to M_{m \times n}(\K)
            $$
            definida
            $$
            T \mapsto [T]_{\mathcal B \mathcal B'},
            $$
            es un isomorfismos de espacios vectoriales. 
        \end{teorema}
        \begin{proof}[Demostracion]
            Primero probaremos que $\kappa$  es lineal y luego que tiene inversa.
            
            Sean $T,T' \in L(V,W)$ y $\lambda \in \K$, veamos que $\kappa(\lambda T+ T') = \lambda \kappa(T)+ \kappa(T')$,  es decir
            \begin{equation}\label{kappa-lineal}
                [\lambda T+ T']_{\mathcal B \mathcal B'} = \lambda[T]_{\mathcal B \mathcal B'}+ [T']_{\mathcal B \mathcal B'}.
            \end{equation}
            Para $1 \le j \le n$, sean 
            \begin{equation*}
                T(v_j) = \sum_{i=1}^m a_{ij} w_i\qquad \text{ y } \qquad T'(v_j) = \sum_{i=1}^m a'_{ij} w_i,
            \end{equation*}
            es  decir
            \begin{equation*}
            [T]_{\mathcal B \mathcal B'} =	[a_{ij}] \qquad\text{ y } \qquad [T']_{\mathcal B \mathcal B'} =	[a'_{ij}],
            \end{equation*}
            entonces
            \begin{align*}
                (\lambda T+ T')(v_j) &= \lambda T(v_j)+ T'(v_j) \\
                &=\lambda \sum_{i=1}^m a_{ij} w_i+ \sum_{i=1}^m a'_{ij} w_i \\
                &= \sum_{i=1}^m (\lambda a_{ij}+ a'_{ij}) w_i,
            \end{align*}
            por lo tanto
            \begin{equation*}
                [\lambda T+ T']_{\mathcal B \mathcal B'} = [\lambda a_{ij}+ a'_{ij}] =
                \lambda[T]_{\mathcal B \mathcal B'}+ [T']_{\mathcal B \mathcal B'}
            \end{equation*}
            y hemos probado (\ref{kappa-lineal}) y, en consecuencia, $\kappa$  es lineal.
            
            
            Definamos ahora la inversa de $\kappa$: sea $A = [a_{ij}]$ matriz $m \times n$ y sea $T: V \to W$ la única transformación lineal que satisface, para $1 \le j \le n$, que
            $$
            T(v_j) = \sum_{i=1}^m a_{ij} w_i.
            $$
            Es claro que esta aplicación tiene dominio en $M_{m \times n}(\K)$ y su imagen está contenida en  $L(V,W)$. Más aún,  es muy sencillo comprobar que es la aplicación inversa a $\kappa$.
            
        \end{proof}
    
        
        
        \begin{teorema}\label{th-producto-de-matrices-bases}
            Sean $V$, $W$ y $Z$ espacios vectoriales de dimensión finita sobre el cuerpo $\K$; sean $T: V \to W$ y $U: W \to Z$  transformaciones lineales.  Si $\mathcal B$, $\mathcal B'$ y $\mathcal B''$ son bases ordenadas de los espacios $V$, $W$ y $Z$, respectivamente, entonces
            \begin{equation}
                [UT]_{\mathcal B\mathcal B''} = 	[U]_{\mathcal B'\mathcal B''} 	[T]_{\mathcal B\mathcal B'}.  
            \end{equation} 
        \end{teorema}
        \begin{proof} 
            Sean
            \begin{equation*}
                 \mathcal B = \{v_1,\ldots,v_n\},\quad\mathcal B' = \{w_1,\ldots,w_m\},\quad  \mathcal B''= \{z_1,\ldots,z_l\}
            \end{equation*}
            y
            \begin{equation*}
            T(v_j) = \sum_{i=1}^m a_{ij} w_i, \; 1 \le j \le n; \qquad U(w_i) = \sum_{k=1}^l b_{ki} z_k, \; 1 \le i \le m.
            \end{equation*}
            Es decir 
            $$
            [T]_{\mathcal B\mathcal B'} = [a_{ij}]\qquad \text{y} \qquad [U]_{\mathcal B'\mathcal B''} = [b_{ij}]. 
            $$
            Entonces
            \begin{align*}
                (UT)(v_j) &= U(\sum_{i=1}^m a_{ij} w_i) \\
                &=  \sum_{i=1}^m a_{ij} U(w_i) \\
                &=  \sum_{i=1}^m a_{ij} \sum_{k=1}^l b_{ki} z_k \\
                &= \sum_{k=1}^l(\sum_{i=1}^m  b_{ki}a_{ij})  z_k.
            \end{align*}
            Luego el coeficiente $kj$ de la matriz $[UT]_{\mathcal B\mathcal B''}$ es $\sum_{i=1}^m  b_{ki}a_{ij}$ que es igual a la fila $k$ de $[U]_{\mathcal B'\mathcal B''}$ por la columna $j$ de  $[T]_{\mathcal B\mathcal B'}$,  en símbolos,  si $A= 	[T]_{\mathcal B\mathcal B'}$, $B = 	[U]_{\mathcal B'\mathcal B''}$ y $C = 	[UT]_{\mathcal B\mathcal B''}$, entonces
            \begin{equation*}
                [C]_{kj} = \sum_{i=1}^m  b_{ki}a_{ij} = F_k(B)C_j(A) = [BA]_{kj}.
            \end{equation*}
        \end{proof}	
            
        \begin{corolario}\label{cor-inversa-matriz-cambio-de-base} Sea $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y sean $\mathcal B$ y $\mathcal B'$ bases ordenadas de $V$. La matriz de cambio de base  $P =[\Id]_{\mathcal B' \mathcal B}$ es invertible y su  inversa es $P^{-1} =[\Id]_{\mathcal B \mathcal B'}$
        \end{corolario}
        \begin{proof}
            \begin{equation*}
                P^{-1} P =[\Id]_{\mathcal B \mathcal B'}[\Id]_{\mathcal B' \mathcal B} = [\Id]_{\mathcal B'} = \Id.
            \end{equation*}
        \end{proof}
    

        \begin{corolario}\label{cor-5.6} Sean $V$ espacio vectorial de dimensión finita, $\mathcal B = \{v_1,\ldots,v_n\}$ base ordenada de $V$ y $T,U: V \to V$ operadores lineales. Entonces
            \begin{enumerate}
                \item\label{itm-cor-cambio-1} $[UT]_{\mathcal B} = [U]_{\mathcal B} [T]_{\mathcal B}$.
                \item\label{itm-cor-cambio-2} Si $\Id: V \to V$  es el operador identidad, entonces $[\Id]_{\mathcal B} =\Id$,  donde $\Id$  es la matriz identidad $n \times n$.
                \item\label{itm-cor-cambio-3} Si $T$  es invertible,  entonces $[T]_{\mathcal B}$  es una matriz invertible y  $$[T^{-1}]_{\mathcal B} = [T]_{\mathcal B}^{-1}.$$
            \end{enumerate}
        \end{corolario}
        \begin{proof}
            \ref{itm-cor-cambio-1}. Es inmediato del teorema anterior tomado $\mathcal B' =\mathcal B'' = \mathcal B$. 
            
            \ref{itm-cor-cambio-2}. $\Id(v_i) = v_i$ y por lo tanto
            \begin{equation*}
                [\Id]_{\mathcal B} = \begin{bmatrix} 1&0&\cdots&0 \\0&1&\cdots&0\\\vdots&&&\vdots\\0&0&\cdots&1 	\end{bmatrix} = \Id.
            \end{equation*}
            
            
            \ref{itm-cor-cambio-3}. $\Id = T T^{-1}$, luego
            \begin{equation*}
                \Id = [\Id]_{\mathcal B} =  [T T^{-1}]_{\mathcal B} =  [T]_{\mathcal B} [T^{-1}]_{\mathcal B}.
            \end{equation*}
             Análogamente, $\Id = T^{-1} T$, luego
            \begin{equation*}
            \Id = [\Id]_{\mathcal B} =  [T^{-1} T]_{\mathcal B} =  [T^{-1}]_{\mathcal B} [T]_{\mathcal B}. 
            \end{equation*}
            Por  lo tanto $[T]_{\mathcal B}^{-1} = [T^{-1}]_{\mathcal B}$.
        \end{proof}	
    
    
            
        \begin{teorema}\label{th-cambio-de-base}
            Sea $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y sean
            $$
            \mathcal B = \{v_1,\ldots,v_n \}, \qquad \mathcal B' = \{w_1,\ldots,w_n \}
            $$
            bases ordenadas de $V$. Sea $T$ es un operador lineal sobre V. Entonces, si $P$ es la matriz de cambio de base de $\mathcal B'$ a $\mathcal B$, se cumple que  
            \begin{equation*}
            [T]_{\mathcal B'} = P^{-1}[T]_{\mathcal B}  P.
            \end{equation*}
            Es decir
            \begin{equation}\label{eq-cambio de base}
                [T]_{\mathcal B'} = [\Id]_{\mathcal B \mathcal B'} [T]_{\mathcal B} [\Id]_{\mathcal B' \mathcal B}.
            \end{equation}
        \end{teorema}
        \begin{proof} Tenemos que $T = \Id T$ y $T =T  \Id$, luego 
            \begin{align*}
                [T]_{\mathcal B'\mathcal B'} &=  [\Id T]_{\mathcal B'\mathcal B'}& \qquad& \\
                &= [\Id]_{\mathcal B \mathcal B'} [T]_{\mathcal B' \mathcal B}& &(\text{teorema \ref{th-producto-de-matrices-bases}})  \\
                &= [\Id]_{\mathcal B \mathcal B'} [T \Id]_{\mathcal B' \mathcal B}& &\\
                &= [\Id]_{\mathcal B \mathcal B'} [T]_{\mathcal B\mathcal B} [\Id]_{\mathcal B' \mathcal B}& &(\text{teorema \ref{th-producto-de-matrices-bases}}) \\
                &=P^{-1}[T]_{\mathcal B\mathcal B}P& &(\text{corolario \ref{cor-inversa-matriz-cambio-de-base}}).
            \end{align*} 
        \end{proof}
    
        La fórmula (\ref{eq-cambio de base})  es importante por si misma y debemos recordarla.

        El teorema \ref{th-cambio-de-base} nos permite definir el determinante de un operador lineal.	Sea $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y $T$ un operador lineal sobre $V$. Sean  $\mathcal B$, $\mathcal B'$ 
        bases ordenadas de $V$, entonces $[T]_{\mathcal B'} = P^{-1}[T]_{\mathcal B}P$, para $P$ una matriz invertible. Por lo tanto, 
        \begin{equation*}
            \det([T]_{\mathcal B'}) = \det(P^{-1}[T]_{\mathcal B}  P) =  \det([T]_{\mathcal B}  PP^{-1}) =  \det([T]_{\mathcal B}). 
        \end{equation*}
        Es decir,  el determinante de la matriz de $T$ en cualquier base siempre es igual.
        
        \begin{definicion} 
                Sea $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y $T$ un operador lineal sobre $V$. El \textit{determinante de $T$}\index{determinante de una transformación lineal} es el determinante de la matriz de $T$ en alguna base de $V$.  
        \end{definicion}
        \end{section}    
    
        \begin{section}{Operadores diagonalizables}\label{seccion-autovalores-y-autovectores-de-una-tl}
        Vimos en la sección  \ref{seccion-autovalores-y-autovectores-de-matrices} la definición de autovalores y autovectores de una matriz. Por otro lado,  en la sección \ref{seccion-matriz-de-una-tl} vimos que dada una base podemos asignarle a cada transformación lineal una matriz. En  esta sección veremos,  entro otros temas,  los autovalores y autovectores desde una perspectiva de las transformaciones lineales. Por lo dicho anteriormente  verán que muchos conceptos y demostraciones se repiten o son similares al caso de la matrices. 
        
        Sea $V$ espacio vectorial de dimensión finita. Un operador lineal en $V$ es \textit{diagonalizable}\index{matriz!diagonalizable}  si existe una base ordenada $\mathcal B= \{v_1,\ldots,v_n\}$ de $V$ y $\lambda_1,\ldots,\lambda_n \in \K$ tal que 
        \begin{equation}\label{eq-auto-49}
            T(v_i) = \lambda_i v_i,\qquad 1\le i \le n. 
        \end{equation}
        En  general, los operadores diagonalizables permiten hacer cálculos sobre ellos en forma sencilla, por ejemplo el núcleo del  operador definido por (\ref{eq-auto-49}) es $\nuc(T)=\langle v_i: \lambda_i =0 \rangle$ y  su imagen es $\img(T)=\langle v_i: \lambda_i \not=0 \rangle$ (vermos la demostración de estos resultado más adelante). 
        Otra propiedad importante de los operadores diagonalizables es que la matriz de la transformación lineal en una base adecuada es diagonal (de allí viene el nombre de diagonalizable). En  el caso del  operador definido por (\ref{eq-auto-49}) tenemos que
        $$
        [T]_{\mathcal B} = 
        \begin{bmatrix}
        \lambda_1&0&0&\cdots&0 \\
        0&\lambda_2&0&\cdots&0\\
        0&0&\lambda_3&\cdots&0\\
        \vdots&\vdots&\vdots&&\vdots\\
        0&0&0&\cdots&\lambda_n
        \end{bmatrix}.
        $$
        
        
        No todo operador lineal es diagonalizable y no es inmediato, ni sencillo, de la definición de un operador lineal decidir si es diagonalizable o no. En esta sección veremos herramientas para estudiar un operador lineal $T$ y su posible diagonalización. La ecuación (\ref{eq-auto-49}) sugiere se estudien los vectores que son transformados por $T$ en múltiplos de sí mismos.
        
        \begin{definicion}
            Sea $V$ un espacio vectorial sobre el cuerpo $\K$ y sea $T$ un operador lineal sobre $V$. Un \textit{valor propio}\index{valor propio} o \textit{autovalor}\index{autovalor} de $T$ es un escalar $\lambda$ de $\K$ tal que existe un vector no nulo $v \in V$ con $T(v) = \lambda v$. Si $\lambda$ es un autovalor de $T$, entonces
            \begin{enumerate}
                \item  cualquier  $v \in V$ tal que $T(v) = \lambda v$  se llama un \textit{vector propio}\index{vector propio} o  \textit{autovector}\index{autovector} de $T$ asociado al valor propio $\lambda$;
                \item la colección de todos los $v \in V$ tal que $T(v) = \lambda v$  se llama \textit{espacio propio}\index{espacio propio} o \textit{autoespacio}\index{autoespacio} 	asociado a $\lambda$. 
            \end{enumerate}
            
            Los valores propios se llaman también a menudo raíces características, eigenvalores. valores característicos o valores espectrales. Nosotros usaremos, preferentemente, ``autovalores''.
            
            Sea ahora $\lambda \in \K$, definimos
            $$
            V_\lambda := \{v \in V: Tv = \lambda v \}.
            $$
            Observar que $V_\lambda \ne 0$ si y sólo si $\lambda$ es autovalor y en ese caso $V_\lambda$  es el autoespacio asociado a $\lambda$. 
        \end{definicion}
        
        
        \begin{teorema}
            Sea $V$ un espacio vectorial y sea $T:V \to V$ una aplicación lineal. Sea $\lambda \in \K$ entonces, $V_\lambda$  es subespacio de $V$.
        \end{teorema}
        \begin{proof}
            Sean $v_1,v_2 \in V$ tales que $Tv_1 = \lambda v_1$ y $Tv_2 = \lambda v_2$. Entonces
            $$
            T(v_1+v_2) = T(v_1)+ T(v_2) = \lambda v_1 + \lambda v_2 = \lambda (v_1 + v_2),
            $$
            es decir si  $v_1,v_2 \in V_\lambda$, probamos que $v_1+v_2 \in V_\lambda$. 
            
            Sea ahora $c \in F$, entonces $T(cv_i) = cT(v_1) = c\lambda v_1 = \lambda (cv_1)$. Por lo tanto, si  $v_1\in V_\lambda$ y $c \in F$, probamos que $cv_1 \in V_\lambda$.
            
            Esto termina de probar el teorema.
        \end{proof}

        
        
        \begin{teorema}
            Sea $V$ espacio vectorial y sea $T: V \to V$ una aplicación lineal.  Sean $v_1,\ldots,v_m$ autovectores de $T$, con autovalores $\lambda_1,\ldots,\lambda_m$ respectivamente. Suponga que estos  autovalores son distintos entre si, esto es, $\lambda_i \ne \lambda_j$ si $i \ne j$. Entonces $v_1,\ldots,v_m$ son linealmente independientes.
        \end{teorema}
        \begin{proof}
            Hagamos la demostración por inducción sobre $m$.
            
            \textit{Caso base.} Si $m=1$, no hay nada que demostrar puesto que un vector no nulo el LI.
            
        
            
            \textit{Paso inductivo.} Supongamos que el enunciado es verdadero para el caso $m-1$ con $m>1$, (hipótesis inductiva o HI), y probemos entonces que esto implica que es cierto para $m$. Debemos ver  que si 
            \begin{equation}
            c_1v_1+	c_2v_2+ \cdots c_mv_m = 0 \tag{$*$}
            \end{equation}
            entonces $c_1 = \cdots c_m = 0$.
            Multipliquemos $(*)$ por $\lambda_1$, obtenemos:
            \begin{equation}
            c_1\lambda_1v_1+ c_2\lambda_1v_2+\cdots c_m\lambda_1v_m = 0. \tag{$**$}
            \end{equation}
            También apliquemos $T$ a $(*)$ y obtenemos
            \begin{equation}
            c_1\lambda_1v_1+ c_2\lambda_2v_2+\cdots c_m\lambda_mv_m = 0. \tag{$***$}
            \end{equation}
            Ahora a $(**)$ le restamos $(***)$ y obtenemos:
            \begin{equation}
            c_2(\lambda_1 -\lambda_2)v_2+\cdots c_m(\lambda_1 -\lambda_m)v_m = 0. 	 
            \end{equation}
            Como, por hipótesis inductiva, $v_2,\ldots,v_m$ son LI, tenemos que $c_i(\lambda_1 -\lambda_i)=0$ para $i\ge 2$. Como $\lambda_1 -\lambda_i \ne 0$ para $i\ge 2$, obtenemos que $c_i = 0$ para $i\ge 2$. Por $(*)$ eso implica que $c_1=0$ y por lo tanto $c_i=0$ para todo $i$.
        \end{proof}
        
        \begin{corolario}\label{cor-aut-li}
            Sea $V$ espacio vectorial de dimensión $n$ y sea $T: V \to V$ una aplicación lineal que tiene $n$
            autovectores $v_1,\ldots, v_n$ cuyos autovalores $\lambda_1,\ldots,\lambda_n$ son distintos entre
            si. Entonces $\{v_1,\ldots, v_n\}$ es una base de $V$.
        \end{corolario}
        
        Recordemos que si $T$  es una transformación lineal, el determinante de $T$  se define como el determinante de la matriz de la transformación lineal en una base dada y que este determinante no depende de la base.    
        
        \begin{definicion}
            Sea $A \in M_n(\K)$   el \textit{polinomio característico}\index{polinomio característico} de $A$ es $$\chi_A(x) = \det(A-x \Id),$$ donde $x$ es una indeterminada. 
            
            Sea $V$ espacio vectorial de dimensión finita y sea $T: V \to V$ lineal, el  \textit{polinomio característico de $T$} es $\chi_T(x) = \det(T- x \Id)$.
        \end{definicion}
    
            
        En general,  si $A = [a_{ij}]$ matriz $n \times n$, tenemos que
        \begin{equation}
        \chi_A(x) = \det(A- x\Id) = \det
        \begin{bmatrix}
        a_{11}-x&a_{12}&\cdots&a_{1n}\\
        a_{21}&a_{22}-x&\cdots&a_{2n}\\
        \vdots&\vdots&\ddots&\vdots\\
        a_{n1}&a_{n2}&\cdots&a_{nn}-x\\
        \end{bmatrix}
        \end{equation} 
        y el polinomio característico de $A$ es un polinomio  de grado $n$,  más precisamente  
        $$
        \chi_A(x) =(-1)^nx^n + a_{n-1}x^{n-1}+ \cdots + a_1x + a_0.
        $$ 
        Esto se puede demostrar fácilmente por inducción. 
        
        \begin{ejemplo*}
            Sea $T: \R^2 \to \R^2$ y su matriz en la base canónica es
            \begin{equation*}
                A = \begin{bmatrix}
                    a&b\\c&d
                \end{bmatrix},
            \end{equation*}
        entonces
        \begin{equation*}
                 \det \begin{bmatrix}
                a-x & b \\ c &d-x
                \end{bmatrix} = 
                (a-x)(d-x) - bc = x^2 -(a+d)x + (ad -bc).
        \end{equation*}
        Es decir,
        $$
        \chi_T(x) = x^2 -(a+d)x + (ad -bc).
        $$ 
        \end{ejemplo*}

    
    
        
        \begin{ejemplo} \label{ej-autovectores}
            Consideremos la transformación lineal de $T:\R^3 \to \R^3$  definida por (con abuso de notación incluido)
            \begin{equation*}
                T
                \begin{bmatrix} x\\y\\z \end{bmatrix} =
                \begin{bmatrix} 10x-10y+6z\\8x -8y +6z \\-5x+5y-3z\end{bmatrix}.
            \end{equation*}
            Es decir,  si $\mathcal C$ es la base canónica de $\R^3$, 
            $$ 
            [T]_{\mathcal C}=\begin{bmatrix}10&-10&6\\8& -8& 6\\-5& 5& -3\end{bmatrix}.
            $$
            Entonces el  polinomio característico de $T$ es
            $$
            \det \begin{bmatrix}10-x&-10&6\\8& -8-x& 6\\-5& 5& -3-x\end{bmatrix} = -x^3  - x^2 + 6 x .
            $$
            Es posible factorizar esta expresión y obtenemos
            $$
            \chi_A(x) = -x (x-2)(x+3).
            $$
        \end{ejemplo}
        
    \begin{comment}
        \begin{ejemplo*}
        Sea
        $$ 
        A=\begin{bmatrix}1&2&1\\ 6&-1&0\\ -1&-2&-1\end{bmatrix},
        $$
        entonces el  polinomio característico de $A$ es
        $$
        \det \begin{bmatrix}1-x&2&1\\ 6&-1-x&0\\ -1&-2&-1-x\end{bmatrix} = -x^3 - x^2 + 12 x.
        $$
        Es posible factorizar esta expresión y obtenemos
        $$
        \chi_A(x) = -x(x-3)(x+4).
        $$
        \end{ejemplo*}
    \end{comment}	
        
    

        
        \begin{proposicion}\label{autovalores}
            Sea $V$ espacio vectorial de dimensión $n$ y sea $T: V \to V$ lineal. Entonces $\lambda\in \K$ es autovalor si y sólo si $\lambda$ es raíz del polinomio característico.  
        \end{proposicion}
        \begin{proof}${}^{}$
            
            ($\Rightarrow$) Si $\lambda$ es autovalor, entonces existe $v \in V$, no nulo, tal que $Tv = \lambda v$, luego 
            $$
            0 = \lambda Tv -v  =   Tv - \lambda \Id v =  (T-\lambda \Id)v.
            $$
            Por lo tanto, $T-\lambda \Id$ no es invertible, lo cual implica que $0 = \det(T-\lambda \Id) = \chi_T(\lambda)$. Es decir, $\lambda$ es raíz del polinomio característico. 
            
            ($\Leftarrow$) Si $\lambda$ es raíz del polinomio característico, es decir si $0 = \chi_T(\lambda) = \det(T-\lambda \Id)$, entonces $T-\lambda \Id$ no es una transformación lineal  invertible, por lo tanto  su núcleo es no trivial. Es decir existe $v \in V$ tal que $(T-\lambda \Id)v =0$, luego $Tv =\lambda v$, por lo tanto $v$ es autovector con autovalor $\lambda$.   
        \end{proof}
        
        Repetimos ahora algunos conceptos ya expresados al comienzo de la sección. 
        
        \begin{definicion}
            Sea $V$ espacio vectorial de dimensión finita y sea $T: V \to V$ lineal. Diremos que \textit{$T$ es diagonalizable} si existe una base de $V$ de autovectores de $T$. 
        \end{definicion}	
        
        En el caso que $T$ sea una transformación lineal diagonalizable y $\mathcal{B} = \{v_1,\ldots,v_n \}$ sea una base de autovectores con autovalores $\lambda_1,\ldots,\lambda_n$, entonces
        $$
        T(v_i) = \lambda_i v_i, \qquad 1 \le i \le n,
        $$ 
        y, por lo tanto, la matriz de $T$ en  la base $\mathcal{B}$ es diagonal, más precisamente
        $$
        [T]_\mathcal{B} = \begin{bmatrix}
        \lambda_1 &0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots &0 \\
        \vdots & &\ddots & \vdots \\
        0 & 0 & \ldots &\lambda_n
        \end{bmatrix}
        $$
        
        \begin{ejemplo*} Consideremos la transformación lineal de $T:\R^3\to\R^3$  definida en el ejemplo \ref{ej-autovectores}.

        Ya vimos que  el  polinomio característico de esta aplicación es 
        $$
        \chi_A(t) = t (t-2)(t+3).
        $$
        Luego, por 	proposición \ref{autovalores}, los autovalores de $A$ son $0$, $2$ y $-3$. Debido al corolario \ref{cor-aut-li} existe una base de autovectores de $T$. Veamos cuales son. Si $\lambda$ autovalor de $T$, para encontrar los autovectores con autovalor $\lambda$  debemos resolver la ecuación $Tv -\lambda v=0 $,  en este caso sería
        \begin{equation*}
        \begin{bmatrix}10- \lambda &-10&6\\8& -8- \lambda & 6\\-5& 5& -3- \lambda \end{bmatrix}
        \begin{bmatrix} x\\y\\z \end{bmatrix} =
        \begin{bmatrix} 0\\0 \\0\end{bmatrix},
        \end{equation*}  
        para $\lambda =0, 2,-3$. Resolviendo estos tres sistemas, obtenemos que 
        \begin{equation*}
            V_0 = \{(y,y,0): y \in \R \},\quad V_2 = \{(-2z,-z,z): z \in \R \},\quad V_{-3} = \{(-2z,-2z,z): z \in \R \}. 
        \end{equation*}
        Por lo tanto, $\{(1,1,0), (-2,-1,1), (-2,-2,1)\}$ es una base de autovectores de la transformación lineal. 
        \end{ejemplo*}
        
        \begin{proposicion} Sea $V$ espacio vectorial de dimensión $n$ y sea $T: V \to V$ lineal tal que tiene una base de autovectores $\mathcal{B} = \{v_1,\ldots,v_n \}$  con autovalores $\lambda_1,\ldots,\lambda_n$. Entonces $\nuc(T)=\langle v_i: \lambda_i =0 \rangle$ e  $\img(T)=\langle v_i: \lambda_i \not=0 \rangle$.			
        \end{proposicion}
        \begin{proof} Reordenemos la base de tal forma que  $\lambda_i =0$ para $1 \le i \le k$ y $\lambda_i \ne 0$ para $k < i \le n$. 
        Todo $v \in V$ se escribe en términos de la base como 
        $$
        v = x_1v_1 + \cdots+ x_k v_k+ x_{k+1} v_{k+1}+\cdots+ x_n v_n,\quad (x_i \in \K),
        $$
        y entonces
        \begin{equation}\label{eq-av-01}
            T(v) =  \lambda_{k+1}x_{k+1} v_{k+1}+\cdots+ \lambda_nx_n v_n.
        \end{equation}
        Luego, $T(v) =0$ si y sólo si $x_{k+1} = \cdots = x_n=0$, y esto se cumple si y solo si $v =  x_1v_1 + \cdots+ x_k v_k$,  es decir $v \in \langle v_i: \lambda_i =0 \rangle$. 
        También es claro por la ecuación (\ref{eq-av-01}) que 
        \begin{align*}
            \img(T) &= \{\lambda_{k+1}x_{k+1} v_{k+1}+\cdots+ \lambda_nx_n v_n: x_i \in \K \} \\
            &=\{\mu_{k+1} v_{k+1}+\cdots+ \mu_n v_n: \mu_i \in \K \}\\
            &= \langle v_i: \lambda_i \not=0 \rangle.
        \end{align*}
        \end{proof}
        
        \begin{ejemplo*}
            Sea $T:\R^2 \longrightarrow \R^2$ el operador definido por $T(x,y)=(y,x)$. Probar que $T$ es diagonalizable y encontrar una base de autovectores. 
        \end{ejemplo*}
        \begin{proof}
            Por la proposición \ref{autovalores}, los autovalores de $T$  son las raíces del polinomio característico,  es decir las raíces de 
            $$
            \chi_T(\lambda)= \det\left[\begin{matrix}
            -\lambda& 1 \\ 1 & -\lambda
            \end{matrix} \right]= \lambda^2 -1  =(\lambda -1)(\lambda +1). 
            $$
            Luego los autovalores son $1$ y $-1$. Para hallar un autovector con autovalor $1$ debemos resolver la ecuación $T(x,y) = (x,y)$. Ahora bien, 
            $$
            (x,y) = T(x,y) = (y,x),
            $$
            luego $x =y$ y claramente $(1,1)$ es autovector con autovalor $1$.
            
            Por otro lado $T(x,y) = -(x,y)$, implica  que $(y,x)            = -(x,y)$, es decir $y = -x$ y claramente podemos elegir $(1,-1)$ como autovector con autovalor $-1$.
            
            Luego $\mathcal{B} = \{(1,1),(1,-1) \}$  es una base de $\R^2$ de autovectores de  $T$.
        \end{proof}
        
        No todas la matrices son diagonalizables,  como veremos en el ejemplo a continuación. 
        
        
        \begin{ejemplo*}
            Sea $T:\R^2 \longrightarrow \R^2$ el operador definido por $T(x,y)=(2x-y,x+4y)$. Probar que $T$ tiene un único autovalor $\lambda$ cuyo autoespacio $V_\lambda=\{v\in \R^2: Tv=\lambda v\}$ es de dimensión 1.
        \end{ejemplo*}
        \begin{proof} La matriz de  $T$ en la base canónica es 
            $$
            A = \begin{bmatrix}
            2 & -1 \\ 1 & 4
            \end{bmatrix}. 
            $$
            Por la proposición \ref{autovalores}, los autovalores de $T$  son las raíces del polinomio característico,  es decir las raíces de 
            $$
            \det\left[\begin{matrix}
            2-x & -1 \\ 1 & 4-x
            \end{matrix} \right] = (2-x)(4-x)+1 = x^2 -6x + 9 = 
            (x -3)^2. 
            $$
            Es decir el  único autovalor posible es 3.
            
            Debemos ver para que valores  $(x,y)\in \R^2$ se satisface la ecuación
            $$
            T(x,y) = 3(x,y).
            $$
            tiene solución. Esta ecuación es equivalente a 
            \begin{equation*}
                \begin{array}{rcll}
                (2x-y,x+4y)&=&(3 x, 3 y)\quad &\Rightarrow \\
                2x-y = 3 x&,& x+4y = 3y \quad &\Rightarrow \\
                -y =  x&,& x = -y \quad &\Rightarrow \\
                y &=&  -x \quad &
                \end{array}
            \end{equation*}
            Luego $V_3 = \{(x,-x): x \in \R \}$ que es de dimensión 1.  	
            
        \end{proof}
    
        \begin{proposicion}
             Sea $T$ un operador lineal diagonalizable sobre un espacio vectorial $V$ de dimensión finita. Sean $\lambda_1,\ldots,\lambda_k$ los autovalores distintos de $T$. Entonces,  el  polinomio característico de $T$ es
                 $$
                 \chi_T(x) = (\lambda_1 -x)^{d_1}\ldots(\lambda_k- x)^{d_k}
                 $$
                 con
                 $$
                 d_i =  \dim V_{\lambda_i},
                 $$
                 para  $i=1, \ldots, k$.
        \end{proposicion}
        \begin{proof}[Demostración ($*$)]
            $T$ es un operador lineal diagonalizable y $\lambda_1,\ldots,\lambda_k$ los valores propios distintos de $T$. Entonces existe una base ordenada $\mathcal B$ con respecto a la cual $T$ está representado por una matriz diagonal; es decir, los elementos de la diagonal son los escalares $\lambda_j$ cada uno de los cuales se repite un cierto número de veces. Más específicamente, si
             $v_{j1},\ldots,v_{jd_j}$ son los vectores en $\mathcal{B}$ con autovalor $\lambda_j$ ($1 \le j \le k$),  reordenamos la base de tal forma que primero estén los autovectores con autovalor $\lambda_1$, a continuación los de autovalor $\lambda_2$, etc.: 
            \begin{equation*}
                \mathcal{B} = \{v_{11},\ldots,v_{1d_1},\ldots,v_{k1},\ldots,v_{kd_k}\}. 
            \end{equation*}
            Ahora bien,  si $v \in V$,  entonces 
            \begin{align*}
            v &=x_1v_{11}+\cdots+x_{d_1}v_{1d_1}+\cdots+x_nv_{n1}+\cdots+x_{d_n}v_{nd_n} \\
            &= v_1 + v_2 +\cdots + v_k
            \end{align*}
            con $v_i = x_iv_{i1}+\cdots+x_{d_i}v_{id_i} \in V_{\lambda_i}$. Luego
            \begin{equation}\label{eq-desc-vectores-propios}
                T(v)  =\lambda_1v_1+\lambda_2v_2\cdots+\lambda_kv_k
            \end{equation} 
            
            Veamos que $V_{\lambda_i} = <v_{i1},\ldots,v_{id_i}>$ para $1 \le i \le k$. Es claro que  $<v_{i1},\ldots,v_{id_i}> \subset V_{\lambda_i}$. Probemos  ahora que,  $V_{\lambda_i} \subset <v_{i1},\ldots,v_{id_i}>$: si  $v \in V_{\lambda_i}$,  entonces $T(v)$ es como en (\ref{eq-desc-vectores-propios}) y, por lo tanto, si $v_j \ne 0$ para $j\not=i$ entonces $T(v) \ne \lambda_j v$, lo que contradice la hipótesis. Es decir $v = v_i \in <v_{i1},\ldots,v_{id_i}>$. Hemos probado que    $V_{\lambda_i} = <v_{i1},\ldots,v_{id_i}>$ y como $v_{i1},\ldots,v_{id_i}$ son LI, entonces $\dim V_{\lambda_i} = d_i$.
            
            Por otro lado, la matriz de $T$ en la base $\mathcal{B}$ tiene la forma
            \begin{equation*}
                \begin{bmatrix}
                \lambda_1 \Id_1 &0&\cdots&0 \\0&\lambda_2 \Id_2&\cdots&0 \\\vdots&\vdots&&\vdots \\0&0&\cdots&\lambda_n \Id_n 
                \end{bmatrix}
            \end{equation*}
            donde $\Id_j$ es la matriz identidad $d_j \times d_j$. Luego,  el polinomio característico de $T$ es el producto
            \begin{align*}
                (\lambda_1 -x)^{d_1}\ldots(\lambda_k- x)^{d_k}.
            \end{align*}
        \end{proof}
    
        \begin{ejemplo*}
            Sea $T$ un operador lineal sobre $\R^3$  representado en la base ordenada canónica por la matriz
            \begin{equation*}
                A = 
                \begin{bmatrix}
                5 &-6 &-6\\ -1& 4& 2\\3 &-6& -4
                \end{bmatrix}.
            \end{equation*}
            El  polinomio característico de $A$ es
            \begin{equation*}
                \chi_A(x) = \det \begin{bmatrix}
                    5-x &-6 &-6\\ -1& 4-x& 2\\3 &-6& -4-x
                \end{bmatrix} 
                = -x^3 + 5 x^2 - 8 x + 4 = -(x-2)^2(x-1).
            \end{equation*}
            ¿Cuáles son las dimensiones de los espacios de los vectores propios asociados
            con los dos valores propios? Se deben resolver las ecuaciones asociadas a las matrices
            \begin{equation*}
            A - 2 \Id = 
            \begin{bmatrix}
            3 &-6 &-6\\ -1& 2& 2\\3 &-6& -6
            \end{bmatrix} 
            \end{equation*}
            y 
            \begin{equation*}
            A -\Id= 
            \begin{bmatrix}
            4 &-6 &-6\\ -1& 3& 2\\3 &-6& -5
            \end{bmatrix}.
            \end{equation*}
            Las soluciones de estos sistemas son los autoespacios de autovalor 2 y 1 respectivamente. En  el primer caso, 
            \begin{equation*}
            \begin{bmatrix} 3 &-6 &-6\\ -1& 2& 2\\3 &-6& -6 \end{bmatrix}
            \underset{F_3+3F_2}{\stackrel{F_1+3 F_2}{\longrightarrow}} 
            \begin{bmatrix} 0 &0 &0\\ -1& 2& 2\\0 &0& 0 \end{bmatrix}.
            \end{equation*}
            Luego,  la solución del sistema asociado a $A-2\Id$ es 
            $$
            V_2 = \{(2y+2z,y,z): y,z \in \R\} = <(2,1,0),(2,0,1)>
            $$
            cuya dimensión es 2. 
            
            Por otro lado, 
            \begin{equation*}
            \begin{bmatrix}4 &-6 &-6\\ -1& 3& 2\\3 &-6& -5 	\end{bmatrix}
            \underset{F_3+3F_2}{\stackrel{F_1+4 F_2}{\longrightarrow}} 
            \begin{bmatrix}0 &6 &2\\ -1& 3& 2\\0 &3& 1 	\end{bmatrix}
            \underset{F_2-F_3}{\stackrel{F_1-2 F_3}{\longrightarrow}}
            \begin{bmatrix}0 &0 &0\\ -1& 0& 1\\0 &3& 1 	\end{bmatrix}.
            \end{equation*}
            Luego,  la solución del sistema asociado a  $A-\Id$ es 
            $$
            V_1 = \{(z,-\frac13z,z): z \in \R\} = <(1,-\frac13,1)>.
            $$
            
            Entonces, una base de autovectores de $T$ podría ser
            $$
            \mathcal{B} = \{(2,1,0),(2,0,1),(1,-\frac13,1) \}
            $$
            y en esa base la matriz de la transformación lineal es
            \begin{equation*}
            [T]_{\mathcal{B}} = \begin{bmatrix}2 &0 &0\\ 0& 2& 0\\0 &0& 1 	\end{bmatrix}.
            \end{equation*}
        \end{ejemplo*}
    
        
        \end{section}
    
    
        
    

\begin{section}{Operadores simétricos en $\R^n$}\label{seccion-operadores-simetricos-rn}
    
    \begin{definicion}
        Sea $T$ un operador lineal  en $\R^n$,  diremos que $T$ es un \textit{operador simétrico} si la matriz de $T$  en la base canónica es simétrica, es decir si 
        $[T]_{\mathcal{C}}^{\t} = [T]_{\mathcal{C}}$
    \end{definicion}
    
    
    Observar, como ya hemos visto anteriormente,  que en $\R^n$ el producto escalar es 
    \begin{equation*}
        \la (x_1,\ldots,x_n),(y_1,\ldots,y_n)  \ra = \sum_i x_iy_i = \begin{bmatrix} x_1& \cdots &x_n\end{bmatrix}\begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}.
    \end{equation*}
    Es decir, si usamos la convención que un vector en $\R^n$  se escribe como una matriz columna (de $n$ filas y una columna),  tenemos que dados $x,y \in \R^n$, 
    \begin{equation*}
        \la x,y \ra = x^\t y.
    \end{equation*}
    
Sea $T:\R^n \to \R^n$ un operador simétrico y $A$ la matriz asociada a  $T$,  es decir $A = [T]_{\mathcal{C}}$,  donde $\mathcal{C}$  es la base canónica. Si trabajamos en las coordenadas canónicas es claro que $T(x) = Ax$ y debido a esto a menudo intercambiaremos  $T$ por $A$ y viceversa.  

\begin{definicion}
    Sea $V$  espacio vectorial y  $T: V \to V$ operador lineal. Sea $U \subset V$ subespacio de $V$. Diremos que $U$  es  \textit{invariante por $T$} si $T(U) := \{T(v): v \in U\} \subset U$. 
\end{definicion}

    
\begin{proposicion}\label{prop-t-simetrico-autoadjunto}
    Si $T$ es un operador simétrico, entonces 
    \begin{enumerate}[label=\emph{\alph*})]
        \item 
        \begin{equation*}
        \la Tx,y \ra =  \la x,Ty \ra .
        \end{equation*}
        \item Si $W$ un subespacio de $\R^n$ invariante por $T$, entonces $W^\perp$ es invariante por $T$.
    \end{enumerate}
\end{proposicion}
\begin{proof} Sea $A=[T]_{\mathcal{C}}$. 


    (a) Notar que
    \begin{equation*}
    \la Ax,y \ra = (Ax)^\t y = x^\t A^\t y  = x^\t A y = \la x,Ay \ra 
    \end{equation*}

    (b)  Sea $u \in W^\perp$,  debemos ver que $Au \in W^\perp$,  es decir que $\la Au,w\ra =0$ para todo $w \in W$. Ahora bien,
    \begin{equation*}
        \la Au,w\ra =(Au)^\t w = u^\t A^\t w =u^\t A w = u^\t (A w) = \la u, Aw\ra=0.  
    \end{equation*}
    La última igualdad es válida pues $u \in W^\perp$ y  como $w \in W$, por hipótesis $Aw \in W$.
\end{proof}
    
    
 
    Veremos ahora que un operador simétrico, o equivalentemente, una matriz simétrica, es diagonalizable, es decir que hay una base de autovectores del operador o, equivalentemente, que existe una matriz $P$ invertible tal que $P^{-1}AP$ es diagonal. 
    
    Usaremos el siguiente resultado sin demostración.
    
    \begin{teorema}[Teorema fundamental del álgebra]\label{th-fundamental-del-algebra}
        Todo polinomio no constante con coeficientes complejos tiene al menos una raíz compleja. Es decir si 
        \begin{equation*}
            \text{$p(x) = a_nx^n+ a_{n-1}x^{n-1} +\cdots+a_0$, con $a_i \in \mathbb{C}$,  $a_n\ne 0$ y $n\ge 1$,}
        \end{equation*}
        entonces existe $\alpha \in \mathbb{C}$ tal que $p(\alpha)=0$.
    \end{teorema}

    Pese a llamarse ``Teorema fundamental del álgebra'', este resultado  no suele demostrarse en los cursos de álgebra, pues su demostración requiere del uso de análisis matemático.
    
    Si $\alpha$ es raíz de $p$, un polinomio de grado $n$, por  el teorema del resto,  $p(x) = (x-\alpha)p_1(x)$, con  $p_1$ un polinomio de grado $n-1$.  Aplicando inductivamente este procedimiento, podemos deducir:
    
    \begin{corolario} Si $p$ es un polinomio de de grado $n\ge 1$ con coeficientes en $\C$,  entonces
        \begin{equation*}
            p(x)= c(x-\alpha_1)(x-\alpha_2)\ldots(x-\alpha_n),
        \end{equation*}
        con $c,\alpha_i \in \C$.
    \end{corolario}
    
    
    \begin{obs}\label{5.9} 	Recordemos que si $a+bi \in \mathbb{C}$, $a$ es la \textit{parte real} y $b$ es la \textit{parte imaginaria}. El conjugado $a+b_i$ es $\overline{a +bi} = a-bi$. La conjugación cumple que $\overline{\overline{z}} = z$, $\overline{z +w} = \overline{z}+\overline{w}$ y $\overline{z w} = \overline{z}\,\overline{w}$ ($z,w \in \mathbb{C}$). Recordemos también que $z\overline{z} = |z|^2$.
        
        Si $x \in \mathbb{C}^n$,  entonces cada coordenada de $x$ es un número complejo,  es decir $x_i = a_i + ib_i$, con $a_i,b_i \in \R$. Luego si $v = (a_1,\ldots,a_n)$ y $w = (b_1,\ldots,b_n)$, tenemos que  $x = v + wi$  con  $v,w \in \R^n$.  En  este caso, diremos que $v$  es la parte real de $x$ y $w$ la parte imaginaria. También podemos extender la conjugación a $\mathbb{C}^n$ y  $\mathbb{C}^{n \times m}$ coordenada a coordenada y entonces no es difícil verificar que si $A, B \in \mathbb{C}^{n \times m}$
        \begin{equation*}\overline{\overline{A}} = A,\qquad \overline{A+B} = \overline{A}+\overline{B},
        \end{equation*}
        y que si $A\in \mathbb{C}^{n \times m}$, $ B \in \mathbb{C}^{m \times k}$, $\alpha \in \mathbb{C}$, entonces $\overline{\alpha A} =\overline{\alpha}\, \overline{A}$ y además
        \begin{equation*}\overline{\alpha A B} =\overline{\alpha}\, \overline{A}\,\overline{B} = \overline{A}\,\overline{\alpha B}.
        \end{equation*} 
        Notar también que si  $z = (z_1,\ldots,z_n)$, 
        \begin{equation*}z^\t \, \overline{z} = [ z_1 \, \ldots \, z_n ]
        {\begin{bmatrix} \overline{z_1} \\\vdots \\\overline{z_n} \end{bmatrix}} = |z_1|^2+\cdots+|z_n|^2,
        \end{equation*}   
        que es $>0$ si el vector no es nulo. Denotaremos la expresión de arriba como $||z||^2$.
        
    \end{obs}
    
    \begin{teorema}\label{th-existe-autovalor-simetrica}
        Sea $T$ un operador simétrico de $\R^n$. Entonces existe $\lambda \in \R$ autovalor real de $T$.   
    \end{teorema}
    \begin{proof} Sea $A = [T]_{\mathcal{C}}$. Extendamos $T$  a una transformación lineal de $T: \mathbb{C}^n \to \mathbb{C}^n$ de manera natural, con el producto de matrices $T(x) = Ax$ con $x \in \C^n$. Sea $\chi_A$ el polinomio característico de $A$. Por el teorema fundamental  del álgebra, existe $\lambda \in \mathbb{C}$ tal que $\chi_A(\lambda)=0$. Luego existe $x \in \mathbb{C}^n$, no nulo,  tal que $Ax = \lambda x$.  Veremos que $\lambda$ es un número real. Por un lado, como $A$ tiene coeficientes reales, tenemos que $A = \overline{A}$ y entonces:
        \begin{equation*}
        x^\t A\overline{x} = x^\t \overline{A}\overline{x} =x^\t\overline{Ax} =x^\t\overline{\lambda x} = \overline{\lambda} x^\t\overline{x} = \overline{\lambda} ||x||^2 .
        \end{equation*}
    Por otro lado,  como $A$  es simétrica,
        \begin{equation*}
        x^\t A\overline{x} = x^\t A^t\overline{x}= (A x)^\t\overline{x} = (\lambda x)^\t\overline{x} = \lambda x^\t\overline{x} = \lambda ||x||^2. 
        \end{equation*}
    Por lo tanto, $\lambda = \overline{\lambda}$, lo cual nos dice que ${\lambda} \in \R$.  Es decir, existe un vector $x \in \C^n$ no nulo y $\lambda \in \R$, tal que $Ax=\lambda x$. Si  $x = v +iw$ con $v,w \in \R^n$, entonces 
        \begin{equation*}
        \lambda v + i \lambda w = \lambda x = Ax = Av + i Aw.
        \end{equation*}
    Como $A$ es una matriz real $Av,Aw \in \R^n$ y como $\lambda \in \R$, tenemos que $Av = \lambda v$ y $Aw = \lambda w$ Como $x = v + iw$ es no nulo,  entonces o $v$ o $w$ son no nulos y por  lo tanto  hay al menos un autovector en $\R^n$ con autovalor $\lambda \in \R$. 
    \end{proof}
    
 
    
    El siguiente resultado, el \textit{teorema espectral}, requiere para su demostración una generalización del resultado anterior para espacios de producto interno  de dimensión finita y matrices (transformaciones lineales)  simétricas respecto a este producto interno. Todos estos conceptos y resultados son generalizaciones sencillas, pero llevan algún tiempo desarrollarlas y el lector interesado las puede ver en la sección \ref{secccion-operadores-autoadjuntos}. La demostración que daremos del teorema espectral se hará por inducción en la dimensión del espacio y utiliza en el paso inductivo la generalización del teorema \ref{th-existe-autovalor-simetrica}.   
        
    \begin{teorema}[Teorema espectral]\index{Teorema espectral} Sea $A$ matriz simétrica $n \times n$. Entonces existe $\mathcal{U} = \{u_1,\ldots,u_n\}$ una BON de $\R^n$ de autovectores de $A$.
    \end{teorema}
    \begin{proof}[Idea de la demostración (*)]		
        Se hará por inducción en $n= \dim(V)$.
        
        Si $n=1$ es trivial. 
        
        Supongamos que vale  para $n-1$ con $n >1$, y  probaremos el resultado para $n$. Por el teorema  \ref{th-existe-autovalor-simetrica} existe $\lambda \in \R$ y $x \in \R^n$ tal que $Ax = \lambda x$. Si $u_n = x/||x||$, $u_n$ tiene norma 1 y  cumple también que $Au_n = \lambda u_n$. Sea $W =\la u_n \ra$. Entonces $W$ es invariante y por proposición \ref{prop-t-simetrico-autoadjunto} tenemos que $W^\perp$ es invariante por $A$. De esta forma, podemos considerar entonces a  $A$ como una transformación lineal de $W^\perp$ a $W^\perp$. 
        
        No es difícil verificar que el producto interno canónico es un producto interno en $W^\perp$ y que $A:W^\perp \to W^\perp$ es una matriz simétrica con este producto interno restringido.  
        
        Como $\dim(W^\perp) = n-1$, por hipótesis inductiva existe   $\{u_1,\ldots,u_{n-1}\}$ una BON de $W^\perp$ de autovectores de $A: W^\perp \to W^\perp$. Es claro entonces que $\mathcal{U} = \{u_1,\ldots,u_n\}$  una BON de $\R^n$ de autovectores de $A$. 
    \end{proof}

    \begin{corolario}
         Sea $A$ matriz simétrica $n \times n$, entonces $A$ es diagonalizable.
    \end{corolario}


    \begin{ejemplo*} Encontremos autovalores y autovectores de  la matriz
        \begin{equation*}
            A = \begin{bmatrix} 2&-1&0\\-1&2&-1\\0&-1&2 \end{bmatrix}.
        \end{equation*}
        Como es una matriz simétrica sabemos que es diagonalizable, es decir tiene una base de autovectores. 
        El polinomio característicos es
        \begin{equation*}
            \chi_A(x) = \det  \begin{bmatrix} 2-x&-1&0\\-1&2-x&-1\\0&-1&2-x \end{bmatrix} =	-x^3 + 6 x^2 - 10 x + 4.
        \end{equation*}
        Ahora bien, las raíces de $-x^3 + 6 x^2 - 10 x + 4 $  son 
        \begin{align*}
            \lambda_1 &= 2 + \sqrt 2 \\
            \lambda_2 &= 2 \\
            \lambda_3 &= 2 - \sqrt 2
        \end{align*}
        Para averiguar los autovectores debemos plantear las ecuaciones $Ax = \lambda_ix$,  que resultan en los siguiente sistema de ecuaciones
        \begin{align*}
        \begin{split}
        2x_1 - x_2  &= \lambda_i x_1 \\
        -x_1 + 2x_2 -x_3 &=\lambda_ix_2 \\
        -x_2 + 2x_3 &= \lambda_ix_3,  
        \end{split}
        \end{align*}
        ($i=1,2,3$),  o equivalentemente,
        \begin{align*}
        \begin{split}
        (2 -\lambda_i)x_1+ x_2  &= 0 \\
        -x_1 +(2-\lambda_i)x_2  -x_3 &=0 \\
        -x_2 + (2-\lambda_i)x_3 &= 0,  
        \end{split}
        \end{align*}
        ($i=1,2,3$).
        En el caso de $\lambda_1= 2 + \sqrt 2$,  resulta
        \begin{align*}
        \begin{split}
        -\sqrt 2x_1+ x_2  &= 0 \\
        -x_1 -\sqrt 2x_2  -x_3 &=0 \\
        -x_2 -\sqrt 2x_3 &= 0,  
        \end{split}
        \end{align*}
        cuya solución es $\lambda(1, -\sqrt2, 1)$. Si continuamos resolviendo los sistemas de ecuaciones, podemos encontrar la siguiente base de autovectores:
        \begin{align*}
        v_1 &= (1, -\sqrt2, 1) \\		
        v_2 &= (-1, 0, 1) \\		
        v_3 &= (1, \sqrt2, 1).
        \end{align*}
        
    \end{ejemplo*}
    
    \end{section}

        
    \end{chapter}