\documentclass[a4paper,12pt,twoside,spanish]{amsbook}
\tolerance=10000
\renewcommand{\baselinestretch}{1.3}


%\renewcommand{\familydefault}{\sfdefault} % pasa las font a sans serif
% Para hacer el  indice en linea de comando hacer 
% makeindex main
%% En http://www.tug.org/pracjourn/2006-1/hartke/hartke.pdf hay ejemplos de packages de fonts libres, como los siguientes:
%\usepackage{cmbright}
%\usepackage{pxfonts}
%\usepackage[varg]{txfonts}
%\usepackage{ccfonts}
%\usepackage[math]{iwona}
\usepackage[math]{kurier}

\usepackage{etex}
\usepackage{t1enc}
\usepackage[spanish,es-nodecimaldot]{babel}
\usepackage{latexsym}
\usepackage[utf8]{inputenc}
\usepackage{verbatim}
\usepackage{multicol}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amsfonts,amssymb}
\usepackage{amsthm}
\usepackage{calc}         % From LaTeX distribution
\usepackage{graphicx}     % From LaTeX distribution
\usepackage{ifthen}
%\usepackage{makeidx}
\input{random.tex}        % From CTAN/macros/generic
\usepackage{subfigure} 
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{matrix}
\tikzset{
	every picture/.append style={
		execute at begin picture={\deactivatequoting},
		execute at end picture={\activatequoting}
	}
}
\usetikzlibrary{decorations.pathreplacing,angles,quotes}
\usetikzlibrary{shapes.geometric}
%\usetikzlibrary{graphs}
%\usepackage{tikz-3dplot} %for tikz-3dplot functionality
%\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage{stackrel}
%\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{tkz-graph}
\usepackage{polynom}

\polyset{%
	style=B,
	delims={(}{)},
	div=:
}

\renewcommand\labelitemi{$\circ$}


%% \theoremstyle{plain} %% This is the default
\oddsidemargin 0.0in \evensidemargin -1.0cm \topmargin 0in
\headheight .3in \headsep .2in \footskip .2in
\setlength{\textwidth}{16cm} %ancho para apunte
\setlength{\textheight}{21cm} %largo para apunte
%\leftmargin 2.5cm
%\rightmargin 2.5cm
\topmargin 0.5 cm

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
}
\usepackage{hypcap}

%\begin{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% CONTROLADORES DE ESTILO %%%
%%%--------------------------------------------------------------------------
%%% CAPÍTULOS 
%%%--------------------------------------------------------------------------
\usepackage{titlesec}%--
\newcommand{\hsp}{\hspace{15pt}}
\titleformat{\chapter}[hang]{\huge\bfseries}{{
		\fontsize{6em}{6em}\selectfont
		\thechapter}\hsp%\textcolor{}%
	{\vrule height 4em width 3pt}\hsp}{0pt}{\huge\bfseries}
%--------------------------------------------------------------------------
% SECCIONES 
%--------------------------------------------------------------------------
%\usepackage{titlesec}
\titleformat{\section}[frame]{\normalfont}%
{\filright\footnotesize\enspace \large {\large SECCIÓN \thesection} \enspace}%
{16pt}{\huge\bfseries\filcenter}%
%%%--------------------------------------------------------------------------	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{comment}
\renewcommand{\thesection}{\thechapter.\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}

\newtheorem{teorema}{Teorema}[section]
\newtheorem{proposicion}[teorema]{Proposici\'on}
\newtheorem{corolario}[teorema]{Corolario}
\newtheorem{lema}[teorema]{Lema}
\newtheorem{propiedad}[teorema]{Propiedad}
\newtheorem{resultado}[teorema]{Resultado}
\theoremstyle{definition}

\newtheorem{definicion}{Definici\'on}[section]
\newtheorem{ejemplo}{Ejemplo}[section]
\newtheorem{ejemplos}{Ejemplos}[section]
\newtheorem{problema}{Problema}[section]
\newtheorem{ejercicio}{Ejercicio}[section]
\newtheorem{ejerciciof}{}[section]

\theoremstyle{remark}
\newtheorem{observacion}{Observaci\'on}[section]
\newtheorem{obs}{Observaci\'on}[section]
\newtheorem{nota}{Nota}[section]


\renewcommand{\abstractname}{Resumen}
\renewcommand{\partname }{Parte }
\renewcommand{\indexname}{Indice }
\renewcommand{\figurename }{Figura }
\renewcommand{\tablename }{Tabla }
\renewcommand{\proofname}{Demostraci\'on}
\renewcommand{\refname }{Referencias }
\renewcommand{\appendixname }{Ap\'endice }
\renewcommand{\contentsname }{Contenidos }
\renewcommand{\chaptername }{Cap\'\i tulo }
\renewcommand{\bibname }{Bibliograf\'\i a }


\setcounter{MaxMatrixCols}{20}


\newcommand{\img}{\operatorname{Im}}
\newcommand{\nuc}{\operatorname{Nu}}
\newcommand\im{\operatorname{Im}}
\renewcommand\nu{\operatorname{Nu}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\renewcommand{\t}{{\operatorname{t}}}
\renewcommand{\sin}{{\,\operatorname{sen}}}
\newcommand{\Q}{\mathbb Q}
\newcommand{\R}{\mathbb R}
\newcommand{\C}{\mathbb C}
\newcommand{\K}{\mathbb K}
\newcommand{\F}{\mathbb F}
\newcommand{\Z}{\mathbb Z}
\newcommand{\sen}{\operatorname{sen}}
\newcommand \circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\makeindex
\begin{document}
	\baselineskip=0.55truecm %original
	%\baselineskip=0.43truecm
	
	\pagenumbering{roman}
	
	\title{Álgebra II / Álgebra - Notas del teórico  \\ Año 2020 - 2º Cuatrimestre  \\ FAMAF  - UNC \\ Versión: \today
	}
	
	\maketitle
	
	\newpage
	
	\author{
		${}^{}$
		\\${}^{}$
		\\${}^{}$
		\center{\textbf{AUTORES/COLABORADORES}} \\${}^{}$\\ 
		\flushleft 
		\begin{itemize}
			\item \textbf{Obra original:} Silvina Riveros,  Alejandro Tiraboschi y Agustín García  Iglesias. 
			\item \textbf{Colaboración: } -.   
			\item \textbf{Correcciones y sugerencias:} -. 
		\end{itemize}
	}
	
	%\author{A. L. Tiraboschi}
	
	%\address{FaMAF}
	%\subjclass{47A56, 47A10; Secondary 47A55, 47A70, 47A68, 47B15}
	%\date{July 19, 1992}
	%\translator{H. H. McFaden}
	
	\vskip 2cm 
	\thanks{
		\center{\textbf{LEER}} \\
		${}^{}$\\
		{\flushleft 
			Este material es distribuido bajo la licencia Creative Commons} \\
		{\center \textbf{Atribución--CompartirIgual 4.0 Internacional}}
		\\ 
		\center  lo cual significa 
		\\
		\flushleft
		- En cualquier explotación de la obra autorizada por la licencia será necesario reconocer los autores, colaboradores, etc.\\
		- La distribución de la obra u obras derivadas se debe hacer con una licencia igual a la que regula la obra original.\\
		${}^{}$
		\\
		Los detalles de la licencia pueden encontrarse en \href{https://creativecommons.org/licenses/by/4.0/deed.es}{Creative Commons}
	}
	
	\tableofcontents 
	
	\pagenumbering{arabic}
	
	\chapter*{Prefacio} 

	
	Las siguientes notas se han utilizado para el dictado del curso ``Álgebra II / Álgebra'' del primer año de las licenciaturas y profesorados de FAMAF. 	Han sido las notas principales en el dictado del año 2018 y 2020, y se limitan casi exclusivamente al contenido dictado en el curso. Las partes señaladas con (*) y los apéndices son optativos.
	
	Estas notas están basadas principalmente en \textit{Apuntes de Álgebra II - Año 2005} de Silvina Riveros y han sido revisadas, modificadas y ampliadas por Alejandro Tiraboschi  y Agustín García Iglesias. 
	
	También hemos utilizado como bibliografía de apoyo  los siguientes: 
	
	\begin{enumerate}
		\item [-] \textit{Álgebra Lineal.} Autores: Gabriela Jeronimo, Juan Sabia y Susana Tesauri. Año 2008. Puede descargarse del Departamento de Matemática de la UBA, en la dirección
		
		\href{http://mate.dm.uba.ar/~jeronimo/algebra_lineal/AlgebraLineal.pdf}	 {http://mate.dm.uba.ar/~jeronimo/algebra\_lineal/AlgebraLineal.pdf}
		\item [- ] \textit{Linear Algebra.} Autores: Jim Hefferon. Se descarga en
		
		\href{http://joshua.smcvt.edu/linearalgebra/}{http://joshua.smcvt.edu/linearalgebra/}
	\end{enumerate}
	
	
	Otra bibliografía de interés:
	\begin{enumerate}
		\item [-] \textit{Álgebra Lineal.}  Autor: Serge Lang. Año: 1976. Editorial: Fondo Educativo Interamericano. Ficha en biblioteca de FAMAF: \href{http://bit.ly/2stBTzs}{http://bit.ly/2stBTzs}
		\item [-] \textit{Álgebra Lineal.} Autores: Kenneth Hoffman y Ray Kunze.  Año: 1973. Editorial: Prentice Hall. Ficha en biblioteca de FAMAF:  \href{http://bit.ly/2tn3eRc}{http://bit.ly/2tn3eRc}
	\end{enumerate}
	
	\newpage
	\begin{center}
		\textbf{\large Contenidos mínimos}
	\end{center}
	
	\vskip .5cm
	
	\noindent Resolución de ecuaciones lineales. Matrices. Operaciones elementales. Matriz inversa. Espacios vectoriales sobre $\mathbb R$ y $\mathbb C$. Subespacios. Independencia lineal. Bases y dimensión Rectas y planos en $\mathbb R^n$. Transformaciones lineales y matrices. Isomorfismos. Cambio de bases. Núcleo e imagen de transformaciones lineales. Rango fila y columna. Determinante de una matriz. Cálculo y propiedades básicas. Espacios con producto interno. Desigualdad de Cauchy-Schwartz. Desigualdad triangular. Teorema de Pitágoras. Ortonormalización de Gram-Schmidt. Ecuaciones de rectas y planos en $\mathbb R^n$. Distancias. Introducción a vectores y valores propios. Aplicaciones. Diagonalización de matrices simétricas.
	\pagenumbering{roman}


	\maketitle
	
	

	
	
	\pagenumbering{arabic}
	
	\setcounter{chapter}{0}
	
\begin{chapter}{Vectores}\label{chap-vectores}

	El concepto de vector es básico para el estudio de funciones de varias variables y proporciona la motivación geométrica para todo el curso. Por lo tanto, las propiedades de los vectores, tanto algebraicas como geométricas, serán discutidas en forma resumida en este capítulo.
	
	\begin{section}{Álgebra lineal en $\R^2$ y $\R^3$}  
		Sabemos que se puede usar un número para representar un punto en una línea, una vez que se selecciona la longitud de una unidad.
		
		\begin{figure}[h]\label{fig-punto-en-R}
			\begin{tikzpicture}
			\draw[thick,->] (-4.0,0) -- (4.0,0) node[right] {$x$}; % eje x
			\foreach \x in {-3,...,3}
			\draw (\x,3pt) -- (\x,-3pt)
			node[anchor=north] {\x};
			\end{tikzpicture}
			\caption{La recta real y algunos números enteros.}
		\end{figure}
		
		Se puede usar un par de números $(x, y)$ para representar un punto en el plano. Estos pueden ser representados como en la figura \ref{fig-punto-en-R2}.
		\begin{figure}[h]
			\begin{tikzpicture}
			\draw[thick,->] (-4.0,0) -- (4.0,0) node[right] {$x$}; % eje x
			\draw[thick,->] (0,-3) -- (0,3) node[above] {$y$}; % eje y
			\foreach \x in {-3,...,-1}
			\draw (\x,3pt) -- (\x,-3pt)
			node[anchor=north] {\x};
			\foreach \x in {1,...,3}
			\draw (\x,3pt) -- (\x,-3pt)
			node[anchor=north] {\x};
			\foreach \y in {-2,...,-1}
			\draw (3pt,\y) -- (-3pt,\y) 
			node[anchor=east] {\y}; 
			\foreach \y in {1,...,2}
			\draw (3pt,\y) -- (-3pt,\y)
			node[anchor=east] {\y}; 
			\draw[fill] (2,1) circle [radius=0.07];
			%\draw[thick,->] (0,0) -- (2,1);
			\node [right] at (2,1) {$(2,1)$};
			\draw [dashed] (0,1) -- (2,1);
			\draw [dashed] (2,0) -- (2,1);
			\draw[fill] (-1,2.5) circle [radius=0.07];
			%\draw[thick,->] (0,0) -- (-1,2.5);
			\node [left] at (-1,2.5) {$(-1,2.5)$};
			\draw [dashed] (0,2.5) -- (-1,2.5);
			\draw [dashed] (-1,0) -- (-1,2.5);
			\draw[fill] (-2.5,-2.5) circle [radius=0.07];
			%\draw[thick,->] (0,0) -- (-2.5,-2.5);
			\node [below] at (-2.5,-2.5) {$(-2.5,-2.5)$};
			\draw [dashed] (0,-2.5) -- (-2.5,-2.5);
			\draw [dashed] (-2.5,0) -- (-2.5,-2.5);
			\end{tikzpicture}
			\caption{Representación gráfica de los puntos $(2,1)$, $(-1,2.5)$ y  $(-2.5,-2.5)$ en $\R^2$.}
			\label{fig-punto-en-R2}
		\end{figure}
		
		
		Ahora observamos que un triple de números $(x, y, z)$ se puede usar para representar un punto en el espacio, es decir, espacio tridimensional, o 3-espacio. Simplemente, introducimos un eje más. La figura \ref{fig-punto-en-R3} ilustra esto.
		\begin{figure}[h]
			\begin{tikzpicture}
			%draw the main coordinate system axes
			\draw[thick,->] (0,0,0) -- (5,0,0) node[right]{$y$};
			\draw[thick,->] (0,0,0) -- (0,5,0) node[above]{$z$};
			\draw[thick,->] (0,0,0) -- (0,0,5) node[anchor=north east]{$x$};
			\draw[fill] (3,2.5,3.5) circle [radius=0.07] node[anchor=west]{\,$v$};
			%\draw[thick,->] (0,0,0) -- (3,2.5,3.5) node[anchor=west]{$v$};
			\draw [dashed] (0,2.5,0) -- (3,2.5,0);
			\draw [dashed] (0,2.5,0) --  (0,2.5,3.5);
			\draw [dashed] (0,2.5,3.5) -- (3,2.5,3.5) ;
			\draw [dashed] (3,0,0) -- (3,2.5,0);
			\draw [dashed] (0,0,3.5) -- (3,0,3.5);
			\draw [dashed] (0,0,3.5) -- (0,2.5,3.5);
			\draw [dashed] (3,0,3.5) -- (3,0,0);
			\draw [dashed] (3,2.5,3.5) -- (3,2.5,0);
			\draw [dashed] (3,2.5,3.5) -- (3,0,3.5);
			\foreach \y in {1,...,4}
			\draw (\y,0.1,0) -- (\y,-0.1,0)
			node[anchor=north] {\y};
			\foreach \y in {1,...,4}
			\draw (-0.1,\y,0) -- (0.1,\y,0)
			node[anchor=east] {\y\;};
			\foreach \y in {1,...,4}
			\draw (0.1,-0.1,\y) -- (-0.1,0.1,\y)
			node[anchor=south east] {\y\,};
			\end{tikzpicture}
			\caption{Representación gráfica del punto $v = (3.5,\,3,\,2.5)$ en $\R^3$. }
			\label{fig-punto-en-R3}
		\end{figure}			
		
		En lugar de usar $(x,y,z)$, también suele usarse la notación $(x_1,x_2,x_3)$. La línea podría llamarse el 1-espacio, y el plano podría llamarse el 2-espacio. Por lo tanto, podemos decir que un solo número representa un punto en el 1-espacio. Un par representa un punto en el 2-espacio. Un triple representa un punto en el 3-espacio.
		
		Aunque no podemos hacer un dibujo para generalizar lo anterior a $4$-espacios, no hay nada que nos impida considerar un cuádruple de números y decretar que este es un punto en el $4$-espacio. Un quíntuple sería un punto en el 5-espacio, luego vendría un séxtuple, séptuple, óctuple, etc.
		
		Podemos generalizar y definir un punto en el $n$-espacio, para $n$ un entero positivo, como una $n$-tupla de números. Vamos a denotar tal $n$-tupla con letras $v$, $w$, $u$, ... y usaremos otras letras minúsculas para los números. Si $v =(x_1,x_2,\ldots,x_n)$, llamamos a los números $x_1,x_2,\ldots,x_n$ las \textit{coordenadas} del punto $v$. Más precisamente, $x_i$ será la \textit{coordenada $i$-ésima} de $v$. Por ejemplo, en el 3-espacio, 2 es la primera coordenada\index{coordenada} del punto $(2,3, -4)$, y $-4$ es la tercera coordenada. Denotamos a los  $n$-espacios por $\R^n$. Para formalizar:
		\begin{definicion}
			Sea $\R$ el cuerpo de los números reales,  entonces
			\begin{equation*}
				\R^n:= \{(x_1,x_2,\ldots,x_n): x_i \in \R, 1 \le i \le n \}.
			\end{equation*}
			Tode $v$ en $\R^n$ será llamado {\em punto}\index{punto}. Alternativamente, también podemos decir que $v$  es un \textit{vector en el origen} o simplemente un \textit{vector}. 
		\end{definicion}
		
		La mayoría de nuestros ejemplos tendrán lugar cuando $n = 2$ o $n = 3$. Por lo tanto, el lector puede visualizar cualquiera de estos dos casos a lo largo del apunte. Para ello usaremos el {\em sistema  de coordenadas cartesianas}\index{sistema  de coordenadas cartesianas}\index{coordenadas cartesianas} para representar los elementos de  $\R^2$ y  $\R^3$, tal como se ha hecho en las figuras \ref{fig-punto-en-R2} y \ref{fig-punto-en-R3}.
		
		
	
	
		\begin{ejemplo} \label{ej-3espacio-industria}
			Un ejemplo clásico de 3-espacio es, por supuesto, el espacio en el que vivimos. Después de seleccionar un origen y un sistema de coordenadas, podemos describir la posición de un punto (cuerpo, partícula, etc.) mediante 3 coordenadas. Además, como se sabía hace mucho tiempo, es conveniente extender este espacio a un espacio de 4 dimensiones, donde la cuarta coordenada es el tiempo, seleccionándose el origen del tiempo, por ejemplo, como el nacimiento de Cristo, aunque esto es puramente arbitrario. Entonces, un punto con coordenada de tiempo negativo es un punto antes de Cristo, y un punto con coordenada  de tiempo positiva es un punto después de Cristo.
			
			Sin embargo, no es que obligatoriamente ``el tiempo es la cuarta dimensión''. El espacio 4-dimensional anterior es solo un ejemplo posible. Hagamos un ejemplo relacionado a la economía: tomamos como coordenadas la cantidad de dinero gastado por una industria a lo largo de un año. 
			Por ejemplo, podríamos tener un espacio de 6 dimensiones con coordenadas correspondientes a las siguientes industrias: 1. acero, 2. automotriz, 3. productos agrícolas,  4. productos químicos, 5. indumentaria y 6. transporte. Las coordenadas de las 6-tuplas representarían el gasto anual de las industrias correspondientes. Por  ejemplo, 
			\begin{equation*}
			(1000, 800, 550, 300, 700, 200)
			\end{equation*}
			significaría que la industria del acero gastó 1000 en un año determinado, la automotriz 800, etc.
		\end{ejemplo} 

		También podemos visualizar los 3-espacios  como ``productos de espacios de dimensiones inferiores''. Por ejemplo, podemos ver las coordenadas de los 3-espacios como dos coordenadas en un 2-espacio acompañada por una coordenada en el 1-espacio. Esto es, $(x,y,z)$ indica el mismo punto que $((x,y),z)$.  Esto se escribe como 
		\begin{equation*}
			\R^3 = \R^2 \times \R^1.
		\end{equation*}
		Utilizamos el signo del producto, que no debe confundirse con otros ``productos'', como el producto de los números. Del mismo modo, podemos escribir
		\begin{equation*}
		\R^4 = \R^3 \times \R^1.
		\end{equation*}
		Hay otras formas de expresar $\R^4$ como un producto, a saber:
		\begin{equation*}
		\R^4 = \R^2 \times \R^2.
		\end{equation*}
		Esto significa que al punto $(x_1,x_2,x_3,x_4)\in \R^4$  lo podemos describir por el par ordenado $((x_l, x_2),(x_3, x_4))\in \R^2 \times \R^2$. 
		
		En  general, dado $n>1$, y $n_1,n_2$ tal que $n_1+n_2 = n$, tenemos
		\begin{equation*}
		\R^n= \R^{n_1} \times \R^{n_2}.
		\end{equation*} 
		De forma más general aún,  dado $n>1$, y $n_1,\ldots,n_k$ tal que $n_1+\cdots+n_k = n$, tenemos
		\begin{equation*}
		\R^n= \R^{n_1} \times \cdots\times \R^{n_k}.
		\end{equation*} 	
		Ahora vamos a definir cómo sumar los puntos de $\R^n$. Si $v$, $w$ son dos puntos, digamos en el 2-espacio,  definimos $v + w$ como el punto cuyas coordenadas son la suma de cada coordenada. Es decir, si, por ejemplo,  $v= (1, 2)$ y $w= (- 3, 5)$, entonces $v+w = (- 2, 7)$. En 3-espacios la definición es análoga. Por  ejemplo, si $v= (- 1, y, 3)$ y $w= (x, 7, - 2)$, entonces $v+w = (x - 1, y + 7, 1)$, con $x,y \in \R$.
		
		En  dos y tres dimensiones podemos definir
			Dados $(x_1,x_2), (y_1,y_2) \in \R^2$ o $(x_1,x_2,x_3), (y_1,y_2,y_3) \in \R^3$, definimos
		\begin{itemize}
			\item $(x_1,x_2)+ (y_1,y_2):=(x_1+y_1,x_2+y_2)$, 
			\item $(x_1,x_2,x_3)+ (y_1,y_2,y_3):=(x_1+y_1,x_2+y_2,x_3+y_3)$.
		\end{itemize}
		
		Generalizando, 
			\begin{definicion}
			Si $(x_1,\ldots,x_n), (y_1,\ldots,y_n) \in \R^n$, definimos la suma de los dos vectores como: 
					\begin{equation*}
					(x_1,\ldots,x_n)+ (y_1,\ldots,y_n):=(x_1+y_1,\ldots,x_n+y_n), 
				\end{equation*}
			\end{definicion}
			
		Observemos que se satisfacen las siguientes propiedades: sean $v,w,u$ en $\R^n$,  entonces
		\begin{enumerate}
		\item[\textbf{S1.}] $v + w = w + v$ (\textit{conmutatividad de la suma}),
		\item[\textbf{S2.}] $(v+ w)+ u = v + (w+u)$ (\textit{asociatividad de la suma}),
		\item[\textbf{S3.}] si definimos
		\begin{equation*}
		0 = (0,\dots,0),
		\end{equation*}
		el punto cuyas coordenadas son todas $0$, el \textit{vector cero}, entonces 
			\begin{equation*}
		v +0 = 0 +v = v,
		\end{equation*}
		 (\textit{existencia de elemento neutro de la suma}).
		\item[\textbf{S4.}]si $v = (x_1,\ldots,x_n)$,  definimos $-v = (-x_1,\ldots, -x_n)$. Entonces 
		\begin{equation*}
		v + (-v) = (-v) + v = 0
		\end{equation*} 
		(\textit{existencia de opuesto} o  \textit{inverso aditivo}).
	\end{enumerate}

\vskip .3cm

	Estas propiedades se deducen casi trivialmente de la definición de suma, coordenada a coordenada, y de la validez de las propiedades en el caso de la recta real. Como es usual en otros contextos ya conocidos, si $v,w \in \R^n$,  entonces denotamos $v- w := v + (-w)$.
	
	
	
	
	
	\begin{ejemplo} 
		Vimos al final del ejemplo  \ref{ej-3espacio-industria} que una $n$-tupla puede representar cuestiones relacionadas con las finanzas. En  nuestro ejemplo una 6-tupla representaba el gasto anual de determinadas actividades económicas, por ejemplo los gastos  en los años 2000 y 2001 son 
		\begin{center}
			\begin{tabular}{lcl}
				2000 \quad&$\rightarrow$\quad &$(1000, 800, 550, 300, 700, 200)$ \\
				2001 \quad&$\rightarrow$\quad &$(1200, 700, 600, 300, 900, 250)$ 
			\end{tabular} 
		\end{center}
		Luego los costos totales en los dos años son 
		\begin{align*}
			(1000, 800,& 550, 300, 700, 200) + (1200, 700, 600, 300, 900, 250) = \\
			&=(1000+1200, 800+700, 550+600, 300+300, 700+900, 200+250) \\
			&= (2200, 1500, 1350, 600, 1600, 450). 
		\end{align*} 
	\end{ejemplo}
	
	
	En  el ejemplo anterior es claro que la suma de puntos se corresponde con lo que nosotros esperamos que ocurra. Ahora interpretaremos la suma  geométricamente en el plano.
	
	En  álgebra lineal a veces resultará  conveniente pensar a cada punto como un {\em vector} que comienza en el origen.  Los vectores en  $\R^2$ y $\R^3$ se pueden graficar como ``flechas'' que parten del origen y llegan a las coordenadas del punto. Veamos en los siguientes ejemplos que está interpretación es útil. 
	
	\begin{ejemplo}
		Sea $v =(2,3)$ y $w= (-1, 1)$. Entonces $v+w= (1, 4)$. En el dibujo de los puntos involucrados aparece un \textit{paralelogramo} (fig. \ref{fig-ley-del-paralelogramo})
		\begin{figure}[h]
			\begin{tikzpicture}[scale=1.0]
				\draw[thick,->] (-3.0,0) -- (3.0,0) node[right] {$x$}; % eje x
				\draw[thick,->] (0,-1) -- (0,4) node[above] {$y$}; % eje y
				\foreach \x in {-2,...,2}
				\draw (\x,3pt) -- (\x,-3pt);
				\foreach \y in {0,...,3}
				\draw (3pt,\y) -- (-3pt,\y) ;
				\draw[very thick, ->] (0,0) -- (-1,1);
				\node [left] at (-1,1) {$(-1,1)$};
				\draw[very thick,->] (0,0) -- (2,3);
				\node [right] at (2,3) {$(2,3)$};
				\draw[very thick,-] (0,0) -- (1,4);
				\node [above] at (1,4) {$(1,4)$};
				\draw[fill] (1,4) circle [radius=0.05];
				\draw[-] (-1,1) -- (1,4);
				\draw[-] (2,3) -- (1,4);
			\end{tikzpicture}
			\caption{\;}\label{fig-ley-del-paralelogramo}
		\end{figure} 
	\end{ejemplo}
	
	\begin{ejemplo}
		Sea $v= (3, 1)$ y $w=(1,2)$. Entonces
		\begin{equation*}
			v+w = (4,3). 
		\end{equation*}
		Esta suma la representamos en la fig. \ref{fig-ley-del-paralelogramo-2}. 
		\begin{figure}[h]
			\begin{tikzpicture}[scale=1.0]
				\draw[thick,->] (-3.0,0) -- (4.0,0) node[right] {$x$}; % eje x
				\draw[thick,->] (0,-1) -- (0,3) node[above] {$y$}; % eje y
				\foreach \x in {-2,...,3}
				\draw (\x,3pt) -- (\x,-3pt);
				\foreach \y in {0,...,2}
				\draw (3pt,\y) -- (-3pt,\y) ;
				\draw[very thick, ->] (0,0) -- (3,1);
				\node [right] at (3,1) {$v$};
				\draw[very thick,->] (0,0) -- (1,2);
				\node [above] at (0.9,2) {$w$};
				\draw[very thick,-] (0,0) -- (4,3);
				\node [above] at (4,3) {$v+w$};
				\draw[fill] (4,3) circle [radius=0.05];
				\draw[-] (3,1) -- (4,3);
				\draw[-] (1,2) -- (4,3);
			\end{tikzpicture}
			\caption{\;}\label{fig-ley-del-paralelogramo-2}
		\end{figure}
		
		Vemos de nuevo que en la representación geométrica aparece un paralelogramo. La razón por la cual la figura que aparece es un paralelogramo se puede dar en 	términos de la geometría plana de la siguiente manera. Obtenemos $v = (1, 2)$ comenzando desde el origen $0 = (0, 0)$, y moviéndonos 1 unidad hacia la derecha y 2 hacia arriba. Para obtener $v+w$, comenzamos desde $v$, y de nuevo nos movemos 1 unidad a la derecha y 2 hacia arriba. Así, el segmento entre 0 y $w$, y entre $v$ y $v+w$ son las hipotenusas de los triángulos rectángulos cuyos catetos  correspondientes son de la misma longitud y paralelos. Los segmentos anteriores son por lo tanto paralelos y de la misma longitud, como se ilustra en la fig. \ref{fig-ley-del-paralelogramo-3}.
		\begin{figure}[h]
			\begin{tikzpicture}[scale=1.0]
			\draw[thick,->] (-3.0,0) -- (4.0,0) node[right] {$x$}; % eje x
			\draw[thick,->] (0,-1) -- (0,3) node[above] {$y$}; % eje y
			\foreach \x in {-2,...,3}
			\draw (\x,3pt) -- (\x,-3pt);
			\foreach \y in {0,...,2}
			\draw (3pt,\y) -- (-3pt,\y) ;
			\draw[very thick,-] (0,0) -- (1,2);
			\node [above] at (0.9,2) {$w$};
			\draw[very thick,-] (3,1) -- (4,3);
			\node [above] at (4,3) {$v+w$};
			\node [below] at (3,1) {$v$};
			\node [left] at (0,-0.3) {$0$};
			\draw [dashed] (1,0) -- (1,2);
			\draw [dashed] (3, 1) -- (4,1) -- (4,3);
			\draw[fill] (0,0) circle [radius=0.07];
			\draw[fill] (1,2) circle [radius=0.07];
			\draw[fill] (3,1) circle [radius=0.07];
			\draw[fill] (4,3) circle [radius=0.07];
			\end{tikzpicture}
			\caption{\;}\label{fig-ley-del-paralelogramo-3}
		\end{figure}
	\end{ejemplo}
	
	
	\begin{ejemplo} Sea el punto $v =(3, 1)$ , entonces $-v = (- 3, - 1)$. Si dibujamos $v$ y $-v$ vemos que $-v$ es un vector del mismo ``tamaño'' que $v$ pero con la dirección opuesta. Podemos ver a $-v$ como la reflexión de $v$ a través del origen (fig. \ref{fig-vector-opuesto}). 
	\begin{figure}[h]
		\begin{tikzpicture}[scale=1.0]
		\draw[thick,->] (-4.0,0) -- (4.0,0) node[right] {$x$}; % eje x
		\draw[thick,->] (0,-2) -- (0,2) node[above] {$y$}; % eje y
		\foreach \x in {-3,...,3}
		\draw (\x,3pt) -- (\x,-3pt);
		\foreach \y in {-1,...,1}
		\draw (3pt,\y) -- (-3pt,\y) ;
		\draw[very thick, ->] (0,0) -- (3,1);
		\node [right] at (3,1) {$v$};
		\draw[very thick, ->] (0,0) -- (-3,-1);
		\node [left] at (-3,-1) {$-v$};
		\draw[fill] (0,0) circle [radius=0.1];
		\end{tikzpicture}
		\caption{\;}\label{fig-vector-opuesto}
	\end{figure}
	\end{ejemplo}

	La resta de dos vectores también se puede representar geométricamente: restemos  al vector $v$ el vector $w$. Como primera opción podemos encontrar el  vector $-w$ y sumarlo  a $v$ aplicando la regla del paralelogramo. Esto es equivalente a lo siguiente: los vectores $v$ y $w$ determinan el triángulo determinado por los puntos $0$, $v$ y $w$. Entonces,  el lado determinado por $w$ y $v$,  en ese sentido, trasladado al origen es el vector $v-w$  (fig. \ref{fig-resta-de-vectores}). 
	\begin{figure}[h]
		\begin{tikzpicture}[scale=1.0]
		\draw[thick,->] (-3.0,0) -- (4.0,0) node[right] {$x$}; % eje x
		\draw[thick,->] (0,-1) -- (0,3) node[above] {$y$}; % eje y
		\foreach \x in {-2,...,3}
		\draw (\x,3pt) -- (\x,-3pt);
		\foreach \y in {0,...,2}
		\draw (3pt,\y) -- (-3pt,\y) ;
		% vector w
		\draw[fill] (3,1) circle [radius=0.05];
		\node [right] at (3,1) {$w$};
		\draw[very thick, ->] (0,0) -- (2.96,0.96);
		% vector v
		\draw[fill] (1,2) circle [radius=0.05];
		\draw[very thick,->] (0,0) -- (0.97,1.96);
		\node [above] at (0.9,2) {$v$};
		% flecha punteada de w a v
		\draw[dashed,->,very thick] (3,1) -- (1.055,1.965);
		% vector v - w
		\draw[fill] (-2,1) circle [radius=0.05];
		\node [above] at (-2,1) {$v-w$};
		\draw[very thick,->] (0,0) -- (-2*0.975,1*0.975);	
		% punteada de v-w  a v
		\draw[dashed] (-2,1) -- (1,2);	
		\end{tikzpicture}
		\caption{Al  vector $v$ le restamos $w$.}\label{fig-resta-de-vectores}
	\end{figure}
	
	

	Ahora consideraremos la multiplicación de un vector $v$ por un número. 
	
	\begin{definicion}
		Sea $v = (x_1,\ldots,x_n) \in \R^n$ y $\lambda \in \R$, entonces
		\begin{equation*}
			\lambda.v = (\lambda x_1,\ldots,\lambda x_n).
		\end{equation*}  
		También denotamos a esta multiplicación por $\lambda v$.
	\end{definicion}

	\begin{ejemplo}
		 Si $v= (2, -1,5)$  y $\lambda = 7$, entonces $\lambda v = (14, -7.35)$.
	\end{ejemplo}
	
	Es fácil verificar las siguientes reglas: dados $v,w \in \R^n$, 
	\begin{enumerate}
		\item[\textbf{P1.}] $1.v=v$.
		\item[\textbf{P2.}] $\lambda_1(\lambda_2v) = (\lambda_1\lambda_2)v$, para todo $\lambda_1,\lambda_2 \in \R$.
		\item[\textbf{D1.}] $\lambda(v+w) = \lambda v +\lambda w$, para todo $\lambda \in \R$  (\textit{propiedad distributiva}).
		\item[\textbf{D2.}] $(\lambda_1+\lambda_2)v = \lambda_1v + \lambda_2 v$ para todo $\lambda_1,\lambda_2 \in \R$  (\textit{propiedad distributiva}).
	\end{enumerate}

	También tengamos en cuenta que 
	\begin{equation*}
		(-1)v = -v.
	\end{equation*}
	
	¿Cuál es la representación geométrica de la multiplicación de un vector  por un número?
	\begin{ejemplo}
		Sea $v= (1,2)$ y $\lambda = 3$. Luego $\lambda v = (3,6)$ como en la siguiente figura:
		\begin{center}
			\begin{tabular}{ccc}
				\begin{tikzpicture}[scale=1.0]
				\draw[thick,->] (-1.0,0) -- (4.0,0) node[right] {$x$}; % eje x
				\draw[thick,->] (0,-1) -- (0,6) node[above] {$y$}; % eje y
				\foreach \x in {0,...,3}
				\draw (\x,3pt) -- (\x,-3pt);
				\foreach \y in {-1,...,5}
				\draw (3pt,\y) -- (-3pt,\y) ;
				\draw[very thick, -] (0,0) -- (1,2);
				\node [right] at (1.1,1.9) {$v=(1,2)$};
				\draw[very thick, -] (0,0) -- (3,6);
				\node [right] at (3.1,5.9) {$3v=(3,6)$};
				\draw[very thick, -] (0,0) -- (0.5,1);
				\node [right] at (0.6,0.9) {$\frac12v=(0.5,1)$};
				\draw[fill] (0,0) circle [radius=0.07];
				\draw[fill] (0.5,1) circle [radius=0.07];
				\draw[fill] (3,6) circle [radius=0.07];
				\draw[fill] (1,2) circle [radius=0.07];
				\node [right] at (1.8,-1.5) {(a)};
				\end{tikzpicture}
				&\qquad &
				\begin{tikzpicture}[scale=0.58]
				\draw[thick,->] (-4.0,0) -- (4.0,0) node[right] {$x$}; % eje x
				\draw[thick,->] (0,-6) -- (0,6) node[above] {$y$}; % eje y
				\foreach \x in {-3,...,3}
				\draw (\x,3pt) -- (\x,-3pt);
				\foreach \y in {-5,...,5}
				\draw (3pt,\y) -- (-3pt,\y) ;
				\draw[very thick, -] (0,0) -- (1,2);
				\node [right] at (1.1,1.9) {$v$};
				\draw[very thick, -] (0,0) -- (3,6);
				\node [right] at (3.1,5.9) {$3v$};
				\draw[very thick, -] (0,0) -- (-3,-6);
				\node [right] at (-3.0,-5.9) {$-3v$};
				\draw[fill] (0,0) circle [radius=0.12];
				\draw[fill] (-3,-6) circle [radius=0.12];
				\draw[fill] (3,6) circle [radius=0.12];
				\draw[fill] (1,2) circle [radius=0.12];
				\node [right] at (-0.2,-7) {(b)};
				\end{tikzpicture}
			\end{tabular}
		\end{center}
			
			
	\end{ejemplo}

\vskip .3cm
	 La multiplicación por 3 equivale a ``estirar'' $v$ por 3. Del mismo modo, $\frac12v$ 	equivale a estirar $v$ en $\frac12$, es decir, reducir $v$ a la mitad de su tamaño. En general, si $t$ es un número con $t> 0$, interpretamos $tv$ como un punto en la misma dirección que $v$ con tamaño  $t$-veces el tamaño de $v$.  De hecho, decimos que $v$ y $w$ tienen la misma dirección si existe un número $\lambda> 0$ tal que $v = \lambda w$. La multiplicación por un número negativo invierte la dirección. Así, $-3 v$ se representa como en la figura anterior, en la parte (b). Decimos que $v$ y $w$  (ninguno de los cuales es cero) tienen direcciones opuestas si existe un número $\lambda <0$ tal que $v =\lambda w$. Por lo tanto, $-v$ tiene dirección opuesta a $v$.

		
\vskip .3cm Más allá de las interpretaciones geométricas,  hemos definido  en forma algebraica la suma de vectores en $\R^n$ y  la multiplicación de un vector por un escalar,  y estas operaciones tienen ciertas propiedades de interés. 
\vskip .3cm
Concluyendo, las definiciones y resultados más importantes de esta sección son:
\vskip .3cm

Sean  $(x_1,\ldots,x_n), (y_1,\ldots,y_n) \in \R^n$ y $\lambda \in \R$, definimos
	\begin{itemize}
		\item $(x_1,\ldots,x_n)+ (y_1,\ldots,y_n):=(x_1+y_1,\ldots,x_n+y_n)$, 
		\item $\lambda.v := (\lambda x_1,\ldots,\lambda x_n)$.
	\end{itemize}
Dados  $v,w,u$ en $\R^n$,  se verifican
\begin{enumerate}
	\item[\textbf{S1.}] $v + w = w + v$ (\textit{conmutatividad de la suma}),
	\item[\textbf{S2.}] $(v+ w)+ u = v + (w+u)$ (\textit{asociatividad de la suma}),
	\item[\textbf{S3.}] sea $0: =(0,\ldots, 0)$, el \textit{vector cero}, entonces $0+ v = v + 0 =v$ (\textit{existencia de elemento neutro de la suma}).
	\item[\textbf{S4.}] Si $v =(x_1,\ldots,x_n)$, entonces $-v:=(-x_1,\ldots,-x_n)$ y se satisface  $v + (-v) = (-v)+ v =0$ (\textit{existencia de opuesto} o  \textit{inverso aditivo}).
	\item[\textbf{P1.}] $1.v=v$.
	\item[\textbf{P2.}] $\lambda_1(\lambda_2v) = (\lambda_1\lambda_2)v$, para todo $\lambda_1,\lambda_2 \in \R$.
	\item[\textbf{D1.}] $\lambda(v+w) = \lambda v +\lambda w$, para todo $\lambda \in \R$  (\textit{propiedad distributiva}).
	\item[\textbf{D2.}] $(\lambda_1+\lambda_2)v = \lambda_1v + \lambda_2 v$ para todo $\lambda_1,\lambda_2 \in \R$  (\textit{propiedad distributiva}).
\end{enumerate}

\vskip .3cm

Verán más adelante que las propiedades anteriores son muy parecidas a los  ``axiomas'' que se utilizan en el  capítulo \ref{chap-esp-vect} para definir espacios vectoriales abstractos (ver definición \ref{def-esp-vect}). 

\vskip .3cm

	\begin{definicion}
	Dado, $n \in \mathbb N$, para cada $i\in\{1, ..., n\}$, se denota ${e_i}\in\R^n$ al vector cuyas coordenadas son todas $0$ excepto la coordenada $i$ que es un $1$.
	\begin{align*}
	e_i:=(0, ..., 1, ..., 0)
	\end{align*}
	El conjunto $\{e_1, ..., e_n\}$ se llama\textit{ {base canónica}} de $\R^n$.\index{base canónica de $R^n$}
\end{definicion}

\begin{ejemplo}
	En $\R^3$ los vectores son $
	e_1=(1,0,0)$, $
	e_2=(0,1,0)$, 
	$e_3=(0,0,1)
	$
\end{ejemplo}

\vskip .3cm

Estos vectores jugarán un rol central en la materia, principalmente, por la siguiente propiedad.


\begin{proposicion}
	Todo vector de $\R^n$ se escribe como combinación lineal de la base canónica. Explícitamente, si $(x_1, ..., x_n)\in\R^n$ entonces
	\begin{align*}
	(x_1, ..., x_n)=x_1e_1+x_2e_2+\cdots+x_ne_n.
	\end{align*}
\end{proposicion}


La demostraci\'on es trivial pero por ahora no la haremos. 


\begin{ejemplo}
	
	\begin{align*}
	(1,2,3)&=(1,0,0)+(0,2,0)+(0,0,3)\\
	&=1(1,0,0)+2(0,1,0)+3(0,0,1)\\
	&=1e_1+2e_2+3e_3
	\end{align*}
	
\end{ejemplo}



\vskip .6cm


\end{section}
	
	
	
	\begin{section}{El producto escalar}
		
		En 2-espacios, dados dos vectores $v = (x_1, x_2)$ y $w= (y_l, y_2)$, definimos su \textit{producto escalar}\index{producto escalar} como
		\begin{equation*}
			\langle v , w  \rangle :=x_1y_1 + x_2y_2.
		\end{equation*}
		Para el caso de 3-espacios, sean   $v = (x_1, x_2,x_3)$ y $w= (y_l, y_2,y_3)$,  entonces el \textit{producto escalar de $v$ y $w$} es
		\begin{equation*}
			\langle v , w  \rangle :=x_1y_1 + x_2y_2+x_3y_3.
		\end{equation*}
		Finalmente, en los $n$-espacios,  generalizamos la definición de la manera obvia: 
		
		\begin{definicion}
			Sean  $v = (x_1, \ldots,x_n)$ y $w= (y_l, \ldots,y_n)$ vectores de $\R^n$,  el \textit{producto escalar de $v$ y $w$} se define como		
			\begin{equation*}
			\langle v , w \rangle :=x_1y_1 + x_2y_2+\cdots+x_ny_n.
			\end{equation*}
		\end{definicion}
		
		
		
		Es importante notar  que este producto es un número real. Por ejemplo, si
		\begin{equation*}
			v= (1, 3, - 2) \quad\text{ y } \quad w= (- 1, 4, - 3),
		\end{equation*}
		entonces
		\begin{equation*}
			\langle v , w \rangle= - 1 + 12 + 6 = 17.
		\end{equation*}
		
		Por el momento, no le damos una interpretación geométrica a este producto escalar y veremos esto en la sección \ref{secccion-norma-de-un-vector}. Ahora derivaremos algunas propiedades importantes. Los básicas son las siguientes: sean $v$, $w$, $u$  tres vectores, entonces
		
		
		\begin{enumerate}[label=\textbf{P\arabic*.},ref=P\arabic*]
			\item\label{prop-P1}	$\langle v , w \rangle = \langle w , v \rangle$.
			\item\label{prop-P2} 
			\begin{equation*}
				\langle v , w + u \rangle =\langle v , w \rangle + \langle v , u \rangle = \langle w +u , v \rangle.
			\end{equation*}
			\item\label{prop-P3} Si $\lambda$ es un número, entonces 
			\begin{equation*}
				\langle \lambda v , w \rangle = \lambda \langle v , w \rangle \quad \text{ y } \quad  \langle v , \lambda w \rangle = \lambda \langle v , w \rangle.
			\end{equation*}
			\item\label{prop-P4} Si $v=0$ es el vector cero, entonces $\langle v , v \rangle =0$,  de lo contrario
			\begin{equation*}
				\langle v , v \rangle >0
			\end{equation*}
		\end{enumerate}
		
		Ahora vamos a probar estas propiedades. En cuanto a la primera, tenemos que
		\begin{equation*}
			x_1y_1 + x_2y_2+\cdots+x_ny_n = y_1x_1 + y_2x_2+\cdots+y_nx_n
		\end{equation*}
		porque para cualquiera de los dos números $x, y$, tenemos que $xy=yx$. Esto prueba la propiedad \ref{prop-P1}. 
		
		Para \ref{prop-P2}, sea $u = (z_1, \ldots, z_n)$. Entonces
		\begin{equation*}
			w + u = (y_1+z_1, \ldots, y_n+ z_n)
		\end{equation*}
		y
		\begin{align*}
			\langle v , w + u \rangle &= \langle (x_1, \ldots,x_n) , (y_1+z_1, \ldots, y_n+ z_n) \rangle\\
			&= x_1(y_1+z_1) + \cdots x_n(y_n+z_n) \\
			&= x_1y_1+x_1z_1 + \cdots x_ny_n+x_nz_n
		\end{align*}
		Reordenando los términos obtenemos
		\begin{equation*}
				\langle v , w + u \rangle =  x_1y_1+\cdots +  x_ny_n +x_1z_1 + \cdots+x_nz_n,
		\end{equation*}
		que no es otra cosa que $\langle v , w \rangle + \langle v , u \rangle$.
		
		Dejamos la propiedad \ref{prop-P3} como ejercicio.
		
		Finalmente probemos \ref{prop-P4}. Observemos que 
		\begin{equation}\label{eq-v-escalar-v}
			\langle v , v \rangle = x_1^2 + x_2^2 + \cdots + x_n^2.
		\end{equation}
		Como $x_i^2 \ge 0$ para todo $i$,  entonces $\langle v , v \rangle \ge 0$. Además, es claro que si $v$ tiene todas las coordenadas iguales a 0,  entonces  $\langle v , v \rangle =0$. En  el caso que $v\not=0$, entonces,  existe algún $i$  tal que  $x_i \ne 0$, por lo tanto $x_i^2>0$ y por la ecuación (\ref{eq-v-escalar-v}), tenemos que  $\langle v , v \rangle>0$.
		
		\vskip .3cm
		
		Por  la propiedad \ref{prop-P1} diremos que el producto escalar es \textit{simétrico}, por las propiedades  \ref{prop-P2} y \ref{prop-P3} diremos que es una \textit{forma bilineal} y, finalmente, por la propiedad \ref{prop-P4} diremos que es \textit{definido positivo.} 
		
		\vskip .3cm
		
		
		
		
		
		El producto escalar $\langle v , w \rangle$ puede ser  igual a 0 para determinados  vectores,  incluso ambos distintos de 0.  Por ejemplo, si $v = (1,2,3)$ y $w = (2, 1, -\frac43)$,  entonces
		\begin{equation*}
			\langle v , w \rangle = 2 + 2 -4 =0.
		\end{equation*}
		\begin{definicion}
			Decimos que dos vectores $v$ y $w$ en $\mathbb R^n$ son  \textit{perpendiculares} u \textit{ortogonales} si  $\langle v , w \rangle=0$. Cuando $v$ y $w$ son ortogonales  denotamos $v \perp w$.
		\end{definicion}
		
		Por el momento, no es claro que en el plano la definición anterior coincida con nuestra noción geométrica e intuitiva de perpendicularidad. Esto lo veremos en la siguiente sección. Aquí nos limitaremos a observar un ejemplo.
		
		\begin{ejemplo}
			En  $\R^3$ consideremos los vectores
			\begin{equation*}
			e_1 = (1,0,0),\quad e_2 = (0, 1,0),\quad e_3 = (0,0,1),
			\end{equation*}
			representados en la fig. \ref{fig-canonicos-en-R3}
			\begin{figure}[h]
				\begin{tikzpicture}[scale=1.3]
				%draw the main coordinate system axes
				\draw[thick,->] (0,0,0) -- (2,0,0) node[right]{$y$};
				\draw[thick,->] (0,0,0) -- (0,2,0) node[above]{$z$};
				\draw[thick,->] (0,0,0) -- (0,0,2) node[anchor=north east]{$x$};
				\draw[very thick,->] (0,0,0) -- (1,0,0);
				\node [below] at (1,0,0.1) {$e_2$};
				\draw[very thick,->] (0,0,0) -- (0,1,0) node[anchor=west]{$e_3$};
				\draw[very thick,->] (0,0,0) -- (0,0,1);
				\node [below] at (0.2,0,1) {$e_1$};
				\draw [dashed] (0,0.8,0) -- (1.5,0.8,0);
				\draw [dashed] (1.5,0,0) -- (1.5,0.8,0);
				\draw[very thick,->] (0,0,0) -- (1.5,0.8,0) node[right]{$v$};
				\end{tikzpicture}
				\caption{}
				\label{fig-canonicos-en-R3}
			\end{figure}
			
			Luego, vemos que $\langle e_i , e_j \rangle = 0$, si $i \ne j$ y por lo tanto $e_i$  es perpendicular a $e_j$ si $i \ne j$, lo cual concuerda con nuestra intuición. 
		\end{ejemplo} 
			
			Observemos que si $v = (x_1,x_2,x_3)$,  entonces $\langle v , e_i \rangle = x_i$. Por lo tanto,  si la coordenada $i$-ésima de $v$ es cero,  $v$  es ortogonal a $e_i$. Esto nos dice,  por ejemplo,  que si $v$  es un vector contenido en el plano que incluye $e_2$ y $e_3$,  es decir si la primera coordenada es cero,  entonces $v$  es ortogonal a $e_1$. 
			
		
		\begin{ejemplo} Sea $(a,b)$ un vector en $\R^2$, entonces $(-b,a)$ es un vector ortogonal a $(a,b)$ debido a que
			\begin{equation*}
				\langle (a,b) , (-b,a) \rangle = a\cdot b + (-b) \cdot a = 0.
			\end{equation*}
			Si graficamos con un ejemplo, $a=1$, $b=3$; vemos que esto se corresponde con nuestra intuición de perpendicularidad. 
				
			\begin{center}
				\begin{tikzpicture}[scale=0.8]
				\draw[thick,->] (-4.0,0) -- (4.0,0) node[right] {$x$}; % eje x
				\draw[thick,->] (0,-1) -- (0,3) node[above] {$y$}; % eje y
				\foreach \x in {-3,...,3}
				\draw (\x,3pt) -- (\x,-3pt);
				\foreach \y in {0,...,2}
				\draw (3pt,\y) -- (-3pt,\y) ;
				% vector w
				\draw[very thick,->] (0,0) -- (1,3);
				\node [above] at (1,3) {$(a,b)$};
				\draw[very thick,->] (0,0) -- (-3,1);
				\node [above] at (-3,1) {$(-b,a)$};
				\end{tikzpicture}
			\end{center}	
				
		\end{ejemplo}
		
		
	\end{section}

	\begin{section}{La norma de un vector}\label{secccion-norma-de-un-vector}
	Si $v$  es vector,  entonces $\langle v , v \rangle \ge 0$ y definimos como la \textit{norma de $v$}\index{norma de un vector} o \textit{longitud de $v$} al número
	\begin{equation*}
		||v|| = \sqrt{\langle v , v \rangle}.
	\end{equation*}
	
	Cuando $v$ pertenece al plano  y $v =(x,y)$,  entonces $||v|| = \sqrt{x^2 + y^2}$ y si graficamos el vector en la fig. \ref{fig-pitagoras}, vemos que la noción de norma o longitud en $\R^2$ se deduce del teorema de Pitágoras.  
	\begin{figure}[h]
		\begin{tikzpicture}
		\draw[thick,->] (0,0) coordinate (O) -- (3,0);
		\draw[thick,->] (O) -- (0,4);
		\node[inner sep=1.5pt,fill,circle,label={60:$(x,y)$}] at (2,3) (point) {};
		\draw[very thick] (0,0) -- (point);
		%\draw (1.8,0) -- ++(0,0.2) -- ++(0.2,0);
		\draw[dashed] (2,0) coordinate (pointx) -- (point); 
		\draw[decoration={brace,mirror,raise=5pt},decorate]
		(2,0) -- node[right=6pt] {$y$} (point);
		\draw[decoration={brace,mirror,raise=5pt},decorate]
		(0,0) -- node[below=6pt] {$x$} (2,0); 
		\draw[decoration={brace,raise=5pt},decorate]
		(0,0) -- node[below=6pt] {${}$} (2,3);
		\node [right] at (0.5,1.8) {$r$};
		\end{tikzpicture}
		\caption{$r= \sqrt{x^2 + y^2}$.}
		\label{fig-pitagoras}
	\end{figure} 

	Si $n=3$,  el dibujo es como en la fig. \ref{fig-pitagoras-3d}, para $v =(x,y,z)$.  Es decir,  por la aplicación reiterada del teorema de Pitágoras obtenemos que la longitud de $v$ es $\sqrt{x^2 + y^2 +z^2}$.
	\begin{figure}[h]
		\begin{tikzpicture}[scale=0.8]
		\draw[thick,->] (0,0,0) -- (5,0,0) node[right]{$y$};
		\draw[thick,->] (0,0,0) -- (0,5,0) node[above]{$z$};
		\draw[thick,->] (0,0,0) -- (0,0,5) node[anchor=north east]{$x$};
		\draw[very thick,->] (0,0,0) -- (4,4.5,3) node[anchor=west]{$v$};
		\draw[dashed]  (0,0,0) -- (4,0,3); 
		\draw[dashed]  (0,0,3) -- (4,0,3); 
		\draw[dashed]  (4,0,0) -- (4,0,3); 
		\draw[dashed]  (4,0,3) -- (4,4.5,3); 
		%\draw[decoration={brace,raise=5pt},decorate]
		%(0,0,0) -- node[below=6pt] {${}$} (4,4.5,3);
		\node [right] at (2,2.2,1.5) {$r$};
		\node [right] at (1.2,-0.1,1.5) {$w$};
		\node [below] at (4.2,0.2,3.5) {$(x,y)$};
		\end{tikzpicture}
		\caption{$w= \sqrt{x^2 + y^2}$, $r = \sqrt{w^2 + z^2} = \sqrt{x^2 + y^2 +z^2}$.}
		\label{fig-pitagoras-3d}
	\end{figure} 
	
	En  general,  si $v =(x_1,x_2,\ldots,x_n) \in \R^n$,  entonces
	\begin{equation*}
		||v|| = \sqrt{x_1^2+x_2^2+\cdots+x_n^2}
	\end{equation*} 
	y la aplicación reiterada del  teorema de Pitágoras nos dice que esta es la definición correcta de longitud o norma de un vector. 
	
	\begin{proposicion}\label{prop-lambda-norma}
		Sea $v \in \R^n$ y $\lambda \in \R$,  entonces
		\begin{equation*}
			||\lambda v|| = |\lambda|||v||.
		\end{equation*}
	\end{proposicion}
	\begin{proof}
		$||\lambda v||^2 = \la\lambda v, \lambda v \ra$, por la propiedad \ref{prop-P3} del producto escalar, 
		\begin{equation*}
			\la\lambda v, \lambda v \ra = \lambda\la v, \lambda v \ra = \lambda^2\la v, v  \ra.
		\end{equation*}
		Es decir 	$||\lambda v||^2 =  \lambda^2 ||v||^2$, por lo tanto (sacando raíz cuadrada), $||\lambda v|| = |\lambda|||v||$.
	\end{proof}
	
	\vskip .3cm
	
El producto escalar no sólo es útil para definir la longitud de un vector,  sino que también nos dice cual es el ángulo entre dos vectores: sean   $v_1= (x_1,y_1)$ y $v_2= (x_2,y_2)$ en $\R^2$; veremos a continuación que 
		\begin{equation}\label{eq-cos-theta}
		\langle v_1 , v_2 \rangle = ||v_1||\, ||v_2|| \cos(\theta),
		\end{equation}
		donde  $\theta$ es el ángulo comprendido entre $v_1$ y $v_2$.
	
		
		Sea $\alpha_1$ el ángulo comprendido  entre $v_1$ y el eje horizontal y $\alpha_2$ el ángulo comprendido  entre $v_2$ y el eje horizontal.  Entonces,
		\begin{equation*}
		v_1 = ||v_1||(\cos (\alpha_1), \sen(\alpha_1) ), \quad v_2 = ||v_2||(\cos (\alpha_2), \sen(\alpha_2) ),
		\end{equation*}
		por lo tanto
		\begin{equation*}
		\langle v_1 , v_2 \rangle =  ||v_1|| \,  ||v_2|| (\cos (\alpha_1)\cos(\alpha_2)+  \sen(\alpha_1)\sen\alpha_2)). 
		\end{equation*}
		Por otro  lado, por la propiedad de la suma de los cosenos tenemos que 
		\begin{equation}\label{eq-ley-cosenos}
		\cos (\alpha_1)\cos(\alpha_2)+  \sen(\alpha_1)\sen(\alpha_2) = \cos(\alpha_1- \alpha_2).
		\end{equation}
		Es decir, 
		\begin{equation*}
		\langle v_1,v_2 \rangle =   ||v_1|| \,  ||v_2|| \cos (\alpha_1-\alpha_2), 
		\end{equation*}
		y precisamente, $\theta =  \alpha_1-\alpha_2$ es el ángulo comprendido entre  $v_1$ y $v_2$.
		
		Esto se puede generalizar a $\R^3$  y ahí en vez de la fórmula (\ref{eq-ley-cosenos}) se debe usar la ley esférica de los cosenos. Los resultados se puede generalizar a  $\R^n$ y  en general vale  que si $v_1, v_2 \in \R^n$,  entonces el ángulo comprendido entre $v_1$ y $v_2$ es 
		\begin{equation}\label{eq-ang-comprendido}
		\theta = 	\operatorname{arcos}\left(\frac{\langle v_1 , v_2 \rangle}{||v_1|| \,  ||v_2|| }\right).
		\end{equation}  
		
		\vskip .3cm 
		
		Terminaremos esta sección dando la noción de  distancia entre dos vectores o dos puntos. 
		
		\begin{definicion}
			Sea $v,w \in \R^n$, entonce las \textit{distancia}\index{distancia en $\mathbb R^n$} entre $v$ y $w$ es $||v-w||$.
		\end{definicion}
	
	Vemos en la fig. \ref{fig-distancia-de-vectores} que la norma del vector $v-w$ es la longitud del segmento que une $w$ con $v$.

\begin{figure}[h]
	\begin{tikzpicture}[scale=0.8]
	\draw[thick,->] (-3.0,0) -- (4.0,0) node[right] {$x$}; % eje x
	\draw[thick,->] (0,-1) -- (0,3) node[above] {$y$}; % eje y
	\foreach \x in {-2,...,3}
	\draw (\x,3pt) -- (\x,-3pt);
	\foreach \y in {0,...,2}
	\draw (3pt,\y) -- (-3pt,\y) ;
	% vector w
	\draw[fill] (3,1) circle [radius=0.05];
	\node [right] at (3,1) {$w$};
	\draw[very thick, ->] (0,0) -- (2.96,0.96);
	% vector v
	\draw[fill] (1,2) circle [radius=0.05];
	\draw[very thick,->] (0,0) -- (0.97,1.96);
	\node [above] at (0.9,2) {$v$};
	% flecha punteada de w a v
	\draw[-, thick,blue] (3,1) -- (1.055,1.965);
	% vector v - w
	\draw[fill] (-2,1) circle [radius=0.05];
	\node [above] at (-2,1) {$v-w$};
	\draw[very thick,->] (0,0) -- (-2*0.975,1*0.975);	
	% punteada de v-w  a v
	\draw[dashed] (-2,1) -- (1,2);	
	\end{tikzpicture}
	\caption{Distancia de $v$ a $w$.}\label{fig-distancia-de-vectores}
\end{figure}


Una de las desigualdades más notables referentes a la norma de un vector es la \textit{desigualdad triangular}:\index{desigualdad traingular}

\begin{proposicion}
	Sean $v, w \in \R^n$, entonces
	\begin{equation*}
		||v+w|| \le ||v||+||v||
	\end{equation*}
\end{proposicion}
\begin{proof}
	Ejercicio. 
\end{proof}

La desigualdad triangular expresa en forma algebraica el resultado, más conocido, ``en todo triángulo, un lado es menor que la suma de los otros dos'', que graficamos a continuación.

\begin{center}
		\begin{tikzpicture}[scale=1.0]
		\draw[thick,->] (-3.0,0) -- (4.0,0) node[right] {$x$}; % eje x
		\draw[thick,->] (0,-1) -- (0,3) node[above] {$y$}; % eje y
		\foreach \x in {-2,...,3}
		\draw (\x,3pt) -- (\x,-3pt);
		\foreach \y in {0,...,2}
		\draw (3pt,\y) -- (-3pt,\y) ;
		\draw[-] (0,0) -- (3,1);
		\node [right] at (3,1) {$v$};
		\draw[very thick,-,blue] (0,0) -- (1,2);
		\node [above] at (0.9,2) {$w$};
		\draw[very thick,-,blue] (0,0) -- (4,3);
		\node [above] at (4,3) {$v+w$};
		\draw[fill] (4,3) circle [radius=0.05];
		\draw[fill] (1,2) circle [radius=0.05];
		\draw[fill] (3,1) circle [radius=0.05];
		\draw[fill] (0,0) circle [radius=0.05];
		\draw[-] (3,1) -- (4,3);
		\draw[very thick,-,blue] (1,2) -- (4,3);
		\node [right,blue] at (-0.1,1.7) {{\footnotesize$||w||$}};
		\node [right,blue] at (2.4,1.6) {{\footnotesize$||v+w||$}};
		\node [right, blue] at (2.1,2.8) {{\footnotesize$||v||$}};
		\end{tikzpicture}
\end{center}
	

\vskip .5cm


	\begin{section}{Vectores afines}
		En  esta sección veremos el concepto de vector afín, que nos servirá  para entender más geométricamente los conceptos de rectas y planos en $\R^2$ y $\R^3$,  respectivamente (secciones \ref{seccion-rectas-en-r2} y \ref{seccion-planos-en-r3}). 
		\begin{figure}[h]
			\begin{tikzpicture}[scale=0.5]
			\draw[thick,-] (-1,0) -- (4,0);
			\draw[thick,-] (0,-1) -- (0,3);
			\draw[very thick,->] (0.7,1) -- (3,2) node[right]{$w$};
			\node[inner sep=1.5pt,fill,circle] at (0.7,1) {};
			\node[inner sep=1.5pt,fill,circle] at (3*1.015,2*1.015) {};
			\node[left] at (0.7,0.8){$v$};
			\end{tikzpicture}
			\caption{Un vector afín.}
			\label{fig-vector-afin}
		\end{figure}
		Definimos un \textit{vector afín}\index{vector afín} como un par ordenado de puntos  $v$ y $w$, que escribimos $\overrightarrow{vw}$ y lo visualizamos como una flecha entre $v$ y $w$. Llamamos a $v$ el \textit{punto inicial}\index{vector afín!punto inicial} y $w$ el \textit{punto final}\index{vector afín!punto final} del vector afín (fig. \ref{fig-vector-afin}).
		
		
		
		Sean $\overrightarrow{vw}$ y $\overrightarrow{pq}$ dos vectores afines. Diremos que son \textit{equivalentes}\index{vector afín!equivalencia} si $w-v= q-p$. 
		\begin{figure}[h]
			\begin{tikzpicture}
			\draw[thick,-] (-1,0) -- (4,0);
			\draw[thick,-] (0,-1) -- (0,3);
			\draw[very thick,->] (0.7,1) -- (3,2) node[right]{\;$w$};
			\node[inner sep=1.5pt,fill,circle] at (0.7,1) {};
			\node[inner sep=1.5pt,fill,circle] at (3*1.015,2*1.015) {};
			\node[left] at (0.7,0.8){$v$};
			\draw[very thick,->] (0.7-1.5,1+0.9) -- (3-1.5,2+0.9) node[right]{\;$q$};
			\node[inner sep=1.5pt,fill,circle] at (0.7-1.5,1+0.9) {};
			\node[inner sep=1.5pt,fill,circle] at (3*1.015-1.5,2*1.015+0.9) {};
			\node[left] at (0.7-1.5,0.8+0.9){$p$};
			\draw[very thick,->] (0,0) -- (3-0.7,2-1) node[right]{\;$w-v=q-p$};
			\node[inner sep=1.5pt,fill,circle] at (0,0) {};
			\node[inner sep=1.5pt,fill,circle] at (2.3*1.015,1*1.015) {};
			\end{tikzpicture}
			\caption{Dos vectores equivalentes.}
			\label{fig-vectores-equivalerntes}
		\end{figure}
		
		Cada vector afín $\overrightarrow{vw}$ es equivalente a uno cuyo punto de inicial es el origen, pues $\overrightarrow{vw}$ es equivalente a $\overrightarrow{0(w-v)}$ (ver fig. \ref{fig-vectores-equivalerntes}).
		
		Claramente este es el único vector cuyo punto inicial es el origen y que es equivalente a $\overrightarrow{vw}$. Si visualizamos la ley del paralelogramo en el plano, entonces está claro que la equivalencia de dos vectores afines se puede interpretar geométricamente diciendo que las longitudes de los segmentos de línea determinadas por el par de puntos son iguales, y que las ``direcciones'' de los dos vectores son las mismos. 
		
		A una $n$-upla  la podemos interpretar como un  vector cuyo punto inicial es el origen. En vista de esto, llamaremos, como lo venimos haciendo,  a una $n$-upla punto o vector, dependiendo de la interpretación que tenemos en mente.
		
		Se dice que dos vectores afines $\overrightarrow{vw}$ y $\overrightarrow{pq}$ son \textit{paralelos} si hay un 	número $\lambda\ne 0$  tal que $w-v= \lambda(q-p)$. Se dice que tienen la \textit{misma dirección} si hay un número $\lambda >0$ tal que $w-v= \lambda(q-p)$, y que tienen \textit{direcciones opuestas} si hay un número $\lambda <0$ tal que $w-v= \lambda(q-p)$.
		
		En los siguientes dibujos, ilustramos vectores afines paralelos. En el primer dibujo  con la misma dirección,  en el segundo, con direcciones opuestas.
		
		\vskip .3cm
		
		\begin{center}
			\begin{tabular}{lcl}
				\begin{tikzpicture}[scale=0.8]
				\draw[thick,-] (-1,0) -- (4,0);
				\draw[thick,-] (0,-1) -- (0,3);
				\draw[very thick,->] (0.5*0.7,0.5*1) -- (0.5*3,0.5*2) node[right]{\;$w$};
				\node[inner sep=1.5pt,fill,circle] at (0.5*0.7,0.5*1) {};
				\node[inner sep=1.5pt,fill,circle] at (0.5*3*1.015,0.5*2*1.015) {};
				\node[below] at (0.5*0.7,0.5*0.8){$v$};
				\draw[very thick,->] (0.7-1.5,1+0.9) -- (3-1.5,2+0.9) node[right]{\;$q$};
				\node[inner sep=1.5pt,fill,circle] at (0.7-1.5,1+0.9) {};
				\node[inner sep=1.5pt,fill,circle] at (3*1.015-1.5,2*1.015+0.9) {};
				\node[left] at (0.7-1.5,0.8+0.9){$p$};

				\end{tikzpicture} & 
				\qquad & 
				\begin{tikzpicture}[scale=0.8]
				\draw[thick,-] (-1,0) -- (4,0);
				\draw[thick,-] (0,-1) -- (0,3);
				\draw[very thick,->] (-1.5*0.7+4,-1.5*1+3.5) -- (-1.5*3+4,-1.5*2+3.5) node[left]{\;$w$};
				\node[inner sep=1.5pt,fill,circle] at (-1.5*0.7+4,-1.5*1+3.5) {};
				\node[inner sep=1.5pt,fill,circle] at (-1.5*3*1.015+4,-1.5*2*1.015+3.5) {};
				\node[right] at (-1.5*0.7+4,-1.5*0.8+3.5){$v$};
				\draw[very thick,->] (0.7-1.5,1+0.9) -- (3-1.5,2+0.9) node[right]{\;$q$};
				\node[inner sep=1.5pt,fill,circle] at (0.7-1.5,1+0.9) {};
				\node[inner sep=1.5pt,fill,circle] at (3*1.015-1.5,2*1.015+0.9) {};
				\node[left] at (0.7-1.5,0.8+0.9){$p$};
				
				\end{tikzpicture}
			\end{tabular}
		\end{center}

		\vskip .3cm
		
	\end{section}




	\end{section}

	\begin{section}{Rectas en $\R^2$}\label{seccion-rectas-en-r2}
	Conocemos de la secundaria y de cursos anteriores el concepto de recta, por ejemplo en el  sitio on-line EcuRed dice: 
	
	\textit{``Una recta puede ser expresada mediante una ecuación del tipo $y = m x + b$, donde $x$, $y$ son variables en un plano. En dicha expresión $m$ es denominada pendiente de la recta y está relacionada con la inclinación que toma la recta respecto a un par de ejes que definen el Plano. Mientras que $b$ es el término independiente y es el valor del punto en el cual la recta corta al eje vertical en el plano.''}
	
	Dicho en otros términos una recta,  según esta definición, es el conjunto de puntos $(x,y) \in \R^2$ que satisfacen la ecuación $y = m x + b$ y puede verse como el gráfico de la función $f(x) = m x + b$. Si, por ejemplo, $m=\frac12$ y $b=1$, podemos dibujar la recta en el plano cerca del origen, como en fig.\ref{fig-recta-funcion}.
	\begin{figure}[h]
		\begin{tikzpicture}
		\draw[thick,->] (-4.0,0) -- (4.0,0) node[right] {$x$}; % eje x
		\draw[thick,->] (0,-2) -- (0,3) node[above] {$y$}; % eje y
		\foreach \x in {-2,...,-1} \draw (\x,3pt) -- (\x,-3pt) node[anchor=north] {\x};
		\foreach \x in {1,...,3} \draw (\x,3pt) -- (\x,-3pt) node[anchor=north] {\x};
		\foreach \y in {-1,...,-1} \draw (3pt,\y) -- (-3pt,\y) node[anchor=east] {\y}; 
		\foreach \y in {1,...,2} \draw (3pt,\y) -- (-3pt,\y) node[anchor=east] {\y}; 
		\draw[scale=1,domain=-3:3,smooth,variable=\x,blue] plot ({\x},{0.5*\x +1});
		\end{tikzpicture}
		\caption{La recta $y = \frac12x +1$.}
		\label{fig-recta-funcion}
	\end{figure} 
	
	Sin embargo, con la definición anterior no es posible considerar las rectas verticales. Las rectas verticales están dadas por una ecuación del tipo $x= b$,  es decir son todos los puntos $(x,y)$  tal que $x=b$ e $y$ puede tomar cualquier valor. Por ejemplo, la recta $x=2.5$ se  grafica  como en la fig. \ref{fig-recta-vertical}.
	\begin{figure}[h]
		\begin{tikzpicture}
		\draw[thick,->] (-4.0,0) -- (4.0,0) node[right] {$x$}; % eje x
		\draw[thick,->] (0,-2) -- (0,3) node[above] {$y$}; % eje y
		\foreach \x in {-2,...,-1} \draw (\x,3pt) -- (\x,-3pt) node[anchor=north] {\x};
		\foreach \x in {1,...,3} \draw (\x,3pt) -- (\x,-3pt) node[anchor=north] {\x};
		\foreach \y in {-1,...,-1} \draw (3pt,\y) -- (-3pt,\y) node[anchor=east] {\y}; 
		\foreach \y in {1,...,2} \draw (3pt,\y) -- (-3pt,\y) node[anchor=east] {\y}; 
		\draw[scale=1,domain=-2:3,smooth,variable=\x,blue] plot ({2.5},{\x});
		\end{tikzpicture}
		\caption{La recta $x=2.5$.}
		\label{fig-recta-vertical}
	\end{figure} 
	
	No es difícil dar una definición que englobe todas las rectas posibles del plano:
	
	\begin{definicion}[Definición general de la recta] Sean  $a,b,c \in \R$ y tal que $a,b$ no son simultáneamente $0$. La \textit{recta con ecuación implícita}\index{recta} \index{recta!ecuación implícita}
	\begin{equation}\label{eq-implicita-de-la-recta}
		ax +by =c,
	\end{equation}
	es el conjunto de puntos $(x,y)$ en $\R^2$ que satisfacen la ecuación (\ref{eq-implicita-de-la-recta}).  Es decir, si denotamos $L$  a la recta,
	\begin{equation*}
		L = \{ (x,y)\in \R^2: ax +by =c\}.
	\end{equation*}
	\end{definicion}

\vskip .3cm

	Observar  que si $b\ne0$, entonces la recta es \, $y= -\displaystyle\frac{a}{b}x + \displaystyle\frac{c}{b}$ \, y que si $b=0$,  entonces $a\ne 0$ y la recta es \, $x =\displaystyle\frac{c}{a}$.
	
	\begin{observacion}
		Si  consideramos  el vector $(a,b)$ en $\R^2$, $c \in \R$ y $L$ la recta definida por los puntos $(x,y)$ tal que $ax +by =c$,  entonces $L$ es la recta formada por el conjunto de puntos $(x,y)$ en $\R^2$ que satisfacen 
		\begin{equation*}
			\langle (x,y), (a,b) \rangle = c.
		\end{equation*}
	Ahora bien, consideremos $(x_0,y_0)$ un punto de la recta, entonces,  obviamente tenemos que $\langle (x_0,y_0), (a,b) \rangle = c$, por lo tanto la recta se puede describir como los puntos $(x,y)$ que satisfacen la ecuación 
	\begin{equation*}
	\langle (x,y), (a,b) \rangle = \langle (x_0,y_0), (a,b) \rangle.
	\end{equation*}	
	Por la propiedad \ref{prop-P2} del producto escalar, llegamos a la conclusión que 
	\begin{equation*}
	L = \{(x,y) \in \R^2: \langle (x,y)-(x_0,y_0)\,,\, (a,b) \rangle = 0 \}.
	\end{equation*}
	Sea $v_0 =(x_0,y_0)$ y $v =(x,y)$,   representemos gráficamente la situación: 
	\begin{figure}[h]
		\begin{tikzpicture}
		\def\vx{3}
		\def\vy{2}
		\def\wx{4}
		\def\wy{1}	
		\def\tv{3.0}
		\def\tw{1.4}
		\def\taa{1.5}
		\def\tab{-1.5}
		\def\rr{\tv*\vy-\tv*\wy}	
		\def\ss{\tv*\wx-\tv*\vx}		
		\def\rrm{-\tw*\vy+\tw*\wy}	
		\def\ssm{-\tw*\wx+\tw*\vx}		
		\draw[thick,-] (-4,0) -- (4,0);
		\draw[thick,-] (0,-1) -- (0,3);
		%\node[inner sep=1.5pt,fill,circle] at (\vx,\vy) {};
		\draw[very thick,->] (0,0) -- (\vx,\vy) node[above]{$v_0$};
		\draw[very thick,->] (0,0) -- (\wx,\wy) node[above]{\;$v$};
		\draw[very thick,->] (0,0) -- (\wx-\vx,\wy-\vy) node[right]{$v-v_0$};
		\draw[thick,blue,-] (\vx- \tab*\vx + \tab*\wx ,\vy - \tab*\vy + \tab*\wy) --  (\vx- \taa*\vx + \taa*\wx ,\vy - \taa*\vy + \taa*\wy);
		\draw[very thick,->] (0,0) -- (\tw*\vy-\tw*\wy,\tw*\wx-\tw*\vx) node[left]{$(a,b)$};
		\draw[dashed, thick,-] (0,0) -- (\rr,\ss);
		\draw[dashed, thick,-] (0,0) -- (\rr,\ss);
		\draw[dashed, thick,-] (\rrm,\ssm) -- (0,0);
		%\draw[dashed,thick] (0,0) (\tw*\wx,\tw*\wy) -- (\vx+\tw*\wx,\vy+\tw*\wy+0.1);
		%\node[inner sep=1.5pt,fill,circle,blue] at (\vx+\tw*\wx+0.04*\wx,\vy+\tw*\wy+0.04*\wy)  {};
		%\node[right] at (\vx+\tw*\wx,\vy+\tw*\wy+0.1){$v+tw$};
		\end{tikzpicture}
		\caption{Una recta en el plano.}
		\label{fig-recta-parametrica}
	\end{figure} 


	La recta $L$ es, entonces, la recta perpendicular a $(a,b)$ y que pasa por $v_0$. 
	
	\end{observacion}
\vskip .3cm
	El  razonamiento también es posible hacerlo en el otro sentido: 
	\vskip .2cm
	
	\begin{resultado}\label{res-recta-perp}
		{La ecuación implícita de la recta $L$ perpendicular a $(a,b)$ y que pasa por $(x_0,y_0)$ es
			\begin{equation*}
			ax +by = \langle (x_0,y_0), (a,b) \rangle.
			\end{equation*}}
	\end{resultado}
	

\vskip .3cm

\begin{ejemplo}
	Encontrar la ecuación implícita de la recta que pasa por $(2,-1)$ y  es perpendicular a $(-2,3)$.
\end{ejemplo}
\begin{proof}[Solución]
Por lo visto anteriormente la recta esta formada por los puntos  $(x,y)$ tales que
	\begin{equation*}
			-2x+3y = c
	\end{equation*}
	 y debemos determinar el valor de $c$. Como $(2,-1)$ pertenece a la recta
	 \begin{equation*}
	 c = -2\cdot 2+3\cdot (-1) = -7.
	 \end{equation*}
	 Luego, la ecuación implícita de la recta es
	 \begin{equation*}
	 -2x+3y = -7.
	 \end{equation*}
\end{proof}
\vskip .3cm

	Una definición equivalente de recta es la siguiente:
	
		\begin{definicion}
			Sean $v, w \in \R^2$ tal que  $w \not=0$. Sea 
			\begin{equation*}
			L = \{v + tw: t \in \R\}. 
			\end{equation*}
			Diremos entonces que  \textit{$L$ es la recta que pasa por $v$ paralela a $w$.} 
		\end{definicion}
		
	Observemos que la recta $L$ está dada por todos los puntos que se obtienen de la función
	\begin{equation}\label{eq-ecuacion-parametrica-recta}
	X(t) =v +tw, \quad \text{para $t \in \R$.} 
	\end{equation} 
	En  el espacio $\R^2$, diremos que (\ref{eq-ecuacion-parametrica-recta}) es la \textit{ecuación paramétrica} o la \textit{representación paramétrica}\index{recta!ecuación paramétrica} de la recta $L$ que pasa por el punto $v$ y es paralela a $w \ne 0$.
		

		
		Podemos representar una recta dada en forma paramétrica como en la figura \ref{fig-recta-parametrica}.
		\begin{figure}[h]
			\begin{tikzpicture}
			\def\vx{1.2}
			\def\vy{0.7}
			\def\wx{-2.2}
			\def\wy{1.5}	
			\def\tw{1.4}					
			\draw[thick,-] (-4,0) -- (4,0);
			\draw[thick,-] (0,-1) -- (0,3);
			\draw[thick,blue,-] (\vx+1.5*\wx,\vy+1.5*\wy) -- (\vx-1*\wx,\vy-1*\wy);
			%\node[inner sep=1.5pt,fill,circle] at (\vx,\vy) {};
			\draw[very thick,->] (0,0) -- (\vx,\vy) node[above]{$v$};
			\draw[very thick,->] (0,0) -- (\wx,\wy) node[above]{\;$w$};
			\draw[dashed, thick,->] (0,0) -- (\tw*\wx,\tw*\wy) node[left]{$tw$};
			\draw[dashed,thick] (0,0) (\tw*\wx,\tw*\wy) -- (\vx+\tw*\wx,\vy+\tw*\wy+0.1);
			\node[inner sep=1.5pt,fill,circle,blue] at (\vx+\tw*\wx+0.04*\wx,\vy+\tw*\wy+0.04*\wy)  {};
			\node[right] at (\vx+\tw*\wx,\vy+\tw*\wy+0.1){$v+tw$};
			\end{tikzpicture}
			\caption{Una recta en el plano.}
			\label{fig-recta-parametrica}
		\end{figure} 
		Cuando damos tal representación paramétrica, podemos pensar en un móvil que comienza en el punto $v$ en el tiempo $t = 0$, y moviéndose en la dirección de $w$. En el momento $t$, el móvil está en la posición $v+tw$. Por lo tanto, podemos interpretar físicamente la representación paramétrica como una descripción del movimiento, en que $w$ se interpreta como la velocidad del móvil. En un momento dado $t$, el móvil está en el punto $X(t) =v +tw$ que es  llamada la \textit{posición} del móvil en el tiempo $t$.
		
		Esta representación paramétrica también es útil para describir el conjunto de los puntos que se encuentran en el segmento de línea entre dos puntos dados. Sean $v$, $u$ dos puntos, entonces el segmento entre $v$ y $u$ consiste en todos los puntos
		\begin{equation}\label{eq-segmento-parametrico}
			S(t) = v + t(u-v)\quad  \text{con}\quad 0 \le t \le 1.
		\end{equation}
		Observar que en tiempo 0, $S(0) = v$ y  en tiempo 1, $S(1)= v + (u-v)= u$. Como $t$ ``va'' de 0 a 1,  el móvil va de $v$ a $u$,  en linea recta. %En  el lenguaje de vectores afines, (\ref{eq-segmento-parametrico}) es \textit{la ecuación paramétrica del vector $\overrightarrow{vu}$}.
		
		Extendiendo a ambos lados el segmento, podemos describir la recta que pasa por $v$ y $u$ por la ecuación paramétrica (fig. \ref{fig-recta-entre-puntos})
		\begin{equation*}
		S(t) = v + t(u-v)\quad  \text{con}\quad t \in \R.
		\end{equation*}
		\begin{figure}[h]
			\begin{tikzpicture}
			\draw[thick,-] (-4,0) -- (4,0);
			\draw[thick,-] (0,-1) -- (0,2);
			\draw[blue,-] (-4,2) -- (4,-0.5);
			\node[inner sep=1.5pt,fill,circle,blue] at (-4 + 8/4,2 -2.5/4) {};
			\draw[very thick,-] (0,0) -- (-4 + 8/4,2 -2.5/4) node[above]{$u$};
			\draw[very thick,-] (0,0) -- (-4 + 2.5*8/4,2 -2.5*2.5/4) node[above]{$v$};
			\node[inner sep=1.5pt,fill,circle,blue] at (-4 + 2.5*8/4,2 -2.5*2.5/4)  {};
			\draw[thick,blue,-] (-4 + 8/4,2 -2.5/4) -- (-4 + 2.5*8/4,2 -2.5*2.5/4);
			\draw[thick,blue,-] (-4,2) -- (4,-0.5);
			\end{tikzpicture}
			\caption{La recta que pasa por $v$ y $u$.}
			\label{fig-recta-entre-puntos}
		\end{figure} 
		
		\begin{ejemplo}
			Encontrar una representación paramétrica para la recta que contiene los puntos $(1, - 3, 1)$ y $(- 2,4,5)$.
		\end{ejemplo}	
		\begin{proof}[Solución] Llamemos $v = (1, - 3, 1)$ y $u =(- 2,4,5)$. Entonces $u-v = (- 2,4,5) - (1,-3,1) = (-3,7,4)$ y la representación paramétrica de la recta que pasa por $u$ y $v$  es 
		\begin{equation*}
			X (t) = v + t(u-v) = (1, -3,1) + t (-3, 7, 4),\quad t\in \R.
		\end{equation*}
		\end{proof}
	
		Ahora discutiremos la relación entre una representación paramétrica y la ecuación implícita de una recta en el plano.
		
		Supongamos que trabajamos en el plano y tenemos $v, w \in \R^2$ con $w \not= 0$ y la recta descrita en forma paramétrica:
		\begin{equation*}
		X(t) =v +tw.
		\end{equation*}
		Sea $v = (x_1,y_1)$, $w = (x_2,y_2)$,  entonces, todo punto de la recta es de la forma
		\begin{equation*}
		(x,y) =(x_1,y_1) +t(x_2,y_2) = (x_1+tx_2,y_1+ty_2),
		\end{equation*}
		es decir, los puntos de la recta $X$ son los $(x,y)$ tal que
		\begin{equation*}
			x = x_1+tx_2, \qquad y = y_1+ty_2, \qquad
		\end{equation*}
		para $t \in \R$. Dado  que $(x_2,y_2) \ne 0$, podemos despejar $t$  de alguna de las ecuaciones y usando la otra ecuación eliminamos $t$ y obtenemos una ecuación implícita. Veremos esto en un ejemplo.
		
		\begin{ejemplo}
			Sean $v = (2, 1)$ y $w = (- 1, 5)$ y sea $X$ la recta que pasa por $v$  en la dirección $w$. Encontrar la ecuación  implícita de $L$.
	\end{ejemplo}
\begin{proof}[Solución]	
			La representación paramétrica de la recta que pasa por  $v$ en la dirección de $w$ es 
			\begin{equation*}
				X(t)= (2,1)+ t(-1,5) = (2-t,1+5t).
			\end{equation*}
			 Es decir, si  miramos cada coordenada, 
			\begin{equation}\label{eq-par-des-ej}
			x = 2 - t, \qquad\quad y = 1 + 5t. \qquad\quad\tag{*}
			\end{equation}
			Despejando $t$ de la primera ecuación obtenemos $t = 2-x$. Reemplazando este valor de $t$  en la segunda ecuación obtenemos $y = 1 + 5t=1 + 5(2-x)t= y = 11- 5x$, luego
			\begin{equation}\label{eq-impl-rec-ej}
				5x + y = 11,\tag{**}
			\end{equation}
			que es la ecuación implícita de la recta.
			
			\end{proof}	
			\vskip .1cm		
			
			Esta eliminación de $t$ muestra que cada par $(x, y)$ que satisface la representación paramétrica (\ref{eq-par-des-ej}) para algún valor de $t$ también satisface la ecuación (\ref{eq-impl-rec-ej}).
			
			\vskip .2cm	
			
			Recíprocamente, de la ecuación implícita podemos obtener la representación paramétrica. 
			
			\begin{ejemplo} Encontrar la representación paramétrica de la recta definida por 
				\begin{equation*}
				5x + y = 11. 
				\end{equation*}
			\end{ejemplo}
			\begin{proof}[Solución]
				 Supongamos que tenemos un par de números $(x, y)$ que satisfacen la ecuación  implícita, $5x + y = 11$, luego $y = (-5)x + 11$, remplazando $x$ por $t$ (sólo por notación) obtenemos que 
				 \begin{equation*}
				 	Y(t) = (t, -5t+11)
				 \end{equation*}
				es la representación paramétrica de la recta.
		\end{proof}	 
	
	 \vskip .2cm	
	 
	 De los ejemplos anteriores se deduce que la recta 
	 \begin{equation*}
	 	X(t)=  (2-t,1+5t)
	 \end{equation*}
	 es la misma que la recta 
	 \begin{equation*}
	 	Y(t) = (t, -5t+11).
	 \end{equation*}     
	 Observar que, pese a que hablamos de ``la representación paramétrica de la recta'', una recta tiene muchas formas de ser representada paramétricamente. 
	
	\vskip .3cm 
		Los procedimientos  de los ejemplos anteriores se pueden generalizar a cualquier recta y de esa forma se puede demostrar que la definición paramétrica y la definición implícita de la recta son equivalentes. 
		
		\vskip .3cm 
		
		Finalmente, podemos obtener la representación paramétrica de la recta a partir de un vector ortogonal a ella y otro vector perteneciente a ella. 
				
		\begin{proposicion}
			Sean $(a,b),(x_0,y_0)\in\R^2$ con $(a,b)\neq0$.  La recta perpendicular a $(a,b)$ que pasa por $(x_0,y_0)$ es
			\begin{align*}
			L=\left\{(x_0,y_0)+t(b,-a)\mid t\in\R\right\} 
			\end{align*}
		\end{proposicion}
	\begin{proof}
	El vector $(b,-a)$  es perpendicular a a $(a,b)$  y por lo tanto tiene la dirección de la recta. Luego  la ecuación paramétrica de la recta es $v_0+t(b,-a)$ para algún $v_0$  en la recta. Como $(x_0,y_0)$ pertenece a la recta, obtenemos el resultado que queríamos probar.
	\end{proof}
		

\begin{ejemplo}
	Encontrar una representación paramétrica para la recta que contiene los puntos $(2,2)$ y y es perpendicular a $(2,1)$.
\end{ejemplo}	
\begin{proof}[Solución]

	El vector ortogonal a $(2,1)$  es $(1,-2)$. Luego:
	
	\begin{align*}
	L&=\left\{(2,2)+t(1,-2)\mid t\in\R\right\} \\
	&= \left\{(2 +t,2-2t)\mid t\in\R\right\}
	\end{align*}

\end{proof}


		
		\vskip .3cm 
		
		Debemos observar que en $\R^3$ no alcanza una sola ecuación lineal del tipo $ax+by+cz=d$ para definir una recta. Veremos en la sección siguiente que una ecuación lineal define un plano en $\R^3$. Genéricamente hablando, con las soluciones de una ecuación en $\R^n$ se obtiene un objeto ``con una dimensión menos''. Todo esto quedará claro al final de la materia cuando estudiemos subespacios vectoriales de un espacio vectorial. 
	\vskip .3cm 		

	
	\end{section}

\vskip .8cm 

	\begin{section}{Planos en $\R^3$}\label{seccion-planos-en-r3} 
		En  la sección anterior vimos (aunque no lo demostramos) que existe una equivalencia entre la definición implícita y la definición paramétrica de la recta. En esta sección definiremos un plano en $\R^3$ utilizando la forma implícita, que es la forma más usual y además es geométricamente intuitiva. Luego veremos la definición del plano en su versión paramétrica . 
		
		Comenzaremos, debido a que es más simple, con planos que  pasan por el origen,  como el de la fig. \ref{fig-plano-por-origen}.
		\begin{figure}[h]
			\begin{tikzpicture}[plane/.style={trapezium,draw,fill=black!20,trapezium left angle=60,trapezium right angle=120,minimum height=1.5cm},scale=0.7]
			\node (p)[plane,rotate=-30] at (0,0){.};
			\draw[rotate=-30] (p.center) edge ++(0,2cm) edge[densely dashed] (p.south) (p.south) edge ++(0,-1cm);
			\draw[->] (0,0,0) -- (2*0.5,2*0.866,0) node[right]{$u$};
			\draw[thick,->] (0,0,0) -- (3,0,0) node[right]{$y$};
			\draw[thick,->] (0,0,0) -- (0,2.5,0) node[above]{$z$};
			\draw[thick,->] (0,0,0) -- (0,0,3) node[anchor=north east]{$x$};
			\node [right] at (1.2,-0.1,1.5) {$P$};
			\end{tikzpicture}
			\caption{El plano $P$ y $u$, un vector  perpendicular al plano.}
			\label{fig-plano-por-origen}
		\end{figure} 
	
		En  este caso,  es claro que el plano está determinado por un vector perpendicular al mismo, es decir si $P$  es un plano que pasa por el origen y $u$ es un punto de $\R^3$, no nulo, tal que $u \perp P$,  entonces
		\begin{equation*}
			P = \{ v \in \R^3: \la v,u \ra=0 \}. 
		\end{equation*}
		Sea ahora un  plano $P$ que no pasa por el origen.  Tomo $v_0 \in P$ y entonces observamos que
		\begin{equation}\label{eq-p0-plano-1}
			P_0 = \{v-v_0: v \in P \}
		\end{equation} 
		es un plano que pasa por el origen (pues $v_0-v_0 \in P_0$). Luego,  si $u$ perpendicular a $P_0$ tenemos que
		\begin{equation}\label{eq-p0-plano-2}
		P_0 = \{w: \la w,u \ra=0\}.
		\end{equation}
		De las ecuaciones (\ref{eq-p0-plano-1}) y  (\ref{eq-p0-plano-2}) deducimos que 
		\begin{equation*}
			v \in P \Leftrightarrow v-v_0 \in P_0 \Leftrightarrow \la v-v_0,u \ra=0,
		\end{equation*}
		 es decir
		\begin{equation*}
			P = \{ v \in \R^3: \la v-v_0,u \ra=0 \}. 
		\end{equation*}  
		Observemos que  $\la v-v_0,u \ra=0$ sii $\la v,u \ra-\la v_0,u \ra=0$ sii $\la v,u \ra=\la v_0,u \ra$. Es decir, si $d = \la v_0,u \ra$, tenemos
		\begin{equation*}
		P = \{ v \in \R^3: \la v,u \ra=d \}. 
		\end{equation*} 
		
		Esta interpretación geométrica del plano se puede formalizar en la siguiente definición.
		
		
		\begin{definicion}\label{def-eq-implicita-plano} Sean $a,b, c,d \in \R$ tal que $(a,b,c) \ne (0,0,0)$ y sea 
		\begin{equation*}
			P = \{(x,y,z): ax +by +cz =d\}.
		\end{equation*}
		Entonces diremos que $P$  es  un \textit{plano con ecuación implícita}\index{plano en $\R^3$!ecuación implícita}  $ax +by +cz =d$ y  que $(a,b,c)$ es un \textit{vector normal al plano $P$.} A esta forma de describir el plano también suele llamársela la \textit{ecuación normal del plano.}
		\end{definicion} 
		
		Observar que la ecuación $ ax +by +cz =d$ no es más que la ecuación $\la(x,y,z),(a,b,c) \ra=d$. 	
		
		\begin{ejemplo}
			El plano determinado por la ecuación
			\begin{equation*}
				2x - y 	+ 3z = 	5
			\end{equation*}
			es perpendicular al vector $(2, - 1, 3)$. Si queremos encontrar un punto en ese plano, por supuesto que tenemos muchas opciones. Podemos dar un valor arbitrario a $x$ e $y$, y luego despejamos $z$. Para obtener un punto concreto, sea $x = 1$, $y = 1$. Luego resolvemos para $z$, a saber
			\begin{equation*}
				3z 	= 5 - 2 + 1 = 4,
			\end{equation*}
			luego $z = \displaystyle\frac43$ y entonces
			\begin{equation*}
				(1,1,\frac43)
			\end{equation*}
			es un punto en el plano.
		\end{ejemplo}
		
		Se dice que dos planos son \textit{paralelos} (en el 3-espacio) si sus vectores normales son paralelos,  es decir son proporcionales. Se dice que son \textit{perpendiculares} si sus vectores normales son perpendiculares. El \textit{ángulo entre dos planos} se define como el ángulo entre sus vectores normales.
		
		
		
		Como $(a,b,c) \ne (0,0,0)$,  entonces una de las tres componentes del vector normal al plano no es cero. Supongamos que $a\ne 0$,  luego es fácil despejar $x$ en función de las constantes $a$, $b$, $c$ y $d$; y las variables $y$ y $z$, por lo tanto cada coordenada del plano depende paramétricamente de $y$ y $z$ y así obtenemos una ecuación paramétrica de $P$ (que depende de 2 parámetros). Se puede hacer de forma análoga cuando $b\ne 0$ o $c \ne 0$. 
		
		\begin{ejemplo}
			Dado el plano $P = \{(x,y,z): x -2y +z =1\}$,  hallaremos una ecuación paramétrica de $P$. Como  $x -2y +z =1$ sii  $x = 2y-z +1$, tenemos que
			\begin{equation*}
				P = \{(2y-z +1,y,z): y,z \in \R\},
			\end{equation*}
			o,  escrito de una forma más estándar,
			\begin{equation*}
			P = \{(2s-t +1,s,t): s,t \in \R\}. 
			\end{equation*}
			Observemos que $(2s-t +1,s,t) = (1,0,0) + (2s-t,s,t)= (1,0,0) + (2s,s,0) + (-t,0,t) = (1,0,0) + s(2,1,0) + t(-1,0,1) $, por lo tanto, podemos también escribir
			\begin{equation*}
			P = \{(1,0,0) + s(2,1,0) + t(-1,0,1): s,t \in \R\}. 
			\end{equation*}
			Cualquiera de las formas paramétricas de describir $P$  es correcta, pero la última es la que se utiliza para definir formalmente el plano en forma paramétrica. 
			
		\end{ejemplo}
	
	\begin{definicion}
		Sean $v, w_1,w_2 \in \R^3$ tal que  $w_1$,$w_2$ no  nulos y tal que $w_2$ no sea un múltiplo de $w_1$. Sea 
		\begin{equation*}
		P = \{v + sw_1 + tw_2: s,t \in \R\}. 
		\end{equation*}
		Diremos entonces que  \textit{$P$ es el  plano a través de $v$ paralelo a los vectores $w_1$ y $w_2$.}\index{plano en $\R^3$!ecuación paramétrica} 
	\end{definicion}
	
		Claramente,  en la definición de arriba, el vector $v$ pertenece al plano y el plano 
		\begin{equation*}
		P_0 = \{sw_1 + tw_2: s,t \in \R\}
		\end{equation*}
		es el plano que pasa por el origen y paralelo a $P$. 
	\end{section}
	
	Ya  hemos visto que de la ecuación implícita del plano podemos pasar a al ecuación paramétrica fácilmente. Es un poco menos directo pasar de la ecuación paramétrica a la ecuación implícita, pero podemos describir un procedimiento general: sea $P = \{v + sw_1 + tw_2: s,t \in \R\}$,  entonces $v \in P$ y $P_0 = \{sw_1 + tw_2: s,t \in \R\}$  es el plano paralelo a $P$  que pasa por el origen. Si encontramos $u \ne 0$ tal que $\la u,w_1 \ra =0$ y $\la u,w_2 \ra =0$, entonces $ \la  sw_1 + tw_2, u \ra =0$ para  $s,t$ arbitrarios y 
	\begin{equation*}
	P_0 = \{(x,y,z): \la (x,y,z),u \ra =0\}. 
	\end{equation*}
	Sea $d = \la v, u \ra$, entonces $\la v + sw_1 + tw_2, u\ra = \la v , u\ra =d$, para $s,t$ arbitrarios. Es decir
	\begin{equation*}
	P = \{(x,y,z): \la (x,y,z),u \ra =d\}. 
	\end{equation*}
	  
	\begin{ejemplo}
		Sea $P$ el plano definido en forma paramétrica por 
		\begin{equation*}
			P = \{ (1,1,0) + s(-1,0,-1) + t(0,1,-2): s,t \in \R\}.
		\end{equation*}
		 Encontremos la ecuación implícita de  $P$. 
		 
		 Sea $u= (a,b,c)$,  entonces 
		\begin{align*}
			\la u,(-1,0,-1) \ra = 0 \quad &\Leftrightarrow \quad -a -c=0, \\
			\la u,(0,1,-2) \ra = 0 \quad &\Leftrightarrow \quad b -2c=0.
		\end{align*} 
		Estas dos ecuaciones se cumplen sii $a = -c$ y $b=2c$, es decir si $u=(-c,2c,c)$. Si, por ejemplo, $c=1$, tenemos $u=(-1,2,1)$, luego el plano paralelo a $P$  que pasa por el origen es
		\begin{equation*}
		P_0 = \{(x,y,z): -x+2y+z =0\}. 
		\end{equation*}
		Como $\la (1,1,0),(-1,2,1) \ra = 1 $, obtenemos
		\begin{align*}
		P &= \{(x,y,z): \la (x,y,z),(-1,2,1) \ra =1\} \\
		&= \{(x,y,z): -x+2y+z =1\}.
		\end{align*}
		    
		
	\end{ejemplo}

	\end{chapter}
	
	
	\begin{chapter}{Sistemas lineales}\label{chap-sist-lin}
		
		En  este capítulo estudiaremos en forma sistemática los sistemas de ecuaciones lineales,  es decir las soluciones de un conjunto finito de ecuaciones  donde la relación entre las incógnitas se expresa en forma lineal.  
		
		
		\begin{section}{Sistemas de ecuaciones lineales}
		
		\vskip .3cm 
		
			El problema a resolver será el siguiente: buscamos números  $x_1,\ldots,x_n$ en el cuerpo $\K$ ($= \R$ o $\C$)  que satisfagan las siguientes condiciones
			\begin{equation}\label{sist-eq}
			\begin{matrix}
			a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &y_1\\
			\vdots&  &\vdots& &&  &\vdots \\
			a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&y_m
			\end{matrix}
			\end{equation}
			donde $y_1, \ldots,y_m$ y $a_{i,j}$ ($1 \le i \le m$, $1 \le j \le n$) son números en $\K$.
			
				\vskip .3cm 
			
			Llamaremos a (\ref{sist-eq}) un \textit{sistema de $m$ ecuaciones lineales con $n$ incógnitas.}\index{sistema de ecuaciones lineales} A una  $n$-upla $(x_1,\ldots,x_n)$ de elementos de $\K^n$ que satisface cada una de las ecuaciones de  (\ref{sist-eq}) la llamaremos una \textit{solución del sistema}. Si $y_1 = \cdots = y_m=0$, el sistema se llamará \textit{homogéneo.} En caso contrario el sistema se denominará \textit{no-homogéneo.}
			
			\begin{ejemplo}
				Los siguientes son sistemas de 2 ecuaciones lineales con 2 incógnitas:
				\begin{equation*}
				\begin{matrix}
				\text{(1)} & 
				\begin{matrix}
				2x_1 + 8x_2 &= 0 &  \\
				2x_1 + x_2 &=  1& 
				\end{matrix} &\quad&
				\text{(2)} & 
				\begin{matrix}
				2x_1 + x_2 &= 0 &  \\
				2x_1 - x_2 &=  1& 
				\end{matrix} &\quad&
				\text{(3)} & 
				\begin{matrix}
				2x_1 + x_2 &= 1 &  \\
				4x_1 +2 x_2 &=  2& 
				\end{matrix} 
				\end{matrix}
				\end{equation*}
			\end{ejemplo} 
			
			\begin{ejemplo}\label{ej-sist-1}
				Resolvamos ahora un sistema de ecuaciones homogéneo sencillo:
				\begin{equation*}
				\begin{matrix}
				{\footnotesize{\footnotesize\circled{1}}}&\qquad& 2x_1 -x_2 + x_3 &=& 0 \\
				{\footnotesize{\footnotesize\circled{2}}}&\qquad& x_1 + 3x_2 + 4x_3 &=& 0.
				\end{matrix}
				\end{equation*}
			\end{ejemplo}
			\begin{proof}[Solución]
				Observar que $(0,0,0)$ es solución. Busquemos otras soluciones manipulado las ecuaciones.
				\begin{align*}
				\begin{matrix}
				&\text{Si  hacemos}&-2\,{\footnotesize\circled{2}} +  {\footnotesize\circled{1}}& \text{obtenemos:} & & {\footnotesize\circled{$1^\prime$}}&  -7x_2 - 7x_3&=&0 &\;\Rightarrow\;& x_2 &=& -x_3\\
				&\text{Si  hacemos}&3 \,{\footnotesize\circled{1}} + {\footnotesize\circled{2}}& \text{obtenemos:} && {\footnotesize\circled{$2^\prime$}}& 7x_1 + 7x_3&=&0&\;\Rightarrow\;& x_1 &=& -x_3,
				\end{matrix}
				\end{align*}
				y esto nos dice que las soluciones son de la forma $\{(-x_3,-x_3, x_3): x_3 \in \R \}$, por ejemplo $(-1,-1,1)$ es solución y $(1,2,3)$ no es solución.
			\end{proof}
			
				
			En  el ejemplo anterior hemos encontrado soluciones por \textit{eliminación de incógnitas},  es decir multiplicando por constantes adecuadas  ciertas ecuaciones y sumándolas hemos eliminado en  {\footnotesize\circled{$1$}} a $x_1$ y en  {\footnotesize\circled{$2$}} a $x_2$, con lo cual la solución del sistema se deduce inmediatamente por pasaje de término.   
			
			\vskip .6cm
			
			\begin{ejemplo}\label{ej-sist-2}
					Encontrar las soluciones $(x,y,z)$ del sistema de ecuaciones:
					\begin{equation*}
					\begin{matrix}
					{\footnotesize{\footnotesize\circled{1}}}&\qquad&x &  & +2z & =& 1 \\
					{\footnotesize{\footnotesize\circled{2}}}&\qquad&x& -3y & +3z & =&2 \\
					{\footnotesize{\footnotesize\circled{3}}}&\qquad&2x& -y & +5z & =&3
					\end{matrix} \tag{S1}
					\end{equation*}
					Es decir, queremos encontrar los n\'umeros reales $x$, $y$ y $z$ que satisfagan las ecuaciones anteriores.
				
			\end{ejemplo}
				
				\begin{proof}[Solución]
					Veremos que la única solución es $(x,y,z)=(-1,0,1)$.  El  método que usaremos, similar al del ejemplo anterior,  será el de eliminación de variables o incógnitas: vemos  en el sistema que queremos resolver 8 variables, algunas repetidas. Trataremos de eliminar en cada ecuación la mayor cantidad de variables posibles de tal forma de llegar a una formulación equivalente del sistema que nos de inmediatamente la solución. 
\vskip .3cm	





	Supongamos que $(x,y,z)$ es una solución de nuestro sistema. Entonces también vale que: 
	\begin{equation*}
	\begin{matrix}
	&{\footnotesize{\footnotesize\circled{2}}}&\qquad&x& -3y & +3z & = & 2 \\
	(-1)\cdot&{\footnotesize{\footnotesize\circled{1}}}&\qquad(-1)\cdot&(x &  & +2z) & = &(-1)\cdot1 \\
	\hline
	&{\footnotesize{\footnotesize\circled{2'}}}&\qquad&& -3y & +z & = & 1  
	\end{matrix}
	\end{equation*}
	
	 
	Por lo tanto $(x,y,z)$ también es solución del sistema
	\begin{equation*}
	\begin{matrix}
	{\footnotesize{\footnotesize\circled{1}}}&\qquad&x &  & +2z & = 1 \\
	{\footnotesize{\footnotesize\circled{2'}}}&\qquad&& -3y & +z & = 1   \\
	{\footnotesize{\footnotesize\circled{3}}}&\qquad&2x& -y & +5z & =3
	\end{matrix}\tag{S2}
	\end{equation*}
 


 
Dado que $(x,y,z)$ es solución del sistema (S2),  entonces también vale que:
\begin{equation*}
\begin{matrix}
&{\footnotesize{\footnotesize\circled{3}}}&\qquad&&2x& -y & +5z & = &3 \\
(-2)\cdot&{\footnotesize{\footnotesize\circled{1}}}&\qquad&(-2)\cdot&(x &  & +2z) & = &(-2)\cdot1 \\
\hline
&{\footnotesize{\footnotesize\circled{3'}}}&\qquad&& & -y & + z & = & 1  
\end{matrix}
\end{equation*}
 

Por lo tanto $(x,y,z)$ también es solución del sistema
\begin{equation*}
\begin{matrix}
{\footnotesize{\footnotesize\circled{1}}}&\qquad&x &  & +2z & = 1 \\
{\footnotesize{\footnotesize\circled{2'}}}&\qquad&& -3y & +z & = 1   \\
{\footnotesize{\footnotesize\circled{3'}}}&\qquad&& -y & +z & =1
\end{matrix}\tag{S3}
\end{equation*}
 

 
Dado que $(x,y,z)$ es solución del sistema (S3), entonces también vale que: 
\begin{equation*}
\begin{matrix}
&{\footnotesize{\footnotesize\circled{2'}}}&\qquad&& -3y & +z & = & 1 \\
(-3)\cdot&{\footnotesize{\footnotesize\circled{3'}}}&\qquad&(-3)\cdot&( -y & +z) & = &(-3)\cdot1 \\
\hline
&{\footnotesize{\footnotesize\circled{2''}}}&\qquad&&   & -2z & = & -2  
\end{matrix}
\end{equation*}

 
Por lo tanto $(x,y,z)$ también es solución del sistema
\begin{equation*}
\begin{matrix}
x &  & +2z & = 1 \\
&    & -2z & = -2\\
& -y & +z & =1 
\end{matrix}
\qquad\mbox{equivalentemente}\qquad
\begin{matrix}
x &  & +2z & = 1 \\
&    & z & = 1\\
& -y & +z & =1 
\end{matrix}
\end{equation*}
 

 
Dado $(x,y,z)$ es solución del sistema
\begin{equation*}
\begin{matrix}
{\footnotesize{\footnotesize\circled{1}}}&x &  & +2z & = 1 \\
{\footnotesize{\footnotesize\circled{2''}}}&&    & z & = 1 \\
{\footnotesize{\footnotesize\circled{3'}}}&& -y & +z & =1
\end{matrix}\tag{S4}
\end{equation*}
o equivalentemente 
\begin{equation*}
\begin{matrix}
{\footnotesize{\footnotesize\circled{1}}}&x &  & +2z & = 1 \\
{\footnotesize{\footnotesize\circled{3'}}}&& -y & +z & =1\\
{\footnotesize{\footnotesize\circled{2''}}}&&    & z & = 1 
\end{matrix}\tag{S5}
\end{equation*}

Entonces también vale que: 
\begin{center}
\begin{tabular}{rrr}
	{\footnotesize{\footnotesize\circled{1}}}&$x  +2z$  = & 1 \\
	$(-2)\cdot${\footnotesize{\footnotesize\circled{2''}}}&$(-2)\cdot$(\quad\;\;\;$z$ ) = &$(-2)\cdot 1$ \\
	\hline
	{\footnotesize{\footnotesize\circled{1'}}}  &$x$\qquad\; = & $-1$    
\end{tabular}\quad\; y \quad\;
\begin{tabular}{rrr}
	{\footnotesize{\footnotesize\circled{3'}}}&$-y  +z$  = & 1 \\
	$(-1)\cdot${\footnotesize{\footnotesize\circled{2''}}}&$(-1)\cdot$(\quad\;\;\;$z$ ) = &$(-1)\cdot 1$ \\
	\hline
	{\footnotesize{\footnotesize\circled{3''}}}  &$-y$\quad\; = & $0$    
\end{tabular}
\end{center}



Por lo tanto $(x,y,z)$ también es solución del sistema
\begin{equation*}
\begin{matrix}
x &  &  & =& -1 \\
& -y &  & =&0 \\
&    & z & =& 1
\end{matrix} \tag{S6}
\end{equation*}
 o equivalentemente
\begin{equation*}
\begin{matrix}
	x & =& -1 \\
	y & =&0 \\
	z & =& 1
	\end{matrix}\tag{S7}
\end{equation*}


 

 
En resumen, supusimos que $(x,y,z)$ es una solución del sistema
\begin{equation*}
\begin{matrix}
x &  & +2z & = 1 \\
x& -3y & +3z & =2 \\
2x& -y & +3z & =1
\end{matrix}
\end{equation*}

y probamos que 
\begin{equation*}
x=-1\quad y=0,\quad z=1. 
\end{equation*}
				\end{proof}




Tanto en el ejemplo \ref{ej-sist-1} como en el ejemplo \ref{ej-sist-2} eliminamos variables usando alguna de las siguientes operaciones entre ecuaciones:
	\begin{enumerate}
		\item[E1.] multiplicar una ecuación por una constante no nula,  
		\item[E2.] sumar a una ecuación una constante por otra, y
		\item[E3.] permutar ecuaciones.  
	\end{enumerate}
	Veremos en  las secciones siguientes que estas operaciones son ``reversibles'' y que con ellas podemos reducir todo sistema de ecuaciones a uno cuyas soluciones son obvias. 
	
	
\begin{ejemplo}
Así como  haciendo operaciones del tipo E1, E2 y E3  en la ecuaciones del ejemplo \ref{ej-sist-2}   llegamos de 
	
	\begin{equation*}
	\begin{matrix}
	x &  & +2z & =& 1 \\
	x& -3y & +3z & =&2 \\
	2x& -y & +5z & =&3
	\end{matrix}
	\qquad \text{ a } 
	\qquad 
	\begin{matrix}x&=&-1\\ y&=&0 \\ z&=&1,
	\end{matrix}
	\end{equation*}
haciendo  las ``operaciones inversas'' (que son del mismo tipo)  podemos llegar de  
\begin{equation*}
\begin{matrix}x&=&-1\\ y&=&0 \\ z&=&1.
\end{matrix}
\qquad \text{ a } 
\qquad 
\begin{matrix}
x &  & +2z & =& 1 \\
x& -3y & +3z & =&2 \\
2x& -y & +5z & =&3
\end{matrix}.
\end{equation*}
Luego,  ambos sistemas son  tienen las mismas soluciones.

\end{ejemplo}

\vskip .5cm


	\end{section}




		\begin{section}{Equivalencia de sistemas de ecuaciones lineales}

		
		
		

			
			
			
			\vskip .3cm 
			Dado el sistema
			\begin{equation}\label{sist-eq-2}
			\begin{matrix}
			a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &y_1\\
			\vdots&  &\vdots& &&  &\vdots \\
			a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&y_m
			\end{matrix}
			\end{equation}
			donde $y_1, \ldots,y_m$ y $a_{i,j}$ ($1 \le i \le m$, $1 \le j \le n$) son números en $\K$, si multiplicamos cada ecuación por $c_i$ ($1 \le i \le m$) y sumamos miembro a miembro obtenemos
			\begin{equation*}
			\sum_{i=1}^m c_i(a_{i1}x_1 + a_{i2}x_2 + \cdots + a_{in}x_n) = \sum_i c_iy_i.
			\end{equation*}
			Expandiendo la ecuación y tomando como factor común los $x_j$ ($1 \le j \le n$) obtenemos la ecuación
			\begin{multline*}
			(c_{1}a_{11} + c_2a_{21}+ \cdots + c_ma_{m1})x_1 + \cdots +  	(c_{1}a_{1n} + c_2a_{2n}+ \cdots + c_ma_{mn})x_n = \\ = 	c_{1}y_{1} + c_2y_{2}+ \cdots + c_my_{m},
			\end{multline*}
			o,  escrito de otra forma, 
			\begin{equation}\label{comb-lin-eq}
			\left(\sum_{i=1}^{m}c_{i}a_{i1}\right)x_1 + \cdots +  	\left(\sum_{i=1}^{m}c_{i}a_{in}\right)x_n = \sum_{i=1}^{m}	c_{i}y_{i},
			\end{equation}
			la cual es una \textit{combinación lineal} de las ecuaciones dadas en (\ref{sist-eq-2}). Observar que la ecuación (\ref{comb-lin-eq}),  es  una ecuación lineal con $n$ incógnitas, es decir  es del mismo tipo que cada una de las ecuaciones que componen el sistema de ecuaciones original.
			
			\begin{proposicion}\label{sist-impl}
				Sean $c_1,\ldots,c_m$ en $\K$. Si $(x_1,\ldots,x_n) \in \K^n$  es solución del sistema de ecuaciones
				\begin{equation*}
				\begin{matrix}
				a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &y_1\\
				\vdots&  &\vdots& &&  &\vdots \\
				a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&y_m.
				\end{matrix}
				\end{equation*}
				 entonces $(x_1,\ldots,x_n)$ también es solución de la ecuación
				 \begin{equation*}
				 \left(\sum_{i=1}^{m}c_{i}a_{i1}\right)x_1 + \cdots +  	\left(\sum_{i=1}^{m}c_{i}a_{in}\right)x_n = \sum_{i=1}^{m}	c_{i}y_{i},
				 \end{equation*}
			\end{proposicion}
			\begin{proof}
				Por hipótesis
				\begin{equation*}
				a_{i1}x_1 + a_{i2}x_2 + \cdots + a_{in}x_n = y_i,\; \text{ para } 1 \le i \le m.
				\end{equation*}
				Luego, 
				\begin{equation*}
				\sum_{i=1}^m c_i(a_{i1}x_1 + a_{i2}x_2 + \cdots + a_{in}x_n) = \sum_i c_iy_i
				\end{equation*}
				y  esta, como vimos, es otra escritura de la ecuación (\ref{comb-lin-eq}).
			\end{proof}
			
			La idea de hacer combinaciones lineales de ecuaciones es fundamental en el proceso de eliminación de incógnitas. En principio, no es cierto que si obtenemos un sistema de ecuaciones por combinaciones lineales de otro sistema, ambos tengan las mismas soluciones   (por ejemplo, hacer combinaciones lineales triviales con todos los coeficientes iguales a 0).
		
			\begin{definicion}
			Decimos que dos sistemas de ecuaciones lineales son \textit{equivalentes}\index{sistemas lineales equivalentes} si cada ecuación de un sistema es combinación lineal del otro.
			\end{definicion}
			
			
			\begin{teorema}\label{sistemas-equiv}
				Dos sistemas de ecuaciones lineales equivalentes tienen las mismas soluciones.
			\end{teorema}
			\begin{proof}
				Sea 
				\begin{equation}\label{sist-01} \tag{*}
				\begin{matrix}
				a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &y_1\\
				\vdots&  &\vdots& &&  &\vdots \\
				a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&y_m
				\end{matrix}
				\end{equation}
				equivalente a
				\begin{equation}\label{sist-02} \tag{**}
				\begin{matrix}
				b_{11}x_1& + &b_{12}x_2& + &\cdots& + &b_{1n}x_n &= &z_1\\
				\vdots&  &\vdots& &&  &\vdots \\
				b_{k1}x_1& + &b_{k2}x_2& + &\cdots& + &b_{kn}x_n &=&z_k,
				\end{matrix}.
				\end{equation}
				En particular, las ecuaciones  de (\ref{sist-02}) se obtienen a partir de combinaciones lineales de las ecuaciones del sistema  (\ref{sist-01}). Luego, por proposición \ref{sist-impl}, si  $(x_1,\ldots,x_n)$  es solución de  (\ref{sist-01}), también será solución de cada una de las ecuaciones de (\ref{sist-02}) y por lo tanto solución  del sistema. 
				
				Recíprocamente, como también las ecuaciones  de (\ref{sist-01}) se obtienen a partir de combinaciones lineales de las ecuaciones del sistema  (\ref{sist-02}), toda solución  de 	
				(\ref{sist-02}) es solución de  (\ref{sist-01}).
			\end{proof}
			
			
			
			
			
			
			\vskip .2cm  
			\begin{observacion}
				La equivalencia de sistemas lineales es una relación de equivalencia,  en particular  vale la propiedad transitiva: si el sistema (A) es equivalente al sistema (B) y  el sistema (B)  es equivalente al sistema (C),  entonces (A) es equivalente a (C). Esto nos permite, ir paso a paso para eliminar las incógnitas. 
			\end{observacion}

			\vskip .2cm
			\begin{ejemplo}
				Encontrar las soluciones del siguiente sistema de ecuaciones
				\begin{equation*}\label{sist-000} \tag{S0}
				\begin{matrix}
				{\footnotesize\circled{1}}&\qquad& 2x_1 &+&4x_2 &-& 6x_3 &=& 0 \\
				{\footnotesize\circled{2}}&\qquad& 3x_1 &-& x_2 &+& 5x_3 &=& 0.
				\end{matrix}
				\end{equation*}
			\end{ejemplo}
			\begin{proof}[Solución]
				Si reemplazamos la ecuación ${\footnotesize\circled{1}}$ por ${\footnotesize\circled{1}}/2$, obtenemos el sistema
				\begin{align*}\label{sist-001} \tag{S1}
				\begin{matrix}
				{\footnotesize\circled{$1^\prime$}}&\qquad x_1 &+&2x_2 &-& 3x_3 &=& 0 \\
				{\footnotesize\circled{$2^\prime$}}&\qquad 3x_1 &-& x_2 &+& 5x_3 &=& 0.
				\end{matrix}
				\end{align*}
				Reemplazando {\footnotesize\circled{$2^\prime$}} por ${\footnotesize\circled{$2^\prime$}}-3{\footnotesize\circled{$1^\prime$}}$, obtenemos
				\begin{align*}\label{sist-002} \tag{S2}
				\begin{matrix}
				{\footnotesize\circled{$1^{\prime\prime}$}}&\qquad x_1 &+&2x_2 &-& 3x_3 &=& 0 \\
				{\footnotesize\circled{$2^{\prime\prime}$}}&\qquad &-&7 x_2 &+& 14x_3 &=& 0.
				\end{matrix}
				\end{align*} 
				Reemplazando {\footnotesize\circled{$2^{\prime\prime}$}} por ${\footnotesize{\footnotesize\circled{$2^{\prime\prime}$}}}/(-7)$, obtenemos
				\begin{align*}\label{sist-003} \tag{S3}
				\begin{matrix}
				{\footnotesize\circled{$1^{\prime\prime\prime}$}}&\qquad x_1 &+&2x_2 &-& 3x_3 &=& 0 \\
				{\footnotesize\circled{$2^{\prime\prime\prime}$}}&\qquad && x_2 &-& 2x_3 &=& 0.
				\end{matrix}
				\end{align*} 
				Reemplazando {\footnotesize\circled{$1^{\prime\prime\prime}$}} por ${\footnotesize\circled{$1^{\prime\prime\prime}$}}-2{\footnotesize\circled{$2^{\prime\prime\prime}$}}$, obtenemos
				\begin{align*}\label{sist-004} \tag{S4}
				\begin{matrix}
				{\footnotesize\circled{$1^{\prime\prime\prime\prime}$}}&\qquad x_1 && &+& x_3 &=& 0 \\
				{\footnotesize\circled{$2^{\prime\prime\prime\prime}$}}&\qquad && x_2 &-& 2x_3 &=& 0.
				\end{matrix}
				\end{align*} 
				
				Luego $x_1 = -x_3$ y $x_2 = 2x_3$, y esto nos dice que las soluciones son de la forma $\{(-x_3,2x_3, x_3): x_3 \in \R \}$.
				
				Por otro lado, observar que 
				\begin{itemize}
					\item a partir de (\ref{sist-004}) podemos obtener (\ref{sist-003}) reemplazando {\footnotesize\circled{$1^{\prime\prime\prime\prime}$}} por ${\footnotesize\circled{$1^{\prime\prime\prime\prime}$}}+2 \,{\footnotesize\circled{$2^{\prime\prime\prime\prime}$}}$;
					\item a partir de (\ref{sist-003}) podemos obtener (\ref{sist-002}) reemplazando {\footnotesize\circled{$2^{\prime\prime\prime}$}} por $-7\,{\footnotesize\circled{$2^{\prime\prime\prime}$}}$;
					\item a partir de (\ref{sist-002}) podemos obtener (\ref{sist-001}) reemplazando {\footnotesize\circled{$2^{\prime\prime}$}} por ${\footnotesize\circled{$2^{\prime\prime}$}}+3\, {\footnotesize\circled{$1^{\prime\prime}$}}$;
					\item a partir de (\ref{sist-001}) podemos obtener (\ref{sist-000}) reemplazando \circled{$1^{\prime}$} por $2\,\circled{$1^{\prime}$}$.
				\end{itemize}
				
				Es decir los sistemas  (\ref{sist-000}) y (\ref{sist-004}) son equivalentes y por lo tanto  tienen las mismas soluciones.  Como el conjunto de soluciones de (\ref{sist-004}) es $\{(-x_3,2x_3, x_3): x_3 \in \R \}$,  éste también es el conjunto de soluciones del sistema original. 
			\end{proof}
			
			\begin{ejemplo}
				Encontrar las soluciones del siguiente sistema de ecuaciones
				\begin{equation*}
				\begin{matrix}
				{\footnotesize\circled{1}}&\qquad& 2x_1 &-&x_2 &+& x_3 &=& 1 \\
				{\footnotesize\circled{2}}&\qquad& x_1 &+& 3x_2 &+& 3x_3 &=&  2 \\
				\circled{3}&\qquad& x_1 &+& &+& 2x_3 &=&   1 .
				\end{matrix}
				\end{equation*}
				(observar que en $\circled{3}$ el coeficiente de $x_2$ es cero.)
			\end{ejemplo}
			\begin{proof}[Solución] Si
				\begin{align*}
				\begin{matrix*}[l]
				&\text{reemplazamos {\footnotesize\circled{1}} por }&{\footnotesize\circled{1}}-2\,{\footnotesize\circled{2}} + & \text{obtenemos:} &\quad& {\footnotesize\circled{$1^\prime$}}&\; -7x_2 - 5x_3&=&-3, \\
				&\text{reemplazamos {\footnotesize\circled{2}} por }&{\footnotesize\circled{2}}+3 \,{\footnotesize\circled{1}}& \text{obtenemos:} &\quad& {\footnotesize\circled{$2^\prime$}}&\; 7x_1 + 6x_3&=& 5, \\
				&\text{no cambiamos}&\circled{3} & \text{y obtenemos:} &\quad& \circled{$3^\prime$}&\; x_1 + 2x_3&=& 1.
				\end{matrix*}
				\end{align*}
				Ahora reemplazando  ${\footnotesize\circled{$2^\prime$}}$ por  $ {\footnotesize\circled{$2^\prime$}}- 7\, {\footnotesize\circled{$1^\prime$}}$, obtenemos $- 8x_3= -2$, que es equivalente a que $x_3 = \frac14$. Por lo tanto, obtenemos el sistema
				\begin{align*}
				\begin{matrix*}[r]
				{\footnotesize\circled{$1^{\prime\prime}$}}&\qquad -7x_2 - 5x_3&=&-3 \\
				{\footnotesize\circled{$2^{\prime\prime}$}}&\qquad x_3&=& \frac14 \\
				\circled{$3^{\prime\prime}$}&\qquad x_1 + 2x_3&=& 1 ,
				\end{matrix*}
				\end{align*}
				Luego, reemplazando $x_3$ por $\frac14$ y  despejando en  {\footnotesize\circled{$1^{\prime\prime}$}} y {\footnotesize\circled{$2^{\prime\prime}$}}:
				\begin{align*}
				\begin{matrix*}[r]
				{\footnotesize\circled{$1^{\prime\prime\prime}$}}&\qquad -7x_2 &=&-3 +5\frac14 \\
				{\footnotesize\circled{$2^{\prime\prime\prime}$}}&\qquad x_3&=& \frac14 \\
				\circled{$3^{\prime\prime\prime}$}&\qquad x_1 &=& 1 -2\frac14.
				\end{matrix*}
				\end{align*}
				Por lo tanto, $x_1 = \frac12$, $x_2 = \frac14$, $x_3 = \frac14$. 
			\end{proof}
			

		\end{section}
		

		
		
		\begin{section}{Matrices}
			

			\vskip .4cm En  esta sección introduciremos el concepto de matriz y veremos un sistema de ecuaciones se puede describir en el lenguaje de las matrices. También veremos que sistemas de ecuaciones lineales equivalentes se corresponden con matrices equivalentes por filas. Esto nos permitirá,  en la próxima sección, explicitar en forma clara y concisa el  método de Gauss.  
			
			Debemos tener claro que el lenguaje de las matrices en el contexto de esta sección, no es más que una notación más cómoda para el problema de resolver sistemas de ecuaciones lineales.  
			
				\vskip .4cm 
			Estudiaremos la solución de un sistema de ecuaciones lineales 
			\begin{equation}\label{sist-eq-hom}
			\begin{matrix}
			a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &y_1\\
			\vdots&  &\vdots& &&  &\vdots \\
			a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&y_m.
			\end{matrix}
			\end{equation}
			Observemos que podemos escribir los coeficientes de  las fórmulas de la izquierda en  un arreglo rectangular de $m$ filas y $n$ columnas:
			\begin{equation}\label{matriz}
			A = \begin{bmatrix}
			a_{11}& a_{12}& \cdots &a_{1n} \\
			\vdots&\vdots  &  &\vdots \\
			a_{m1} &a_{m2}&\cdots &a_{mn}
			\end{bmatrix}
			\end{equation} 
			También  podemos escribir los $x_1,\ldots,x_n$ e $y_1,\ldots,y_n$ como \textit{matrices columna}
			\begin{equation}\label{matriz-columna}
			X = \begin{bmatrix}
			x_1 \\
			\vdots \\
			x_{n}
			\end{bmatrix}, \qquad
			Y = \begin{bmatrix}
			y_1 \\
			\vdots \\
			y_{m}
			\end{bmatrix}
			\end{equation}
			
			
			\begin{definicion} Sea $\K$ cuerpo. Una \textit{matriz}\index{matriz} $m \times  n$ o de \textit{orden $m \times  n$} es un arreglo rectangular de elementos de $\K$ con $m$ filas y $n$ columnas. A cada elemento de la matriz la llamamos \textit{entrada} o \textit{coeficiente}. Si $A$ es una matriz $m \times  n$, denotamos $[A]_{ij}$ la entrada que se ubica en la fila $i$ y la columna $j$. Al conjunto de matrices de orden $m \times  n$ con entradas en $\K$ lo denotamos $M_{m\times n}(\K)$, o simplemente $M_{m\times n}$ si $\K$  está sobreentendido. 
			\end{definicion}
			
			\begin{observacion}
				Más formalmente, podemos ver una matriz como un elemento del producto cartesiano $(\K^n)^m$, es decir como $m$-uplas donde en cada coordenada hay una $n$-upla. Esta es la forma usual de describir una matriz en los lenguajes de programación modernos. 
			\end{observacion}
			
			\begin{ejemplo}
				El siguiente es un ejemplo de una matriz $2 \times 3$:
				\begin{equation*}
				A = \begin{bmatrix}
				2& -1& 4 \\
				-3 &0&1
				\end{bmatrix}
				\end{equation*} .
			\end{ejemplo}
			
			Usualmente escribiremos a una matriz $m \times  n$   con entradas  $[A]_{ij} = a_{ij}$ como en (\ref{matriz}). A esta matriz también la podemos denotar como $A = [a_{ij}]$. Dos matrices $A = [a_{ij}]$ y $B = [b_{ij}]$, de orden $m \times n$, son iguales si $a_{ij} = b_{ij}$ para todo $i = 1,\ldots,m$  y $j = 1,\ldots,n$. Es decir, dos matrices son iguales si los elementos que ocupan la misma posición en ambas matrices coinciden.
			
			\vskip .3cm
			
			Como hicimos al comienzo de la sección, a un sistema de $m$ ecuaciones con $n$ incógnitas le asignaremos una matriz $m \times  n$  lo cual nos permitirá trabajar en forma más cómoda y, como veremos en la próxima sección,  podremos resolver los sistemas de ecuaciones lineales en forma algorítmica, realizando operaciones elementales por fila en las matrices correspondientes. 
			
			Sean
			$$
				A = \begin{bmatrix}
			a_{11}& a_{12}& \cdots &a_{1n} \\
			\vdots&\vdots  &  &\vdots \\
			a_{m1} &a_{m2}&\cdots &a_{mn}
			\end{bmatrix}, \quad 
			X = \begin{bmatrix}
			x_1 \\ \vdots \\ x_n
			\end{bmatrix}\quad \text{ e } \quad
			Y = \begin{bmatrix}
			y_1 \\ \vdots \\ y_m
			\end{bmatrix}.
			$$
			Entonces, podemos escribir el sistema de ecuaciones (\ref*{sist-eq-hom}) como
			\begin{equation}\label{sist-eq-2}
				A = \begin{bmatrix}
				a_{11}& a_{12}& \cdots &a_{1n} \\
				\vdots&\vdots  &  &\vdots \\
				a_{m1} &a_{m2}&\cdots &a_{mn}
				\end{bmatrix}
				\begin{bmatrix}
				x_1  \\ \vdots \\  x_n
				\end{bmatrix} = 
				\begin{bmatrix}
				y_1 \\ \vdots \\ y_m
				\end{bmatrix}.
			\end{equation}
			
			 En forma resumida:
			\begin{equation}\label{sis-eq-hom-2}
			AX = Y.
			\end{equation}
			Más adelante, veremos que esta notación tiene un sentido algebraico (el término de la izquierda es un ``producto de matrices'').
			
			\begin{subsection}{Operaciones elementales por fila}
				Sea $A = [a_{ij}]$ una matriz $m \times  n$,  entonces la fila $i$ es 
				$$
				\begin{bmatrix} a_{i1}& a_{i2}& \cdots &a_{in} 	\end{bmatrix},
				$$
				y la denotamos $F_i(A)$ o simplemente $F_i$ si $A$ está sobreentendido. Si $c\in \K$,  entonces 
				$$
				cF_i = \begin{bmatrix} ca_{i1}& ca_{i2}& \cdots &ca_{in} 	\end{bmatrix}
				$$
				y
				$$
				F_r + F_s = \begin{bmatrix} a_{r1}+a_{s1}& a_{i2}+a_{s2}& \cdots &a_{in}+a_{sn} 	\end{bmatrix}.
				$$ 
				Diremos que la fila $i$ es nula si 
				$$
				F_i = \begin{bmatrix} 0& 0& \cdots &0 	\end{bmatrix},
				$$
				
				\begin{definicion}
					Sea $A = [a_{ij}]$ una matriz $m \times  n$, diremos que $e$ es una  \textit{operación elemental por fila}\index{operación elemental por fila} si aplicada a la matriz $A$ se obtiene  $e(A)$ de la siguiente manera:
					\begin{enumerate}
						\item[E1.]\label{elem-1} multiplicando la fila $r$ por una constante $c\not=0$, o
						\item[E2.]\label{elem-2} cambiando la fila $F_r$ por $F_r + tF_s$ con $r\not=s$, para algún $t \in \K$, o
						\item[E3.]\label{elem-3} permutando la fila $r$ por la fila $s$.   
					\end{enumerate}
					E1, E2 y E3 son tres tipos de operaciones elementales, más precisamente sea 
					\begin{equation*}
					A = \begin{bmatrix} 
					F_1 \\  \vdots \\	F_m
					\end{bmatrix},
					\end{equation*} entonces 
					\begin{enumerate}
						\item[E1.] si multiplicamos la fila $r$ por $c \not=0$, 
						$$ e(A) = \begin{bmatrix} 
						F_1 \\ 	\vdots \\ cF_r \\ \vdots \\	F_m
						\end{bmatrix} $$ con $c \not=0$, o
						\item[E2.] si $r\not=s$, multiplicamos la fila $s$ por  $t \in \K$ y la sumamos a la fila $r$, 
						$$ e(A)= \begin{bmatrix} 
						F_1 \\  \vdots \\ F_r + t F_s\\ \vdots \\	F_m
						\end{bmatrix}.$$
						\item[E3.] La última operación elemental es  permutar la fila $r$ por la fila $s$:
						$$
						A= \begin{bmatrix} 
						F_1 \\ 	\vdots \\ F_r \\ \vdots \\ F_s\\ \vdots \\	F_m
						\end{bmatrix} \quad \Rightarrow \quad
						e(A)= \begin{bmatrix} 
						F_1 \\ 	\vdots \\ F_s \\ \vdots \\ F_r\\ \vdots \\	F_m
						\end{bmatrix}.$$
					\end{enumerate}
				\end{definicion} 
				\vskip .3cm
				Podemos describir en forma más compacta una operación elemental por fila de la matriz $A=[a_{ij}]$.
				\begin{enumerate}
					\item[E1.] Multiplicar la fila $r$ por $c \not=0$
					$$
					e(A)_{ij} = \left\{ \begin{matrix}
					a_{ij}& \quad &\text{si $i\not=r$} \\
					ca_{ij}& \quad &\text{si $i=r$}
					\end{matrix}\right.
					$$
					\item[E2.] Si $r\not=s$, multiplicar la fila $s$ por $t \in \K$ y sumarla a la fila $r$
					$$
					e(A)_{ij} = \left\{ \begin{matrix}
					a_{ij}& \quad &\text{si $i\not=r$} \\
					a_{rj} + t a_{sj}& \quad &\text{si $i=r$}
					\end{matrix}\right.
					$$
					con $t \in \K$.
					\item[E3.] Permutar la fila $r$ por la fila $s$
					$$
					e(A)_{ij} = \left\{ \begin{matrix}
					a_{ij}& \quad &\text{si $i\not=r,s$}
					\\ a_{sj}& \quad &\text{si $i=r$}
					\\ a_{rj}& \quad &\text{si $i=s$}
					\end{matrix}\right.
					$$
				\end{enumerate} 
				
				\vskip .3cm 
				\begin{ejemplo}
					Sea 
					$$
					A = 
					\begin{bmatrix}
					2&1\\-1&0\\4&-5
					\end{bmatrix}.
					$$
					Ejemplificaremos las operaciones elementales
					
					E1. Multipliquemos la fila 2 por -2, obtenemos
					$$
					e(A) = 
					\begin{bmatrix}
					2&1\\2&0\\4&-5
					\end{bmatrix}.
					$$
					
					E2. Sumemos a la fila 3 dos veces la fila 1,
					$$
					e(A) = 
					\begin{bmatrix}
					2&1\\-1&0\\8&-3
					\end{bmatrix}.
					$$
					
					E3. Permutemos la fila 2 con la fila 3.
					$$
					e(A) = 
					\begin{bmatrix}
					2&1\\4&-5\\-1&0
					\end{bmatrix}.
					$$
				\end{ejemplo}
				
				Una característica importante de las operaciones elementales es que cada una tiene como ``inversa'' otra operación elemental. 
				
				\begin{teorema}\label{op-elem}
					A cada operación elemental por fila $e$ le corresponde otra operación elemental $e^\prime$ (del mismo tipo que $e$) tal que $e^\prime(e(A)) = A$ y $e(e^\prime(A)) = A$. En otras palabras, la operación inversa de una operación elemental es otra operación elemental del mismo tipo.  
				\end{teorema}
				\begin{proof} \
					\begin{enumerate}
						\item[E1.] La operación inversa de multiplicar la fila $r$ por $c\not=0$ es multiplicar la misma fila por $1/r$.
						\item[E2.] La operación inversa de multiplicar la fila $s$ por  $t \in \K$ y sumarla a la fila $r$ es multiplicar  la fila $s$ por  $-t \in \K$ y sumarla a la fila $r$.
						\item[E3.] La operación inversa de permutar la fila $r$ por la fila $s$ es la misma operación.
					\end{enumerate}
				\end{proof}
				
				\begin{definicion} 
					Sean $A$ y $B$ dos matrices $m \times n$. Diremos que $B$ es \textit{equivalente por filas}\index{matrices equivalentes por filas} a $A$, si $B$ se puede obtener de $A$ por un número finito de operaciones elementales por fila. 
				\end{definicion}
				
				\begin{obs} Denotamos $A \sim B$, si $B$ es equivalente a $A$ por filas. Entonces esta relación es una \textit{relación de equivalencia}\index{relación de equivalencia}, es decir es reflexiva, simétrica y transitiva. En  nuestro caso, sean $A$, $B$ y $C$ matrices $m \times n$, entonces ``$\sim$'' cumple: 
					\begin{enumerate}
						\item  $A \sim A$ (reflexiva), 
						\item $A \sim B$, entonces $B \sim A$ (simétrica), y
						\item si $A \sim B$ y $B \sim C$, entonces $A \sim C$.   
					\end{enumerate}
					Claramente ``$\sim$'' es reflexiva (admitamos que no hacer nada es una equivalencia por filas). 
					
					Si podemos obtener $B$ de $A$ por operaciones elementales por fila, entonces, 
					$$
					B = e_k(e_{k-1}(\cdots(e_1(A)))\cdots),
					$$
					con $e_1,\ldots,e_k$ operaciones elementales por fila. Por el teorema \ref{op-elem},  tenemos $e'_1,\ldots,e'_{k-1},e'_k$ operaciones elementales inversas de  $e_1,\ldots,e_{k-1},e_k$, respectivamente. Luego, 
					$$
					A = e'_1(e'_{2}(\cdots(e'_k(B)))\cdots).
					$$
					Es decir, podemos  obtener $A$ de $B$ por operaciones elementales por fila, luego ``$\sim$'' es simétrica. Observar que para obtener $A$ a partir de $B$ tenemos que hacer las operaciones inversas en orden inverso. 
					
					Finalmente,   si podemos obtener $B$ de $A$ por operaciones elementales por fila y  podemos obtener $C$ de $B$ por operaciones elementales por fila, entonces podemos obtener $C$ de $A$ por operaciones elementales por fila (haciendo las primeras operaciones y luego las otras).
				\end{obs}
				
				\begin{ejemplo}
					Veamos que la matriz 
					\begin{equation*}
					A= 	\begin{bmatrix}
					3 & 9 & 6 \\ 4&8&4 \\ 0&2&2
					\end{bmatrix}
					\end{equation*}
					es equivalente por fila a la matriz
					\begin{equation*}
					B = \begin{bmatrix}
					1&0&-1 \\ 0&0&0\\  0&-1&-1
					\end{bmatrix}.
					\end{equation*}
						\end{ejemplo}
					
					\begin{proof}[Solución]				
					Hasta ahora, no hemos aprendido ningún algoritmo o método que nos lleve una matriz a otra por operaciones elementales por fila, pero no es difícil, en este caso, encontrar una forma de llevar la matriz $A$ a la matriz $B$:
					\begin{multline*}
					\begin{bmatrix}
					3 & 9 & 6 \\ 4&8&4 \\ 0&2&2
					\end{bmatrix} \stackrel{F_1/3}{\longrightarrow}
					\begin{bmatrix}
					1 & 3 & 2 \\ 4&8&4 \\ 0&2&2
					\end{bmatrix} \stackrel{F_2 -4F_1}{\longrightarrow}
					\begin{bmatrix}
					1 & 3 & 2 \\ 0&-4&-4 \\ 0&2&2
					\end{bmatrix} \stackrel{F_2/4}{\longrightarrow}
					\\
					\stackrel{F_2/4}{\longrightarrow}  \begin{bmatrix}
					1 & 3 & 2 \\ 0&-1&-1 \\ 0&2&2 
					\end{bmatrix} \stackrel{F_1 + 3F_2}{\longrightarrow} 
					\begin{bmatrix}
					1 & 0& -1 \\ 0&-1&-1 \\ 0&2&2 
					\end{bmatrix} \stackrel{F_3 + 2F_2}{\longrightarrow} 
					\begin{bmatrix}
					1 & 0& -1 \\ 0&-1&-1 \\ 0&0&0 
					\end{bmatrix} \stackrel{F_3\leftrightarrow F_2}{\longrightarrow} 
					\begin{bmatrix}
					1 & 0& -1 \\ 0&0&0\\  0&-1&-1 
					\end{bmatrix}.
					\end{multline*}
					
					Comprobamos fácilmente la propiedad reflexiva, pues podemos llegar de la matriz $B$ a la matriz $A$ haciendo, sucesivamente, la operaciones inversas en orden inverso:
					\begin{multline*}
					\begin{bmatrix}1 & 0& -1 \\ 0&0&0\\  0&-1&-1 \end{bmatrix}
					\stackrel{F_3\leftrightarrow F_2}{\longrightarrow} 
					\begin{bmatrix}1 & 0& -1 \\ 0&-1&-1 \\ 0&0&0 \end{bmatrix}
					\stackrel{F_3 - 2F_2}{\longrightarrow} 
					\begin{bmatrix}1 & 0& -1 \\ 0&-1&-1 \\ 0&2&2 \end{bmatrix} 
					\stackrel{F_1 - 3F_2}{\longrightarrow}
					\\ \stackrel{F_1 - 3F_2}{\longrightarrow}
					\begin{bmatrix}1 & 3 & 2 \\ 0&-1&-1 \\ 0&2&2 \end{bmatrix}
					\stackrel{4F_2}{\longrightarrow} 
					\begin{bmatrix}1 & 3 & 2 \\ 0&-4&-4 \\ 0&2&2 \end{bmatrix} 
					\stackrel{F_2 +4F_1}{\longrightarrow} 
					\begin{bmatrix} 1 & 3 & 2 \\ 4&8&4 \\ 0&2&2\end{bmatrix} 
					\stackrel{3F_1}{\longrightarrow} 
					\begin{bmatrix} 3 & 9 & 6 \\ 4&8&4 \\ 0&2&2 \end{bmatrix}.
					\end{multline*}
				\end{proof}
				
				\vskip .5cm 
				
					
				\begin{definicion}
					Consideremos un sistema como en (\ref{sist-eq-hom}) y sea  $A$ la matriz correspondiente al sistema. La \textit{matriz  ampliada}\index{sistema de ecuaciones lineales!matriz  ampliada} del sistema es 
					\begin{equation}\label{mtrx-ampliada}
					A' = \left[\begin{array}{ccc|c}  
					a_{11} & \cdots & a_{1n} &  y_1 \\
					a_{21} & \cdots & a_{2n} &  y_2 \\
					\vdots &  & \vdots  &  \vdots  \\
					a_{m1} & \cdots & a_{mn} &  y_m 
					\end{array}\right]
					\end{equation}
					que también podemos denotar
					\begin{equation*}
					A' = [A | Y].
					\end{equation*}
				\end{definicion}
				
				
				\begin{teorema}\label{th-equiv-op-elem} Sea $[A | Y]$ la matriz ampliada de un sistema no homogéneo y sea $[B | Z]$ una matriz que se obtiene a partir de $[A | Y]$ por medio de operaciones elementales. Entonces, los sistemas correspondientes a $[A | Y]$ y  $[B | Z]$ tienen las mismas soluciones. 
			\end{teorema}
			\begin{proof}
				 Supongamos que $[B | Z]$ se obtiene por una operación elemental por fila a partir de $[A | Y]$,  entonces las ecuaciones de $[B | Z]$ son combinaciones lineales de las ecuaciones de $[A | Y]$. Como toda operación elemental por fila tiene inversa, podemos obtener $[A | Y]$ a partir de $[B | Z]$ y por lo tanto las ecuaciones de $[A | Y]$ son combinaciones lineales de las ecuaciones de $[B | Z]$. Es decir $[A | Y]$ y $[B | Z]$ determinan sistemas de ecuaciones lineales equivalentes y por lo tanto tiene las mismas soluciones (teorema \ref{sistemas-equiv}).
				 
				 En el caso que $[B | Z]$ se obtenga a partir $[A | Y]$ haciendo varias operaciones elementales,  se aplica el razonamiento de arriba las veces que sea necesario. 
			\end{proof}
				
				\begin{ejemplo}\label{ejemplo2.11}
					Resolvamos el siguiente sistema:
					\begin{align}\label{sist-eq-01}
					\begin{split}
					2x_1 - x_2 + x_3 + 2x_4 &= 2 \\
					x_1 - 4x_2 -x_4 &=1 \\
					2x_1 +6x_2 -x_3 +3x_4 &= 0,  
					\end{split}
					\end{align}
					para  $x_i \in \R$ ($1 \le i \le 4$). 
				
					
					La matriz ampliada  correspondiente a este sistema de ecuaciones es 
					$$
					\left[\begin{array}{cccc|c} 
					 2& -1&1& 2&2 \\ 1&-4 &0&-1&1 \\ 2&6&-1&3&0 \end{array}\right].
					$$
					Encontraremos una matriz que nos dará un sistema de ecuaciones equivalente, pero con soluciones mucho más evidentes:
					\begin{multline*}
					\left[\begin{array}{cccc|c}  2& -1&1& 2&2 \\ 1&-4 &0&-1&1 \\ 2&6&-1&3&0 \end{array}\right]
					\stackrel{F_1\leftrightarrow F_2}{\longrightarrow} 
					\left[\begin{array}{cccc|c}  1&-4 &0&-1&1 \\ 2& -1&1& 2&2 \\ 2&6&-1&3&0 \end{array}\right]
					\stackrel{F_2-2 F_1}{\longrightarrow} 
					\left[\begin{array}{cccc|c}  1&-4 &0&-1&1 \\ 0& 7&1& 4&0 \\ 2&6&-1&3&0 \end{array}\right]
					\\
					\stackrel{F_3-2 F_1}{\longrightarrow} 
					\left[\begin{array}{cccc|c}  1&-4 &0&-1&1 \\ 0& 7&1& 4&0 \\ 0&14&-1&5&-2 \end{array}\right] 
					\stackrel{F_3-2 F_2}{\longrightarrow} 
					\left[\begin{array}{cccc|c}  1&-4 &0&-1&1 \\ 0& 7&1& 4&0 \\ 0&0&-3&-3&-2 \end{array}\right] 
					\stackrel{F_3/(-3)}{\longrightarrow} 
					\left[\begin{array}{cccc|c}  1&-4 &0&-1&1 \\ 0& 7&1& 4&0 \\ 0&0&1&1&\frac{2}{3} \end{array}\right]
					\\
					\stackrel{F_2- F_3}{\longrightarrow} 
					\left[\begin{array}{cccc|c}  1&-4 &0&-1&1 \\ 0& 7&0& 3&-\frac{2}{3} \\ 0&0&1&1&\frac{2}{3} \end{array}\right]
					\stackrel{F_2/7}{\longrightarrow} 
					\left[\begin{array}{cccc|c}  1&-4 &0&-1&1\\ 0& 1&0& \frac37&-\frac{2}{21}\\ 0&0&1&1&\frac{2}{3}\end{array}\right]
					\stackrel{F_1 +4F_2}{\longrightarrow} 
					\left[\begin{array}{cccc|c} 1&0&0&\frac{5}{7}&\frac{13}{21}\\0&1&0&\frac37&-\frac{2}{21} \\ 0&0&1&1&\frac{2}{3}\end{array}\right].
					\end{multline*}
					Volvamos a las ecuaciones: el nuevo sistema de ecuaciones, equivalente al original, es
					\begin{align*}
					x_1 +\frac{5}{7}x_4 &= \frac{13}{21} \\
					x_2 + \frac{3}{7}x_4 &=-\frac{2}{21} \\
					x_3 +x_4 &= \frac{2}{3}, 
					\end{align*}
					luego 
					\begin{align*}
					x_1  &=-\frac{5}{7}x_4 + \frac{13}{21}\\
					x_2  &=- \frac{3}{7}x_4 -\frac{2}{21} \\
					x_3  &= -x_4+\frac{2}{3}. 
					\end{align*}
					Por lo tanto, el conjunto de soluciones del sistema de ecuaciones (\ref{sist-eq-01}) es
					$$
					\left\{(-\frac{5}{7}t+ \frac{13}{21},\,- \frac{3}{7}t-\frac{2}{21},\, -t+\frac{2}{3},\,t): t \in \R \right\}.
					$$
					Luego, el sistema tiene infinitas soluciones parametrizadas por una variable $t \in \R$.
				\end{ejemplo}
				
				
				\begin{ejemplo}
					Consideremos ahora el siguiente sistema sobre los números complejos:
					\begin{align}\label{sist-eq-03}
					\begin{split}
					2x_1 +i x_2 &= 0 \\
					-ix_1 +3x_2  &=0 \\
					x_1 +2x_2  &= 0.
					\end{split}
					\end{align}
					Al ser un sistema homogéneo $x_1=x_2 = 0$ es solución. Veamos si hay otras soluciones: 
					\begin{multline*}
					\begin{bmatrix} 2&i \\ -i&3 \\ 1&2 \end{bmatrix}
					\stackrel{F_1\leftrightarrow F_3}{\longrightarrow} 
					\begin{bmatrix} 1&2 \\ -i&3 \\ 2&i \end{bmatrix}
					\stackrel{F_2+iF_1}{\longrightarrow} 
					\begin{bmatrix} 1&2 \\ 0&3+2i \\ 2&i \end{bmatrix}
					\stackrel{F_3-2F_1}{\longrightarrow} 
					\begin{bmatrix} 1&2 \\ 0&3+2i \\ 0&-4+i \end{bmatrix}
					\\
					\stackrel{F_2/(3+2i)}{\longrightarrow} 
					\begin{bmatrix} 1&2 \\ 0&1 \\ 0&-4+i \end{bmatrix}
					\stackrel{F_3-(-4+i)F_2}{\longrightarrow} 
					\begin{bmatrix} 1&2 \\ 0&1 \\ 0&0 \end{bmatrix}
					\stackrel{F_1-2F_2}{\longrightarrow} 
					\begin{bmatrix} 1&0 \\ 0&1 \\ 0&0 \end{bmatrix}.
					\end{multline*}
					Luego  el sistema (\ref{sist-eq-03}) es equivalente al sistema  $x_1=x_2 = 0$, que resulta ser la única solución.
				\end{ejemplo}
			\end{subsection} 
		\end{section}
	
	
	
			\begin{section}{Método de Gauss } Ahora avanzaremos en una forma sistemática para hallar todas las soluciones de un sistema de ecuaciones.
	
			
			\begin{subsection}{Matrices reducidas por filas} 
				
				\begin{definicion}
					Una matriz $A$ de $m \times n$ se llama \textit{reducida por filas}\index{matriz!reducida por filas} o \textit{MRF}\index{MRF} si 
					\begin{enumerate}
						\item[(a)] la primera entrada no nula de una fila de $A$ es 1. Este 1 es llamado \textit{1 principal}\index{1 principal de una MRF}.
						\item[(b)] Cada columna de $A$ que contiene un  1 principal tiene todos los otros elementos iguales a 0. 
					\end{enumerate} 
				Una matriz $A$ de $m \times n$ es \textit{escalón reducida por fila}\index{matriz!escalón reducida por fila} o \textit{MERF}\index{MERF} si,  es  MRF y
				\begin{enumerate}
					\item[\textit{c})] todas las filas cuyas entradas son todas iguales a cero están al final de la matriz, y
					\item[\textit{d})] en dos filas consecutivas no nulas el 1 principal de la fila inferior está más a la derecha que el 1 principal de la fila superior. 
				\end{enumerate}
				
				\end{definicion} 
				
				\begin{ejemplo} Las siguientes matrices son MRF, pero  no MERF:
					\begin{equation*}
					\begin{bmatrix}1 & 0& -1 \\  0&0&0\\ 0&1&3 \end{bmatrix}\;  \text{no cumple (c),} \qquad
					\begin{bmatrix} 0&1&3 \\1 & 0& -1\\  0&0&0 \end{bmatrix}\; \text{no cumple (d).} \qquad
					\end{equation*}
					Las siguientes matrices, no son MRF:
					\begin{equation*}
					\begin{bmatrix}1 & 0& 1 \\ 0&2&3\\  0&0&0 \end{bmatrix}\; \text{no cumple (a),} \qquad
					\begin{bmatrix}1 & 0& -1 \\ 0&1&3\\  0&0&1 \end{bmatrix}\; \text{no cumple (b)}. 
					\end{equation*}
					Las siguientes son MERF:
					\begin{equation*}
					\begin{bmatrix} 1&0&0&2 \\ 0&1&0&5 \\ 0&0&1&4\end{bmatrix}, \qquad
					\begin{bmatrix} 1&0&0 \\ 0&1&0 \\ 0&0&0\end{bmatrix}, \qquad
					\begin{bmatrix} 0&0 \\ 0&0\end{bmatrix}, \qquad
					\begin{bmatrix} 0&1&2&0&1 \\ 0&0&0&1&0 \\ 0&0&0&0&0 \\ 0&0&0&0&0\end{bmatrix}.
					\end{equation*}
				\end{ejemplo}
				
				
				En general una matriz  MERF tiene la forma
				\begin{equation}\label{merf-general}
				\begin{bmatrix}
				0& \cdots & 1 & *&0&* &* &0& * &*\\
				0& \cdots &0 &\cdots & 1& *  & * & 0& * &*\\
				\vdots&  &\vdots &  &\vdots && &\vdots& &\vdots\\
				0& \cdots &0& \cdots & 0&\cdots &\cdots & 1& * &* \\
				0& \cdots &0& \cdots & 0&\cdots &\cdots & 0& \cdots &0 \\
				\vdots&  &\vdots &  &\vdots && &\vdots& &\vdots\\
				0& \cdots &0& \cdots & 0&\cdots &\cdots & 0& \cdots &0
				\end{bmatrix}
				\end{equation}
				
				\vskip .5cm
				
				
				
				
				\begin{definicion}
					Sea $I_n$ la matriz  $n \times n$ definida 
					\begin{equation*}
					[I_n]_{ij} = \left\{ \begin{matrix}
					1 && \text{si $i=j$,}\\
					0 && \text{si $i\not=j$,}
					\end{matrix}\right.\qquad \text{ o bien }\qquad I_n =
					\begin{bmatrix}
					1 & 0 & \cdots & 0 \\
					0 & 1 & \cdots & 0 \\
					\vdots & \vdots & \ddots & \vdots \\
					0 & 0 & \cdots & 1
					\end{bmatrix}
					\end{equation*}
					(la matriz cuadrada con 1's en la diagonal y 0's en las otras entradas). Llamaremos a $I_n$ la \textit{matriz identidad $n \times n$}\index{matriz!identidad $n \times n$}.
				\end{definicion}
				
				Observar que $I_n$ es una matriz escalón reducida por fila.
				
				\begin{teorema}\label{th-merf}
					Toda matriz $m \times n$ sobre $\K$ es equivalente por fila a una matriz escalón reducida por fila.
				\end{teorema}
				\begin{proof}[Demostración (*)]
					Sea $A = [a_{ij}]$ una matriz $m \times n$. Trabajaremos fila por fila, de la primera a la última, de tal forma de ir encontrando matrices equivalentes por fila en cada paso, con ciertas características que ya detallaremos. Cuando terminemos llegaremos a una MRF.  
					
					Si la primera fila es nula pasamos a la segunda fila. Si la primera fila no es nula sea $a_{1k}$ la primera entrada no nula, es decir
					\begin{equation*}
					A = \begin{bmatrix}
					0 & \cdots & 0 & a_{1k} & \cdots & a_{1n} \\
					a_{21}& \cdots & a_{2,k-1} & a_{2k} & \cdots & a_{2n} \\
					\vdots&  &  &  &  & \vdots \\
					a_{m1}& \cdots & a_{m,k-1} & a_{mk} & \cdots & a_{mn} \\
					\end{bmatrix}.
					\end{equation*} 
					Esta matriz es equivalente por fila a $A_1$ donde $A_1$ se obtiene dividiendo la fila 1 por $a_{1k}$. Luego 
					\begin{equation*}
					A_1 = \begin{bmatrix}
					0 & \cdots & 0 & 1 & \cdots & a_{1n} \\
					a_{21}& \cdots & a_{2,k-1} & a_{2k} & \cdots & a_{2n} \\
					\vdots&  &  &  &  & \vdots \\
					a_{m1}& \cdots & a_{m,k-1} & a_{mk} & \cdots & a_{mn} \\
					\end{bmatrix}
					\end{equation*}
					(donde los nuevos $a_{1j}$ son  los originales divididos por $a_{1k}$).  
					
					Haciendo $m-1$ equivalencias por fila (reemplazamos $F_i$ por $F_i - a_{ik}F_1$) podemos hacer nulas todas las entradas debajo del  1 principal y obtener la matriz equivalente por fila
					\begin{equation*}
					A_2 = \begin{bmatrix}
					0 & \cdots & 0 & 1 & \cdots & a_{1n} \\
					a_{21}& \cdots & a_{2,k-1} & 0 & \cdots & a_{2n} \\
					\vdots&  &  &  &  & \vdots \\
					a_{m1}& \cdots & a_{m,k-1} &0 & \cdots & a_{mn} \\
					\end{bmatrix}
					\end{equation*}
					(obviamente los nuevos $a_{ij}$ están transformados por las equivalencias).
					
					El mismo procedimiento que hicimos arriba lo podemos hacer en la fila 2,  de tal forma que la fila 2 es cero u obtenemos otro 1 principal y todas las demás entradas de la columna donde se encuentra el 1 principal son nulas. 
					
					Repitiendo este procedimiento en todas las filas hasta la última, obtenemos que cada fila es, o bien 0, o bien la primera entrada no nula es  1 y todas las entradas en la misma columna de este 1 principal son nulas.   Esto, claramente, nos dice que hemos obtenido una matriz reducida por fila.  
					
				 Finalmente, intercambiando filas podemos entonces obtener una matriz escalón reducida por fila.

					
				\end{proof}
				
				El procedimiento que usamos en la demostración del teorema anterior nos da un algoritmo para obtener una MERF a partir de una matriz arbitraria.
				
				\begin{observacion}[\sc Método para reducir a MERF] \label{metodo-merf}  
					\
									
				\begin{enumerate}
					\item \label{paso-0} Nos ubicamos en la primera fila.
					\item \label{paso-1} Si la fila es 0 y no es la última, pasar a la fila siguiente.
					\item \label{paso-2} Si la fila no es 0, 
					\begin{enumerate}
						\item si el primera entrada no nula está en  la columna $k$ y su valor es $c$, dividir la fila por $c$ (ahora la primera entrada no nula vale 1),
						\item con operaciones elementales del tipo $F_r+ tF_s$ hacer 0  todas las entradas en la columna $k$ (menos la de la columna actual).    
					\end{enumerate}
					\item \label{paso-3} Si la fila es la última, pasar al (\ref{paso-4}). Si la fila no es la última, pasar a la fila siguiente e ir al paso (\ref{paso-1}).  
					\item \label{paso-4} Permutar las filas hasta obtener una MERF. 
				\end{enumerate}
			\end{observacion}
				
				
				\begin{ejemplo} Ejemplifiquemos con la matriz que aparece en el ejemplo  \ref{ejemplo2.11},  es decir con la matriz
					$$
					\begin{bmatrix} 2& -1&1& 2 \\ 1&-4 &0&-1 \\ 2&6&-1&3 \end{bmatrix}.
					$$
					Siguiendo estrictamente el algoritmo:
					\begin{multline*}
					\begin{bmatrix*}[r] 2& -1&1& 2 \\ 1&-4 &0&-1 \\ 2&6&-1&3 \end{bmatrix*}
					\stackrel{F_1/2}{\longrightarrow} 
					\begin{bmatrix*}[r] 1& -\frac12&\frac12& 1 \\ 1&-4 &0&-1 \\ 2&6&-1&3 \end{bmatrix*}
					\underset{F_3-2F_1}{\stackrel{F_2- F_1}{\longrightarrow}} 
					\begin{bmatrix*}[r] 1& -\frac12&\frac12& 1 \\ 0&-\frac72 &-\frac12&-2 \\ 0&7&-2&1 \end{bmatrix*}
					\\
					\stackrel{F_2/(-\frac72)}{\longrightarrow} 
					\begin{bmatrix*}[r] 1& -\frac12&\frac12& 1 \\ 0&1 &\frac17&\frac47 \\ 0&7&-2&1 \end{bmatrix*}
					\underset{F_3-7F_2}{\stackrel{F_1 +\frac12 F_2}{\longrightarrow}} 
					\begin{bmatrix*}[r] 1& 0&\frac47& \frac97 \\ 0&1 &\frac17&\frac47 \\ 0&0&-3&-3 \end{bmatrix*}
					\\
					\stackrel{F_3/(-3)}{\longrightarrow} 
					\begin{bmatrix*}[r] 1& 0&\frac47& \frac97 \\ 0&1 &\frac17&\frac47 \\ 0&0&1&1 \end{bmatrix*}
					\underset{F_2-\frac17F_3}{\stackrel{F_1 -\frac47 F_3}{\longrightarrow}} 
					\begin{bmatrix*}[r] 1& 0&0& \frac{5}7 \\ 0&1 &0&\frac37 \\ 0&0&1&1 \end{bmatrix*}.
					\end{multline*}
					Observemos que llegamos a la misma matriz que en el ejemplo \ref{ejemplo2.11}, pese a que hicimos otras operaciones elementales.
				\end{ejemplo}
				

				
			
				
	
				
				Llevar una matriz a una matriz  escalón reducida por fila equivalente nos provee un método para encontrar todas las soluciones de sistemas de ecuaciones homogéneos.
				
				\begin{ejemplo}
					Supongamos que queremos encontrar las soluciones del sistema $AX = 0$ donde $A$ es una matriz $4 \times 5$ y que $A$ es equivalente por filas a la matriz
					\begin{equation*}
					\begin{bmatrix} 1&0&0&2&-3 \\ 0&1&0&0&-5 \\ 0&0&1&-1&2 \\ 0&0&0&0&0\end{bmatrix}.
					\end{equation*} 
					Entonces el sistema de ecuaciones original, de 4 ecuaciones y 5 incógnitas,  es equivalente al sistema
					\begin{equation*}
					\begin{matrix*}[r]
					x_1 +2x_4 -3x_5 &= 0 \\ x_2 -5x_5 &= 0 \\ x_3-x_4+2x_5 &= 0
					\end{matrix*} 
					\qquad \Rightarrow \qquad 
					\begin{matrix*}[l]
					x_1  &= -2x_4 +3x_5 \\ x_2  &= 5x_5 \\ x_3 &= x_4-2x_5
					\end{matrix*}. 
					\end{equation*}
					Por  lo tanto el conjunto de soluciones del sistema es
					\begin{equation*}
					\{(-2s +3t,  5t,  s-2t, s, t): s,t \in \R\}
					\end{equation*}
				\end{ejemplo}   
			\end{subsection}
			
			
			\begin{subsection}{Método de Gauss} Consideremos el siguiente sistema homogéneo de $m$ ecuaciones lineales con $n$ incógnitas:
				\begin{equation*}
				\begin{matrix}
				a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &y_1\\
				\vdots&  &\vdots& &&  &\vdots \\
				a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&y_n
				\end{matrix}
				\end{equation*}
				con $a_{ij} \in \K$. Sea $[A | Y]$ la matriz asociada a este sistema. Mediante operaciones elementales de fila obtenemos una matriz $[B|Z]$ escalón reducida por fila (teoremas \ref{th-mrf} y \ref{th-merf}). Por el teorema \ref{th-equiv-op-elem} los sistemas de ecuaciones $AX=Y$ y $BX=Z$ tiene las mismas soluciones. 
				
				
				 Veamos ahora formalmente cuales son las soluciones del sistema $BX=Z$. Sea $r$ el número de filas no nulas de $B$ y $k_1,\ldots, k_r$ las columnas donde aparecen los primeros 1's en las primeras $r$ filas. Entonces, $k_1 < k_2 < \cdots< k_r$ y el sistema de ecuaciones asociado a $B$ es:
				\begin{equation}
				\begin{matrix}
				&x_{k_1}& + &\sum_{j \not= k_1,\ldots, k_r} b_{1j}\,x_j= &z_1\\
				&x_{k_2}& + &\sum_{j \not= k_1,\ldots, k_r} b_{2j}\,x_j= &z_2\\
				& \vdots& &  &\vdots \\
				&x_{k_r}& + &\sum_{j \not= k_1,\ldots, k_r} b_{rj}\,x_j= &z_r\\
				\end{matrix}
				\end{equation}  
				y, por lo tanto,
				\begin{equation}
				\begin{matrix}\label{sist-eq-hom-merf0}
				x_{k_1} &= z_1-\sum_{j \not= k_1,\ldots, k_r} b_{1j}\,x_j\\
				x_{k_2} &= z_2-\sum_{j \not= k_1,\ldots, k_r} b_{2j}\,x_j\\
				\vdots& \vdots \\
				x_{k_r}  &= z_r-\textstyle\sum_{j \not= k_1,\ldots, k_r} b_{rj}\,x_j
				\end{matrix}.
				\end{equation} 
				Llamaremos a  $x_{k_1}, x_{k_2}, \ldots, x_{k_r}$ las \textit{variables principales}\index{sistema de ecuaciones lineales!variables principales} del sistema  y  las $n-r$ variables restantes son las \textit{variables libres}.\index{sistema de ecuaciones lineales!variables libres} Es claro entonces que variando de forma arbitraria todas las variables libres obtenemos todas las soluciones del sistema.
				
				 Las soluciones del sistema son, entonces,  los $(x_1,\ldots,x_n)\in \K^n$ tal que $x_{k_1},\ldots,x_{k_r}$ satisfacen las ecuaciones (\ref{sist-eq-hom-merf0}) y $x_i \in \K$ para $x_i$ libre. 
				
				\vskip .3cm
				El método anterior es denominado \textit{método de Gauss.}\index{sistema de ecuaciones lineales!método de Gauss} 
				\vskip .3cm
				En particular,  hemos probado:
				
				\begin{teorema}\label{inf-sol}
					Sea $A$ matriz $n \times n$ con coeficientes en $\K$. Entonces, si $A$ es equivalente por filas a  $B$ una  MERF y $B$ tiene filas nulas, el sistema $AX=Y$ tiene infinitas soluciones. 
				\end{teorema}
				\begin{proof}
					Sea $r$  el número de filas no nulas de $B$, como $B$ tiene filas nulas,  entonces $r<n$ y, por lo tanto, existe al menos una variable libre. Esto implica que hay infinitas soluciones. 
					
					Por ser $A$ y $B$ equivalentes por fila, $AX=Y$ y $BX=Z$ tienen las mismas soluciones y, por consiguiente, $AX=Y$ tiene infinitas soluciones. 
				\end{proof}
				
				\begin{teorema}\label{soluciones-m-menor-n}
					Sea $A$ una matriz $m \times n$ con  $m < n$ e $Y$ matriz $m \times 1$ . Entonces, el sistema de ecuaciones lineales $AX=Y$ tiene infinitas soluciones.
				\end{teorema}
				\begin{proof}
					La matriz $A$ es equivalente por fila a una matriz $B$ escalón reducida por fila 
					y $B$ tiene $r$ filas no nulas con $r \le m <n$. Por lo tanto hay $n-r \ge 1$ variables libres, y en consecuencia $BX=Z$ (y $AX=Y$) tienen infinitas soluciones.
				\end{proof}
				
				\begin{lema}\label{mtrx-merf-id}
					Sea $R$ una matriz $n \times n$ escalón reducida por fila tal que no tiene filas nulas. Entonces $R=I_n$. 
				\end{lema}
				\begin{proof}
					Como  $R$ es reducida por fila y no tiene filas nulas, cada fila tiene  un 1 en alguna entrada y en la columna donde está el 1 todos las otras entradas son nulas, por  lo tanto hay $n$ 1's principales distribuidos en $n$ columnas. Concluyendo: hay un 1 por columna y en esa columna todas las demás entradas son nulas. 
					
					Ahora bien como $R$ es una MERF, la primera fila contiene el 1 que está más a la izquierda, que no puede estar en otra ubicación que no sea la primera (pues si no la primera columna sería nula). Con el mismo razonamiento vemos que en la segunda fila hay un 1 en la columna 2 y en general en la fila $k$-ésima hay un 1 en la columna $k$. Luego $R=I_n$.
				\end{proof}			
				
				\begin{teorema}
					Sea $A$ una matriz $n \times n$. Entonces, $A$ es equivalente por filas a la matriz $I_n$  si y sólo si el sistema $AX = Y$ tiene un única solución. 
				\end{teorema}
				\begin{proof}
					($\Rightarrow$) Como $A$ es equivalente por filas a la matriz $I_n$, las soluciones de $AX =Y$ son las mismas que las de $I_nX=Z$, para algún $Z$. Ahora bien,  en la fila $i$ de la matriz $I_n$ tenemos $[I_n]_{ii} =1$ y las otras entradas son cero, luego la ecuación correspondiente a esa fila es $x _i =0$, y esto ocurre en todas las filas, luego el sistema de ecuaciones es
					\begin{align*}
					x_1 &=z_1 \\ x_2 &= z_2 \\ \vdots \\ x_n &= z_n
					\end{align*}
					cuya única solución es la solución trivial.
					
					($\Leftarrow$) Sea $R$ la matriz escalón reducida por filas asociada a $A$. Por hipótesis, $AX=Y$ tiene una sola solución y  por lo tanto $RX=Z$, para algún $Z$, tiene una sola solución. Luego, no hay variables libres, es decir hay $n$ filas no nulas en $R$, como $R$ tiene $n$ filas, lo anterior implica que $R$ no tiene filas nulas. Entonces, por el lema anterior, $R=I_n$.
				\end{proof}
			\end{subsection}
			
			
			
				\begin{ejemplo} Resolvamos el sistema
					\begin{align*}
					x_1 -2x_2 + x_3  &= 1\\
					2x_1 +x_2 + x_3  &= 2\\
					5x_2 - x_3  &= 0.
					\end{align*}
					La matriz  aumentada correspondiente a este sistema es
					\begin{equation*}
					 \left[\begin{array}{rrr|r} 
					 1 & -2 & 1 &  1 \\ 2 & 1 & 1 &  2 \\ 0 & 5 & -1 &  0 
					\end{array}\right]
					\end{equation*}
					apliquemos el método de Gauss:
					\begin{multline*}
					\left[\begin{array}{rrr|r}1 & -2 & 1 &  1 \\ 2 & 1 & 1 &  2 \\	0 & 5 & -1 &  0  \end{array}\right]
					\stackrel{F_2 - 2F_1}{\longrightarrow} 
					\left[\begin{array}{rrr|r}1 & -2 & 1 &  1 \\ 0 & 5 & -1 &  0 \\	0 & 5 & -1 &  0  \end{array}\right]
					\stackrel{F_3-F_1}{\longrightarrow} 
					\left[\begin{array}{rrr|r}1 & -2 & 1 &  1 \\ 0 & 5 & -1 &  0 \\	0 & 0 & 0 & 0  \end{array}\right]
					\\
					\stackrel{F_2/5}{\longrightarrow} 
					\left[\begin{array}{rrr|r}1 & -2 & 1 &  1 \\ 0 & 1 & -1/5 &  0 \\	0 & 0 & 0 &  0  \end{array}\right]
					\stackrel{F_1+2 F_2}{\longrightarrow} 
					\left[\begin{array}{rrr|r}1 & 0 & 3/5 & 1 \\ 0 & 1 & -1/5 &  0 \\	0 & 0 & 0 &  0  \end{array}\right].
					\end{multline*}	
					Luego,  el sistema se reduce  a
					\begin{align*}
					x_1  + 3/5x_3  &= 1\\
					x_2 -1/5 x_3  &= 0.
					\end{align*}
					Es decir, 
					\begin{align*}
					x_1    &= - 3/5x_3 + 1\\
					x_2   &= 1/5 x_3.
					\end{align*}
					En consecuencia,  las soluciones de esta ecuación son
					\begin{equation*}
					\left\{(-\frac{3}5s + 1, \frac15 s, s ): s \in \K \right\}.
					\end{equation*}
				\end{ejemplo} 
				
			
			
			
		\end{section}
		
		
		
		
		
		
		
		
		\begin{section}{Álgebra de matrices}\label{seccion-alg-de-matrices}
			
			Ahora estudiaremos propiedades algebraicas de las matrices,  en particular veremos que dado $n\in \mathbb N$, entonces podemos definir una suma y un producto en el conjunto de matrices $n \times n$ con la propiedad de que estas operaciones satisfacen muchos de los axiomas que definen a  $\mathbb Z$.  
			
			
			\begin{subsection}{Algunos tipos de matrices}
				
				\
				\vskip .3cm
				
				\textbf{Matriz cuadrada.} Es aquella que tiene igual número de filas que de columnas, es decir si es una matriz $n \times n$ para algún $n \in\mathbb N$. En ese caso, se dice que la matriz es de orden $n$. Por ejemplo, la matriz
				\begin{equation*}
				A= \begin{bmatrix}
				1&3&0\\-1&4&7\\-2&0&1
				\end{bmatrix}
				\end{equation*}
				es cuadrada de orden 3.
				
				Denotaremos el conjunto de todas las matrices cuadradas de orden $n$ con entradas en $\K$ por $M_n(\K)$ o simplemente $M_n$ si $\K$  está sobreentendido. Así, en el 	ejemplo anterior $A \in M_3$.
				
				Los elementos de la \textit{diagonal principal} de una matriz\index{diagonal principal de una matriz} cuadrada son aquellos que están situados
				en la diagonal que va desde la esquina superior izquierda hasta la inferior derecha. En otras
				palabras, la diagonal principal de una matriz $A=[a_{ij}]$ está formada por los elementos
				$a_{11},a_{22},\ldots,a_{nn}$.  En el ejemplo anterior la diagonal principal está compuesta por los elementos: $a_{11} = 1$, $a_{22} = 4$ , $a_{33} = 1$.
				
				\vskip .3cm
				
				\textbf{Matriz diagonal y matriz escalar.} Una matriz cuadrada, $A=[a_{ij}]$ de orden $n$, es \textit{diagonal}\index{matriz!diagonal} si $a_{ij}  =0$ , para $i \not= j$ . Es decir, si todos los elementos situados fuera de la diagonal principal son cero. Por ejemplo, la siguiente matriz es diagonal:
				\begin{equation}
				\begin{bmatrix}
				2&0&0&0\\0&-1&0&0\\0&0&5&0\\0&0&0&3
				\end{bmatrix}.
				\end{equation}
				
				Un matriz $n \times n$  es \textit{escalar}\index{matriz!escalar} si  es diagonal y todos los elementos de la diagonal son iguales, por ejemplo, en el caso $4 \times 4$ las matrices escalares son 
				
				\begin{equation}
				\begin{bmatrix}
				c&0&0&0\\0&c&0&0\\0&0&c&0\\0&0&0&c
				\end{bmatrix},
				\end{equation}
				con $c \in \K$.
				
				\vskip .3cm
				
				\textbf{Matriz unidad o identidad.} Esta matriz ya la hemos definido anteriormente. Recordemos que es una matriz diagonal cuya diagonal principal está compuesta de 1's. 
				
				Más adelante veremos que la matriz identidad, respecto a la multiplicación de matrices, juega un papel similar al número 1 respecto a la multiplicación de números reales o enteros (elemento neutro del producto).
				
				\vskip .3cm
				
				\textbf{Matriz nula}. La \textit{matriz nula}\index{matriz!nula} de orden $m\times n$, denotada $0_{m \times n}$ o simplemente $0$ si $m$ y $n$ están sobreentendidos,  es la  matriz $m \times n$ cuyas entradas son todas nulas ($=0$). 
				
				Por ejemplo, la matriz nula $2 \times 3$ es
				\begin{equation*}
				\begin{bmatrix} 0&0&0\\ 0&0&0\end{bmatrix}.
				\end{equation*}
				Veremos luego que la matriz nula juega un papel similar al número 0 en el álgebra de matrices (elemento neutro de la suma).  
				
				\textbf{Matriz triangular.} Una matriz cuadrada es \textit{triangular superior} o \textit{escalón}\index{matriz!triangular superior}\index{matriz!escalón} si todos los elementos situados por debajo de la diagonal principal son cero. Por ejemplo, la siguiente matriz es triangular superior:
				\begin{equation}
				\begin{bmatrix}
				2&-1&3&1\\0&-1&0&2\\0&0&5&1\\0&0&0&3
				\end{bmatrix}.
				\end{equation}
				Análogamente, una matriz cuadrada es \textit{triangular inferior}\index{matriz!triangular inferior} si todos los elementos situados por encima de la diagonal principal son cero. Un matriz triangular (superior o inferior) se dice \textit{estricta} si la diagonal principal es 0.
				
				En forma más precisa, sea $A =[a_{ij}]\in M_n(\K)$,  entonces
				\begin{itemize}
					\item $A$ es \textit{triangular superior} (\textit{triangular superior estricta}) si $a_{ij} =0$ para $i < j$ (resp. $i \le j$),
					\item $A$ es \textit{triangular inferior} (\textit{triangular inferior estricta}) si $a_{ij} =0$ para $i > j$ (resp. $i \ge j$).
				\end{itemize}
				
				Por  ejemplo, cualquier matriz diagonal es triangular superior y también triangular inferior. 
				
				No es difícil comprobar que si $R$ es una matriz cuadrada $n \times n$  que es una MERF,  entonces $R$  es triangular superior. 
				
			 
			\end{subsection}
			
			
			\begin{subsection}{Suma  de matrices}
				Sean $A=[a_{ij}]$, $B=[b_{ij}]$ matrices  $m \times n$. La matriz $C= [a_{ij} + b_{ij}]$ de orden $m \times n$,  es decir la matriz cuyo valor en la posición $ij$ es  $a_{ij} + b_{ij}$, es llamada \textit{la suma de las matrices $A$ y $B$} y se denota $A+B$. En otras palabras, la suma de dos matrices es la matriz que resulta de sumar ``coordenada a coordenada'' ambas matrices. 
				
				Veamos un ejemplo, consideremos las siguientes matrices:
				\begin{equation*}
				A = \begin{bmatrix} -1&3&5 \\ 2&0&-1  \end{bmatrix}, \qquad
				B = \begin{bmatrix} 5&2&-3 \\ 0&-1&1  \end{bmatrix}, \qquad
				M = \begin{bmatrix} 2&1&-3 \\ 3&0&-1 \\ 0&8&5 \end{bmatrix}.
				\end{equation*}
				Las matrices $A$ y $B$ son de orden $2 \times 3$, mientras la matriz $M$ es cuadrada de orden 3. Por tanto, no podemos calcular la suma de $A$ y $M$ y tampoco la suma de $B$ y $M$, en cambio, sí podemos sumar $A$ y $B$ ya que tienen el mismo orden. Esto es,
				\begin{equation*}
				A + B = \begin{bmatrix} -1&3&5 \\ 2&0&-1  \end{bmatrix}+
				\begin{bmatrix} 5&2&-3 \\ 0&-1&1  \end{bmatrix} =
				\begin{bmatrix} -1+5&3+2&5-3 \\ 2+0&0-1&-1+1  \end{bmatrix}=
				\begin{bmatrix} 4&5&2 \\ 2&-1&0  \end{bmatrix}
				\end{equation*}
				
				
				Dadas $A$, $B$ y $C$ matrices $m \times n$, podemos deducir fácilmente las siguientes propiedades de la suma de matrices de matrices:
				\begin{itemize}
					\item Conmutativa: $A + B = B + A$,
					\item Asociativa: $A + (B + C) = (A + B) + C$,
					\item Elemento neutro (la matriz nula): $A + 0 = 0 + A = A$,
					\item Elemento opuesto: existe una matriz $-A$ de orden $m \times n$ tal que  $A + (-A) = (-A) + A = 0$.
				\end{itemize}
				Debemos explicitar la matriz opuesta: si $A = [a_{ij}]$,  entonces $-A = [-a_{ij}]$. Usualmente denotaremos $A + (-B)$ como $A-B$ y $(-A)+B$ como $-A+B$. 
				
				La demostración de las propiedades anteriores se deduce de que las mismas propiedades valen coordenada a coordenada y se dejan a cargo del lector. 
			\end{subsection}		
			
			\begin{subsection}{Multiplicación de matrices} Sean $A=[a_{ij}]$ matriz $m \times n$ y 		 $B=[b_{ij}]$ matriz $n \times p$, entonces $C=[c_{ij}]$ matriz $m \times p$  es el \textit{producto} de $A$ y $B$, si 
				\begin{equation}\label{mtrx-mult}
				c_{ij} = a_{i1}b_{1j}+a_{i2}b_{2j}+\cdots+a_{in}b_{nj}= \sum_{k=1}^{n}a_{ik}b_{kj}.
				\end{equation}
				Es decir, los elementos que ocupan la posición $ij$ en la matriz producto, se obtienen sumando 	los productos que resultan de multiplicar los elementos de la fila $i$ en la primera matriz por los
				elementos de la columna $j$ de la segunda matriz. Al producto de $A$ por $B$ lo denotamos $AB$. 
				
				\vskip .2cm 
				
				Es muy importante recalcar que por la definición, se puede multiplicar una matriz $m \times n$ por una matriz $r \times p$, sólo si $n=r$ y en ese caso, la multiplicación resulta ser una matriz $m \times p$. 
				
				\vskip .2cm 
						
				Podemos visualizar la multiplicación así:
				\begin{align*}
					\left[
					\begin{array}{cccc}
						a_{11} & a_{12} & \cdots & a_{1n}\\ 
						a_{21} & a_{22} & \cdots & a_{2n}\\
						\vdots & \vdots & & \vdots\\
						\textcolor{red}{a_{i1}} & \textcolor{red}{a_{i2}} & \cdots & \textcolor{red}{a_{in}}\\  
						\vdots & \vdots & & \vdots\\
						a_{m1} & a_{m2} & \cdots & a_{mn}
					\end{array}
					\right] 
					\cdot
					\left[
					\begin{array}{ccc}
						\cdots & \textcolor{blue}{b_{1j}} & \cdots\\ 
						\cdots & \textcolor{blue}{b_{2j}} & \cdots\\
						& \textcolor{blue}{\vdots} & \\
						\cdots & \textcolor{blue}{b_{nj}} & \cdots 
					\end{array}
					\right]
					=
					\left[
					\begin{array}{ccc}
						&\vdots&\\
						\cdots&\sum_{k=1}^n\textcolor{red}{a_{ik}}\cdot
						\textcolor{blue}{b_{kj}}&\cdots\\
						&\vdots&
					\end{array}
					\right]
				\end{align*} 
				
	
				
				\vskip .2cm 
				
				
				\begin{obs}\label{mtrx-filasxcols}
					 Sean $A=[a_{ij}]$ matriz $m \times n$ y $B=[b_{ij}]$ matriz $n \times p$, entonces si 
					 multiplicamos la matriz que se forma con la fila $i$ de $A$ por la matriz que determina la columna $j$ de $B$, obtenemos el coeficiente $ij$ de $AB$.
					Esquemáticamente
					\begin{equation*}
					\begin{bmatrix} a_{i1}& a_{i2}& \cdots &a_{in}\end{bmatrix}
					\begin{bmatrix} b_{1j}\\ b_{2j}\\ \vdots \\b_{nj}\end{bmatrix} =  \sum_{k=1}^{n}a_{ik}b_{kj} = c_{ij}.
					\end{equation*}
					Por lo tanto diremos a veces, que el coeficiente $ij$ de la matriz $AB$ es la fila $i$ de $A$ por la columna $j$ de $B$. 
					
					El lector recordará el producto escalar definido en  el capítulo \ref{chap-vectores} y notará que el coeficiente $ij$ de $AB$ es el producto escalar de la fila $i$ de A por la columna $j$ de B, ambos pensados como vectores. 
				\end{obs}	
				
				\begin{ejemplo}
					Si 
					\begin{equation*}
						A = \begin{bmatrix}1&0\\-3&1\end{bmatrix}, \qquad B = \begin{bmatrix}5&-1&2\\15&4&8\end{bmatrix},
					\end{equation*}
					como $A$ es $2 \times 2$ y $B$ es $2 \times 3$, la matriz $AB$ será $2 \times 3$ y  aplicando la regla (\ref{mtrx-mult}), obtenemos:
					\begin{equation*}
						AB = \begin{bmatrix}1\times  5 + 0\times 15&1\times (-1) + 0\times 4&1\times 2 + 0\times 8
							\\-3\times 5 + 1\times 15&-3\times (-1) + 1\times 4&-3\times 2 + 1\times 8
						\end{bmatrix} =
						\begin{bmatrix} 5 &-1 &2 
							\\ 0 &7 &2
						\end{bmatrix}.
					\end{equation*}
					Observemos que, debido a nuestra definición, no es posible multiplicar $B$ por $A$, pues no está definido multiplicar una matriz $2 \times 3$ por una $2 \times 2$.
				\end{ejemplo}
				 	
				
				Hay casos, como veremos en el siguiente ejemplo, en los que se pueden calcular ambos productos aunque se obtienen resultados diferentes. Consideremos las siguientes matrices:
				\begin{equation*}
				A = \begin{bmatrix}2&1\\-3&1\end{bmatrix}, \qquad 
				B = \begin{bmatrix}1&3\\-1&1\end{bmatrix}
				\end{equation*}
				Entonces, por un lado,
				\begin{equation*}
				AB = \begin{bmatrix}2&1\\-3&1\end{bmatrix}
				\begin{bmatrix}1&3\\-1&1\end{bmatrix} =
				\begin{bmatrix}1&7\\-4&-8\end{bmatrix}, 
				\end{equation*}
				y por otro lado,
				\begin{equation*}
				BA = \begin{bmatrix}1&3\\-1&1\end{bmatrix} \begin{bmatrix}2&1\\-3&1\end{bmatrix}
				=\begin{bmatrix}-7&4\\-5&0\end{bmatrix}.
				\end{equation*}
				
				Según se pudo comprobar a través del ejemplo anterior, la multiplicación de matrices
				no cumple la propiedad conmutativa. Veamos algunas propiedades que sí cumple esta operación:
				\begin{itemize}
					\item Asociativa: 
					\begin{equation*}
					A (B C) = (A B) C, \quad\forall\, A \in M_{m \times n}, \;B \in M_{n \times p}, \;C  \in M_{p \times q},
					\end{equation*}
					
					\item Elemento neutro: si  $A$ es matriz $m \times n$,  entonces
					\begin{equation*}
					AI_{n} = A = I_{m}A,
					\end{equation*}
					\item Distributiva:
					\begin{equation*}
					A(B + C) = AB + AC,\qquad \forall\, A \in M_{m \times n}, \;B, C \in M_{n \times p},
					\end{equation*}
					y
					\begin{equation*}
					(A+ B)C = AC + BC,\qquad \forall\, A, B \in M_{m \times n}, \; C \in M_{n \times p}.
					\end{equation*}
				\end{itemize}	
				
				Como en el caso de la suma,  la demostración las propiedades anteriores se deja a cargo del lector. 
				
				En virtud de estas propiedades y de las anteriores de la suma de matrices, resulta que el conjunto $(M_n ,+,.)$  de las matrices cuadradas de orden $n$, respecto a las dos leyes de composición interna, ``+'' y ``·'', tiene estructura de anillo unitario no conmutativo (ver \href{ https://es.wikipedia.org/wiki/Anillo\_(matemática)}{ https://es.wikipedia.org/wiki/Anillo\_(matemática)}) . 
				
				Cuando las matrices son cuadradas podemos multiplicarlas por si mismas y  definimos,  de forma análoga a lo que ocurre en los productos de números, la potencia de una matriz: sea $A$ matriz $n \times n$, y sea $m \in \mathbb N$ entonces
				\begin{equation*}
				A^0 = I_{n}, \quad A^m = A^{m-1}A,		
				\end{equation*}
				es decir $A^m$ es multiplicar $A$ consigo mismo $m$-veces.  
				
				\begin{observacion}
					Un  caso especial de multiplicación es la multiplicación por matrices diagonales. Sea 
				\begin{equation*}
					D = \begin{bmatrix}
					d_1 & 0 & \cdots &0 \\
					0 & d_2 & \cdots &0 \\
					\vdots & \vdots & \ddots &\vdots \\
					0 & 0 & \cdots & d_n 
					\end{bmatrix}
				\end{equation*}
				matriz $n \times n$ diagonal con valor $d_i$  en la posición $ii$,  entonces si $A$ es matriz $n \times p$, tenemos que $DA$  es la matriz que en la fila $i$ tiene a la fila $i$ de $A$ multiplicada por $d_i$.  Es decir,
				\begin{equation*}
				\begin{bmatrix}
				d_1 & 0 & \cdots &0 \\
				0 & d_2 & \cdots &0 \\
				\vdots & \vdots & \ddots &\vdots \\
				0 & 0 & \cdots & d_n 
				\end{bmatrix}
				\begin{bmatrix}
				a_{11} & a_{12} & \cdots &a_{1p} \\
				a_{21} & a_{22} & \cdots &a_{2p}\\
				\vdots & \vdots & \ddots &\vdots \\
				a_{n1} & a_{n2} & \cdots & a_{np} 
				\end{bmatrix}
				=
				\begin{bmatrix}
				d_1a_{11} & d_1a_{12} & \cdots &d_1a_{1p} \\
				d_2a_{21} & d_2a_{22} & \cdots &d_2a_{2p}\\
				\vdots & \vdots & \ddots &\vdots \\
				d_n a_{n1} & d_n a_{n2} & \cdots & d_n a_{np} 
				\end{bmatrix}.
				\end{equation*}
				Esto es claro, pues el coeficiente $ij$  de $DA$ es la fila $i$ de $D$ por la columna $j$ de $A$, es decir
				$$
				[DA]_{ij} = 0.a_{1j}+\cdots + 0.a_{i-1,j}+d_i.a_{ij}+0.a_{i+1,j}+\cdots + 0.a_{nj} = d_ia_{ij}. 
				$$
				Observar que en el caso de que $D$ sea una matriz escalar (es decir $d_1=d_2=\cdots=d_n$), $DA$ es multiplicar por el mismo número todos los coeficientes de $A$. En particular, en este caso, si $A$  es $n \times n$,  $DA = AD$.
				 
				
				
				 Si $B$ es $m \times n$, ¿qué se obtiene haciendo $BD$? 
				 
				
				\end{observacion}
				
				\vskip .3cm
				
				Otras observaciones importantes: 
				
				\begin{itemize}
					\item multiplicar cualquier matriz por la matriz nula resulta la matriz nula,
					\item existen divisores de cero: en general, $AB = 0$ no implica que $A = 0$ o $B = 0$  o,  lo que es lo mismo, el producto de matrices no nulas puede resultar en una matriz nula. Por
					ejemplo,
					\begin{equation*}
					\begin{bmatrix}1&0\\2&0\end{bmatrix} \begin{bmatrix}0&0\\8&1\end{bmatrix} = \begin{bmatrix}0&0\\0&0\end{bmatrix}.
					\end{equation*}
					\item En general no se cumple la propiedad cancelativa: si $A\not=0$ y  $AB = AC$ no necesariamente se cumple que $B = C$. Por
					ejemplo,
					\begin{equation*}
					\begin{bmatrix}2&0\\4&0\end{bmatrix}=
					\begin{bmatrix}1&0\\2&0\end{bmatrix} \begin{bmatrix}2&0\\8&1\end{bmatrix} =
					\begin{bmatrix}1&0\\2&0\end{bmatrix} \begin{bmatrix}2&0\\5&3\end{bmatrix}
					\end{equation*}
					\item No se cumple la fórmula del binomio: 	sean $A, B$ matrices $n \times n$, entonces		
					\begin{align*}
					(A+B)^2 &= (A+B)(A+B) \\&= A(A+B) + B(A+B) \\&= AA + AB + BA + BB \\&= A^2 + AB + BA + B^2,
					\end{align*}
					y  esta última expresión puede no ser  igual a $A^2 + 2AB + B^2$ ya que el producto de matrices no es conmutativo (en general). 
				\end{itemize}
			\end{subsection}
		
		
		\begin{subsection}{Multiplicación de una matriz por un escalar} Otra operación importante es la multiplicación de una matriz por un elemento de $\K$: sea $A=[a_{ij}]$ matriz $m \times n$ y $c \in \K$,  entonces el producto de $c$ por $A$ es la matriz
			$$
			cA=[ca_{ij}].
			$$ 
		Por ejemplo, 
		$$
		2\begin{bmatrix*}[r]
		-1& 0& 3 & 4\\
		5& 1& -2 & 1\\
		3& 2& 1 & -3
		\end{bmatrix*} =
		\begin{bmatrix*}[r]
		-2& 0& 6& 8\\
		10& 2& -4 & 2\\
		6& 4& 2 & -6
		\end{bmatrix*}.
		$$
		
		Observar que multiplicar por $c$ una matriz $m \times n$,  es lo mismo que multiplicar por la matriz escalar $m \times m$ con los coeficientes de la diagonal iguales a $c$,  es decir
		\begin{equation}
			cA = \begin{bmatrix*}[r]
			c     &     0& \cdots & 0\\
			0     &     c& \cdots & 0\\
			\vdots&\vdots&  \ddots      &\vdots\\
			0     &    0  & \cdots & c
			\end{bmatrix*}
			 \begin{bmatrix}
			a_{11}&a_{12}& \cdots & & a_{1n}\\
			a_{21}&a_{22}& \cdots & &a_{2n}\\
			\vdots&\vdots&  & &\vdots\\
			a_{m1}&a_{m2}& \cdots & & a_{mn}
			\end{bmatrix} =
			 \begin{bmatrix}
			ca_{11}&ca_{12}& \cdots & & ca_{1n}\\
			ca_{21}&ca_{22}& \cdots & &ca_{2n}\\
			\vdots&\vdots&  & &\vdots\\
			ca_{m1}&ca_{m2}& \cdots & & ca_{mn}
			\end{bmatrix}
		\end{equation}
		
		Debido a esta observación y a las propiedades del producto de matrices, se cumple lo siguiente:
			\begin{align*}
			 c(A B) = (cA) B,\qquad &\forall\, c \in \K,  A \in M_{m \times n}, \;B \in M_{n \times p}, \\
			 (cd)A = c(dA),  \qquad &\forall\, c,d \in \K,  A \in M_{m \times n},, \\
			1.A = A ,\qquad&\forall\, c \in \K, A \in M_{m \times n}\\
			c(A + B) = cA + cB,\qquad& \forall\, c \in \K, A,B \in M_{m \times n}, \\
			(c+ d)A = cA + dA,\qquad& \forall\, c,d \in \K, \; A \in M_{m \times n}.
			\end{align*}
			
			Si $A$ es $n \times n$,  entonces $DA = AD$ cuando $D$  es una matriz escalar. Por lo tanto
			\begin{align*}
			c(A B) = (cA) B = A(cB),\qquad &\forall\, c \in \K,  A \in M_{n \times n}, \;B \in M_{n \times n}.
			\end{align*} 

		\end{subsection}
			
			
		\end{section}
		
		\begin{section}{Matrices elementales}
			Veremos ahora la relación entre el álgebra de matrices y la solución de sistemas de ecuaciones lineales. 
			
			Primero recordemos que dado un sistema de $m$  ecuaciones lineales con $n$ incógnitas
			\begin{equation}\label{sist-eq-gen-3}
			\begin{matrix}
			a_{11}x_1& + &a_{12}x_2& + &\cdots& + &a_{1n}x_n &= &y_1\\
			\vdots&  &\vdots& &&  &\vdots \\
			a_{m1}x_1& + &a_{m2}x_2& + &\cdots& + &a_{mn}x_n &=&y_m
			\end{matrix}
			\end{equation}
			donde $y_1, \ldots,y_m$ y $a_{i,j}$ ($1 \le i \le m$, $1 \le j \le n$) son números en $\K$. Si denotamos
			\begin{equation*}
			A = \begin{bmatrix}
			a_{11}& a_{12}& \cdots &a_{1n} \\
			\vdots&\vdots  &  &\vdots \\
			a_{m1} &a_{m2}&\cdots &a_{mn}\end{bmatrix},\qquad
			X = \begin{bmatrix}
			x_1 \\ \vdots \\ x_n 
			\end{bmatrix},
			\qquad 
			Y = \begin{bmatrix}
			y_1 \\ \vdots \\ y_n
			\end{bmatrix},
			\end{equation*}
			entonces 
			\begin{equation*}
			AX = \begin{bmatrix}
			a_{11}& a_{12}& \cdots &a_{1n} \\
			\vdots&\vdots  &  &\vdots \\
			a_{m1} &a_{m2}&\cdots &a_{mn}\end{bmatrix} \begin{bmatrix} 
			x_1 \\ \vdots \\ x_n 
			\end{bmatrix}
			=
			\begin{bmatrix}
			a_{11}x_1+ a_{12}x_2+ \cdots +a_{1n}x_n \\
			\vdots \\
			a_{m1}x_1 +a_{m2}x_2+\cdots +a_{mn}x_n\end{bmatrix} =
			\begin{bmatrix}
			y_1 \\ \vdots \\ y_n
			\end{bmatrix} = Y
			\end{equation*}
			(producto de matrices). Es decir, la notación antes utilizada es consistente con el, ahora definido, producto de matrices.  
			
			\begin{definicion} Una matriz $m \times m$  se dice  \textit{elemental}\index{matriz!elemental} si fue obtenida por medio de una única operación elemental a partir de la matriz identidad $I_m$.
			\end{definicion}
			
			\begin{ejemplo} Veamos cuales son las matrices elementales  $2 \times 2$:
				\begin{enumerate}
					\item Si $c \not=0$, multiplicar por  $c$ la primera fila y multiplicar $c$ por la segunda fila son, respectivamente,
					\begin{equation*}
					\begin{bmatrix} c& 0\\ 0&1\end{bmatrix}\;\text{ y }\; \begin{bmatrix} 1& 0\\ 0&c\end{bmatrix},
					\end{equation*}
					\item si  $c \in \K$, sumar a la fila 2 la fila 1 multiplicada por $c$ o sumar a la fila 1 la fila 2 multiplicada por $c$ son, respectivamente,
					\begin{equation*}
					\begin{bmatrix} 1& 0\\ c&1\end{bmatrix}\;\text{ y }\; \begin{bmatrix} 1& c\\ 0&1\end{bmatrix}.
					\end{equation*}
					\item Finalmente, intercambiando la fila 1 por la fila 2 obtenemos la matriz
					\begin{equation*}
					\begin{bmatrix} 0& 1\\ 1&0\end{bmatrix}.
					\end{equation*}
				\end{enumerate}
			\end{ejemplo}
			
			\vskip .5cm
			
			En el caso de matrices $m \times m$ tampoco es difícil encontrar las matrices elementales:
			\begin{enumerate}
				\item Si $c \not=0$, multiplicar por  $c$ la fila $k$ de la matriz identidad, resulta en la matriz elemental que tiene todos 1's en la diagonal, excepto en la posición $k,k$ donde vale $c$,  es decir si $e(I_m) = [a_{ij}]$,  entonces
				\begin{equation}\label{elem-tipo-1}
				a_{ij} = \left\{ 
				\begin{matrix*}[l]
				1 &\text{si $i=j$ e $i\ne k$,}\\
				c &\text{si $i=j=k$,} \\
				0 \quad&\text{si $i \ne j$.}
				\end{matrix*}\right.
				\end{equation}
				Gráficamente,
				\begin{align*}
				&\begin{matrix}
				{}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{k}{\downarrow}&{}^{}&{}^{}&{}^{}
				\end{matrix} \\
				\begin{matrix}
				{}^{}\\
				{}^{}\\
				\overset{k}{\to}\\
				{}^{}\\
				{}^{}
				\end{matrix}
				&\begin{bmatrix}
				1 & 0 &  &\cdots & 0  \\
				\vdots  & \ddots  & & & \vdots \\
				0 & \cdots &c &\cdots &0 \\
				\vdots  &   & &\ddots & \vdots \\
				0  & \cdots  & &\cdots & 1
				\end{bmatrix}
				\end{align*}
				\item si  $c \in \K$, sumar a la fila $r$  la fila $s$ multiplicada por $c$, resulta en la matriz elemental que tiene todos 1's en la diagonal, y todos los demás coeficientes son 0,  excepto en la fila  $r$ y columna $s$ donde  vale $c$,  es decir si $e(I_m) = [a_{ij}]$,  entonces
				\begin{equation}\label{elem-tipo-2}
				a_{ij} = \left\{ 
				\begin{matrix*}[l]
				1 &\text{si $i=j$}\\
				c &\text{si $i=r$, $j=s$,} \\
				0 \quad&\text{otro caso.}
				\end{matrix*}\right.
				\end{equation}
				Gráficamente, 
				\begin{align*}
				&\begin{matrix}
				{}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{r}{\downarrow}&{}^{}&{}^{}&{}^{}{}^{}\overset{s}{\downarrow}&{}^{}
				\end{matrix} \\
				\begin{matrix}
				{}^{}\\{}^{}\\
				\overset{r}{\to}\\
				{}^{}\\
				{}^{}\\
				{}^{}
				\end{matrix}
				&\begin{bmatrix}
				1 & 0 &  &\cdots &&& 0  \\
				\vdots  & \ddots  & & &&& \vdots \\
				0 & \cdots &1 &\cdots&c&\cdots &0 \\
				\vdots  &   & &\ddots &&& \vdots \\
				\vdots  &   & & &\ddots&& \vdots \\
				0  & \cdots  & &\cdots &&& 1
				\end{bmatrix}
				\end{align*}
				
				
				\item Finalmente, intercambiar la fila $r$ por la fila $s$ resulta ser
				\begin{equation}\label{elem-tipo-3}
				a_{ij} = \left\{ 
				\begin{matrix*}[l]
				1 &\text{si ($i=j$, $i \ne r$, $i \ne s$) o ($i=r$, $j=s$) o ($i=s$, $j=r$) }\\
				0 \quad&\text{otro caso.}
				\end{matrix*}\right.
				\end{equation}
					Gráficamente, 
				\begin{align*}
				&\begin{matrix}
				{}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{r}{\downarrow}&{}^{}&{}^{}&{}^{}{}^{}\overset{s}{\downarrow}&{}^{}
				\end{matrix} \\
				\begin{matrix}
				{}^{}\\{}^{}\\
				\overset{r}{\to}\\
				{}^{}\\
				\overset{s}{\to}\\{}^{}\\
				{}^{}
				\end{matrix}
				&\begin{bmatrix}
				1 & \cdots &  &\cdots &&\cdots& 0  \\
				\vdots  & \ddots  & & &&& \vdots \\
				0 & \cdots &0 &\cdots&1&\cdots &0 \\
				\vdots  &   & &\ddots &&& \vdots \\
				0  & \cdots  &1 &\cdots &0& \cdots& 0 \\
				\vdots  &   & & &&\ddots& \vdots \\
				0  & \cdots  & &\cdots &&\cdots& 1
				\end{bmatrix}
				\end{align*}
			\end{enumerate}
			
			Veamos ahora que, dada una matriz $A$,   hacer una operación elemental en $A$ es igual a multiplicar $A$ a izquierda por una matriz elemental. Más precisamente:
			
			\begin{teorema}\label{th-mrtx-elem}
				Sea $e$ una operación elemental por fila y sea $E$ la matriz elemental $E=e(I)$. Entonces $e(A) = EA$.
			\end{teorema}
			\begin{proof}
				Hagamos la prueba para matrices $2 \times 2$. La prueba en general es similar, pero requiere de un complicado manejo de índices.
				\begin{enumerate}
					\item[E1.] Sea  $c \in \K$, y  sea $e$ la operación elemental de a la fila 2 le sumarle  la fila 1 multiplicada por $c$. Entonces. $E:=e(I_2)$ resulta en la matriz elemental:
					\begin{equation*}
					E= \begin{bmatrix} c& 0\\ 0&1\end{bmatrix}.
					\end{equation*}
					Ahora bien,
					\begin{align*}
					E A&=\begin{bmatrix} c& 0\\ 0&1\end{bmatrix}
					\begin{bmatrix} a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix} \\
					&\qquad= 
					\begin{bmatrix} 
					c\,.\,a_{11} + 0 \,.\,a_{21}&c\,.\,a_{12}+0\,.\,a_{22}\\
					0\,.\,a_{11} + 1 \,.\,a_{21}&0\,.\,a_{12}+1\,.\,a_{22}\end{bmatrix} 
					=
					\begin{bmatrix} 
					c\,.\,a_{11}&c\,.\,a_{12}\\
					a_{21}&a_{22}\end{bmatrix} = e(A).
					\end{align*}
					De forma análoga se demuestra en el caso que la operación elemental sea  multiplicar la segunda fila por $c$.
					
					\item[E2.] Sea  $c \in \K$, y  sea $e$ la operación elemental de a la fila 2 le sumarle  la fila 1 multiplicada por $c$. Entonces. $E:=e(I_2)$ resulta en la matriz elemental: 
					\begin{equation*}
					E=\begin{bmatrix} 1& 0\\ c&1\end{bmatrix}.
					\end{equation*}
					Luego 
					\begin{equation*}
					EA= \begin{bmatrix} 1& 0\\ c&1\end{bmatrix}	\begin{bmatrix} a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix} = 
					\begin{bmatrix} 
					a_{11} &a_{12}\\
					c\,.\,a_{11} + a_{21}&c\,.\,a_{12}+a_{22}\end{bmatrix} = e(A).
					\end{equation*}
					La demostración es análoga si la operación elemental es sumar a la fila 1 la fila 2 multiplicada por $c$.
					
					\item[E3.]  Finalmente, sea $e$ la operación elemental que intercambia la fila 1 por la fila 2. Entonces,  $E:=e(I_2)$ es la matriz 
					\begin{equation*}
					E=\begin{bmatrix} 0& 1\\ 1&0\end{bmatrix}.
					\end{equation*}
					Luego
					\begin{equation*}
					EA= \begin{bmatrix} 0& 1\\ 1&0\end{bmatrix}	\begin{bmatrix} a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix} = 
					\begin{bmatrix}
					a_{21} &a_{22}\\
					a_{11} &a_{12}\end{bmatrix} = e(A).
					\end{equation*}
				\end{enumerate}
				
	
			\end{proof}
			
			
			
			
			\begin{corolario}\label{coro-mrtx-elem}
				Sean $A$ y $B$ matrices $m \times n$. Entonces $B$ equivalente por filas a $A$ si y sólo si $B=PA$ donde $P$ es  producto de matrices elementales. Más aún, si $B = e_k(e_{k-1}(\cdots(e_1(A))\cdots))$ con $e_1,e_2,\ldots,e_k$ operaciones elementales de fila y $E_i=e_i(I)$ para $i=1,\ldots,k$,  entonces $B =  E_kE_{k-1}\cdots E_1A$.
			\end{corolario}
			\begin{proof} 
				\
				
				($\Rightarrow$) Si $B$ equivalente por filas a $A$  existen operaciones elementales $e_1,\ldots,e_k$ tal que $B = e_k(e_{k-1}(\cdots(e_1(A))\cdots))$, más formalmente
				\begin{equation*}
				\text{si } A_1 = e_1(A)\text{ y } A_i = e_i(A_{i-1})\text{ para }i=2,\ldots,k\text{, entonces } e_k(A_{k-1})= B.
				\end{equation*}
				Sea $E_i = e_i(I_m)$, entonces, por el teorema anterior $A_1= E_1A$ y $A_i = E_iA_{i-1}$ ($i=2,\ldots,k$). Por  lo tanto $B=E_kA_{k-1}$, en otras palabras $B =  E_kE_{k-1}\cdots E_1A$, luego $P = E_kE_{k-1}\cdots E_1$.
				
				($\Leftarrow$) Si $B= PA$, con  $P = E_kE_{k-1}\cdots E_1$ donde $E_i=e_i(I_m)$ es una matriz elemental,  entonces
				$$
				B = PA = E_kE_{k-1}\cdots E_1A \stackrel{\text{Teor. \ref{th-mrtx-elem}}}{=\joinrel=}  e_k(e_{k-1}(\cdots(e_1(A))\cdots)). 
				$$
				Por lo tanto, $B$ es equivalente por filas a $A$.
			\end{proof} 
			
		\end{section}
		
		\begin{section}{Matrices invertibles}
			\begin{definicion} Sea $A$ una  matriz $n \times n$ con coeficientes en $\K$. Una matriz $B \in M_{n\times n}(\K)$  es \textit{inversa  de $A$}\index{matriz!inversa} si $BA=AB=I_n$. En  ese caso,  diremos que  $A$ es \textit{invertible}.\index{matriz!invertible}
			\end{definicion}
			
			\begin{ejemplo}
				La matriz $\begin{bmatrix} 2&-1\\0&1\end{bmatrix}$ tiene inversa 
				$\begin{bmatrix} \frac12&\frac12\\0&1\end{bmatrix}$ pues es fácil comprobar que 
				\begin{equation*}
				\begin{bmatrix} 2&-1\\0&1\end{bmatrix}
				\begin{bmatrix} \frac12&\frac12\\0&1\end{bmatrix} =
				\begin{bmatrix} 1&0\\0&1\end{bmatrix}\quad\text{ y } \quad
				\begin{bmatrix} \frac12&\frac12\\0&1\end{bmatrix} 
				\begin{bmatrix} 2&-1\\0&1\end{bmatrix}=
				\begin{bmatrix} 1&0\\0&1\end{bmatrix}.
				\end{equation*}
			\end{ejemplo}
			
			\vskip .5cm
			
			\begin{proposicion}
				Sea $A  \in M_{n\times n}(\K)$, 
				\begin{enumerate}
					\item	sean $B, C \in M_{n \times n}(\K)$ tales que $BA =I_n$ y $AC = I_n$, entonces $B=C$;
					\item  si $A$ invertible la inversa es única.
				\end{enumerate}
			\end{proposicion}
			\begin{proof} (1)
				\begin{equation*}
				B = BI_n = B(AC) = (BA)C = I_nC = C.
				\end{equation*}
				
				(2) Sean $B$ y $C$ inversas de $A$, es decir $BA=AB=I_n$ y  $CA=AC=I_n$. En particular, $BA=I_n$ y $AC= I_n$, luego, por (1), $B=C$.  
			\end{proof}
			
			
			
			\begin{definicion}
				Sea $A\in M_{n\times n}(\K)$ invertible. A la única matriz inversa de $A$ la llamamos \textit{la matriz inversa de $A$} y la denotamos $A^{-1}$.
			\end{definicion}
			
			Veremos más adelante que si una matriz $n \times n$ admite una inversa a izquierda,  es decir si existe $B$ tal que $BA=I_n$, entonces la matriz es invertible. Lo mismo vale si $A$  admite inversa a derecha.
			
			\begin{ejemplo}
				Sea $A$ la matriz 
				\begin{equation*}
				\begin{bmatrix} 2&1&-2\\ 1&1&-2\\ -1&0&1
				\end{bmatrix}.
				\end{equation*}
				Entonces,  $A$ es invertible y su inversa es
				\begin{equation*}
				A^{-1} = \begin{bmatrix} 1&-1&0\\ 1&0&2\\ 1&-1&1
				\end{bmatrix}.
				\end{equation*}
				Esto se resuelve comprobando que $AA^{-1}=I_3$ (por lo dicho más arriba es innecesario comprobar que $A^{-1}A=I_3$).
			\end{ejemplo} 
			
			\begin{observacion}
				No toda matriz tiene inversa, por ejemplo  la  matriz nula (cuyos coeficientes son todos iguales a $0$) no tiene inversa pues $0\,.\, A= 0 \not= I$.  También existen matrices no nulas no invertibles,  por ejemplo la matriz 
				\begin{equation*}
				A = \begin{bmatrix} 2&1\\ 0&0\end{bmatrix}
				\end{equation*}
				no tiene inversa.
				Si  multiplicamos a $A$ por una cualquier matriz  $B =[b_{ij}]$ obtenemos
				\begin{equation*}
				AB = \begin{bmatrix} 2&1\\ 0&0\end{bmatrix}
				\begin{bmatrix} b_{11}&b_{12}\\ b_{21}&b_{22}\end{bmatrix} =
				\begin{bmatrix} 2b_{11}+b_{21}&2b_{12}+b_{22}\\0 &0\end{bmatrix}.
				\end{equation*}
				Luego $AB$, al tener una fila identicamente nula, no puede ser nunca la identidad. 
			\end{observacion}
			
			
			
			\begin{teorema}\label{th-prod-inv-impl-inv}
				Sean $A$ y $B$ matrices $n \times n$ con coeficientes en $\K$. Entonces
				\begin{enumerate}
					\item \label{inv-itm1} si $A$ invertible,  entonces $A^{-1}$  es invertible y su inversa es $A$,  es decir $(A^{-1})^{-1}=A$;
					\item \label{inv-itm2} si $A$ y $B$ son invertibles, entonces $AB$ es invertible y $(AB)^{-1} = B^{-1}A^{-1}$.
				\end{enumerate}
			\end{teorema}
				\begin{proof}
					(\ref{inv-itm1}) La inversa a izquierda de $A^{-1}$ es $A$, pues $AA^{-1}=I_n$. Análogamente, la inversa a derecha de $A^{-1}$ es $A$, pues $A^{-1}A = I_n$. Concluyendo: $A$  es la inversa de $A^{-1}$.
					
					(\ref{inv-itm2}) Simplemente debemos comprobar que $B^{-1}A^{-1}$ es inversa a izquierda y derecha de $AB$:
					\begin{equation*}
					(B^{-1}A^{-1})AB = B^{-1}(A^{-1}A)B = B^{-1}I_nB =B^{-1}B = I_n,
					\end{equation*}
					y,  análogamente,
					\begin{equation*}
					AB(B^{-1}A^{-1}) = A(BB^{-1})A^{-1} = AI_nA^{-1} =AA^{-1} = I_n.
					\end{equation*} 
				\end{proof}
			
			
			\begin{observacion}
				Si $A_1,\ldots,A_k$  son invertibles,  entonces $A_1\ldots A_k$ es invertible y su inversa es  $$(A_1\ldots A_k)^{-1} = A_k^{-1}\ldots A_1^{-1} .$$
				El resultado es una generalización del punto (2) del teorema anterior y su demostración se hace por inducción en $k$ (usando (2)  del teorema anterior). Se deja como ejercicio al lector. 
			\end{observacion}
			
			\begin{observacion}
				La suma de matrices invertibles no necesariamente es invertible, por ejemplo $A+ (-A)= 0$ que no es invertible. 
			\end{observacion}	
			
			
			\begin{teorema}\label{th-elmental-impl-invertible}
				Una matriz elemental es invertible.
			\end{teorema}
			\begin{proof}
				Sea $E$ la matriz elemental que se obtiene a partir de $I_n$ por la operación elemental $e$. Se $e_1$ la operación elemental inversa (teorema \ref{op-elem}) y $E_1 = e_1(I_n)$. Entonces 
				\begin{align*}
				EE_1 &= e(e_1(I_n)) = I_n \\
				E_1E &= e_1(e(I_n)) = I_n.
				\end{align*}
				Luego  $E_1 = E^{-1}$. 
			\end{proof}	
			
			
			\begin{ejemplo}
				Es fácil encontrar explícitamente la matriz inversa de una matríz elemental, por ejemplo, en el caso $2 \times 2$ tenemos:
				\begin{enumerate}
					\item Si $c \not=0$,
					\begin{equation*}
					\begin{bmatrix} c& 0\\ 0&1\end{bmatrix}^{-1}=\begin{bmatrix} 1/c& 0\\ 0&1\end{bmatrix}
					\;\text{ y }\; \begin{bmatrix} 1& 0\\ 0&c\end{bmatrix}^{-1}=\begin{bmatrix} 1& 0\\ 0&1/c\end{bmatrix},
					\end{equation*}
					\item si  $c \in \K$, ,
					\begin{equation*}
					\begin{bmatrix} 1& 0\\ c&1\end{bmatrix}^{-1}=\begin{bmatrix} 1& 0\\ -c&1\end{bmatrix}
					\;\text{ y }\; \begin{bmatrix} 1& c\\ 0&1\end{bmatrix}^{-1}=\begin{bmatrix} 1& -c\\ 0&1\end{bmatrix}.
					\end{equation*}
					\item Finalmente, 
					\begin{equation*}
					\begin{bmatrix} 0& 1\\ 1&0\end{bmatrix} ^{-1}= 	\begin{bmatrix} 0& 1\\ 1&0\end{bmatrix}.
					\end{equation*}
				\end{enumerate}
			En  el caso general tenemos:
			
			(1) 
			\begin{align*}
			&\begin{matrix}
			{}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{k}{\downarrow}&{}^{}&{}^{}&{}^{}
			\end{matrix} \\
			\text{La inversa de \quad}\begin{matrix}
			{}^{}\\
			{}^{}\\
			\overset{k}{\to}\\
			{}^{}\\
			{}^{}
			\end{matrix}
			&\begin{bmatrix}
			1 & 0 &  &\cdots & 0  \\
			\vdots  & \ddots  & & & \vdots \\
			0 & \cdots &c &\cdots &0 \\
			\vdots  &   & &\ddots & \vdots \\
			0  & \cdots  & &\cdots & 1
			\end{bmatrix}
			\text{\quad es \quad}
			\begin{bmatrix}
			1 & 0 &  &\cdots & 0  \\
			\vdots  & \ddots  & & & \vdots \\
			0 & \cdots &1/c &\cdots &0 \\
			\vdots  &   & &\ddots & \vdots \\
			0  & \cdots  & &\cdots & 1
			\end{bmatrix}
			\end{align*}
			
			(2)
			\begin{align*}
			&\begin{matrix}
			{}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{r}{\downarrow}&{}^{}&{}^{}&{}^{}{}^{}\overset{s}{\downarrow}&{}^{}
			\end{matrix} \\
			\text{La inversa de \quad}\begin{matrix}
			{}^{}\\{}^{}\\
			\overset{r}{\to}\\
			{}^{}\\
			{}^{}\\
			{}^{}
			\end{matrix}
			&\begin{bmatrix}
			1 & 0 &  &\cdots &&& 0  \\
			\vdots  & \ddots  & & &&& \vdots \\
			0 & \cdots &1 &\cdots&c&\cdots &0 \\
			\vdots  &   & &\ddots &&& \vdots \\
			\vdots  &   & & &\ddots&& \vdots \\
			0  & \cdots  & &\cdots &&& 1
			\end{bmatrix}
			\text{\quad es \quad}
			\begin{bmatrix}
			1 & 0 &  &\cdots &&& 0  \\
			\vdots  & \ddots  & & &&& \vdots \\
			0 & \cdots &1 &\cdots&-c&\cdots &0 \\
			\vdots  &   & &\ddots &&& \vdots \\
			\vdots  &   & & &\ddots&& \vdots \\
			0  & \cdots  & &\cdots &&& 1
			\end{bmatrix}
			\end{align*}
			
			(3)
			
			\begin{align*}
			&\begin{matrix}
			{}^{}&{}^{}&{}^{}&{}^{}&{}^{}&\overset{r}{\downarrow}&{}^{}&{}^{}&{}^{}{}^{}\overset{s}{\downarrow}&{}^{}
			\end{matrix} \\
			\text{La inversa de \quad}\begin{matrix}
			{}^{}\\{}^{}\\
			\overset{r}{\to}\\
			{}^{}\\
			\overset{s}{\to}\\{}^{}\\
			{}^{}
			\end{matrix}
			&\begin{bmatrix}
			1 & \cdots &  &\cdots &&\cdots& 0  \\
			\vdots  & \ddots  & & &&& \vdots \\
			0 & \cdots &0 &\cdots&1&\cdots &0 \\
			\vdots  &   & &\ddots &&& \vdots \\
			0  & \cdots  &1 &\cdots &0& \cdots& 0 \\
			\vdots  &   & & &&\ddots& \vdots \\
			0  & \cdots  & &\cdots &&\cdots& 1
			\end{bmatrix}
			\text{\quad es la misma matriz.\quad}
			\end{align*}
		
						
		
	
\end{ejemplo}
		
		
			
			
			\begin{teorema}\label{mtrx-inv-equiv} Sea $A$ matriz $n \times n$ con coeficientes en $\K$. Las siguientes afirmaciones son equivalentes
				\begin{enumerate}
					\item[\textit{i})] $A$ es invertible,
					\item[\textit{ii})] $A$  es equivalente por filas a $I_n$, 
					\item[\textit{iii})] $A$ es producto de matrices elementales.
				\end{enumerate}
			\end{teorema}
			\begin{proof}
				\

				
				\textit{i}) $\Rightarrow$ \textit{ii})\; Sea $R$ la matriz escalón reducida por fila equivalente por filas a $A$. Entonces,  existen $E_1,\ldots,E_k$ matrices elementales tal que $E_1,\ldots,E_kA = R$. Como las matrices elementales son invertibles, el producto de matrices elementales es invertible, luego  $E_1,\ldots,E_k$ es invertible y por lo tanto $R=E_1,\ldots,E_kA$ es invertible. 
				
				Recordemos que las matrices escalón reducidas por fila si tienen filas nulas, ellas se encuentran al final.  Ahora bien,  si la última fila de $R$ es nula entonces,  $RB$ tiene la última fila nula también y por lo tanto no puede ser igual a la identidad, es decir, en ese caso $R$ no es invertible, lo cual produce un absurdo. Concluyendo: la última fila (la fila $n$) de $R$ no es nula y como es MERF, $R$ no tiene filas nulas. Por lo tanto $R=I_n$ (lema \ref{mtrx-merf-id}) y,  entonces, $A$ es equivalente por filas a $I_n$. 
				
				\textit{ii}) $\Rightarrow$ \textit{iii})\; Como $A$  es equivalente por filas a $I_n$, al ser la equivalencia por filas una relación de equivalencia,  tenemos que $I_n$ es equivalente por filas a $A$, es decir  existen $E_1,\ldots,E_k$ matrices elementales, tales que $E_1E_2,\ldots,E_kI_n = A$. Por lo tanto, $A =E_1E_2,\ldots,E_k$ producto de matrices elementales.
				
				\textit{iii}) $\Rightarrow$ \textit{i}) \; Sea $A = E_1E_2,\ldots,E_k$ donde $E_i$  es una matriz elemental ($i=1,\ldots,k$). Como cada $E_i$ es invertible,  el producto de ellos es invertible,  por lo tanto $A$ es invertible.
			\end{proof}	
			
			\begin{corolario}
				Sean $A$ y $B$ matrices $m \times n$. Entonces,   $B$ es equivalente por filas a $A$ si y sólo si existe matriz invertible $P$ de orden $m \times m$ tal que $B =PA$ . 
			\end{corolario}
			\begin{proof}
				
				\
				
				($\Rightarrow$) $B$ es equivalente por filas a $A$,  luego existe $P$ matriz producto de matrices elementales tal que $B =PA$. Como cada matriz elemental es invertible (teorema \ref{th-elmental-impl-invertible}) y el producto de matrices invertibles es invertible (teorema  \ref{th-prod-inv-impl-inv}(\ref{inv-itm2})), se deduce que $P$ es invertible. 
				
				($\Leftarrow$) Sea  $P$  matriz invertible tal que $B =PA$. Como $P$ es invertible, por el teorema anterior, $P$ es producto de matrices elementales, luego $B =PA$ es equivalente por filas a $A$.
			\end{proof}	
			
			\begin{corolario}\label{mtrx-inv-gauss}
				Sea $A$ matriz $n \times n$. Sean $e_1,\ldots,e_k$ las operaciones elementales por filas que reducen a $A$  a una MERF y esta MERF es la identidad,  es decir $e_1(e_{2}(\cdots(e_k(A))\cdots)) =I_n$. Entonces, $A$ invertible y  las mismas operaciones elementales aplicadas a $I_n$ nos llevan a $A^{-1}$,  es decir $e_1(e_{2}(\cdots(e_k(I_n))\cdots)) =A^{-1}$.
			\end{corolario}
			\begin{proof} Por el teorema anterior, al ser $A$ equivalente por filas a la identidad, $A$ es invertible.  
				Sean las matrices elementales  $E_i = e_i(I_n)$ para $i=1,\ldots,k$,  entonces (ver corolario \ref{coro-mrtx-elem}) $E_1E_2\ldots E_kA = I_n$, por lo tanto, multiplicando por $A^{-1}$ a derecha en ambos miembros,    
				\begin{align*}
				E_1E_2\ldots E_kA A^{-1}&= I_nA^{-1} \quad \Leftrightarrow \\
				E_1E_2\ldots E_kI_n&= A^{-1} \quad \Leftrightarrow \\
				e_1(e_{2}(\cdots(e_k(I_n))\cdots)) &=A^{-1}.
				\end{align*}
			\end{proof}
			
			Este último corolario nos provee un método sencillo para calcular la inversa de una matriz $A$ (invertible). Primero,  encontramos $R = I_n$ la MERF  equivalente por filas a $A$, luego, aplicando la mismas operaciones elementales a $I_n$, obtenemos la inversa de $A$. Para facilitar el cálculo es  conveniente comenzar con $A$ e $I_n$ e ir aplicando paralelamente las operaciones elementales por fila. Veamos un ejemplo.

			\begin{ejemplo}
				Calculemos la inversa (si tiene) de 
				\begin{equation*}
				A=\begin{bmatrix}2&-1\\1&3 \end{bmatrix}.
				\end{equation*}
			\end{ejemplo}
			\begin{proof}[Solución] Por lo que ya hemos demostrado 1) si $A$ tiene inversa es reducible por filas a la identidad, 2) las operaciones que llevan a $A$ a la identidad, llevan también la identidad  a $A^{-1}$. Luego  trataremos de reducir por filas a $A$ y todas las operaciones elementales las haremos en paralelo partiendo de la matriz identidad:
				%$$
				%\left[\begin{array}{cccc|c}
				%1 & 0 & 3 & -1 & 0 \\
				%0 & 1 & 1 & -1 & 0 \\
				%0 & 0 & 0 & 0 & 0 \\
				%\end{array}\right]
				%$$
				\begin{align*}
				[A|I] &= \left[\begin{array}{cc|cc}2&-1 &  1&0\\1&3& 0&1\end{array}\right] 
				\stackrel{F_1\leftrightarrow F_2}{\longrightarrow} 
				\left[\begin{array}{cc|cc}1&3& 0&1\\2&-1 &  1&0 \end{array}\right]
				\stackrel{F_2-2 F_1}{\longrightarrow}
				\left[\begin{array}{cc|cc}1&3& 0&1\\0&-7 &  1&-2 \end{array}\right]
				\stackrel{F_2/(-7)}{\longrightarrow}\\
				&\quad\longrightarrow 
				\left[\begin{array}{cc|cc}1&3& 0&1\\0&1 &  -\frac17&\frac27\end{array}\right]
				\stackrel{F_1-3 F_2}{\longrightarrow}
				\left[\begin{array}{cc|cc}1&0&  \frac37&\frac17\\0&1 &  -\frac17&\frac27 \end{array}\right].
				\end{align*}
				Luego, como $A$ se reduce por filas a la identidad, $A$ es invertible y su inversa es  
				\begin{equation*}
				A^{-1}=\begin{bmatrix}\frac37&\frac17\\-\frac17&\frac27 \end{bmatrix}.
				\end{equation*}
				El lector desconfiado  podrá comprobar, haciendo el producto de matrices, que $AA^{-1} = A^{-1}A=I_2$.
			\end{proof}
			
			
			\begin{teorema}\label{mtrx-inv-equiv2} 
				Sea $A$ matriz $n \times n$ con coeficientes en $\K$. Entonces,  las siguientes afirmaciones son equivalentes. 
				\begin{enumerate}
					\item[\textit{i})] $A$ es invertible.
					\item[\textit{ii})] El sistema $AX=Y$ tiene una única solución para toda matriz $Y$ de orden $n \times 1$. 
					\item[\textit{iii})] El sistema homogéneo $AX=0$ tiene una única solución trivial.
				\end{enumerate}
			\end{teorema}
			\begin{proof}
				
				\
				
				\textit{i}) $\Rightarrow$  \textit{ii}) Sea $X_0$ solución del sistema $AX=Y$, luego
				\begin{equation*}
				AX_0=Y  \quad \Rightarrow \quad  A^{-1}AX_0 = A^{-1}Y  \quad \Rightarrow \quad  X_0 = A^{-1}Y.
				\end{equation*}
				Es decir, $X_0$ es único (siempre igual  a $A^{-1}Y$).  
				
				
				\textit{ii}) $\Rightarrow$  \textit{iii}) Es trivial, tomando $Y =0$.
				
				
				
				\textit{iii}) $\Rightarrow$  \textit{i}) Sea $R$ la matriz escalón reducida por filas equivalente a $A$, es decir $R=PA$ con $P$ invertible y $R$ es MERF. Si $R$ tiene una fila nula, entonces por teorema \ref{inf-sol},  el sistema $AX =0$ tiene más de una solución, lo cual es absurdo.  Por lo tanto, $R$ no tiene filas nulas. Como es una matriz cuadrada y es MERF, tenemos que $R=I_n$. Luego $A$ es equivalente por filas a $I_n$ y por teorema \ref{mtrx-inv-equiv} se deduce que $A$ es invertible. 			
				
			\end{proof}
			
			\begin{corolario}
				Sea $A$ una matriz $n \times n$ con coeficientes  en $\K$. Si $A$ tiene inversa a izquierda,  es decir si existe $B$ matriz $n \times n$ tal que $BA=I_n$,   entonces $A$ es invertible.  Lo mismo vale si $A$ tiene inversa a derecha. 
			\end{corolario}	
			\begin{proof}
				Supongamos que  $A$ tiene inversa a izquierda y  que $B$ sea la inversa a izquierda,  es decir $BA=I_n$. El sistema $AX=0$ tiene una única solución, pues $AX_0=0 \Rightarrow BAX_0=B0 \Rightarrow X_0=0$. Luego, $A$  es invertible (y su inversa es $B$). 
				
				Supongamos que  $A$ tiene inversa a derecha y  que $C$ sea la inversa a derecha, es decir $AC=I$. Por lo demostrado más arriba, $C$ es invertible y su inversa es $A$, es decir $AC=I$ y $CA=I$, luego $A$  es invertible. 
			\end{proof}	
			
			
			
			Terminaremos la  sección calculando algunas matrices inversas usando el corolario  \ref{mtrx-inv-gauss}. 
			
			\begin{ejemplo}
				Calcular la inversa (si tiene) de la matriz $A=\begin{bmatrix}
				1&-1&2\\ 3&2&4\\ 0&1&-2
				\end{bmatrix}$. 
			\end{ejemplo}
			\begin{proof}[Solución]
				%\left[\begin{array}{cc|cc}2&-1 &  1&0\\1&3& 0&1\end{array}\right] 
				\begin{align*} 
				&\left[\begin{array}{rrr|rrr}	1&-1&2&1&0&0\\ 3&2&4&0&1&0\\ 0&1&-2&0&0&1 \end{array}\right]
				\stackrel{F_2-3 F_1}{\longrightarrow}
				\left[\begin{array}{rrr|rrr}	1&-1&2
				&1&0&0\\ 0&5&-2&-3&1&0\\ 0&1&-2&0&0&1 \end{array}\right]
				\stackrel{F_2\leftrightarrow F_3}{\longrightarrow} \\
				&\longrightarrow 
				\left[\begin{array}{rrr|rrr}	1&-1&2&1&0&0\\ 0&1&-2&0&0&1 \\ 0&5&-2&-3&1&0 \end{array}\right]
				\stackrel{F_1 + F_2}{\longrightarrow}
				\left[\begin{array}{rrr|rrr}	1&0&0&1&0&1\\ 0&1&-2&0&0&1 \\ 0&5&-2&-3&1&0 \end{array}\right]
				\stackrel{F_3-5F_2}{\longrightarrow} \\
				&\longrightarrow
				\left[\begin{array}{rrr|rrr}	1&0&0&1&0&1\\ 0&1&-2&0&0&1 \\ 0&0&8&-3&1&-5 \end{array}\right]
				\stackrel{F_3/8}{\longrightarrow}
				\left[\begin{array}{rrr|rrr}	1&0&0&1&0&1\\ 0&1&-2&0&0&1 \\ 0&0&1&-\frac38&\frac18&-\frac58 \end{array}\right]
				\stackrel{F_2+2F_3}{\longrightarrow} \\
				&\longrightarrow
				\left[\begin{array}{rrr|rrr}	1&0&0&1&0&1\\ 0&1&0&-\frac34&\frac14&-\frac14 \\ 0&0&1&-\frac38&\frac18&-\frac58 \end{array}\right].
				\end{align*}
				
				Por lo tanto 
				\begin{equation*}
				A^{-1} = 	\begin{bmatrix*}[r]	1&0&1\\ -\frac34&\frac14&-\frac14 \\ -\frac38&\frac18&-\frac58 \end{bmatrix*}
				\end{equation*}
			\end{proof}
			
			\begin{ejemplo}\label{inv-2x2-0}
				Dados $a,b,c,d \in \mathbb R$, determinar cuando la matriz $A = \begin{bmatrix*} a&b\\c&d\end{bmatrix*}$  es invertible y en ese caso,  cual es su inversa. 
			\end{ejemplo}
			\begin{proof}[Solución] Para poder aplicar el método de Gauss, debemos ir haciendo casos. 
				
				1) Supongamos que  $a\not=0$, entonces 
				\begin{equation*}
				\begin{bmatrix}a&b\\c&d\end{bmatrix} \stackrel{F_1/a}{\longrightarrow}
				\begin{bmatrix}1&\dfrac{b}{a}\\[6pt]c&d\end{bmatrix} \stackrel{F_2 -cF_1}{\longrightarrow}
				\begin{bmatrix}1&\dfrac{b}{a}\\[6pt]0&d- c\dfrac{b}{a}\end{bmatrix} =
				\begin{bmatrix}1&\dfrac{b}{a}\\[6pt]0&\dfrac{ad-bc}{a}\end{bmatrix}
				\end{equation*}   
				Si $ad-bc=0$,  entonces la matriz se encuentra reducida por filas y la última fila es $0$, luego en ese caso no es invertible.  Si $ad-bc\not=0$, entonces
				\begin{equation*}
				\begin{bmatrix}1&\dfrac{b}{a}\\[8pt]0&\dfrac{ad-bc}{a}\end{bmatrix} \stackrel{a/(ad-bc)\,F_2}{\longrightarrow}
				\begin{bmatrix}1&\dfrac{b}{a}\\[6pt]0&1\end{bmatrix}
				\stackrel{F1-b/a\,F_2}{\longrightarrow}
				\begin{bmatrix}1&0\\0&1\end{bmatrix}.
				\end{equation*} 
				Luego, en el caso $a\not=0$, $ad-bc\not=0$ hemos reducido por filas la matriz $A$  a la identidad y por lo tanto $A$  es invertible. Además, podemos encontrar $A^{-1}$ aplicando a $I$ las mismas operaciones elementales que reducían $A$ a la identidad:
				\begin{align*}
				\begin{bmatrix}1&0\\0&1\end{bmatrix} 
				&\stackrel{F_1/a}{\longrightarrow}
				\begin{bmatrix}\dfrac1a&0\\[8pt]0&1\end{bmatrix} 
				\stackrel{F_2 -cF_1}{\longrightarrow}
				\begin{bmatrix*}[r]\dfrac1a&0\\[8pt]-\dfrac{c}{a}&1\end{bmatrix*}
				\stackrel{a/(ad-bc)\,F_2}{\longrightarrow}
				\begin{bmatrix*}[c]\dfrac1a&0\\[8pt]-\dfrac{c}{ad-bc}&\dfrac{a}{ad-bc}\end{bmatrix*}
				\stackrel{F1-b/a\,F_2}{\longrightarrow}
				\\&\longrightarrow			
				\begin{bmatrix*}[c]\dfrac1a+\dfrac{bc}{a(ad-bc)}&-\dfrac{b}{ad-bc}\\[8pt]-\dfrac{c}{ad-bc}&\dfrac{a}{ad-bc}\end{bmatrix*} 
				=
				\begin{bmatrix*}[c]\dfrac{d}{ad-bc}&-\dfrac{b}{ad-bc}\\[8pt]-\dfrac{c}{ad-bc}&\dfrac{a}{ad-bc}\end{bmatrix*} .
				\end{align*}  
				Concluyendo, en el caso $a\not=0$, $ad-bc\not=0$, $A$  es invertible y 
				\begin{equation}\label{inv-2x2}
				A^{-1} = \dfrac{1}{ad-bc}
				\begin{bmatrix*}[c]d&-b\\-c&a\end{bmatrix*}.
				\end{equation}
				
				2) Estudiemos el caso $a=0$. Primero observemos que si $c=0$ o $b=0$ , entonces la matriz no es invertible, pues en ambos casos nos quedan matrices que no pueden ser reducidas por fila a la identidad. Luego la matriz puede ser invertible si $bc\not=0$ y en este caso la reducción por filas es:
				\begin{equation*}
				\begin{bmatrix}0&b\\c&d\end{bmatrix} \stackrel{F_1\leftrightarrow F_2}{\longrightarrow}
				\begin{bmatrix}c&d\\0&b\end{bmatrix} \stackrel{F_1/c}{\longrightarrow}
				\begin{bmatrix}1&\dfrac{d}{c}\\[6pt]0&b\end{bmatrix} \stackrel{F_2/b}{\longrightarrow}
				\begin{bmatrix}1&\dfrac{d}{c}\\[6pt]0&1\end{bmatrix}
				\stackrel{F_1 - d/c F_2}{\longrightarrow}
				\begin{bmatrix}1&0\\0&1\end{bmatrix}.
				\end{equation*}   
				Luego $A$  es invertible y aplicando estas mismas operaciones elementales a la identidad  obtenemos la inversa:
				\begin{equation*}
				\begin{bmatrix}1&0\\0&1\end{bmatrix} \stackrel{F_1\leftrightarrow F_2}{\longrightarrow}
				\begin{bmatrix}0&1\\1&0\end{bmatrix} \stackrel{F_1/c}{\longrightarrow}
				\begin{bmatrix}0&\dfrac{1}{c}\\[6pt]1&0\end{bmatrix} \stackrel{F_2/b}{\longrightarrow}
				\begin{bmatrix}0&\dfrac{1}{c}\\[6pt]\dfrac{1}{b}&0\end{bmatrix}
				\stackrel{F_1 - d/c F_2}{\longrightarrow}
				\begin{bmatrix}-\dfrac{d}{bc}&\dfrac{1}{c}\\[6pt]\dfrac{1}{b}&0\end{bmatrix}.
				\end{equation*}  
				Luego, en el caso  que $a=0$, entonces $A$ invertible si  $b c\not=0$ y su inversa es
				\begin{equation*}
				A^{-1} = \begin{bmatrix}-\dfrac{d}{bc}&\dfrac{1}{c}\\[6pt]\dfrac{1}{b}&0\end{bmatrix} = 
				\dfrac{1}{-bc}
				\begin{bmatrix*}[c]d&-b\\-c&0\end{bmatrix*}.
				\end{equation*}
				Es decir, la expresión de la inversa es igual a (\ref{inv-2x2}) (considerando que  $a=0$).
				
				Reuniendo los dos casos:  $A$ es invertible si $a\not=0$ y  $ad-bc\not=0$ o si $a=0$ y $bc\not=0$, pero esto es lógicamente equivalente a pedir solamente  $ad-bc\not=0$, es decir
				\begin{equation*}
				(a\not=0 \wedge ad-bc\not=0) \vee (a=0 \wedge bc\not=0)\; \Leftrightarrow\; ad-bc\not=0
				\end{equation*}
				(ejercicio).
				
				Resumiendo, $\begin{bmatrix*} a&b\\c&d\end{bmatrix*}$ es invertible  $\Leftrightarrow ad-bc\not=0$  y en ese caso,  su inversa viene dada por 
				\begin{equation}
				\begin{bmatrix*} a&b\\c&d\end{bmatrix*}^{-1} =  \dfrac{1}{ad-bc}
				\begin{bmatrix*}[c]d&-b\\-c&a\end{bmatrix*}
				\end{equation} 
				
				\vskip 6pt
				
				Veremos en la próxima sección que el uso de determinantes permitirá establecer la generalización de este resultado para matrices $n \times n$ con $n\ge 1$.
				
			\end{proof}
			
		\end{section}
		
		
	\begin{section}{Determinante}\label{seccion-determinate}
	El determinante puede ser pensado como una función que a cada matriz cuadrada $n \times n$ con coeficientes en $\K$,  le asocia un elemento de $\K$. En  esta sección veremos como se define esta función y algunas propiedades de la misma. Algunas  demostraciones se omitirán, pues se pondrá énfasis en los usos del determinante y no tanto en sus propiedades teóricas. Las demostraciones faltantes se pueden ver en el Apéndice \ref{apend.Determinante}.
	
	\vskip .5cm
	
	
	
	Una forma de definir determinante es mediante una definición recursiva,  es decir para calcular el determinante de una matriz $n \times n$, usaremos el cálculo  del determinante para matrices $n-1 \times n-1$,  que a su vez se calcula usando el determinante de matrices $n-2 \times n-2$ y así sucesivamente hasta llegar al caso base, que es el caso  de matrices $1 \times 1$.
	
	
	\vskip .5cm
	
	\begin{definicion} Sea $A \in M_n(\K)$. Sean  $i, j$ tal que  $1 \le i,j \le n$. Entonces
		$A(i|j)$ es la matriz $n-1 \times n-1$ que se obtiene eliminando la fila $i$ y la columna $j$ de $A$.
	\end{definicion}
	
	\begin{ejemplo}
		Sea $A= \begin{bmatrix}1&-1&3\\4&2&-5 \\ 0&7&3\end{bmatrix}$,  entonces
		\begin{equation*}
		A(1|1)= \begin{bmatrix} 2&-5 \\ 7&3\end{bmatrix}, \qquad
		A(2|3)= \begin{bmatrix} 1&-1\\0&7\end{bmatrix}, \qquad
		A(3|1)= \begin{bmatrix} -1&3\\2&-5 \end{bmatrix}.
		\end{equation*}
	\end{ejemplo}		
	
	\begin{definicion}
		Sea $n \in \mathbb N$ y $A =[a_{ij}] \in M_n(\K)$ , entonces el \textit{determinante de $A$}\index{determinante}, denotado $\det(A)$ se define como:
		\begin{enumerate}
			\item[(1)] si $n=1$,  $\det([a]) =a$;
			\item[($n$)] si $n >1$, 
			\begin{align*}
			\det(A) &=  a_{11}\det A(1|1) - a_{21}\det A(2|1) + \cdots + (-1)^{1+n}  a_{n1}\det A(n|1) \\
			&= \sum_{i=1}^{n} (-1)^{1+i}  a_{i1}\det A(i|1).
			\end{align*}
		\end{enumerate}
		Si  $1 \le i,j \le n$, al número $\det A(i|j)$ se lo llama el \textit{menor $i,j$ de $A$} y a $C^A_{ij}:= (-1)^{i+j} \det A(i|j)$ se lo denomina  el \textit{cofactor $i,j$ de $A$.} Si la matriz $A$ está sobreentendida se denota, a veces, $C_{ij} := C^A_{ij}$.
	\end{definicion}
	
	Observemos, que con las definiciones introducidas  tenemos
	\begin{equation}\label{def-determinante}
	\det(A) = \sum_{i=1}^{n}  a_{i1}C^A_{i1}.
	\end{equation}
	A este cálculo  se lo denomina \textit{calculo del determinante por desarrollo por la primera columna}, debido  a que usamos los coeficientes de la primera columna,  multiplicados por los cofactores correspondientes. A veces, para simplificar,  denotaremos
	$$
	|A| := \det A.
	$$
	
	\begin{obs}\textbf{Determinantes $\mathbf{2 \times 2}$.} Calculemos el determinante de las matrices $2 \times 2$. Sea 
		$$A=\begin{bmatrix}a&b\\c&d\end{bmatrix},$$  entonces
		$$
		\det A = a \det [d] - c \det [b] = ad-bc.
		$$
		
		Hemos visto en el ejemplo \ref{inv-2x2-0} que $A$ es invertible si y solo si $ ad-bc \not=0$,  es decir 
		\begin{equation}
		\text{\textit{$A$ es invertible si y solo si $ \det A \not=0$.}}
		\end{equation}
		Este resultado se generaliza para matrices $n \times n$. Más aún, la fórmula (\ref{inv-2x2}),  que aquí reescribimos como
		\begin{equation*}
			A^{-1} = \dfrac{1}{\det(A)}
			\begin{bmatrix*}[c]C_{11}&C_{12}\\C_{21}&C_{22}\end{bmatrix*},
		\end{equation*} se generaliza también para matrices cuadradas de cualquier dimensión (ver el corolario \ref{cor-inv-x-det}).
	\end{obs}
	
	
	\begin{observacion} \textbf{Determinantes $\mathbf{3 \times 3}$.} Calculemos el determinante de las matrices $3 \times 3$. Sea 
		$$A=\begin{bmatrix}a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33}\end{bmatrix},$$  entonces
		\begin{align*}
		\det A &= a_{11}\left|\begin{matrix}a_{22}&a_{23}\\a_{32}&a_{33}\end{matrix}\right|
		- a_{21}\left|\begin{matrix}a_{12}&a_{13}\\a_{32}&a_{33}\end{matrix}\right|
		+ a_{31}\left|\begin{matrix}a_{12}&a_{13}\\a_{22}&a_{23}\end{matrix}\right|\\
		&= a_{11}(a_{22}a_{33}- a_{23}a_{32})
		- a_{21}(a_{12}a_{33}-a_{13}a_{32}) 
		+ a_{31}(a_{12}a_{23} - a_{13}a_{22}) \\
		&=a_{11}a_{22}a_{33}- a_{11}a_{23}a_{32} 
		- a_{12}a_{21}a_{33}+ a_{13}a_{21}a_{32}+ a_{12}a_{23}a_{31}
		- a_{13}a_{22}a_{31}.	
		\end{align*}
		
		Observar que  el determinante de una matriz $3 \times 3$ es una sumatoria de seis términos cada uno de los cuales es de la forma $\pm a_{1\,i_1}a_{2\,i_2}a_{3\,i_3}$ e $i_1i_2i_3$ puede ser cualquier permutación de $123$. La fórmula 
		\begin{equation}\label{det3x3}
		\det A =a_{11}a_{22}a_{33}- a_{11}a_{23}a_{32} 
		- a_{12}a_{21}a_{33}+ a_{13}a_{21}a_{32}+ a_{12}a_{23}a_{31}
		- a_{13}a_{22}a_{31},
		\end{equation} 
		no es fácil de recordar, pero existe un procedimiento sencillo que nos permite obtenerla y es el siguiente: 
		\begin{enumerate}
			\item a la matriz original le agregamos las dos primeras filas al final, 
			\item ``sumamos''  cada producto de las diagonales descendentes y ``restamos'' cada producto de las diagonales ascendentes.
		\end{enumerate}
			
		%prueba y otra 
		\begin{equation}
		\begin{tikzpicture}[baseline=(A.center)]
		\tikzset{node style ge/.style={circle}}
		\tikzset{BarreStyle/.style =   {opacity=.4,line width=0.5 mm,line cap=round,color=#1}}
		\tikzset{SignePlus/.style =   {above left,,opacity=1}}
		\tikzset{SigneMoins/.style =   {below left,,opacity=1}}
		% les matrices
		\matrix (A) [matrix of math nodes, nodes = {node style ge},,column sep=0 mm] 
		{ a_{11} & a_{12} & a_{13}  \\
			a_{21} & a_{22} & a_{23}  \\
			a_{31} & a_{32} & a_{33}  \\
			a_{11} & a_{12} & a_{13} \\
			a_{21} & a_{22} & a_{13}\\
		};
		
		\draw [BarreStyle=blue] (A-1-1.north west) node[SignePlus=blue] {$+$} to (A-3-3.south east) ;
		\draw [BarreStyle=blue] (A-2-1.north west) node[SignePlus=blue] {$+$} to (A-4-3.south east) ;
		\draw [BarreStyle=blue] (A-3-1.north west) node[SignePlus=blue] {$+$} to (A-5-3.south east) ;
		\draw [BarreStyle=red]  (A-3-1.south west) node[SigneMoins=red] {$-$} to (A-1-3.north east);
		\draw [BarreStyle=red]  (A-4-1.south west) node[SigneMoins=red] {$-$} to (A-2-3.north east);
		\draw [BarreStyle=red]  (A-5-1.south west) node[SigneMoins=red] {$-$} to (A-3-3.north east);
		\end{tikzpicture}
		\end{equation}
		
		Es decir,
		\begin{enumerate}
			\item[(a)]  se suman  $a_{11}a_{22}a_{33}$, $a_{21}a_{32}a_{13}$, $a_{31}a_{12}a_{23}$,  y
			\item[(b)]  se restan  $a_{31}a_{22}a_{13}$,  $a_{11}a_{32}a_{23}$,  $a_{21}a_{12}a_{33}$. 
		\end{enumerate}
	\end{observacion}
	
	\begin{ejemplo}
		Calcular el determinante de 
		$$ A = \begin{bmatrix}1&-2&2\\3&-1&1\\2&5&4\end{bmatrix}.$$
		
		La forma más sencilla es ampliando la matriz y calculando:
		\begin{equation*}
		\begin{tikzpicture}[baseline=(A.center)]
		\tikzset{node style ge/.style={circle}}
		\tikzset{BarreStyle/.style =   {opacity=.4,line width=0.5 mm,line cap=round,color=#1}}
		\tikzset{SignePlus/.style =   {above left,,opacity=1}}
		\tikzset{SigneMoins/.style =   {below left,,opacity=1}}
		% les matrices
		\matrix (A) [matrix of math nodes, nodes = {node style ge},,column sep=0 mm] 
		{ 1&-2&2  \\
			3&-1&1  \\
			2&5&4  \\
			1&-2&2 \\
			3&-1&1\\
		};
		
		\draw [BarreStyle=blue] (A-1-1.north west) node[SignePlus=blue] {$+$} to (A-3-3.south east) ;
		\draw [BarreStyle=blue] (A-2-1.north west) node[SignePlus=blue] {$+$} to (A-4-3.south east) ;
		\draw [BarreStyle=blue] (A-3-1.north west) node[SignePlus=blue] {$+$} to (A-5-3.south east) ;
		\draw [BarreStyle=red]  (A-3-1.south west) node[SigneMoins=red] {$-$} to (A-1-3.north east);
		\draw [BarreStyle=red]  (A-4-1.south west) node[SigneMoins=red] {$-$} to (A-2-3.north east);
		\draw [BarreStyle=red]  (A-5-1.south west) node[SigneMoins=red] {$-$} to (A-3-3.north east);
		\end{tikzpicture}.
		\end{equation*}
		Luego 
		\begin{equation*}
		\begin{matrix*}[l]
		\det A &= 1\times (-1) \times 4 \quad\;\,\,+ 3\times5 \times2\quad\;+ 2\times (-2) \times 1\\
		&\quad- 2\times (-1) \times 2\quad - 1 \times 5 \times 1 \quad - 3 \times (-2) \times 4\\
		&= -4+ 30 -4 +4 -5 +24 \\
		&= 35.
		\end{matrix*}
		\end{equation*}
	\end{ejemplo}
	
	\begin{obs}
		La regla para calcular el determinante de matrices $3 \times 3$ \textbf{\large no} se aplica a matrices $n \times n$ con $n \ne 3$.
	\end{obs}
	
	\begin{comment}
		\begin{proposicion}
		Sea $A \in M_n(\K)$ matriz triangular  cuyos elementos en la diagonal son $d_1,\ldots,d_n$. Entonces $\det A = d_1.d_2.\ldots d_n$.
		\end{proposicion}
		\begin{proof} Si $A$ es triangular superior podemos demostrar el resultado por inducción sobre $n$: es claro que si $n=1$,  es decir si $A = [d_1]$, el determinante vale $d_1$. Por otro lado, si $n>1$,  observemos que $A(1|1)$ es también triangular superior con valores $d_2,\ldots,d_n$  en la diagonal principal. Entonces,  usamos la definición de la fórmula (\ref{def-determinante}) y observamos que el desarrollo por la primera  columna solo tiene un término, pues esta columna solo tiene un coeficiente no nulo, el $d_1$ en la primera posición. Por lo tanto, 
		\begin{equation*}
		\det(A) = d_1 \det(A(1|1)) \stackrel{\text{(HI)}}{=} d_1.(d_2.\ldots.d_n).
		\end{equation*}
		
		En  el caso  que  $A$ sea triangular inferior también lo demostraremos por inducción. El caso $n=1$ es trivial. Si $n >1$, observemos que  $A(1|1)$ es también triangular inferior con valores $d_2,\ldots,d_n$  en la diagonal principal y si $i >1$, $A(i|1)$ es triangular inferior con el primer valor en la diagonal igual a 0. Por hipótesis inductiva $\det(A(1|1)) = d_2.\ldots.d_n$ y si $i >1$, $\det(A(i|1)) = 0\times \ldots = 0$, por lo tanto 
		\begin{equation*}
		\det(A) = d_1 \det(A(1|1))+ \sum_{i=2}^{n} a_{i1}\det(A(i|1)) =  d_1.(d_2.\ldots.d_n) + 0.
		\end{equation*} 
		\end{proof}
	\end{comment}

	\begin{proposicion}\label{det-triang-sup}
	Sea $A \in M_n(\K)$ matriz triangular  superior cuyos elementos en la diagonal son $d_1,\ldots,d_n$. Entonces $\det A = d_1.d_2.\ldots d_n$.
	\end{proposicion}
	\begin{proof} Podemos demostrar el resultado por inducción sobre $n$: es claro que si $n=1$,  es decir si $A = [d_1]$, el determinante vale $d_1$. Por otro lado, si $n>1$,  observemos que $A(1|1)$ es también triangular superior con valores $d_2,\ldots,d_n$  en la diagonal principal. Entonces,  usamos la definición de la fórmula (\ref{def-determinante}) y observamos que el desarrollo por la primera  columna solo tiene un término, pues esta columna solo tiene un coeficiente no nulo, el $d_1$ en la primera posición. Por lo tanto, 
		\begin{equation*}
		\det(A) = d_1 \det(A(1|1)) \stackrel{\text{(HI)}}{=} d_1.(d_2.\ldots.d_n).
		\end{equation*}
	\end{proof}
	
	\begin{corolario}
		$det I_n = 1$.
	\end{corolario}
	\begin{proof}
		Se deduce del hecho que $I_n$  es triangular superior y todo coeficiente de la diagonal principal vale 1.
	\end{proof}
	
	\begin{corolario}
		Si $R$ es una MERF, entonces 
		\begin{align*}
		\det R = \left\{ \begin{matrix*}[l]
		1 \;&\text{si $R$ no tiene filas nulas,}\\
		0&\text{si $R$ tiene filas nulas.}
		\end{matrix*}\right.  
		\end{align*}
	\end{corolario}
	\begin{proof}
		Si $R$ no tiene filas nulas es igual a $I_n$ (lema \ref{mtrx-merf-id}), luego $\det R = 1$. En general, $R$ es una matriz triangular superior y si tiene alguna fila nula $r$, entonces el coeficiente en la diagonal de la fila $r$ es igual a $0$ y por lo tanto 	$\det R = 0$.
	\end{proof}

	\begin{ejemplo} Veamos,  en el caso de una matriz $A= [a_{ij}]$ de orden  $2 \times 2$ que ocurre con el determinante cuando hacemos una operación elemental. 
			\begin{enumerate}
			\item Si $c \not=0$, multiplicar por  $c$ la primera fila y multiplicar $c$ por la segunda fila obtenemos, respectivamente,
			\begin{equation*}
			e(A) = \begin{bmatrix} ca_{11}& ca_{12}\\ a_{21}&a_{22}\end{bmatrix}\;\text{ y }\; e(A) = \begin{bmatrix} a_{11}& a_{12}\\ ca_{21}&ca_{22}\end{bmatrix},
			\end{equation*}
			luego 
			\begin{equation*}
			\det \begin{bmatrix} ca_{11}& ca_{12}\\ a_{21}&a_{22}\end{bmatrix} = ca_{11}a_{22} - ca_{12}a_{21}\;\text{ y }\;\det \begin{bmatrix} ca_{11}& ca_{12}\\ a_{21}&a_{22}\end{bmatrix} = ca_{11}a_{22} - ca_{12}a_{21}. 
			\end{equation*}
			Por lo tanto,  en ambos casos, $\det e(A) = c \det A$. 
			\item Sea  $c \in \K$, si sumamos a la fila 2 la fila 1 multiplicada por $c$ o sumamos a la fila 1  la fila 2 multiplicada por $c$ obtenemos, respectivamente,
			\begin{equation*}
			e(A) = \begin{bmatrix} a_{11}& a_{12}\\ a_{21}+ ca_{11}&a_{22}+ ca_{12}\end{bmatrix}\;\text{ y }\; 
			e(A) = \begin{bmatrix} a_{11} + ca_{21}& a_{12} + ca_{22}\\ a_{21}&a_{22}\end{bmatrix}.
			\end{equation*}
			Por lo tanto, 
			\begin{align*}
				\det \begin{bmatrix} a_{11}& a_{12}\\ a_{21}+ ca_{11}&a_{22}+ ca_{12}\end{bmatrix} &=
				a_{11}(a_{22}+ ca_{12})- a_{12}( a_{21}+ ca_{11}) \\
				&= a_{11}a_{22}+ ca_{11}a_{12}- a_{12} a_{21}- ca_{12} a_{11}\\
				&= a_{11}a_{22}- a_{12} a_{21} \\
				&= \det A.
			\end{align*}
			En  el otro caso también se comprueba que $\det e(A) = \det A$. 
			\item Finalmente, intercambiando la fila 1 por la fila 2 obtenemos la matriz
			\begin{equation*}
			e(A)=\begin{bmatrix} a_{21}& a_{22}\\ a_{11}&a_{12}\end{bmatrix},
			\end{equation*}
			por lo tanto 
			\begin{equation*}
			\det e(A)= \det \begin{bmatrix} a_{21}& a_{22}\\ a_{11}&a_{12}\end{bmatrix} = a_{21}a_{12} - a_{22}a_{11} = -\det A.
			\end{equation*}
		\end{enumerate}
	\end{ejemplo}
	
	Todos los resultado del ejemplo anterior se pueden generalizar. 
	
	\begin{teorema} \label{det-prop-fundamentales}
		Sea $A  \in M_n(\K)$ y sean $1 \le r,s \le n$.
		\begin{enumerate}
			\item Sea $c \in \K$ y $B$ la matriz que se obtiene de $A$ multiplicando la fila $r$ por $c$, es decir $A  \stackrel{cF_r}{\longrightarrow} B$, entonces $\det B = c \det A$.
			\item  Sea $c \in \K$, $r \ne s$ y $B$ la matriz que se obtiene de $A$ sumando a la fila $r$ la fila $s$ multiplicada por $c$, es decir  $A  \stackrel{F_r + cF_s}{\longrightarrow} B$, entonces $\det B = \det A$.
			\item Sea $r \ne s$ y sea $B$ la matriz que se obtiene de $A$ permutando la fila $r$ con la fila $s$, es decir  $A  \stackrel{F_r \leftrightarrow F_s}{\longrightarrow}B$, entonces $\det B = -\det A$.
			
		\end{enumerate}
	\end{teorema}
	\begin{proof}
		Ver  los teoremas \ref{det-prop-mult-esc}, \ref{det-prop-perm-filas}, \ref{det-prop-fila-mas-cfila} y  sus demostraciones.
	\end{proof}

	
	
	Este resultado nos permite calcular el determinante de matrices elementales.
	
	
	
	
	\begin{corolario}\label{det-mtrx-elem} Sea $n \in \mathbb N$ y $c \in \K$. Sean $1 \le r,s \le n$,  con $r \ne s$.
		\begin{enumerate}
			\item Si $c \not=0$, la matriz elemental que se obtiene de multiplicar por  $c$ la fila $r$ de $I_n$, tiene determinante igual a $c$.
			\item Sea $r \ne s$. La matriz elemental que se obtiene de sumar a la fila $r$ de $I_n$  la fila $s$ multiplicada por $c$, tiene determinante $1$.
			\item Finalmente, si $r \ne s$, la matriz elemental que se obtiene de intercambiar la fila $r$ por la fila $s$ de $I_n$ tiene determinante $-1$.
		\end{enumerate}	
	\end{corolario} 
	\begin{proof}
		Se deduce fácilmente del teorema anterior y del hecho de que $det I_n =1$.			
	\end{proof}
	
	
	\begin{corolario}\label{det-filas-iguales} Sea $A  \in M_n(\K)$.
		\begin{enumerate}
			\item Si $A$ tiene dos filas iguales,  entonces $\det A=0$.
			\item Si $A$ tiene una fila nula, entonces $\det A =0$.
		\end{enumerate}
	\end{corolario}
	\begin{proof}
		(1) Sea $A$ matriz donde $F_r = F_s$ con $r\ne s$. Luego, intercambiando la fila $r$ por la fila $s$ obtenemos la misma matriz. Es decir $A  \stackrel{F_r \leftrightarrow F_s}{\longrightarrow} A$. Por el teorema \ref{det-prop-fundamentales} (3), tenemos entonces que $\det A = - \det A$, por lo tanto  $\det A =0$. 
		
		(2) Sea $F_r$ una fila nula de $A$, por lo tanto multiplicar por 2 esa fila no cambia la matriz. Es decir $A  \stackrel{2F_r}{\longrightarrow} A$. Por el teorema \ref{det-prop-fundamentales} (1), tenemos entonces que $\det A = 2\det A$, por lo tanto  $\det A =0$.
	\end{proof}
	
	
	
	\begin{teorema} \label{mtrx-inv-equiv3} Sean $A,B \in M_n(\K)$, entonces
		\begin{enumerate}
			\item $\det (A B) = \det(A)\det(B)$. 
			\item $A$ invertible si y solo si  $\det(A)\ne0$.
		\end{enumerate}
	\end{teorema}
\begin{proof}
	Ver el teorema \ref{th-dem-detAB}.
\end{proof}
		\vskip .2cm
	\begin{corolario}
		$\det (AB) = \det(BA)$.
	\end{corolario}
	\begin{proof}
		$\det (AB) = \det(A)\det(B) = \det(B)\det(A) = \det(BA)$.
	\end{proof}
	
	\begin{definicion}\label{def-transpuesta-simetrica}
		Sea $A$ una matriz $m \times n$ con coeficientes en $\K$. La \textit{transpuesta}\index{matriz!transpuesta} de $A$, denotada $A^\t$, es la matriz  $n \times m$ que en la fila $i$ y columna $j$ tiene el coeficiente $[A]_{ji}$. Es decir
		\begin{equation*}
		[A^\t]_{ij} = [A]_{ji}.
		\end{equation*} 
		Si $A$ es una matriz $n \times n$, diremos que es \textit{simétrica}\index{matriz!simétrica} si $A^\t = A$. 
	\end{definicion}
	
	\begin{ejemplo} Si
		$$A=\begin{bmatrix}a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33}\end{bmatrix},$$ 
		entonces
		$$A^\t=\begin{bmatrix}a_{11}&a_{21}&a_{31}\\a_{12}&a_{22}&a_{32}\\a_{13}&a_{23}&a_{33}\end{bmatrix}.$$ 
	\end{ejemplo}
	
	\begin{ejemplo}
		Si $$A=\begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix},$$ 
		entonces
		$$A^\t=\begin{bmatrix}1&3&5\\2&4&6\end{bmatrix}.$$ 
	\end{ejemplo}
	
	
	En  general $A^\t$ es la matriz cuyas filas son las columnas de $A$. 
	
	\begin{ejemplo}
		Si 
		\begin{equation*}
			A=\begin{bmatrix}1&2&3\\2&-1&4\\3&4&7\end{bmatrix},
		\end{equation*}
		entonces $A^\t =A$, es decir  $A$ es simétrica.
	\end{ejemplo}
	
	\begin{proposicion}\label{prop-matriz-transpuesta} Sea $A$ matriz $m \times n$.
		\begin{enumerate}
			\item $(A^\t)^\t = A$.
			\item  Si $B$ matriz $n \times k$,  entonces
			\begin{equation*}
			(AB)^\t = B^\t A^\t.
			\end{equation*} 
			\item  Sea $A$ matriz $n \times n$, entonces, $A$ invertible si y sólo si  $A^\t$ es invertible y  en ese caso $(A^\t)^{-1} = (A^{-1})^\t$.
		\end{enumerate}
	\end{proposicion}
	\begin{proof}
		(1) 	$[(A^\t)^\t]_{ij} = [A^\t]_{ji} = [A]_{ij}$. 
		
		\vskip .3cm 
		
		(2)	Por definición de transpuesta $(AB)^\t$ es una matriz $k \times m$.  Ahora observemos  que $B^\t$  es una matriz $k \times n$ y $A^\t$ es $n \times m$, luego tiene sentido multiplicar $B^\t$ por $A^\t$ y se obtiene también  una matriz  $k \times m$. La demostración de la proposición se hace comprobando que el coeficiente $ij$ de $(AB)^\t$ es igual al coeficiente $ij$ de $B^\t A^\t$ y se deja como ejercicio para el lector. 
		
		(3) 
		\begin{align*}
			\text{$A$ invertible } &\Leftrightarrow \text{existe $B$ matriz $n \times n$ tal que $AB = I_n = BA$} \\
			&\Leftrightarrow (AB)^\t = I^\t_n = (BA)^\t\\
			&\Leftrightarrow B^\t A^\t = I_n = A^\t B^\t\\
			&\Leftrightarrow \text{$B^\t$ es  la inversa de $A^\t$}. 
		\end{align*}
		Es decir, $A$ invertible si y sólo si  $A^\t$ es invertible y si $B = A^{-1}$, entonces  $(A^\t)^{-1} = B^\t$.
	\end{proof}
	
	Observar que por inducción no es complicado probar que si $A_1,\ldots, A_k$ son matrices,  entonces 
	\begin{equation*}
	(A_1\ldots A_k)^\t = A_k^\t\ldots A_1^\t.
	\end{equation*}
	
	\begin{ejemplo} Veamos las transpuesta de las matrices elementales  $2 \times 2$.
		\begin{enumerate}
			\item Si $c \not=0$, multiplicar por  $c$ la primera fila y multiplicar $c$ por la segunda fila son, respectivamente,
			\begin{equation*}
			E = \begin{bmatrix} c& 0\\ 0&1\end{bmatrix}\;\text{ y }\; E = \begin{bmatrix} 1& 0\\ 0&c\end{bmatrix},
			\end{equation*}
			por lo tanto $E^\t$ es la misma matriz en ambos casos. 
			\item si  $c \in \K$, sumar a la fila 2 la fila 1 multiplicada por $c$ o sumar a la fila 1 la fila 2 multiplicada por $c$ son, respectivamente,
			\begin{equation*}
			E_1 = \begin{bmatrix} 1& 0\\ c&1\end{bmatrix}\;\text{ y }\; E_2 = \begin{bmatrix} 1& c\\ 0&1\end{bmatrix},
			\end{equation*}
			por lo tanto 
			\begin{equation*}
			E_1^\t = \begin{bmatrix} 1& c\\ 0&1\end{bmatrix} = E_2 \;\text{ y }\; E_2^\t = \begin{bmatrix} 1& 0\\ c&1\end{bmatrix} =E_1,
			\end{equation*}
			\item Finalmente, intercambiando la fila 1 por la fila 2 obtenemos la matriz
			\begin{equation*}
			E = \begin{bmatrix} 0& 1\\ 1&0\end{bmatrix},
			\end{equation*}
			por lo tanto $E^\t =E$
		\end{enumerate}
	\end{ejemplo}
	

\begin{obs}
	En  el caso  de matrices $2 \times 2$ podemos comprobar fácilmente que $\det A^\t = \det A$:
	$$
	\det A^\t = \det \begin{bmatrix} a_{11} & a_{21} \\ a_{12}& a_{22}\end{bmatrix} = 
	a_{11}a_{22} - a_{21}a_{12} = \det A. 
	$$
\end{obs}

También vale este resultado para matrices $n \times n$.

\begin{teorema}\label{det-a-trans} Sea $A \in M_n(\K)$,  entonces 
	$\det(A) = \det(A^\t)$
\end{teorema}
\begin{proof}
	Ver el teorema \ref{th-det-a-trans-app}.
\end{proof}
	\vskip .3cm
	
	El resultado anterior permite obtener resultados nuevos del cálculo de determinante a partir de resultados vistos anteriormente. 
	
	
	\begin{proposicion}\label{det-triang-inf}
	Sea $A \in M_n(\K)$ matriz triangular  inferior cuyos elementos en la diagonal son $d_1,\ldots,d_n$. Entonces $\det A = d_1.d_2.\ldots d_n$.
	\end{proposicion}
	\begin{proof}
		Si $A$ es triangular inferior con elementos en la diagonal $d_1,\ldots,d_n$,  entonces $A^\t$ es triangular superior con  elementos en la diagonal $d_1,\ldots,d_n$. Por la proposición \ref{det-triang-sup}, $\det A^\t = d_1\ldots d_n$. Por el teorema  \ref{det-a-trans} obtenemos el resultado. 
	\end{proof}


	
	\begin{teorema}\label{det-opr-col}
		Sea $A  \in M_n(\K)$ y sean $1 \le r,s \le n$.
		\begin{enumerate}
			\item Sea $c \in \K$ y $B$ la matriz que se obtiene de $A$ multiplicando la columna $r$ por $c$, entonces $\det B = c \det A$.
			\item  Sea $c \in \K$ y $B$ la matriz que se obtiene de $A$ sumando a la columna $r$ la columna $s$ multiplicada por $c$, entonces $\det B = \det A$.
			\item Sea $B$ la matriz que se obtiene de $A$ permutando la columna $r$ con la fila $s$, entonces $\det B = -\det A$.
		\end{enumerate}
	\end{teorema}
	\begin{proof}
		Las operaciones por columna del enunciado se traducen a operaciones por fila de la matriz $A^\t$. Luego, aplicando los resultados del teorema  \ref{det-prop-fundamentales} y usando el hecho de que $\det(A) = \det(A^\t)$ y $\det(B) = \det(B^\t)$ en cada caso, se deduce el corolario. 
	\end{proof}
	
	\begin{corolario} Sea $A  \in M_n(\K)$.
		\begin{enumerate}
			\item Si $A$ tiene dos columnas iguales,  entonces $\det A=0$.
			\item Si $A$ tiene una columna nula, entonces $\det A =0$.
		\end{enumerate}
	\end{corolario}
	\begin{proof}
		(1) Si $A$ tiene dos columnas iguales,  entonces $A^\t$ tiene dos filas iguales, luego, por corolario \ref{det-filas-iguales} (1),  $\det A^\t =0$ y por lo tanto $\det A=0$.
		
		(2) Si $A$ tiene una columna nula,  entonces $A^\t$ tiene una fila nula, luego, \ref{det-filas-iguales} (2), $\det A^\t =0$ y por lo tanto $\det A=0$.
	\end{proof}
	
		El siguiente teorema nos dice  que es posible calcular el determinante desarrollándolo por cualquier fila o cualquier columna. 
	
	
	\begin{teorema} \label{th-expancion-cofactores} El determinante de una matriz $A$ de orden $n \times n$ puede ser calculado por la expansión de los cofactores en  cualquier columna o cualquier fila. Más específicamente, 
		\begin{enumerate}
			\item si usamos la expansión por la $j$-ésima columna, $1 \le j \le n$, tenemos
			\begin{align*}
			\det A &= \sum_{i=1}^{n} a_{ij} C_{ij} \\
			& = a_{1j}C_{1j}+a_{2j}C_{2j}+\cdots+a_{nj}C_{nj}.
			\end{align*} 
			\item si usamos la expansión por la $i$-ésima fila, $1 \le i \le n$, tenemos
			\begin{align*}
			\det A &= \sum_{j=1}^{n} a_{ij} C_{ij} \\
			& = a_{i1}C_{i1}+a_{i2}C_{i2}+\cdots+a_{in}C_{in};
			\end{align*} 
		\end{enumerate}
	\end{teorema}
\begin{proof}
	Ver la demostración de el teorema  \ref{th-dessarrollo-por-columnas}.
\end{proof}




\end{section}	
	\end{chapter}




	\begin{chapter}{Espacios vectoriales}\label{chap-esp-vect} En este capítulo estudiaremos en forma general las combinaciones lineales sobre conjuntos abstractos.
		En el primer capítulo desarrollamos el método de Gauss para la resolución de sistemas de ecuaciones lineales. En el método de Gauss se usan sistemáticamente las combinaciones lineales de las filas de una matriz. Podemos ver estas filas como elementos de $\K^n$  y nuestro primer impulso para el estudio de las combinaciones lineales sería trabajar en este contexto, es decir  en $\K^n$. Eso es lo que haremos, en forma introductoria, en la primera sección, donde trabajaremos restringidos a   $\R^2$ y $\R^3$.   Sin embargo, muchos de los resultados sobre combinaciones lineales en $\K^n$ son aplicables también a conjuntos más generales y de gran utilidad en la matemática. Por lo tanto,  apartir de la segunda sección,  nuestros ``espacios vectoriales'' (espacios donde pueden hacerse combinaciones lineales de vectores) serán espacios abstractos. 
		

		

		
		\begin{section}{Definición y ejemplos de espacios vectoriales}
			
			\begin{definicion}\label{def-esp-vect} Sea $\K$ cuerpo. Un \textit{espacio vectorial}\index{espacio vectorial} sobre $\K$ o un \textit{$\K$-espacio vectorial }, consiste de  un  conjunto $V$ no vacío, cuyos elementos son llamados \textit{vectores}, junto a  '$+$' y '$.$' tal que
				\begin{enumerate}
					\item[(\textit{a})] '$+\colon V\times V\to V$' es una operación, llamada \textit{adición} o  \textit{suma de vectores,} tal que a dos vectores $v,w \in V$ les asigna otro vector $v+w \in V$,
					\item[(\textit{b})]  '$.
                     \colon \K\times V\to V$' es una operación tal que a $\lambda \in \K$ y $v \in V$ le asigna el vector $\lambda.v$ (o simplemente $\lambda v$).  '$.$' es llamada  el \textit{producto por escalares}.
					
				\end{enumerate}	 
				Además, estas operaciones deben satisfacer 
				\begin{enumerate}
					\item[\textbf{S1.}] $v + w = w + v$, para $v,w \in V$ (\textit{conmutatividad de la suma}),
					\item[\textbf{S2.}] $(v+ w)+ u = v + (w+u)$, para $v,w,u \in V$ (\textit{asociatividad de la suma}),
					\item[\textbf{S3.}] existe un único vector $0$, llamado \textit{vector cero}, tal que $0+ v = v + 0 =v$, para todo $v \in V$ (\textit{existencia de elemento neutro de la suma}).
					\item[\textbf{S4.}] Para cada $v \in V$, existe un  único vector $-v$ tal que  $v + (-v) = (-v)+ v =0$ (\textit{existencia de opuesto} o  \textit{inverso aditivo}).
					\item[\textbf{P1.}] $1.v=v$ para todo $v \in V$.
					\item[\textbf{P2.}] $\lambda_1(\lambda_2v) = (\lambda_1\lambda_2)v$, para todo $\lambda_1,\lambda_2 \in \K$ y todo $v \in V$.
					\item[\textbf{D1.}] $\lambda(v+w) = \lambda v +\lambda w$, para todo $\lambda \in \K$ y todo $v,w \in V$ (\textit{propiedad distributiva}).
					\item[\textbf{D2.}] $(\lambda_1+\lambda_2)v = \lambda_1v + \lambda_2 v$ para todo $\lambda_1,\lambda_2 \in \K$ y todo $v \in V$ (\textit{propiedad distributiva}).
				\end{enumerate}
			\end{definicion}
			
			Debido a la ley de asociatividad para la suma $(v+w)+u$ es igual a $v+(w+u)$ y por lo tanto podemos eliminar los paréntesis sin ambigüedad. Es decir, 	$\forall\, v,w,u \in V$ denotamos 
			$$
			v + w + u := (v+w)+u =v+(w+u).
			$$
			
			De forma análoga, $\forall\, \lambda_1,\lambda_2 \in V$, $\forall \, v \in V$ usaremos la notación
			$$
			\lambda_1\lambda_2v  = (\lambda_1\lambda_2)v = \lambda_1(\lambda_2v).
			$$
			
			Otra notación importante, e intuitiva, es la siguiente	$\forall\, v,w \in V$
			$$
			v-w := v+(-w),
			$$
			y a menudo diremos que $v-w$ es la \textit{resta} de $v$ menos $w$. 
			
			\begin{ejemplo} {\sc $\K^n$.} Sea $\K$ cuerpo, y sea
				\begin{equation*}
				V = \{(x_1,x_2,\ldots,x_n): x_i \in \K, 1 \le i \le n \} = \K^n.
				\end{equation*}	
				Entonces $V$ es espacio vectorial con las siguientes operaciones: si $(x_1,x_2,\ldots,x_n) \in \K^n$, $(y_1,y_2,\ldots,y_n) \in K^n$, $\lambda \in \K$
				\begin{enumerate}
					\item[(\textit{a})] $(x_1,x_2,\ldots,x_n)+ (y_1,y_2,\ldots,y_n) = (x_1+y_1,x_2+y_2,\ldots,x_n+y_n)$,
					\item[(\textit{b})] $\lambda(x_1,x_2,\ldots,x_n) = (\lambda x_1,\lambda x_2,\ldots,\lambda x_n)$.
				\end{enumerate}
				Observar que las sumas y productos son coordenada a coordenada y, por lo tanto, en cada coordenada son sumas y productos en $\K$.
				
				Comprobemos las propiedades necesarias para que $V$ sea un espacio vectorial. Como la suma de vectores y el producto por escalares es coordenada a coordenada, las propiedades se deducirán fácilmente de los axiomas para la suma y el producto en los cuerpos.  Sean $x =  (x_1,\ldots,x_n)$, $y = (y_1,\ldots,y_n)$, $z = (z_1,\ldots,z_n)$ en $V$ y $\lambda, \lambda_1,\lambda_2 \in \K$:
				\begin{enumerate}
					\item[S1.] $x + y = y +x$, pues $x_i + y_i = y_i + x_i$, $1 \le i \le n$.
					\item[S2.]  $(x+ y)+ z = x + (y+z)$,  pues $(x_i + y_i) + z_i = x_i + (y_i + z_i)$, $1 \le i \le n$. 
					\item[S3.] Sea $0 = (0,\ldots,0)$,  entonces $0+x = (0+x_1,\ldots, 0+x_n) =(x_1,\ldots,x_n) =x$.
					\item[S4.] Sea $-x = (-x_1,\ldots,-x_n)$,  entonces $x + (-x) = (x_1-x_1,\ldots,x_n-x_n) =  (0,\ldots,0)$.
					\item[P1.] $1.x=(1.x_1,\ldots,1.x_n) = (x_1,\ldots,x_n)=x$.
					\item[P2.] $\lambda_1(\lambda_2x) = (\lambda_1\lambda_2)x$ pues $\lambda_1(\lambda_2x_i) =(\lambda_1\lambda_2)x_i$, $1 \le i \le n$.
					\item[D1.] $\lambda(x+y) = \lambda x +\lambda y$,  pues $\lambda(x_i+y_i) = \lambda x_i + \lambda y_i$, $1 \le i \le n$.
					\item[D2.] $(\lambda_1+\lambda_2)x = \lambda_1x + \lambda_2 x$, pues $ (\lambda_1+\lambda_2)x_i = \lambda_1x_i + \lambda_2 x_i$, $1 \le i \le n$.
				\end{enumerate} 
				
				
			\end{ejemplo}
			
			\medspace
			
			\begin{ejemplo}{\sc Matrices $m \times n$.} Sea $\K$ cuerpo,  definimos en $M_{m \times n}(\K)$ la suma  y el producto por escalares de la siguiente forma. Sean $A = [a_{ij}]$, $B = [b_{ij}]$ matrices $m \times n$ y $ \lambda \in \K$, entonces $A+B$, $\lambda A$ son matrices en $M_{m \times n}(\K)$ con coeficientes:
				\begin{align*}
				[A + B]_{ij} &= [a_{ij} + b_{ij}], \\
				[\lambda A]_{ij} &= [\lambda a_{ij}]. 
				\end{align*}
				Es decir, la suma es coordenada a coordenada y el producto es multiplicar el escalar en cada coordenada. Este caso no es más que $\K^{mn}$ presentado de otra manera. 
				
				Ejemplifiquemos, con casos sencillos, la suma de matrices y el producto por escalares
				\begin{equation*}
				\begin{bmatrix} -2&1\\0&4 \end{bmatrix} + \begin{bmatrix} 5&1\\2&-5 \end{bmatrix} =
				\begin{bmatrix} 3&2\\2&-1 \end{bmatrix}, \qquad 
				3\begin{bmatrix} -2&1\\0&4 \end{bmatrix} = \begin{bmatrix} -6&3\\0&12 \end{bmatrix}.
				\end{equation*}	
			\end{ejemplo}
			
			\medspace
			
			\begin{ejemplo}{\sc Polinomios.} Sea 
				\begin{equation*}
				\K[x] = \{a_nx^n + \cdots + a_1x + a_0: n \in \mathbb N_0, a_i \in \K\text{, para } 0\le i \le n  \}
				\end{equation*}
				el conjunto de polinomios sobre $\K$. Entonces si $P(x), Q(x) \in \K[x]$, definimos la suma de polinomios de la siguiente manera: sea $P(x) = a_nx^n + \cdots + a_1x + a_0$ y $Q(x)= b_nx^n + \cdots + b_1x + a_0$ (completamos coeficientes con 0 hasta que ambos tengan el mismo $n$), entonces
				\begin{equation*}
				(P+Q)(x) = (a_n+b_n)x^n + \cdots + (a_1+ b_1)x + (a_0+b_0).
				\end{equation*}
				Si $\lambda \in \K$, 
				\begin{equation*}
				(\lambda P)(x) = \lambda a_nx^n + \cdots + \lambda a_1x + \lambda a_0.
				\end{equation*}
				Por ejemplo,
				\begin{align*}
				(3x^2 + 1)+(x^4 + 2x^3 + 5x^2-x) &= x^4 + 2x^3 + 8x^2-x +1, \qquad \text{ y } \\
				3(x^4 + 2x^3 + 5x^2-x) &= 3x^4 + 6x^3 + 15x^2-3x.
				\end{align*}
			\end{ejemplo}
			
			
			\medspace
			
			\begin{ejemplo}{\sc Espacios de funciones.} Sean
				\begin{align*}
				\operatorname{F}(\R) &= \{f: \R \to \R: \text{ tal que } f \text{ es una función} \},\\
				\operatorname{C}(\R) &= \{f: \R \to \R: \text{ tal que }  f  \text{ es una función continua} \}.
				\end{align*}
				Recordemos que si $f,g$ son funciones, entonces la función suma de $f$ y $g$ está definida por
				\begin{equation*}
				(f+g)(x) = f(x) + g(x).
				\end{equation*}
				Por otro lado, si $\lambda \in \R$, la función multiplicar $f$ por $\lambda $ está definida por
				\begin{equation*}
				(\lambda f)(x) = \lambda f(x).
				\end{equation*}			
								
				Es sencillo ver que con estas dos operaciones, $\operatorname{F}(\R)$ es un $\R$-espacio vectorial.
				
				Con respecto a $\operatorname{C}(\R)$, hemos visto en el primer curso de análisis matemático  que la suma de funciones continuas es una función continua y, por lo tanto, $f+g$ es continua si $f$ y $g$ lo son.  
				
				El producto de un escalar $c$ por una función continua $f$,  puede ser visto como el producto de una función que es constante y vale $\lambda $ (y es continua) y la función $f$. Por lo tanto, $\lambda f$ es producto de funciones continuas y, en consecuencia, es una función continua.  Resumiendo,
				\begin{equation*}
				f, g \in \operatorname{C}(\R) \Rightarrow f+g \in \operatorname{C}(\R), \qquad \lambda \in \R, f \in \operatorname{C}(\R) \Rightarrow \lambda f \in \operatorname{C}(\R).
				\end{equation*}
				No es difícil ver que con estas definiciones $\operatorname{C}(\R)$  es un $\R$-espacio vectorial.
			\end{ejemplo}
			
            
			
			\medspace
			
			\begin{ejemplo}
            Consideremos el conjunto de los números reales positivos:
            \[
            R_{>0}=\{x\in\R : x>0\}.
            \]
            Entonces $V=R_{>0}$ es un $\R$-espacio vectorial con la suma $\oplus:V\times V\to V$ y  y el producto $\odot:\R\times V\to V$ dados por
            \begin{align*}
            x\oplus y&=x\cdot y, & c\odot x&=x^c,
            \end{align*}
            para cada $c\in\R$, $x,y\in R_{>0}$.
Es fácil ver que los axiomas \textbf{S1.} y \textbf{S2.} sobre la conmutatividad y asociatividad, respectivamente, de la suma $\oplus$ se siguen de las propiedades de conmutatividad y asociatividad del producto $\cdot$ en $\R$.

La existencia del vector $\bf0$ del axioma \textbf{S3.}, neutro para la suma $\oplus$, requiere de cierto cuidado. Notar que este vector debe ser un elemento $\bf0$ en $V$ (un real positivo) que cumpla $x\oplus \bf 0=x$ para todo $x$. Ahora, $x\oplus \bf0=x\cdot \bf0$ por definición, de donde se desprende que debemos tomar $bf0=1$. Es decir, {\it el vector cero es el número 1}. De manera similar, se sigue que el opuesto indicado en el  axioma \textbf{S4.} debe estar dado por $-x=x^{-1}$.

Finalmente, las propiedades de los axiomas \textbf{P1.}, \textbf{P2.}, \textbf{D1.} y \textbf{D2.} se siguen de las propiedades conocidas de la exponenciación en $\R$ y quedan a cargo del lector, como un interesante desafío para terminar de comprender este ejemplo.
\end{ejemplo}            
            
			\vskip .3cm
			
			
			\begin{proposicion}\label{prop-basicas-ev} Sea $V$ un espacio vectorial sobre el cuerpo $\K$. Entonces,
				\begin{enumerate}
					\item\label{c*0=0} $\lambda.0=0$, para todo $\lambda \in \K$;
					\item\label{0*c=0} $0.v = 0$, para todo $v \in V$;
					\item\label{c*v=0} si $\lambda \in \K$, $v \in V, v\ne 0$ y $\lambda.v=0$,  entonces $\lambda =0$;
					\item\label{-1*v=-v} $(-1).v = -v$, para todo $v \in V$.
				\end{enumerate}
			\end{proposicion}
			\begin{proof} Tanto la prueba de (1), como la de (2) son similares a la demostración de que $0.a=0$  en $\mathbb Z$ (o en  $\R$).
				 
				(1) Como $0$ es el elemento neutro de la suma en $V$, entonces $0 = 0 +0$, luego 
				\begin{equation*}
					\begin{array}{rllll}
					\lambda.0 &=& \lambda.(0+0)&\quad&\text{(propiedad distributiva $\Rightarrow$)}  \\
					\lambda.0&=& \lambda.0 + \lambda.0 && \text{(sumando a la izquierda $-\lambda.0$ $\Rightarrow$)} \\
				\lambda.0 - \lambda.0 &=& \lambda.0 + \lambda.0 -\lambda.0 && \text{(opuesto $\Rightarrow$)}\\
					0 &=& \lambda.0 +0  &&\text{(elemento neutro $\Rightarrow$)}\\
					0 &=& \lambda.0.
					\end{array}
				\end{equation*}
				
				(2) Análoga a (1).
				
				(3) Supongamos que  $\lambda.v=0$ y  $\lambda \ne 0$, entonces, por (1), $ \lambda^{-1}(\lambda.v)=0$, pero $\lambda^{-1}(\lambda.v) = (\lambda^{-1}\lambda).v = 1.v = v$. Luego $0=v$,  que contradice la hipótesis. El absurdo vino de suponer que $\lambda \ne 0$.
				
				(4)  $(-1).v +v = (-1).v +1.v = (-1+1).v$,  esto último es por la propiedad distributiva. Ahora bien $(-1+1).v = 0.v \overset{(2)}{=} 0$. Es decir   $(-1).v +v = 0$ y por lo tanto  $(-1).v$ es el opuesto de $v$ (que es $-v$).  
			
			\end{proof}
		
		\begin{definicion}
			Sea $V$ espacio vectorial sobre $\K$ y $v_1,\ldots,v_n$ vectores en $V$. Dado $v \in V$, diremos que\textit{ $v$  es combinación lineal de los $v_1,\ldots,v_n$ }\index{combinación lineal} si existen escalares $\lambda_1,\ldots,\lambda_n$ en $\K$,  tal que 
			$$
			v = \lambda_1v_1+\cdots+\lambda_nv_n.
			$$
		\end{definicion}
	
	\begin{ejemplo}
		\begin{enumerate}
			\item Sean $v_1 = (1,0)$, $v_2 = (0,1)$ en $\C^2$ ¿es $v = (i,2)$ combinación lineal de $v_1,v_2$? La respuesta es sí, pues 
			$$
			v = iv_1+2v_2.
			$$
			Observar además que es la única combinación lineal posible, pues si 
			$$
			v = \lambda_1v_1+ \lambda_2 v_2,
			$$
			entonces
			$$
			(i,2) = (\lambda_1,0)+(0,\lambda_2) = (\lambda_1,\lambda_2),
			$$
			luego $\lambda_1=i$ y $\lambda_2= 2$.
			
			Puede ocurrir que un vector sea combinación lineal de otros vectores de varias formas diferentes. Por ejemplo,   si   $v = (i,2)$ y $v_1 = (1,0)$, $v_2 = (0,1)$, $v_3 = (1,1)$,  tenemos que
			\begin{align*}
				v &= iv_1+2v_2+0v_3,\quad\quad\quad\text{y también}\\
				v &= (i-1)v_1 + v_2 + v_3.  
			\end{align*}
			
			\item Sean $(0,1,0)$, $(0,1,1)$ en $\C^3$ ¿es $(1,1,0)$ combinación lineal de $(0,1,0)$, $(0,1,1)$? La respuesta es no, pues si 
			$$
			(1,1,0) = \lambda_1(0,1,0)+ \lambda_2(0,1,1) = (0,\lambda_1,0)+ (0,\lambda_2,\lambda_2) = (0,\lambda_1+\lambda_2,\lambda_2),
			$$ 
			luego, la primera coordenada nos dice que $1=0$, lo cual es absurdo. Por lo tanto,  no existe un par $\lambda_1,\lambda_2 \in \K$ tal que  $(1,1,0) = \lambda_1(0,1,0)+ (0,1,1)$.
			
		\end{enumerate}
	\end{ejemplo}
		
		\vskip .3cm
		
		\begin{observacion}
			La pregunta de si un vector $v =(b_1,\ldots,b_m) \in \K^m$ es combinación lineal de vectores $v_1,\ldots,v_n \in \K^m$ se resuelve con un sistema de ecuaciones lineales: si 
			$$
			v_i = (a_{1i},\ldots,a_{mi}), \quad \text{para $1 \le i \le n$,}
			$$
			entonces $v = \lambda_1v_1 + \cdots +\lambda_nv_n$ se traduce, en coordenadas, a
			\begin{align*}
				(b_1,\ldots,b_m) &= \lambda_1(a_{11},\ldots,a_{m1}) + \cdots +\lambda_n(a_{1n},\ldots,a_{mn}) \\
				&= (\lambda_1a_{11} + \cdots+ \lambda_na_{1n}, \ldots, \lambda_1a_{m1} + \cdots+ \lambda_na_{mn}).
			\end{align*}
			Luego, $v$  es combinación lineal de los vectores $v_1,\ldots,v_n \in \K^n$ si y sólo si el tiene solución el siguiente sistema de ecuaciones con incógnitas $\lambda_1,\ldots, \lambda_n$: 
			\begin{equation*}
			\begin{matrix}
			a_{11}\lambda_1& + &a_{12}\lambda_2& + &\cdots& + &a_{1n}\lambda_n &= &b_1\\
			\vdots&  &\vdots& &&  &\vdots \\
			a_{m1}\lambda_1& + &a_{m2}\lambda_2& + &\cdots& + &a_{mn}\lambda_n &=&b_m
			\end{matrix}.
			\end{equation*}
		\end{observacion}
		
		
		\begin{ejemplo} Demostrar que $(5,12,5)$  es combinación lineal de los vectores $(1,-5,2), (0,1,-1), (1,2,-1)$. Planteamos la ecuación:
			\begin{align*}
				(5,12,5) &= \lambda_1(1,-5,2)+\lambda_2 (0,1,-1)+\lambda_3 (1,2,-1) 
				\\&= (\lambda_1+\lambda_3,-5\lambda_1+\lambda_2+2\lambda_3,2\lambda_1-\lambda_2-\lambda_3).
			\end{align*}
			Por consiguiente,  esta ecuación se resuelve con el siguiente sistema de ecuaciones
			\begin{align*}
				\lambda_1+\lambda_3 &= 5 \\
				-5\lambda_1+\lambda_2+2\lambda_3 &= 12 \\
				2\lambda_1-\lambda_2-\lambda_3 &= 5.
			\end{align*}
			Ahora bien, usando el método de Gauss
								\begin{multline*}
			\left[\begin{array}{rrr|r}1 & 0 & 1 &  5 \\ -5 & 1 & 2 &  12 \\	2 & -1 & -1 &  5  \end{array}\right]
			\stackrel[F_3 -2F_1]{F_2 + 5F_1}{\longrightarrow} 
			\left[\begin{array}{rrr|r}1 & 0 & 1 &  5 \\ 0 & 1 & 7 &  37 \\	0 & -1 & -3 &  -5  \end{array}\right]
			\stackrel{F_3+F_1}{\longrightarrow} 
			\left[\begin{array}{rrr|r}1 & 0 & 1 &  5 \\ 0 & 1 & 7 &  37 \\	0 & 0 & 4 & 32  \end{array}\right]
			\\
			\stackrel{F_3/4}{\longrightarrow} 
			\left[\begin{array}{rrr|r}1 & 0 & 1 & 5 \\ 0 & 1 & 7 & 37 \\	0 & 0 & 1& 8  \end{array}\right]
			\stackrel[F_2 -7F_3]{F_1 - F_3}{\longrightarrow}
			\left[\begin{array}{rrr|r}1 & 0 & 0 & -3 \\ 0 & 1 & 0 &  -19 \\	0 & 0 & 1& 8  \end{array}\right].
			\end{multline*}	
			Luego $\lambda_1= -3$, $\lambda_2 = -19$ y $\lambda_3=8$,  es decir
			\begin{align*}
			(5,12,5) &= -3(1,-5,2)-19 (0,1,-1)+8 (1,2,-1).
			\end{align*}
		\end{ejemplo}
\end{section}
	
\begin{section}{Subespacios vectoriales}
	\begin{definicion}
		Sea $V$ un espacio vectorial sobre $\K$. diremos que $W \subset V$ es \textit{subespacio de $V$}\index{subespacio} si $W \not= \emptyset$ y
		\begin{enumerate}
			\item[(a)] si para cualesquiera $w_1,w_2 \in W$, se cumple que $w_1+w_2 \in W$ y
			\item[(b)] si $\lambda \in \K$ y  $w \in W$, entonces $\lambda w \in W$.
		\end{enumerate}
	\end{definicion}

	\begin{observacion} Si $W$ subespacio de $V$,  entonces $0 \in W$: como  $W \ne \emptyset$, tomo  cualquier $w \in W$ y  por (a) tenemos que $0.w \in W$. Ya vimos, proposición \ref{prop-basicas-ev}(\ref{0*c=0}),  que $0.w =0$ y por lo tanto $0 \in W$. 
	\end{observacion}

	\begin{observacion}
		 Si $W$ subespacio de $V$ y $w_1,\ldots,w_k \in W$,  entonces cualquier combinación lineal de los $w_1,\ldots,w_k$ pertenece a $W$. Es decir, para cualesquiera $\lambda_1,\ldots,\lambda_k \in \K$, se cumple que  $\lambda_1w_1+\cdots\lambda_kw_k \in W$. Esto se debe a que $\lambda_iw_i \in W$ para $1\le i \le k$. Por un argumento inductivo, no es difícil probar que la suma de $k$ términos en $W$ es un elemento de $W$, por lo tanto $\lambda_1w_1+\cdots\lambda_kw_k \in W$.  
	\end{observacion}

	\begin{teorema}
		Sea $V$ un espacio vectorial sobre $\K$ y $W$ subespacio de $V$. Entonces $W$ con las operaciones suma y producto por escalares de $V$ es un espacio vectorial.
	\end{teorema}
	\begin{proof} Debemos verificar las propiedades de la definición de espacio vectorial.
		\begin{enumerate}
			\item[S1.] $w,w' \in W$, entonces $w + w' = w' + w$, pues conmutan en $V$;
			\item[S2.] si $w_1,w_2,w_3 \in W$, entonces $w_1+(w_2+w_3) = (w_1+w_2)+w_3$, pues la suma es asociativa en $V$;
			\item[S3.] ya vimos que $0 \in W$, y como $0$ es el elemento neutro de la suma para $V$, también lo es para $W$.
			\item[S4.] Sea $w \in W$, entonces $(-1)w \in W$ (por SE2). Pero, por proposición \ref{prop-basicas-ev}(\ref{-1*v=-v}), $(-1)w=-w$,  es decir $(-1)w$ es el opuesto aditivo de $w$  en $V$, luego es el opuesto aditivo de $w$ en $W$.
			\item[P1.] $1.v=v$ para todo $v \in V$, luego $1.w =w$ para todo $w \in W$.
			\item[P2.] $\lambda_1(\lambda_2v) = (\lambda_1\lambda_2)v$, para todo $\lambda_1,\lambda_2 \in \K$ y todo $v \in V$, luego $\lambda_1(\lambda_2w) = (\lambda_1\lambda_2)w$, para todo $\lambda_1,\lambda_2 \in \K$ y todo $w \in W$.
			\item[D1.] $\lambda(v_1+v_2) = \lambda v_1 +\lambda v_2$, para todo $\lambda \in \K$ y cualesquiera $v_1,v_2 \in V$, luego   $\lambda(w_1+w_2) = cw_1 +cw_2$, para todo $\lambda \in \K$ y cualesquiera $w_1,w_2 \in W$.
			\item[D2.] $(\lambda_1+\lambda_2)v = \lambda_1v + \lambda_2 v$ para todo $\lambda_1,\lambda_2 \in \K$ y todo $v \in V$, luego $(\lambda_1+\lambda_2)w = \lambda_1w + \lambda_2 w$ para todo $\lambda_1,\lambda_2 \in \K$ y todo $w \in W$.
		\end{enumerate} 
	\end{proof}



\begin{ejemplo} Veremos ahora una serie de ejemplos de subespacios vectoriales.
	\begin{enumerate}
		\item Sea $V$ un $\K$-espacio vectorial, entonces $0$ y $V$ son subespacios vectoriales de $V$. Suelen ser llamados los \textit{subespacios triviales}\index{subespacios triviales} de $V$.
		
		\item Sea $V$ un $\K$-espacio vectorial y sea $v \in V$, entonces
		$$
		W = \{\lambda v: \lambda \in \K \}
		$$
		es un subespacio vectorial. En  efecto
		\begin{enumerate}
			\item si $\lambda_1v,\lambda_2v \in W$, con $\lambda_1,\lambda_2 \in \K$,  entonces $\lambda_1v + \lambda_2v = (\lambda_1+\lambda_2)v \in W$;
			\item  $\lambda_1v \in W$, con $\lambda_1 \in \K$ y $\lambda \in \K$,  entonces $\lambda(\lambda_1v) = (\lambda \lambda_1)v \in W$.
		\end{enumerate}
		El subespacio $W$ suele ser denotado $\K v$.
	
		
		\item Sean $V=\K^n$ y $1\le j \le n$. Definimos 
		$$
		W = \left\{ (x_1,x_2,\ldots,x_n): x_i \in \K\; (1 \le i \le n), x_j =0\right\}.
		$$
		Es decir $W$  es el subconjunto de $V$ de todas las $n$-uplas con la coordenada $j$ igual a 0. Por ejemplo  si $j=1$ 
		$$
		W = \left\{ (0,x_2,\ldots,x_n): x_i \in \K \;(2 \le i \le n)\right\}.
		$$
		Veamos que este último es un subespacio: 
		\begin{enumerate}
			\item si $(0,x_2,\ldots,x_n), (0,y_2,\ldots,y_n) \in W$,  entonces
			$(0,x_2,\ldots,x_n)+ (0,y_2,\ldots,y_n) = (0,x_2+y_2,\ldots,x_n+y_n)$, el cual pertenece a $W$. 
			\item 	Por otro lado, si $\lambda \in \K$, $\lambda(0,x_2,\ldots,x_n) = (0,\lambda x_2,\ldots,\lambda x_n) \in W$.
		\end{enumerate}
		La demostración para $j >1$ es completamente análoga. 
		\item El  conjunto $\R[x] = \left\{P(x): P(x) \text{ es polinomio en $\R$ } \right\}$, es subespacio de $\operatorname{F}(\R)$, pues $\R[x] \subset \operatorname{F}(\R)$ y las operaciones de suma y producto por un escalar son cerradas en $\R[x]$.
		\item De forma análoga, el conjunto $\R[x]$ es subespacio de $\operatorname{C}(\R)$,  el espacio de funciones continuas de  $\R$.
		\item Sea $W= \left\{A \in M_n(\K): A^\t = A \right\}$. Es claro que  $A \in W$ si y sólo si $[A]_{ij} = [A]_{ji}$. Veamos que  $W$ es subespacio de $M_n(\K)$: 
		\begin{enumerate}
			\item sean $A=[a_{ij}]$, $B= [b_{ij}]$ tales que $A=A^\t$ y $B=B^\t$, entonces debemos verificar que $A+ B \in W$,  es decir que la transpuesta de $A+B$  es la misma matriz: ahora bien, $[A+B]_{ij} = a_{ij}+b_{ij}$, luego 
			\begin{equation*}
				[(A+B)^\t]_{ij} =a_{ji}+b_{ji} = [A]_{ji} + [B]_{ji} = [A]_{ij} + [B]_{ij} = [A+B]_{ij},
			\end{equation*}
			por lo tanto $A+B \in W$.
			\item Si $\lambda \in \K$, $[\lambda A]_{ij} = \lambda a_{ij}$, luego, 
			$$
			[\lambda A^\t]_{ij} = \lambda a_{ji} = \lambda a_{ij} = [\lambda A]_{ij},
			$$
			por lo tanto $\lambda A \in W$.
		\end{enumerate} 	

		\item Sea $A \in M_{m \times n}(\K)$. Si $x = (x_1,\ldots,x_n) \in \K^n$,  entonces $Ax$ denotará la multiplicación de $A$ por la matriz columna formada por $x_1,\ldots,x_n$,  es decir
		$$
		Ax = A\begin{bmatrix} x_1 \\ \vdots \\ x_n\end{bmatrix}.
		$$
		Sea 
		$$
		W = \left\{x \in \K^n: Ax=0 \right\}.
		$$
		Es decir, $W$  es el subconjunto de $\K^n$ de las soluciones del sistema $Ax=0$. Entonces, $W$  es un subespacio de $\K^n$:
		\begin{enumerate}
			\item si $x,y \in W$, es decir si $Ax=0$ y $Ay=0$,  entonces $A(x+y) = Ax + Ay = 0 + 0 = 0$, luego , $x+y \in W$;
			\item si $\lambda \in \K$ y $x \in W$, entonces $A(\lambda x) = \lambda Ax =\lambda.0 =0$, luego $\lambda x \in W$. 
		\end{enumerate}
	\end{enumerate}
\end{ejemplo}

\vskip .5cm

	\begin{teorema}
	Sea $V$ un espacio vectorial sobre $\K$ y sean $v_1,\ldots,v_k \in V$. Entonces
	$$
	W = \{\lambda_1v_1+\cdots+\lambda_kv_k: \lambda_1,\ldots,\lambda_k \in \K \}
	$$
	es un subespacio vectorial. Es decir,  el conjunto de las combinaciones lineales de $v_1,\ldots,v_k$ es un subespacio vectorial.
	\end{teorema}
	\begin{proof}
		\begin{enumerate}
			\item[(a)] Sean $\lambda_1v_1+\cdots+\lambda_kv_k$ y $\mu_1v_1+\cdots+\mu_kv_k$ dos combinaciones lineales de $v_1,\ldots,v_k$, entonces 
			\begin{align*}
				(\lambda_1v_1+\cdots+\lambda_kv_k)+(\mu_1v_1+\cdots+\mu_kv_k) &=  \lambda_1v_1+\mu_1v_1+\cdots+\lambda_kv_k+\mu_kv_k\\
				&= (\lambda_1+\mu_1)v_1+\cdots+(\lambda_k+\mu_k)v_k,
			\end{align*}
			que es  una combinación lineal de  $v_1,\ldots,v_k$ y por lo tanto pertenece a $W$.
			\item[(b)] Si $\lambda \in \K$ y $\lambda_1v_1+\cdots+\lambda_kv_k$  es una combinación lineal de $v_1,\ldots,v_k$,  entonces
			\begin{align*}
			\lambda (\lambda_1v_1+\cdots+\lambda_kv_k) &=  \lambda(\lambda_1v_1)+\cdots+\lambda(\lambda_kv_k)\\
			&= (\lambda\lambda_1)v_1+\cdots+(\lambda\lambda_k)v_k,
			\end{align*}
				que es  una combinación lineal de  $v_1,\ldots,v_k$ y por lo tanto pertenece a $W$.
		\end{enumerate}
	\end{proof}
	


	\begin{definicion}
	Sea $V$ un espacio vectorial sobre $\K$ y sean $v_1,\ldots,v_k \in V$. Al  subespacio vectorial $	W = \{\lambda_1v_1+\cdots+\lambda_kv_k: \lambda_1,\ldots,\lambda_k \in \K \}$ de las combinaciones lineales de $v_1,\ldots,v_k$ se lo denomina \textit{subespacio generado por $v_1,\ldots,v_k$}\index{subespacio generado} y se lo denota  
	\begin{equation*}
		\begin{array}{rcll}
		W &=& \langle v_1,\ldots,v_k \rangle, &\quad \text{ o } \\
		W &=&  \operatorname{gen}\left\{ v_1,\ldots,v_k\right\} &\quad \text{ o } \\
		W &=&  \operatorname{span}\left\{ v_1,\ldots,v_k\right\}.&
		\end{array}
	\end{equation*}
	Además, en este caso, diremos que el conjunto $S = \left\{ v_1,\ldots,v_k \right\}$ \textit{genera} al subespacio $W$.
	
\end{definicion}

\vskip.5cm

		\begin{teorema}
			Sea $V$ un espacio vectorial sobre $\K$. Entonces la intersección de subespacios vectoriales es un subespacio vectorial. 
		\end{teorema}
	\begin{proof}
		Sea $\{W_i \}_{i \in I}$ una familia   de subespacios vectoriales y sea 
		$$W = \bigcap_{i\in I} W_i,$$
		entonces
		\begin{enumerate}
			\item[(a)] si $w_1,w_2 \in W$, tenemos que $w_1, w_2 \in W_i$ para todo $i \in I$, luego, como $W_i$ es subespacio vectorial,  $w_1+ w_2 \in W_i$ para todo $i \in I$, por lo tanto $w_1+ w_2 \in W$;
			\item[(b)] si  $\lambda \in \K$ y $w \in W$,  $w \in W_i$ para todo $i \in I$ y, por lo tanto, $cw \in W_i$ para todo $i \in I$. En consecuencia $w \in W$.
		\end{enumerate}
	\end{proof}
		
		
		\begin{obs}
			Si $V$ es un $\K$-espacio vectorial, $S$ y $T$ subespacios de $V$, entonces $S \cup T$ no es necesariamente un subespacio de $V$. En efecto, consideremos en $\R^2$ los subespacios 
			$S = \R(1,0)$ y $T = \R(0,1)$. 	Observamos que $(1,0)\in  S$ y $(0,1) \in  T$; luego, ambos  pertenecen a $S \cup T$. Pero $(1,0) + (0,1) = (1,1) \not\in S \cup T$, puesto que $(1,1) \not\in S$ y $(1,1) \not\in T$.
		\end{obs}
	
	\begin{teorema}
		Sea $V$ un espacio vectorial sobre $\K$ y sean $v_1,\ldots,v_k \in V$. Entonces,  la intersección de todos los subespacios vectoriales que contienen  a $v_1,\ldots,v_k$ es igual a $\langle v_1,\ldots,v_k \rangle$.	
	\end{teorema}	
	\begin{proof}
		Denotemos $W_1 = \langle v_1,\ldots,v_k \rangle$ y $W_2$ la intersección de todos los subespacios vectoriales que contienen  a $v_1,\ldots,v_k$. Probaremos que  $W_1 = W_2$ con la doble inclusión,  es decir probando que $W_1 \subseteq W_2$ y  $W_2 \subseteq W_1$.
		
		\vskip.3cm
		
		($W_1 \subseteq W_2$). Sea $W$  subespacio vectorial que contiene $v_1,\ldots,v_k$. Como $W$ es subespacio,  entonces  $W$ contiene a cualquier combinación lineal de los $v_1,\ldots,v_k$, por lo tanto $W$ contiene a $W_1$. Es decir, cualquier subespacio que contiene a $v_1,\ldots,v_k$,  también contiene a  $W_1$, por lo tanto la intersección de todos los subespacios que  contienen a $v_1,\ldots,v_k$, contiene a $W_1$. Luego $W_2 \supseteq W_1$.
		
		\vskip.3cm
		
		($W_2 \subseteq W_1$). $W_1$ es un subespacio que contiene a $v_1,\ldots,v_k$, por lo tanto  la intersección de todos los subespacios que  contienen a $v_1,\ldots,v_k$  está contenida en $W_1$. Es decir, $W_2 \subseteq W_1$.
	\end{proof}
		
		
	\begin{definicion} Sea $V$ un espacio vectorial sobre $\K$ y sean $S_1,\ldots,S_k$ subconjuntos  de $V$.
		definimos 
		\begin{equation*}
			S_1+  \cdots +S_k := \left\{s_1+\cdots+s_k: s_i \in S_i, 1 \le i \le k \right\},
		\end{equation*}
		el conjunto \textit{suma de los  $S_1,\ldots,S_k$.}
	\end{definicion}	

	\begin{teorema}
		 Sea $V$ un espacio vectorial sobre $\K$ y sean $W_1,\ldots,W_k$ subespacios  de $V$. Entonces $W= W_1+\cdots+W_k$ es un subespacio de $V$.
	\end{teorema}
	\begin{proof}
		Sean $v = v_1+\cdots+v_k$ y $w = w_1+\cdots+w_k$ en $W$ y $\lambda \in \K$. Entonces
		\begin{enumerate}[label=(\alph*),ref=\alph*]
			\item $v+w =  (v_1+w_1)+\cdots+(v_k+w_k) \in W_1+\cdots+W_k$, pues como $W_i$ es subespacio de $V$, tenemos que $v_i+w_i \in W_i$.
			\item  $\lambda v = \lambda(v_1+\cdots+v_k) = \lambda v_1+\cdots+\lambda v_k \in W_1+\cdots+W_k$, pues como $W_i$ es subespacio de $V$, tenemos que $\lambda v_i \in W_i$.
		\end{enumerate}
	\end{proof}

	\begin{proposicion}
		Sea $V$ un espacio vectorial sobre $\K$ y sean $v_1,\ldots,v_r$ elementos de   de $V$. Entonces
		\begin{equation*}
			\langle v_1,\ldots,v_r \rangle = \langle v_1 \rangle+ \cdots + \langle v_r \rangle.
		\end{equation*}
	\end{proposicion}
	\begin{proof}
		Probemos el resultado viendo que los dos conjuntos se incluyen mutuamente.
		
		($\subseteq$) Sea $w \in \langle v_1,\ldots,v_r \rangle$, luego $w = \lambda_1 v_1 +\cdots+ \lambda_r v_r$. Como $ \lambda_i v_i \in \langle v_i \rangle$, $1 \le i \le r$ ,  tenemos que  $w \in \langle v_1 \rangle+ \cdots + \langle v_r \rangle$.  En  consecuencia, $\langle v_1,\ldots,v_r \rangle \subseteq \langle v_1 \rangle+ \cdots + \langle v_r \rangle$. 
		
		($\supseteq$) Si $w \in \langle v_1 \rangle+ \cdots + \langle v_r \rangle$, entonces $w = w_1 + \cdots+w_r$ con $w_i \in \langle v_i\rangle$ para todo $i$. Por lo tanto, $w_i = \lambda_i v_i$ para algún $\lambda_i \in \K$ y  $w = \lambda_1 v_1 +\cdots+ \lambda_r v_r \in \langle v_1,\ldots,v_r \rangle $. En  consecuencia, $\langle v_1 \rangle+ \cdots + \langle v_r \rangle \subseteq \langle v_1,\ldots,v_r \rangle$. 
	\end{proof}

	\begin{ejemplo}\label{ejemplos2} Veremos una serie de ejemplos de subespacios,  suma e intersección de subespacios.
		\begin{enumerate}
			\item\label{ejemplos2-1} Sea $\K = \C$ y $V= \C^5$. Consideremos los vectores
			\begin{equation*}
				v_1 = (1,2,0,3,0), \qquad v_2 = (0,0,1,4,0), \qquad v_3 = (0,0,0,0,1),
			\end{equation*}
			y sea $W= \langle v_1,v_2,v_3 \rangle$. 
			
			Ahora bien, $w \in W$, si y sólo si $w = \lambda_1 v_1+\lambda_2 v_2+\lambda_3 v_3$, con $\lambda_1,\lambda_2,\lambda_3 \in \C$. Es decir
			\begin{align*}
				w &= \lambda_1 (1,2,0,3,0)+\lambda_2 (0,0,1,4,0)+\lambda_3 (0,0,0,0,1) \\
				&=  (\lambda_1,2\lambda_1,0,3\lambda_1,0)+ (0,0,\lambda_2,4\lambda_2,0)+ (0,0,0,0,\lambda_3) \\ &=(\lambda_1,2\lambda_1,\lambda_2,3\lambda_1+4\lambda_2,\lambda_3)
			\end{align*} 
			Luego,  también podríamos escribir
			\begin{equation*}
				W = \left\{(x_1,x_2,x_3,x_4,x_5)\in \C^5: x_2 = 2x_1, x_4 = 3x_1+4x_3 \right\}.
			\end{equation*}
			
			\item Sea $V = M_2(\C)$ y sean 
			\begin{equation*}
				W_1 = \left\{\begin{bmatrix} x&y\\z&0 \end{bmatrix}: x,y,z \in \C \right\}, 
				\qquad W_2 = \left\{\begin{bmatrix} x&0\\0&y \end{bmatrix}: x,y \in \C \right\}.
			\end{equation*} 
			Es claro que cada uno de estos conjuntos es un subespacio, pues,
			$$
			W_1 = \C \begin{bmatrix} 1&0\\0&0 \end{bmatrix} +
			\C \begin{bmatrix} 0&1\\0&0 \end{bmatrix} +
			\C \begin{bmatrix} 1&0\\0&0 \end{bmatrix}, \quad 
			W_2 = \C \begin{bmatrix} 1&0\\0&0 \end{bmatrix} +
			\C \begin{bmatrix} 0&0\\0&1 \end{bmatrix}. 
			$$
			Entonces, $W_1 + W_2 = V$. En  efecto, sea 
			$\begin{bmatrix} a&b\\c&d\end{bmatrix} \in V$, entonces
			$$
			\begin{bmatrix} a&b\\c&d\end{bmatrix} =  \begin{bmatrix} x_1&y_1\\z_1&0 \end{bmatrix} + 
			\begin{bmatrix} x_2&0\\0&y_2 \end{bmatrix} =
			\begin{bmatrix} x_1+x_2&y_1\\z_1&y_2 \end{bmatrix},
			$$
			y esto se cumple  tomando $x_1 = a, x_2 =0,  y_1 = b, z_1= c, y_2 =d$.
			
			Por otro lado
			\begin{align*}
				W_1 \cap W_2 &= \left\{\begin{bmatrix} a&b\\c&d\end{bmatrix}:  \begin{bmatrix} a&b\\c&d\end{bmatrix} \in W_1, \begin{bmatrix} a&b\\c&d\end{bmatrix} \in W_2\right\} \\
				&= \left\{\begin{bmatrix} a&b\\c&d\end{bmatrix}:  \begin{matrix}
				(a =x_1, b = y_1, c = z_1,d =0)\wedge \\(a = x_2, b=c=0, d= y_2)
				\end{matrix} \right\} \\
				&= \left\{\begin{bmatrix} a&0\\0&0\end{bmatrix}:  a \in \C\right\} .
			\end{align*}
		\end{enumerate}
	\end{ejemplo}

\vskip .3cm

	\begin{definicion}\label{espacio-fila-columna}
		Sea $A = [a_{ij}] \in M_{m \times n}(\K)$. El \textit{vector fila $i$} es el vector  $(a_{i1},\ldots,a_{in}) \in \K^n$. El \textit{espacio fila} de $A$ es el subespacio de $\K^n$ generado por los $m$ vectores fila de $A$.  De forma análoga, se define  el vector columna $i$ al vector $(a_{11},\ldots,a_{mi}) \in \K^m$ y  el \textit{espacio columna} de $A$ es el subespacio de $\K^m$ generado por los $n$ vectores columna de $A$.  
	\end{definicion}

	\begin{ejemplo}
		Sea
		$$
		A = \begin{bmatrix}
		1&2&0&3&0\\ 0&0&1&4&0 \\0&0&0&0&1
		\end{bmatrix},
		$$
		entonces, como vimos en el ejemplo \ref{ejemplos2}(\ref{ejemplos2-1}), el espacio fila es
		\begin{equation*}
		W = \left\{(x_1,x_2,x_3,x_4,x_5)\in \C^5: x_2 = 2x_1, x_4 = 3x_1+4x_3 \right\}.
		\end{equation*}
	\end{ejemplo}
\end{section}		
		
\begin{section}{Bases y dimensión}
	\begin{definicion}
		Sea $V$ un espacio vectorial sobre $\K$. Un subconjunto $S$ de $V$ se dice \textit{linealmente dependiente}\index{linealmente dependiente} (o simplemente, \textit{dependiente}) si existen vectores distintos $v_1,\ldots,v_n \in S$  y escalares $\lambda_1,\ldots,\lambda_n$ de $\K$, no todos nulos, tales que 	
		\begin{equation*}
			\lambda_1v_1+\cdots+\lambda_nv_n=0.
		\end{equation*}
		Un conjunto que no es linealmente dependiente se dice \textit{linealmente independiente}\index{linealmente independiente} (o simplemente, \textit{independiente}).
		
		Si el conjunto $S$ tiene solo un número finito de vectores $v_1,\ldots,v_n$, se dice,
		a veces, que los $v_1,\ldots,v_n$ son dependientes (o independientes), en vez de decir
		que $S$ es dependiente (o independiente).
		
		Cuando un conjunto $S$ es dependiente (independiente), diremos,  para simplificar,  que $S$ es LD (resp. LI).
	\end{definicion}

\begin{observacion} Por definición, un conjunto $S = \left\{v_1,\ldots,v_n \right\}$ es independiente si se cumple cualquiera de las dos afirmaciones siguientes:
	\begin{enumerate}[label=(\alph*),ref=\alph*]
		\item\label{li1} dados $\lambda_1,\ldots,\lambda_n$ en $\K$ tal que $\lambda_i \ne 0$ para algún $i$,  entonces  $\lambda_1v_1+\cdots+\lambda_nv_n\not=0$, o ,
		\item \label{li2}dados $\lambda_1,\ldots,\lambda_n$ en $\K$ tal que $\lambda_1v_1+\cdots+\lambda_nv_n=0$,  entonces $0=\lambda_1=\cdots=\lambda_n$
	\end{enumerate}
	El enunciado (\ref{li1}) se deduce intuitivamente negando  la definición de linealmente dependiente y el resultado (\ref{li2}) es el contrarrecíproco  de (\ref{li1}).
	
	
Para los interesados, esto es un ejercicio de lógica: ser linealmente dependiente se puede enunciar 
$$
(\exists \lambda_1,\ldots,\lambda_n: (\exists i: \lambda_i \ne 0) \wedge (\lambda_1v_1+\cdots+\lambda_nv_n=0 )),
$$
luego, su negación  (es decir ser LI), es 
$$
(\forall \lambda_1,\ldots,\lambda_n:  (\exists i: \lambda_i \ne 0) \Rightarrow \lambda_1v_1+\cdots+\lambda_nv_n\ne 0),
$$
(observar  que $\neg(P \wedge Q) \,\equiv\, \neg P \vee \neg Q \,\equiv\, P \Rightarrow \neg Q$). 

Como $(P \Rightarrow Q) \equiv (\neg Q \Rightarrow \neg P)$, la propiedad anterior  es equivalente a
$$
(\forall \lambda_1,\ldots,\lambda_n:  (\lambda_1v_1+\cdots+\lambda_nv_n=0 ) \Rightarrow (\forall i: \lambda_i = 0)),
$$	
	
\end{observacion}

\vskip .5cm

Las siguientes afirmaciones son consecuencias fáciles de la definición.
\begin{enumerate}
	\item Todo conjunto que contiene un conjunto linealmente dependiente es linealmente dependiente.
	\item  Todo subconjunto de un conjunto linealmente independiente es linealmente independiente.
	\item  Todo conjunto que contiene el vector $0$ es linealmente dependiente; en efecto, $1.0 = 0$.
\end{enumerate}

\begin{ejemplo}
	En $\R^3$  los vectores $(1,-1,1)$ y $(-1,1,1)$ son LI, pues si $\lambda_1(1,-1,1)+\lambda_2(-1,1,1) =0$,  entonces $0= (\lambda_1,-\lambda_1,\lambda_1)+(-\lambda_2,\lambda_2,\lambda_2) =  (\lambda_1-\lambda_2,-\lambda_1+\lambda_2,\lambda_1+\lambda_2)$, y esto es cierto si 
	\begin{equation*}
		\begin{array}{rcl}
		\lambda_1-\lambda_2 &=& 0 \\
		-\lambda_1+\lambda_2 &=& 0 \\
		\lambda_1+\lambda_2 &=& 0 
		\end{array}.
	\end{equation*} 
	Luego $\lambda_1 = \lambda_2$ y $\lambda_1 = -\lambda_2$, por lo tanto $\lambda_1 = \lambda_2 =0$. Es decir,  hemos visto que 
	$$
	\lambda_1(1,-1,1)+\lambda_2(-1,1,1) =0 \quad \Rightarrow \quad\lambda_1 = \lambda_2 =0,
	$$
	y, por lo tanto,  $(1,-1,1)$ y $(-1,1,1)$ son LI.
\end{ejemplo}

\begin{ejemplo} Sea $\K$  cuerpo. En $\K^3$ los vectores
	\begin{align*}
	v_1 &= (\;\;3,\;0,-3) \\
	v_2 &= (-1,\;1,\;\;2) \\
	v_3 &= (\;\;4,\;2,-2) \\
	v_4 &= (\;\;2,\;1,\,\;\;1)
	\end{align*}
	
	son linealmente dependientes, pues
	$$
	2v_1+2v_2 -v_3 +0.v_4 =0.
	$$
	Por otro lado, los vectores
	\begin{align*}
	e_1 &= (1,0,0) \\
	e_2 &= (0,1,0) \\
	e_3 &= (0,0,1) 
	\end{align*}
	son linealmente independientes.
\end{ejemplo}


\begin{observacion}
	En  general,  en $\K^m$, si queremos determinar si  $v_1,\ldots,v_n$ es LI, planteamos la ecuación  
	\begin{equation*}
	\lambda_1v_1+\cdots+\lambda_nv_n=(0,\ldots,0),
	\end{equation*}
	que, viéndola coordenada a coordenada, es equivalente a un sistema de $m$ ecuaciones lineales con  $n$ incógnitas (que son $\lambda_1,\ldots,\lambda_n$). Si  la única solución es la trivial entonces $v_1,\ldots,v_n$ es LI. Si hay alguna solución no trivial, entonces $v_1,\ldots,v_n$ es LD. 
\end{observacion}
 
 \begin{definicion}
 	Sea $V$ un espacio vectorial. Una \textit{base}\index{base de un espacio vectorial} de $V$ es un conjunto $\mathcal B \subseteq V$ tal que
 	\begin{enumerate}
 		\item $\mathcal B$ genera a $V$, y
 		\item $\mathcal B$ es LI.
 	\end{enumerate}
 	 El espacio $V$ es de \textit{dimensión finita} si tiene una base finita,  es decir con  un número finito de elementos.
 \end{definicion}



\begin{ejemplo}[{\sc Base canónica de $\K^n$}] Sea el espacio vectorial $\K^n$ y sean
	\begin{equation*}
	\begin{array}{rcl}
	e_1 &=& (1,0,0,\ldots,0) \\
	e_2 &=& (0,1,0,\ldots,0) \\
	&&\qquad.\,.\,.\,.\,.\,.\,\\ 
	e_n&=& (0,0,0,\ldots,1)
	\end{array}
	\end{equation*}
	($e_i$ es el vector con todas sus coordenadas iguales a cero,  excepto  la coordenada $i$ que vale 1). Entonces veamos que  $e_1,\ldots,e_n$ es una base de $\K^n$.
	\begin{enumerate}
		\item Si $(x_1,\ldots,x_n) \in \K^n$,  entonces
		$$
		(x_1,\ldots,x_n) = x_1e_1+\cdots+x_ne_n.
		$$
		Por lo tanto, $e_1,\ldots,e_n$ genera a  $\K^n$.
		\item Si 
		$$
		x_1e_1+\cdots+x_ne_n =0,
		$$
		entonces
		\begin{align*}
			(0,\ldots,0) &= x_1(1,0,\ldots,0)+ x_2(0,1,\ldots,0)+\cdots+x_n(0,0,\ldots,1)\\ 
			&=  (x_1,0,\ldots,0)+(0,x_2,\ldots,0)+\cdots+(0,0,\ldots,x_n)\\ &= (x_1,x_2,\ldots,x_n).
		\end{align*}
		Luego, $x_1= x_2=\cdots=x_n =0$ y por lo tanto $e_1,\ldots,e_n$ es LI.
		
		Para $1 \le i \le n$, al vector $e_i$ se lo denomina el \textit{$i$-ésimo vector canónico}  y a la base $\mathcal B_n = \left\{e_1,\ldots,e_n \right\}$ se la denomina la \textit{base canónica}\index{base canónica} de $\K^n$. 
		
		
	\end{enumerate}
\end{ejemplo}
 
 
 \begin{ejemplo}
 	Sea $P$ una matriz $n \times n$ invertible con elementos en el cuerpo $\K$. Entonces si $C_1,\ldots,C_n$ son los vectores columna de $P$ (ver definición  \ref{espacio-fila-columna}), estos forman una base de $\K^n$. Eso se verá como sigue. Si $X = (x_1,\ldots,x_n) \in \K^n$, lo podemos ver como columna y 
 	$$
 	PX=x_1C_1+\cdots+x_nC_n.
 	$$
 	Como $PX=0$ tiene solo la solución trivial $X= 0$, se sigue que $\{C_1,\ldots,C_n\}$ es un conjunto linealmente independiente. ¿Por qué generan $\K^n$? Sea $Y \in \K^n$, si $X = P^{-1} Y$, entonces $Y = PX$, esto es
 	$$
 	Y=x_1C_1+\cdots+x_nC_n.
 	$$
 	Así, $\{C_1,\ldots,C_n\}$ es una base de $\K^n$.
 \end{ejemplo}


\begin{ejemplo}
	Sea $\K_n[x]$  el conjunto de polinomios de grado menor  que $n$ con coeficientes en $\K$:
	$$
	\K_n[x] = \left\{a_0 + a_1 x + a_2x^2+\cdots+a_{n-1}x^{n-1}: a_0,\ldots,a_{n-1} \in \K  \right\}.
	$$
	Entonces $1,x,x^2,\ldots,x^n$  es una base de $\K_n[x]$. Es claro que los $1,x,x^2,\ldots,x^n$ generan $\K_n[x]$. Por otro lado, si  $\lambda_0 + \lambda_1 x + \lambda_2x^2+\cdots+\lambda_{n-1}x^{n-1} =0$, tenemos que $\lambda_0=\lambda_1 = \lambda_2 =\cdots =\lambda_{n-1} =0$.
\end{ejemplo}

\begin{ejemplo}[{\sc Base canónica de $M_{m \times n}(\K)$}] 
	Sean $1 \le i \le m$, $1\le j \le m$ y $E_{ij} \in M_{m \times n}(\K)$ definida por
	\begin{equation*}
		[E_{ij}]_{kl} = \left\{ 
		\begin{array}{lll}
		1& &\text{si $i=k$ y $j=l$,} \\
		0& &\text{otro caso}. 
		\end{array}
		 \right.
	\end{equation*}
	Es decir $E_{ij}$  es la matriz cuyas entradas son todas iguales a 0,  excepto la entrada $ij$ que vale 1. En el caso $2 \times 2$  tenemos la matrices
	\begin{equation*}
		E_{11} = \begin{bmatrix} 1&0\\0&0\end{bmatrix}, \quad
		E_{12} = \begin{bmatrix} 0&1\\0&0\end{bmatrix}, \quad
		E_{21} = \begin{bmatrix} 0&0\\1&0\end{bmatrix}, \quad
		E_{22} = \begin{bmatrix} 0&0\\0&1\end{bmatrix}.
	\end{equation*}
	Volviendo al caso general,  es claro que si $A= [a_{ij}] \in M_{m \times n}(\K)$,  entonces
	\begin{equation}\label{base-canonica-matrices}
		A = \sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}E_{ij},
	\end{equation}
	luego $\{E_{ij} \}_{1 \le i \le m, 1\le j \le n}$ genera $M_{m \times n}(\K)$. También, por la ecuación (\ref{base-canonica-matrices}), es claro que si $\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}E_{ij}=0$,  entonces $a_{ij}=0$ para todo $i$ y $j$. Luego,  $\{E_{ij} \}_{1 \le i \le m, 1\le j \le n}$ es LI. 
	
	Concluyendo,  $\{E_{ij} \}_{1 \le i \le m, 1\le j \le n}$ es una base de  $M_{m \times n}(\K)$ y se la denomina la \textit{base canónica} de  $M_{m \times n}(\K)$.
\end{ejemplo}



Si $S$ es un conjunto finito denotemos $|S|$  al \textit{cardinal} de  $S$ es decir, la cantidad de elementos de $S$. 

\begin{teorema}\label{indep-menorigual-gen}
	Sea $V$ un espacio vectorial generado por un conjunto finito de vectores $w_1,\ldots,w_m$. Entonces todo conjunto independiente de vectores de $V$ es finito y  contiene a lo más $m$ elementos. 
\end{teorema}
\begin{proof} Sea $V = \la w_1,\ldots,w_m\ra$ y  $S \subset V$.   El  enunciado del teorema es equivalente a decir:
	$$
	\text{si }S \text{ es LI } \Rightarrow |S| \le m.
	$$
	Para demostrar este teorema es suficiente probar el contrarrecíproco del enunciado, es decir:
	$$
	\text{si }|S| > m \Rightarrow S \text{ es LD},
	$$
	o, dicho  de otra forma, todo subconjunto $S$ de $V$ que contiene más de $m$ vectores es linealmente dependiente. Sea $S$ un tal conjunto,  entonces $S = \{v_1,\ldots,v_n\}$ con $n >m$.  Como  $w_1,\ldots,w_m$ generan $V$, existen escalares $a_{ij}$ en $\K$ tales que
	\begin{equation*}
		v_j = \sum_{i=1}^{m}a_{ij}w_i, \qquad (1 \le j \le n).
	\end{equation*}
	Probaremos ahora que existen $x_1,\ldots,x_n \in \K$ no todos nulos, tal que $x_1v_1 + \cdots+x_nv_n =0$. Ahora bien, para cualesquiera $x_1,\ldots,x_n \in \K$ tenemos
	\begin{align*}
		x_1v_1 + \cdots+x_nv_n &= \sum_{j=1}^{n} x_jv_j& \\
		& = \sum_{j=1}^{n}x_j \sum_{i=1}^{m}a_{ij}w_i& \\
		& = \sum_{j=1}^{n} \sum_{i=1}^{m}(x_ja_{ij})w_i& \\ 
		& = \sum_{i=1}^{m}(\sum_{j=1}^{n} x_ja_{ij})w_i.&  (*)
	\end{align*}
	El sistema de ecuaciones
	\begin{equation*}
		\sum_{j=1}^{n} x_ja_{ij} = 0, \qquad (1 \le i \le m) 
	\end{equation*}
	tiene $m$ ecuaciones  y $n > m$ incógnitas, luego, por el teorema \ref{soluciones-m-menor-n}, existen escalares $x_1,\ldots,x_n \in \K$ no todos nulos, tal que $\sum_{j=1}^{n} x_ja_{ij} = 0$, ($1 \le i \le m$) y, por $(*)$, tenemos que  $x_1v_1 + \cdots+x_nv_n =0$. Esto quiere decir que los $v_1,\ldots,v_n$ son LD.
\end{proof}


\begin{corolario}
	Si $V$ es un espacio vectorial de dimensión finita, entonces dos bases cualesquiera de $V$ tienen el mismo número de elementos.
\end{corolario}
\begin{proof}
		Como $V$ es de dimensión finita, tiene una base finita $\mathcal B$  de $m$ vectores, es decir,  $\mathcal B$ es base de $V$ y  $|\mathcal B| = m.$
		Sea $\mathcal B'$  otra base de $V$, como $\mathcal B$  genera $V$ y  $\mathcal B'$ es  un conjunto LI, entonces, por el teorema anterior, $|\mathcal B'| \le m$. Sea $n = |\mathcal B'|$,  entonces $n \le m$. Por otro lado $\mathcal B'$ es base y, por lo tanto,  genera $V$ y  $\mathcal B$ es LI, luego, por el teorema anterior nuevamente,  $m \le n$, y en consecuencia $m=n$.
\end{proof}

\begin{definicion}
	Sea $V$ espacio vectorial de dimensión finita. Diremos que $n$  es \textit{la dimensión de $V$}\index{dimensión de un espacio vectorial} y  denotaremos $\dim V =n$,  si existe una base de $V$  de $n$  vectores. Si $V = \{0\}$,  entonces definimos $\dim V =0$.
\end{definicion}


Como ya demostramos, si $V$  es un espacio vectorial de dimensión finita y $\mathcal B,\mathcal B' $ dos bases de $V$,  entonces $|\mathcal B|=|\mathcal B'|$.



\begin{ejemplo} Sean $m,n \in \mathbb N$. 
	\begin{enumerate}
		\item $\dim \K^n= n$, pues  la base canónica tiene $n$ elementos.
		\item $\dim M_{m \times n}(\K) = mn$, pues la base canónica de $M_{m \times n}(\K)$ tiene $mn$  elementos.  
		\item $\dim \K_n[x] =n$, pues $1,x,x^2,\ldots,x^{n-1}$ es una base.   
	\end{enumerate}
\end{ejemplo}
		
		
		\begin{corolario}
			Sea $V$ un espacio vectorial de dimensión finita y sea $n = \dim V$. 
			Entonces
			\begin{enumerate}
				\item cualquier subconjunto de $V$ con más de $n$ vectores es linealmente
				dependiente;
				\item ningún subconjunto de $V$ con menos de $n$ vectores puede generar $V$.
			\end{enumerate} 
		\end{corolario}
	\begin{proof}
		\begin{enumerate}
			\item Sea $\{v_1,\ldots,v_n\}$ una base de $V$, entonces $v_1,\ldots,v_n$ generan $V$, luego, por el teorema \ref{indep-menorigual-gen}, cualquier subconjunto de $V$ que contenga más de $n$ vectores es LD.
			\item Sea $S$  subconjunto de $V$ con $m < n$ vectores. Si $S$  genera $V$,  entonces todo subconjunto de más de $m$  vectores es LD (teorema \ref{indep-menorigual-gen}), por lo tanto, un subconjunto de $n$ vectores es LD. En  consecuencia, no puede haber una base de $n$  elementos, lo cual contradice la hipótesis.
		\end{enumerate} 
	\end{proof}

	\begin{lema}\label{li+vec=li}
		Sea $S$ un subconjunto linealmente independiente de un espacio vectorial $V$. Supóngase que $w$ es un vector de $V$ que no pertenece al subespacio generado por $S$. Entonces el conjunto que se obtiene agregando $w$  a $S$, es linealmente independiente.
	\end{lema}
	\begin{proof}
		Supóngase que $v_1,\ldots,v_n$  son vectores distintos de $S$ y sean $\lambda_i,\lambda \in \K$  tales que
		\begin{equation}\label{eq-dep-lin}
			\lambda_1 v_1 + \cdots + \lambda_n v_n + \lambda w =0 .
		\end{equation}
		Debemos probar que $\lambda_i=0$, $1 \le i \le n$, y $\lambda =0$.
		Supongamos que $\lambda \ne 0$, entonces podemos dividir la ecuación por $\lambda$ y haciendo  pasaje de término  obtenemos
		$$
		w = \left(-\frac{\lambda_1}{\lambda}\right) v_1 + \cdots   \left(-\frac{\lambda_n}{\lambda}\right) v_n.
		$$
		Luego $w$ estaría  en el subespacio generado por $S$, lo cual contradice la hipótesis. 
		
		Por  lo tanto $\lambda =0$ y, en consecuencia  
		$$
		\lambda_1 v_1 + \cdots + \lambda_n v_n =0.
		$$ 
		Como $S$ es un conjunto linealmente independiente, todo $\lambda_i = 0$. 
	\end{proof}



	\begin{teorema}\label{completar-bases}
	Sea $V$ espacio vectorial de dimensión finita $n$ y $S_0$ un subconjunto LI de $V$. Entonces $S_0$  es finito  y existen $w_1,\ldots,w_m$ vectores en  $V$ tal que  $S_0 \cup \{w_1,\ldots,w_m\}$ es una base de $V$. 
\end{teorema}
\begin{proof}
	Se extiende $S_0$ a una base de $V$, como sigue. Si $S_0$ genera $V$, entonces $S_0$ es una base de $V$ y está demostrado. Si $S_0$ no genera $V$, por el lema anterior se halla un vector $w_1$ en $V$ tal que el conjunto $S_1 = S_0 \cup \{v_1\}$ es independiente.			
	Si $S_1$ genera $V$, está demostrado. Si no, se aplica el lema para obtener un vector $w_2$ en $V$ tal que el conjunto $S_2 = S_1 \cup \{w_2\} = S_0 \cup \{w_1,w_2\}$ es independiente. Si se continúa de este modo, entonces (y en no más de $\dim V$ de etapas) se llega a un conjunto 
	$$
	S_m =  S_0 \cup \{w_1,\ldots,w_m\}
	$$
	que es independiente y que genera $V$ (si no, continuamos), por lo tanto $S_m$ es base de $V$. 
\end{proof}

Es decir, todo subconjunto LI de un espacio vectorial de dimensión finita se puede completar a una base.  

	\begin{corolario}
		Sea $W$ es un subespacio de un espacio vectorial con de dimensión finita $n$ y $S_0$ un subconjunto LI de $W$. Entonces, $S_0$ se puede completar a una base de $W$. 
	\end{corolario}
	\begin{proof}
		Como $S_0$ es un conjunto linealmente independiente de $W$, entonces $S_0$ es también un subconjunto linealmente independiente de $V$; como $V$ es de dimensión finita, $S_0$ no tiene más de $n$ elementos y por lo tanto es finito.
		
		Como $W$ es un espacio vectorial, aplicando el teorema anterior completamos a una base de $W$. 
	\end{proof}

	\begin{corolario}
		Sea $V$ espacio vectorial de dimensión finita y $V \ne \{0\}$, entonces $\dim V >0$.
	\end{corolario}
	\begin{proof}
		Como $V \ne \{0\}$,  existe $v \in V$ con $v \ne 0$. Entonces, $S_0 = \{v\}$ es LI, pues $\lambda v =0 \Rightarrow \lambda =0$. Por el teorema anterior, $S_0$ se extiende a una base $\mathcal B$. Como $|\mathcal B| \ge |S_0| =1$, tenemos que $\dim V >0$.    
	\end{proof}
	
	\begin{corolario}\label{dimw-menor-dimv}
		Si $W$ es un subespacio propio de un espacio vectorial de dimensión finita $V$, entonces $W$ es de dimensión finita y  $\dim W < \dim V$.
	\end{corolario}
	\begin{proof} Si $W = \{0\}$, entonces $\dim W = 0$,  como $W \subsetneq V$,  tenemos que $V$  es no nulo y por lo tanto $\dim W = 0 < \dim V$. 
		
		Si $W \ne \{0\}$,  sea $w \in W$, $w\not=0$. Por el teorema anterior existe una base $\mathcal B$ de $W$ que contiene a $w$. Como $\mathcal B$ es un conjunto LI en $W$, lo es también en $V$ y por teorema  \ref{indep-menorigual-gen}, $ \dim W = |\mathcal B| \le \dim V$.
		
		Como $W$ es un subespacio propio de $V$ existe un vector $v$ en $V$ que
		no está en $W$. Agregando $v$ a cualquier base de $W$ se obtiene un subconjunto
		linealmente a independiente de $V$ (lema \ref{li+vec=li}). Así, $\dim W < \dim V$.
	\end{proof}

	\begin{comment}
	
	\begin{corolario}
	 En un espacio vectorial V de dimensión finita todo conjunto linealmente independiente de vectores es parte de una base.
	\end{corolario}
	\begin{proof}
		Sea $S$ un conjunto LI en $V$, por el teorema \ref{completar-bases}, existen $v_1,\ldots,v_m$ tal que  $S \cup \{v_1,\ldots,v_m\}$ es una base de $V$. 
	\end{proof}

	\begin{corolario}
		Sea A una matriz $n \times n$ sobre el cuerpo $\K$, y supóngase que los vectores fila de $A$ forman un conjunto linealmente independiente de vectores de $\K^n$. Entonces $A$ es invertible.
	\end{corolario}
	\begin{proof}
		Sean $v_1,\ldots,v_n$ los  vectores fila de $A$, y sea $W = \langle v_1,\ldots,v_n \rangle$.  Como los $v_1,\ldots,v_n$ son linealmente independientes, la dimensión de $W$ es $n$. Por el corolario  \ref{dimw-menor-dimv} se tiene 	que $W$ no es propio y por lo tanto $W = \K^n$.  Luego existen escalares $b_{ij}$ en $\K$ tales que
		$$
		e_i = \sum_{j=1}^{n} b_{ij} v_j, \qquad 1 \le i \le n
		$$
		donde $\{e_1, e_2,\ldots ,e_n\}$ es la base canónica de $\K^n$. 
		Como $v_j= (a_{j1},\ldots,a_{jn})$,  tenemos
		\begin{align*}
			e_i &= \sum_{j=1}^{n} b_{ij} (a_{j1},\ldots,a_{jn}) \\
			&=  (\sum_{j=1}^{n} b_{ij}a_{j1},\ldots,\sum_{j=1}^{n} b_{ij}a_{jn}).
		\end{align*}
		
		Así, para la matriz $B= [b_{ij}]$ se tiene
		$$
		BA=Id.
		$$
	\end{proof}
	
	\end{comment}

Hemos visto que si  $V$  es un espacio de dimensión finita,  entonces todo conjunto LI se puede extender a una base. Veremos ahora que dado un conjunto finito de generadores,  existe un subconjunto que es una base. 


\begin{teorema}\label{gen->base}
	Sea $V \ne 0$ espacio vectorial y $S$ un conjunto finito de generadores de $V$,  entonces existe un subconjunto $\mathcal B$  de $S$ que es una base.  
\end{teorema} 
\begin{proof}
	Sea
	$$
	C = \{|R|: R \subseteq S \;\wedge\; R \text{ es LI}\}.
	$$
	Como $V$ no es nulo  y $S$ genera $V$, $C \ne \emptyset$. $C$  es un subconjunto no vacío de  $\mathbb N$,  acotado superiormente por $|S|$ y por lo tanto tiene máximo. Sea $n$ el máximo de $C$ y sea $\mathcal B \subseteq S$ tal que $|\mathcal B| =n$  y $\mathcal B$ es LI. Veremos que $\mathcal B$  es una base. Para ello, como $\mathcal B $ es LI, sólo falta ver que $\mathcal B$ genera a $V$. 
	
	Supongamos que existe $v \in S$ tal que $ v \not\in \langle \mathcal B \rangle$. Por el lema \ref{li+vec=li},  entonces  $\mathcal B \cup \{v\}$ es LI y este subconjunto LI de $S$ tiene $n+1$  elementos, lo cual contradice la maximalidad de $n$. Es claro entonces, que $v \in S \Rightarrow v \in \mathcal B$,  es decir $S \subset \langle B \rangle$. Como $S \subset \langle B \rangle$,  entonces
	$V = \langle S \rangle \subset \langle B \rangle$, es decir $V =  \langle B \rangle$.
\end{proof}


	\begin{teorema}
		Si $W_1$, y $W_2$ son subespacios de dimensión finita de un espacio vectorial, entonces $W_1+ W_2$ es de dimensión finita y 
		$$\dim W_1 + \dim W_2 = \dim (W_1 \cap W_2) + \dim (W_1 + W_2).$$
	\end{teorema}
	\begin{proof}
		 El  conjunto $W_1 \cap W_2$  es un subespacio de $W_1$ y $W_2$ y por lo tanto un espacio vectorial de dimensión finita. Sea  $u_1,\ldots,u_k$ una base de $W_1 \cap W_2$, por el teorema \ref{completar-bases},  existen $v_1,\ldots,v_n$ vectores en $W_1$ y $w_1,\ldots,w_m$ vectores en $W_2$  tal que
		 $$
		 \{u_1,\ldots,u_k,v_1,\ldots,v_n\}\quad  \text{es una base de $W_1$,}
		 $$
		  y
		  $$
		 \{u_1,\ldots,u_k,w_1,\ldots,w_m\}\quad \text{es una base de $W_2$.}
		 $$
		Es claro que, el subespacio $W_1+ W_2$ es generado por los vectores 
		$$
		u_1,\ldots,u_k,v_1,\ldots,v_n,w_1,\ldots,w_m.
		$$
		Veamos que estos  vectores forman un conjunto independiente. En efecto, supóngase que
		\begin{equation}\label{tres-sumas}
			\sum \lambda_i u_i + \sum \gamma_i v_i + \sum \mu_i w_i =0,
		\end{equation}
		luego
		$$
		\sum \mu_i w_i = - \sum \lambda_i u_i - \sum \gamma_i v_i.
		$$
		Por  lo tanto, $\sum \mu_i w_i \in (W_1 \cap W_2) + W_1 = W_1$. Es  decir,  $\sum \mu_i w_i \in W_2$ y $\sum \mu_i w_i \in W_1$, por lo tanto $\sum \mu_i w_i \in (W_1 \cap W_2)$, y entonces
		$$
		\sum \mu_i w_i  = \sum \alpha_i u_i \Rightarrow 0 = \sum \alpha_i u_i - \sum \mu_i w_i .
		$$
		Como $\{u_1,\ldots,u_k,w_1,\ldots,w_m\}$ es una base y por lo tanto LI, tenemos que $0=\alpha_i=\mu_j$, para todo $i,j$. Por lo tanto, por (\ref{tres-sumas}), 
		\begin{equation}\label{dos-sumas}
		\sum \lambda_i u_i + \sum \gamma_i v_i  =0.
		\end{equation}
		Como $ \{u_1,\ldots,u_k,v_1,\ldots,v_n\}$  es una base de $W_1$,  tenemos que también $0=\lambda_i = \gamma_j$ para todo $i,j$. Luego  $0=\lambda_i = \gamma_j = \mu_r$, para cualesquiera $i,j,r$ y por lo tanto $u_1,\ldots,u_k,v_1,\ldots,v_n,w_1,\ldots,w_m$ es LI y  como  generaban a  $W_1+W_2$ resultan ser una base de $W_1+W_2$, por lo tanto $\dim (W_1+W_2) = k+n+m$.
		
		Finalmente,
		\begin{align*}
			\dim W_1 + \dim W_2&= (k+n)+(k+m) \\&	=k+(k+n+m)\\ &=  \dim (W_1 \cap W_2) + \dim (W_1 + W_2) .
		\end{align*}
	\end{proof}
	\end{section}



	
	\begin{section}{Dimensiones de subespacios}\label{sec-dimensiones-de-subespacios}
		
			
		Dada $A \in M_{m\times n}(\K)$,  ya hemos visto que  las soluciones del sistema $AX=0$ forman un subespacio vectorial. Sea $R $ la MERF equivalente por filas a $A$ y $r$ la cantidad de filas no nulas de $R$. Ahora bien, cada fila no nula está asociada  a una  variable principal y las  $n-r$ variables restantes son variables libres  que generan  todas las soluciones.
		El  hecho de que tenemos $n-r$ variables libres no dice que hay $n-r$ vectores LI que generan $W$, y por lo tanto,  $\dim W = n-r$. Esto lo veremos en ejemplos.
		
		\begin{ejemplo}
			Encontrar una base del subespacio 
			$$
			W = \left\{(x,y,z,w) \in \mathbb{R}: \quad\begin{array}{rcl}
			x-y -3z +\;\;w &=& 0 \\ y +5z +3w &=& 0
			\end{array} \right\}.
			$$
		\end{ejemplo}
		\begin{proof}[Solución]
			$W$  está definido implícitamente y usando el método de Gauss podemos describirlo paramétricamente, pues:
			\begin{equation*}
			\begin{bmatrix}1&-1&-3&1 \\ 0&1&5&3  \end{bmatrix}
			\stackrel{F_1+F_2}{\longrightarrow} 
			\begin{bmatrix}1&0&2&4 \\ 0&1&5&3  \end{bmatrix}.
			\end{equation*}
			Por lo tanto, el sistema de ecuaciones que define $W$ es equivalente a 
			\begin{equation*}
			\begin{array}{rcl}
			x  +2z +4w &=& 0 \\ y +5z +3w &=& 0,
			\end{array}
			\end{equation*}
			es decir 
			\begin{equation*}
			\begin{array}{rcl}
			x  &=& -2z - 4w  \\ y &=& -5z -3w ,
			\end{array}
			\end{equation*}
			y por lo tanto
			\begin{align*}
			W &= \left\{(-2z -4w,-5z -3w,z,w) : z,w\in \mathbb{R} \right\} \\
			&= \left\{(-2,-5,1,0)z+(-4, -3,0,1)w : z,w\in \mathbb{R} \right\}\\
			&= \langle (-2,-5,1,0),(-4, -3,0,1)\rangle.
			\end{align*}
			Concluimos entonces que $(-2,-5,1,0),(-4, -3,0,1)$  es una base de $W$ y, por lo tanto,  su dimensión es 2.
		\end{proof}
		
		\vskip .5cm 
		
	
		
		\begin{teorema}
			Sean $A$ matriz $m \times n$ con coeficientes en $\K$, $P$ matriz $m\times m$ invertible y $B =PA$. Entonces el el espacio fila de $A$ es igual al espacio fila de $B$.
		\end{teorema}
		\begin{proof}
			Sea $A= [a_{ij}]$, $P =[p_{ij}]$ y $B = [b_{ij}]$. Como  $B= PA$, tenemos que la fila $i$ de $B$ es
			\begin{align*}
				(b_{i1},\ldots,b_{in})&= (F_i(P).C_1(A),\ldots,F_i(P).C_n(A)) \\
				&= (\sum_{j=1}^{m} p_{ij}a_{j1}, \ldots, \sum_{j=1}^{m} p_{ij}a_{jn}) \\
				&= \sum_{j=1}^{m} p_{ij}(a_{j1}, \ldots,a_{jn}).
			\end{align*}
		Luego, cada vector fila de $B$ se puede obtener como combinación lineal de los vectores fila de $A$, y por lo tanto el espacio fila de $B$ está incluido en el espacio fila de $A$. 
		
		Ahora bien, como $P$ invertible, podemos multiplicar por $P^{-1}$ a izquierda la fórmula $B= PA$, y obtenemos $P^{-1}B = P^{-1}P A = A$. Haciendo el mismo razonamiento que arriba concluimos que  también el espacio fila de $A$ está incluido en el espacio fila de $B$ y por lo tanto son iguales. 
		\end{proof}
	
	\begin{corolario}\label{subesp-merf}
			Sean $A$ matriz $m \times n$ y $R$ la MERF equivalente por filas a $A$. Entonces, el espacio fila de $A$ es igual al espacio fila de $R$ y las filas no nulas de $R$ forman una base del espacio fila de $A$. 
	\end{corolario}
	\begin{proof}
		$R=PA$, donde $P$ es una matriz $m \times m$ invertible, luego, por el teorema anterior, el espacio fila de $A$  es igual al espacio fila de $R$. Calculemos ahora cual es la dimensión del espacio fila de $R$. Veamos que filas no nulas de $R$ son LI. 
		
		Recordemos  que por definición de MERF cada fila no nula comienza con un 1 y en esa coordenada  todas las demás filas tienen un 0, por lo tanto una combinación lineal no trivial resulta en un vector no nulo: si $v$ es una fila no nula de $R$, con el 1 principal en la coordinada $i$ y $\lambda \ne0$,  entonces $\lambda v$ vale $\lambda$ en la posición $i$ y esta coordenada no puede ser anulada por la combinación de otras filas.  
	\end{proof}

	\begin{corolario}\label{inv-impl-filasgen}
			Sean $A$ matriz $n \times n$. Entonces, $A$ es invertible si y sólo si las filas de $A$ son una base de $\K^n$.
	\end{corolario}
	\begin{proof}
		Si $A$ es invertible entonces la MERF de $A$ es la identidad, por lo tanto  el espacio fila de $A$ genera $\K^n$.
		
		Por otro lado, si el espacio fila de $A$  genera $\K^n$, el espacio fila de  la  MERF es $\K^n$ y por lo tanto  la MERF de $A$ es la identidad y en consecuencia $A$ es invertible.
		
		Hemos probado que $A$ es invertible si y sólo si las $n$ filas de $A$ generan $\K^n$. Como $\dim \K^n = n$,  todo conjunto de $n$ generadores es una base. 
	\end{proof}
	
	\vskip .5cm
	
	El  corolario  \ref{subesp-merf} nos provee un método para encontrar una base de un  subespacio de $\K^n$ generado por $m$ vectores: si $v_1,\ldots,v_m \in \K^n$ y $W = \langle v_1,\ldots,v_m\rangle$, consideramos la matriz 
	$$
	A = \begin{bmatrix}
	v_1 \\ v_2 \\ \vdots \\ v_m
	\end{bmatrix}
	$$
	donde las filas son los vectores $v_1,\ldots,v_m$. Luego calculamos $R$, la MERF equivalente por filas a $A$, y si $R$ tiene $r$ filas no nulas, las $r$ filas no nulas son una base de $W$ y, por consiguiente, $\dim W = r$. 
	
	\begin{ejemplo}\label{ej-4.5}
		Encontrar una base  de $W= \langle (1,0,1), (1,-1,0), (5,-3,2)\rangle$. 
	\end{ejemplo}
	\begin{proof}[Solución]
		Formemos la matriz cuyas filas son los vectores que generan $W$,  es decir 
		$$
		A = \begin{bmatrix} 1&0&1 \\ 1&-1&0 \\ 5&-3&2 \end{bmatrix}.
		$$
		Entonces
		\begin{equation*}
		\begin{bmatrix}1&0&1 \\ 1&-1&0 \\ 5&-3&2  \end{bmatrix}
		\underset{F_3-5F_1}{\stackrel{F_2- F_1}{\longrightarrow}} 
		\begin{bmatrix}1&0&1 \\ 0&-1&-1 \\ 0&-3&-3\end{bmatrix}
		\stackrel{-F_2}{\longrightarrow} 
		\begin{bmatrix}1&0&1 \\ 0&1&1 \\ 0&-3&-3\end{bmatrix}
		\stackrel{F_3 - 3F_2}{\longrightarrow}
		\begin{bmatrix}1&0&1 \\ 0&1&1 \\ 0&0&0\end{bmatrix}.
		\end{equation*}
		Por lo tanto, $\dim W =2$ y $(1,0,1), (0,1,1)$ es una base de  $W$.
	\end{proof}
	
	El método que nos provee el corolario  \ref{subesp-merf} nos permite encontrar una base de un subespacio vectorial de $\K^n$ a partir de un conjunto de generadores del subespacio. Como vimos en el teorema \ref{gen->base},  en todo conjunto finito de generadores existe un subconjunto que es una base. El siguiente teorema nos permite encontrar uno de tales subconjuntos. 

	
	\begin{teorema}
		Sea $v_1,\ldots, v_r$ vectores en $\K^n$ y $W = \langle  v_1,\ldots, v_r \rangle$. Sea $A$ la matriz formada por las filas $v_1,\ldots, v_r$ y $R$ una MRF equivalente por filas a $A$ que se obtiene sin el uso de permutaciones de filas (ver observación \ref{metodo-merf}). Si $i_1,i_2,\ldots,i_s$ son las filas no nulas de $R$,  entonces $v_{i_1},v_{i_2},\ldots,v_{i_s}$ es una base de $W$.
	\end{teorema}
	\begin{proof}
		Se hará por inducción sobre $r$. 
		
		Si $r = 1$ es trivial ver que vale la afirmación. 
		
		Supongamos que tenemos el resultado probado para $1\le k < r$ (hipótesis inductiva). 		
		Sea $W' = \langle  v_1,\ldots, v_{r-1} \rangle$ y sea $A'$ la matriz formada por las $r-1$ filas $v_1,\ldots, v_{r-1}$. 		
		Sea $R'$ la MRF equivalente por filas a $A'$ que se obtiene sin usar permutaciones de filas. Por hipótesis inductiva, si $i_1,i_2,\ldots,i_s$ son las filas no nulas de $R'$,  entonces $v_{i_1},v_{i_2},\ldots,v_{i_s}$ es una base de $W'$.
		
		Sea
		\begin{equation*}
			R_0 = \begin{bmatrix}
			R' \\ v_r
			\end{bmatrix}
		\end{equation*}
		donde $R'$ es la MRF de $A'$. 
		
		Si $v_r \in W'$, entonces  $v_{i_1},v_{i_2},\ldots,v_{i_s}$ es una base de $W$ y 
		\begin{equation*}
		R = \begin{bmatrix}
		R' \\ 0
		\end{bmatrix}
		\end{equation*}
		es la MRF de $A$.
		
		Si $v_r \not\in W'$, entonces  $v_{i_1},v_{i_2},\ldots,v_{i_s}, v_r$ es una base de $W$ (lema \ref{li+vec=li}) y la MRF de $A$ tiene la última fila no nula.  
	\end{proof}


	\begin{ejemplo}
		Sea $S = \{(1,0,1),(1,-1,0), (5,-3,2)\}$ y $W = \langle S\rangle$. Encontrar una base de $W$ que sea un subconjunto de $S$. 
	\end{ejemplo}
	\begin{proof}[Solución] Hemos visto en el ejemplo \ref{ej-4.5} que una  MRF de $A$  es 
		\begin{equation*}
			\begin{bmatrix} 1&0&1\\0&1&1\\0&0&0 \end{bmatrix},
		\end{equation*}
		y que la misma se obtiene sin usar permutaciones. Esta matriz tiene las dos primeras filas no nulas, por lo tanto, $\{(1,0,1),(1,-1,0)\}$ es una base de $W$.
	\end{proof}

Finalmente, terminaremos esta sección con un teorema que resume algunas equivalencias respecto a matrices invertibles.

\begin{teorema}
	Sea $A$ matriz $n \times n$ con coeficientes en $\K$. Entonces son equivalentes
	\begin{enumerate} 
		\item $A$ es invertible.
		\item $A$  es equivalente por filas a $I_n$.
		\item $A$ es producto de matrices elementales.
		\item El sistema $AX=Y$ tiene una única solución para toda matriz $Y$ de orden $n \times 1$. 
		\item El sistema homogéneo $AX=0$ tiene una única solución trivial.
		\item $\det A \ne 0$.
		\item Las filas de $A$ son LI.
		\item Las columnas de $A$ son LI.
	\end{enumerate}
\end{teorema}
\begin{proof} Por teoremas \ref{mtrx-inv-equiv} y  \ref{mtrx-inv-equiv2}, tenemos que 	(1) $\Leftrightarrow$ 	(2) $\Leftrightarrow$ 	(3) $\Leftrightarrow$ 	(4) $\Leftrightarrow$ 	(5).
	
	(1) $\Leftrightarrow$ (6). Por teorema \ref{mtrx-inv-equiv3}.
	
	(1) $\Leftrightarrow$ (7). Por corolario \ref{inv-impl-filasgen}.
	
	(1) $\Leftrightarrow$ (8). $A$ invertible $\Leftrightarrow$ $A^\t$ invertible $\Leftrightarrow$  las filas de $A^\t$ son LI $\Leftrightarrow$  las columnas de $A$ son LI. 
\end{proof}
	
	\end{section}




	\begin{section}{Coordenadas}
		Una de las características útiles de una base $\mathcal B$ en un espacio vectorial  $V$ de dimensión $n$ es que permite introducir coordenadas en $V$ en forma análoga a las ``coordenadas naturales'', $x_i$, de un vector $v = (x_l,\ldots, x_n)$ en el espacio $\K^n$. En este esquema, las coordenadas de un vector $v$ en $V$, respecto de la base $\mathcal B$, serán los escalares que sirven para expresar $v$ como combinación lineal de los vectores de la base. En  el caso  de la base canónica $e_1,\ldots,e_n$ de $\K^n$ tenemos
		$$
		v = (x_1,\ldots,x_n) = \sum_{i=1}^{n} x_ie_i.
		$$
		por lo tanto $x_i$  es la coordenada $i$-ésima de $v$ respecto a la base canónica. 
		
		En  forma análoga veremos que si $v_1,\ldots,v_n$  es una base de $V$,  entonces existe una única forma de  escribir 
		$$
		v =  \sum_{i=1}^{n} x_iv_i,
		$$ 
		y los valores  $x_i$  serán las \textit{coordenadas de $v$}\index{coordenadas de un vector} en la base dada. 
		
		\begin{definicion}
			Si $V$ es un espacio vectorial de dimensión finita, una \textit{base ordenada}\index{base ordenada} de $V$ es una sucesión finita de vectores linealmente independiente y que genera $V$.
		\end{definicion}
		
		
		La diferencia entre la definición de ``base'' y la de ``base ordenada'',  es que en la última es  importante el orden de los vectores de la base. Si la sucesión $v_1,\ldots,v_n$ es una base ordenada de $V$, entonces el conjunto $\{v_1,\ldots,v_n\}$ es una base de $V$. La base ordenada es el conjunto, juntamente con el orden dado. Se incurrirá en un pequeño abuso de notación y se escribirá
		$$
		\mathcal{B} = \{v_1,\ldots,v_n\}
		$$
		diciendo que $\mathcal{B}$ es una base ordenada de $V$.
		
		\begin{proposicion}
			Sea $V$  espacio vectorial de dimensión finita y sea $\mathcal{B} = \{v_1,\ldots,v_n\}$ una base ordenada de $V$. Entonces, para cada $v \in V$,  existen únicos $x_1,\ldots,x_n \in \K$ tales que $$v =   x_1v_1 + \cdots +x_nv_n.$$
		\end{proposicion}
		\begin{proof}
			Como $v_1,\ldots,v_n$  generan $V$,  es claro que existen $x_1,\ldots,x_n \in \K$ tales que $v =   x_1v_1 + \cdots +x_nv_n$. Sean $y_1,\ldots,y_n \in \K$ tales que $v =   y_1v_1 + \cdots +y_nv_n$. Veremos que $x_i = y_i$ para $1 \le i \le n$.
			
			Como $v =  \sum_{i=1}^{n} x_iv_i$ y $v =  \sum_{i=1}^{n} y_iv_i$,  restando miembro a miembro obtenemos 
			$$
			0 =   \sum_{i=1}^{n} (x_i-y_i)v_i.
			$$
			Ahora bien,  $v_1,\ldots,v_n$ son  LI, por lo tanto todos los coeficientes de la ecuación anterior son nulos, es decir $x_i-y_i=0$ para $1 \le i \le n$ y entonces $x_i = y_i$ para $1 \le i \le n$.
		\end{proof}
	
	La proposición anterior permite asociar a cada vector una $n$-upla: sea $V$  espacio vectorial de dimensión finita y sea $\mathcal{B} = \{v_1,\ldots,v_n\}$ una base ordenada de $V$, si $v \in V$ y $$v =   x_1v_1 + \cdots +x_nv_n,$$  entonces \textit{$x_i$ es la coordenada $i$-ésima de $v$} y denotamos
	$$
	v = (x_1,\ldots,x_n)_{\mathcal B}.
	$$
	También nos será útil describir a $v$ como una matriz $n \times 1$: \textit{la matriz de $v$  en la base  $\mathcal{B}$}  es
	$$
	[v]_\mathcal{B} = \begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix}.
	$$
	
	\begin{ejemplo}
		Sea $\mathcal B = \{(1,-1),(2,3)\}$ base ordenada de $\R^2$. Encontrar las coordenadas  de $(1,0)$ y $(0,1)$ en la base $\mathcal B$.
	\end{ejemplo}
	\begin{proof}[Solución] Debemos encontrar $x_1, x_2 \in \R$ tal que 
		$$
		(1,0) = x_1(1,-1)+ x_2(2,3).
		$$
		Es decir 
		\begin{align*}
			x_1+ 2x_2 &= 1\\
			-x_1 + 3x_2 &= 0.
		\end{align*}
		Resolviendo el sistema de ecuaciones obtenemos $x_1 = \frac35$ y $x_2 = \frac15$,  es decir
		$$
		(1,0) =\; \frac35(1,-1)+ \frac15(2,3)\quad \text{o equivalentemente} \quad (1,0) = (\;\frac35,\frac15)_{\mathcal B}.
		$$ 
		De forma análoga podemos ver que
		$$
		(0,1) = -\frac25(1,-1)+ \frac15(2,3)\quad \text{o equivalentemente} \quad (0,1) = (-\frac25,\frac15)_{\mathcal B}.
		$$
	\end{proof}
	
	\begin{proposicion}\label{vectorbase->lineal}
		Sea $\mathcal{B}=\{v_1,\ldots,v_n\}$ una base ordenada de $V$ un $\K$-espacio vectorial. Entonces
		\begin{enumerate}
			\item $[v + w]_\mathcal{B} = [v]_\mathcal{B} +[w]_\mathcal{B}$, para $v,w \in V$,
			\item $[\lambda v]_\mathcal{B} = \lambda[v]_\mathcal{B}$, para $\lambda \in \K$ y $v \in V$.
		\end{enumerate}
	\end{proposicion} 
	\begin{proof}
		(1) Si $v = x_1v_1 + \cdots +x_nv_n$ y $w = y_1v_1 + \cdots +y_nv_n$, entonces 
		$$
		v + w = (x_1+y_1)v_1 + \cdots +(x_n+y_n)v_n,
		$$
		luego
		$$
		[v + w]_\mathcal{B} = \begin{bmatrix}x_1+y_1 \\ \vdots \\ x_n+y_n\end{bmatrix}
		= \begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix}+\begin{bmatrix}y_1 \\ \vdots \\ y_n\end{bmatrix} = [v]_\mathcal{B} +[w]_\mathcal{B}.
		$$
		
		(2) Si $v = x_1v_1 + \cdots +x_nv_n$ y $\ \in \K$, entonces 
		$$
		\lambda v = (\lambda x_1)v_1 + \cdots +(\lambda x_n)v_n,
		$$
		luego
		$$
		[\lambda v ]_\mathcal{B} = \begin{bmatrix}\lambda x_1 \\ \vdots \\ \lambda x_n\end{bmatrix}
		= \lambda \begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix} = \lambda [v]_\mathcal{B}.
		$$
	\end{proof}
	
	\begin{teorema}
			Sea $V$  espacio vectorial de dimensión $n$  sobre el cuerpo $\K$, 	y sean $	\mathcal{B} = \{v_1,\ldots,v_n\}$ y $\mathcal{B'} = \{v'_1,\ldots,v'_n\}$ bases ordenadas de $V$. Entonces existe una única matriz $P$ de orden $n \times n$, invertible, tal que para todo $v \in V$
			\begin{enumerate}
				\item $[v]_\mathcal{B} = P[v]_\mathcal{B'}$ y
				\item $[v]_\mathcal{B'} = P^{-1}[v]_\mathcal{B}$.
			\end{enumerate} 
			Las columnas de $P$ están dadas por
			$$
			C_j = [v'_j]_\mathcal{B},\qquad 1 \le j \le n,
			$$
			es decir, los coeficientes de la columna $j$ de $P$ son las coordenadas de $v'_j$ en la base $\mathcal{B}$. 
	\end{teorema}
	\begin{proof} (1) Tenemos
		\begin{align*}
			v &=   x_1v_1 + \cdots +x_nv_n, \\
			v &=  x'_1v'_1 + \cdots +x'_nv'_n.
		\end{align*}
		 También podemos escribir cada $v'_j$ en coordenadas respecto a $\mathcal B$:
		 $$
		 v'_j =  p_{1j}v_1 + \cdots +p_{nj}v_n, \qquad  1 \le j \le n.
		 $$ 
		Luego, 
		\begin{align*}
			 \sum_{i=1}^{n}x_iv_i &=  \sum_{j=1}^{n}x'_jv'_j \\
			&=  \sum_{j=1}^{n}x'_j(\sum_{i=1}^{n}p_{ij}v_i) \\
			&=  \sum_{i=1}^{n}\sum_{j=1}^{n}x'_jp_{ij}v_i \\
			&=  \sum_{i=1}^{n}(\sum_{j=1}^{n}p_{ij}x'_j)v_i.			
		\end{align*}
		Es decir, $x_i = \sum_{j=1}^{n}p_{ij}x'_j$. Por lo tanto, si $P = [p_{ij}]$ tenemos que
		$$
		\begin{bmatrix}x_1 \\ x_2\\ \vdots \\ x_n\end{bmatrix} = 
		\begin{bmatrix}p_{11} &p_{12} & \ldots & p_{1n} \\p_{21} &p_{22} & \ldots & p_{2n} \\ \vdots &\vdots && \vdots\\ p_{n1} &p_{n2} & \ldots & p_{nn}\end{bmatrix}
		 \begin{bmatrix}x'_1 \\ x'_2\\ \vdots \\ x'_n\end{bmatrix}.
		$$
		
		Veamos que $P$  es invertible: sea $(x_1,\ldots,x_n)$ solución del sistema de ecuaciones $PX =0$. Eso implica que si $v =\sum x_i v'_i$,  entonces $P[v]_{\mathcal B'}=0$.     Eso quiere decir que el vector  escrito en la base $\mathcal B$ tiene coordenadas $0$,  es decir $v = \sum 0. v_i =0$ y entonces $v = \sum 0. v'_i$ por la unicidad de escritura. Por lo tanto, la única solución del sistema $PX=0$ es 0 y en consecuencia $P$  es invertible.
		
		(2)  Como $P$ es invertible, multiplicamos por $P^{-1}$  a izquierda la ecuación $[v]_\mathcal{B} = P[v]_\mathcal{B'}$ y obtenemos el resultado.
	\end{proof}

	Observemos que, por simetría,  los coeficientes de la columna $j$ de $P^{-1}$ son las coordenadas de $v_j$ en la base $\mathcal{B'}$. 
	
	\begin{definicion} 
		Sea $V$  espacio vectorial de dimensión $n$  sobre el cuerpo $\K$, 	y sean $\mathcal{B} = \{v_1,\ldots,v_n\}$ y $\mathcal{B'} = \{v'_1,\ldots,v'_n\}$ bases ordenadas de $V$. Entonces la matriz
		$$
		P = \begin{bmatrix}
		 C_1 &C_2 &\cdots &C_n
		\end{bmatrix}
		$$
		con 
		$$
			C_j = [v'_j]_\mathcal{B},\qquad 1 \le j \le n,
		$$
		es la matriz de \textit{cambio de coordenadas}\index{cambiode coordenadas} de la base $\mathcal{B'}$  a la base $\mathcal{B}$.
	\end{definicion}
	
	
	\begin{ejemplo}
		 $\mathcal{B} = \{v_1,v_2,v_3 \}=\{(1,0,-1),(1,1,1),(1,0,0) \}$ en $\R^3$.
		\begin{enumerate}
			\item Probar que $\mathcal{B}$ es una base de $\R^3$, y 
			\item encontrar las coordenadas del vector  $(2,3,5)$ respecto a la base  $\mathcal{B}$. 
		\end{enumerate}
	\end{ejemplo}
\begin{proof}[Solución]
	(1) Sea $A$ la matriz cuyas filas son los vectores de $\mathcal{B}$ :
	$$
	A = \begin{bmatrix} 1&0&-1\\1&1&1\\1&0&0	\end{bmatrix}.
	$$
	Entonces,  calculando el determinante  por desarrollo de la última fila, obtenemos
	$$
	\det A = 1.(-1)^{3+1}. 
	\left| \begin{matrix} 0&-1\\1&1	\end{matrix} \right| = 1.1.1 =1.
	$$ 
	Por  lo tanto $A$ es invertible y en consecuencia el espacio fila de $A$ es $\R^3$. Como $\mathcal{B}$ es un conjunto de 3  generadores de  $\R^3$, $\mathcal{B}$ es base.
	
	(2) Si $x_1,x_2,x_3$  son las coordenadas de $(2,3,5)$ respecto a la base  $\mathcal{B}$, entonces
	$$
	(2,3,5) = x_1(1,0,-1)+x_2(1,1,1)+x_3(1,0,0)
	$$
	y esto es un sistema de tres ecuaciones y tres incógnitas.  Resolviendo este sistema encontramos la solución buscada. 
	
	Sin embargo,  nos interesa conocer una solución que sirva para cualquier vector, no solo para $(2,3,5)$, es decir buscamos la matriz $P = [p_{ij}]$ de cambio de coordenadas de la base canónica a la base $\mathcal B$. En particular,  esta matriz cumple 
	$$
	\begin{bmatrix} x_1\\x_2\\x_3 \end{bmatrix} = 
	\begin{bmatrix}p_{11} &p_{12}& p_{13} \\p_{21} &p_{22} & p_{23}\\ p_{31} &p_{32}& p_{33}\end{bmatrix}
	\begin{bmatrix} 2\\3\\5 \end{bmatrix}.
	$$
	con $(x_1,x_2,x_3)_{\mathcal B} = (2,3,5)$.
	
	
	Ahora bien, sabemos que $P^{-1}$ es la matriz cuya columna $j$ es la matriz de $v_j$ en la base canónica (fácil de calcular) y luego calculamos su inversa que es $P$.
	
	Por lo dicho en el párrafo anterior, $P^{-1}$  es la matriz cuya  columna $1$ es $(1,0,-1)$,  la columna $2$ es $(1,1,1)$ y la columna $3$ es $(1,0,0)$, luego 
	$$
	P^{-1} = \begin{bmatrix} 1&1&1\\0&1&0\\-1&1&0	\end{bmatrix}.
	$$ 
	Encontremos su inversa,
		\begin{multline*} 
	\left[\begin{array}{rrr|rrr}	1&1&1&1&0&0\\ 0&1&0&0&1&0\\ -1&1&0&0&0&1 \end{array}\right]
	\stackrel{F_3+ F_1}{\longrightarrow}
	\left[\begin{array}{rrr|rrr}	1&1&1&1&0&0\\ 0&1&0&0&1&0\\ 0&2&1&1&0&1 \end{array}\right]
	\underset{F_3-2F_2}{\stackrel{F_1 - F_2}{\longrightarrow}}\\
	\longrightarrow 
	\left[\begin{array}{rrr|rrr}	1&0&1&1&-1&0\\ 0&1&0&0&1&0\\ 0&0&1&1&-2&1 \end{array}\right]
	\stackrel{F_1 - F_3}{\longrightarrow}
	\left[\begin{array}{rrr|rrr}	1&0&0&0&1&-1\\ 0&1&0&0&1&0\\ 0&0&1&1&-2&1 \end{array}\right]
	.
	\end{multline*}
	Entonces 
	$$
	P = \left[\begin{array}{rrr}	0&1&-1\\ 0&1&0\\ 1&-2&1 \end{array}\right]
	$$
	y 
	$$
	\left[\begin{array}{rrr}	0&1&-1\\ 0&1&0\\ 1&-2&1 \end{array}\right]
	\left[\begin{array}{r} 2\\3\\5 \end{array}\right] = 
	\left[\begin{array}{r} -2\\3\\1 \end{array}\right].
	$$
	Por  lo tanto las coordenadas de   $(2,3,5)$ respecto a la base  $\mathcal{B}$ son $(-2,3,1)_{\mathcal B}$ o,  equivalentemente, 
	$$
	(2,3,5) = -2(1,0,-1)+3(1,1,1)+(1,0,0).
	$$
	
	  
\end{proof}
	
	\end{section}

	\end{chapter}
	
		
	\begin{chapter}{Transformaciones lineales}\label{chap-trans-lin}
		Las transformaciones lineales son las funciones con las que trabajaremos en álgebra lineal. Se trata de funciones entre espacios vectoriales que son compatibles con la estructura,  es decir con la suma y el producto por escalares.
		
		
		\begin{section}{Transformaciones lineales}
			\begin{definicion}
				Sean $V$ y $W$ dos espacios vectoriales sobre el cuerpo $\K$. Una 				\textit{transformación lineal}\index{transformación lineal} de $V$ en $W$ es una función $T:V \to W$  tal que
				\begin{enumerate}
					\item $T(v+v') = T(v)+ T(v')$, para $v,v' \in V$,
					\item $T(\lambda v) = \lambda T(v)$, para $v \in V$, $\lambda \in \K$.
				\end{enumerate}
			\end{definicion}
		
			\begin{obs}
				$T:V \to W$ es transformación lineal si y sólo si
				\begin{enumerate}
					\item[({a})]  $T(\lambda v+v') = \lambda T(v)+ T(v')$, para $v,v' \in V$, $\lambda \in \K$.
				\end{enumerate}
			Algunas veces usaremos esto último para comprobar si una aplicación de $V$ en $W$ es una transformación lineal. 
			\end{obs}
			
			\begin{ejemplo}
				Si $V$ es cualquier espacio vectorial, la transformación identidad $I$, definida por $Iv = v$ ($v \in V$), es una transformación lineal de $V$ en $V$. La transformación cero $0$, definida por $0v = 0$, es una transformación lineal de $V$ en $V$.
			\end{ejemplo}
		
			\begin{ejemplo}\label{ejemplo-lineal-3}
				Sea $T : \K^3 \to \K^2$ definida por
				$$
				T(x_1,x_2,x_3) = (2x_1 - x_3, -x_1+3x_2+x_3).
				$$
				Entonces, $T$  es una transformación lineal. La demostración la veremos en la observación que sigue a este ejemplo. 
				
			Observar que si 
			$$
			 A = \begin{bmatrix}
			 2&0&-1 \\ -1&3&1
			 \end{bmatrix},
			$$
			entonces
			$$
			\begin{bmatrix}
			2&0&-1 \\ -1&3&1
			\end{bmatrix} 
			\begin{bmatrix}
			x_1\\x_2\\x_3
			\end{bmatrix} =
			\begin{bmatrix}
			2x_1 - x_3 \\ -x_1+3x_2+x_3
			\end{bmatrix}.
			$$
			Es decir,  si $\mathcal C_n$  es la báse canónica de $\K^n$ y $[x]_{\mathcal C_3}$ es la matriz de $x$ en la base canónica,  entonces 
			$$
			A.[x]_{\mathcal C_3} = [T(x)]_{\mathcal C_2}.
			$$ 
			\end{ejemplo}
		
			\begin{obs}\label{obs-tl-1.5} 	Sea $T: \K^n \to \K^m$. En  general si $T(x_1,\ldots,x_n)$ en cada coordenada tiene una combinación lineal de los $x_1,\ldots,x_n$,  entonces $T$ es una transformación lineal. Mas precisamente, si $T$ está definida por
				\begin{align*}
				T(x_1,\ldots,x_n) &= (a_{11}x_1+\cdots + a_{1n}x_n,\, \ldots\,,a_{m1}x_1+\cdots + a_{mn}x_n )\\
				&=(\sum_{j=1}^n a_{1j} x_j,\ldots,\sum_{j=1}^n a_{mj} x_j),
				\end{align*}
				con $a_{ij} \in \K$, entonces $T$  es lineal. 
			\begin{proof}
				Sean $(x_1,\cdots,x_n), (y_1,\cdots,y_n) \in \K^n$ y $\lambda \in \K$,  entonces
				\begin{align*}
				T(\lambda(x_1,\cdots,x_n)+ (y_1,\cdots,y_n)) &= T(\lambda x_1+y_1,\ldots,\lambda x_n+y_n) \\
				&= (\sum_{j=1}^n a_{1j} (\lambda x_j+y_j),\ldots,\sum_{j=1}^n a_{mj} (\lambda x_j+y_j)) \\
				&= (\lambda\sum_{j=1}^n a_{1j} x_j+\sum_{j=1}^n a_{1j} y_j,\ldots,\lambda\sum_{j=1}^n a_{mj} x_j+\sum_{j=1}^n a_{mj} y_j) \\
				&= \lambda(\sum_{j=1}^n a_{1j} x_j,\ldots,\sum_{j=1}^n a_{mj} x_j) +(\sum_{j=1}^n a_{1j} y_j,\ldots,\sum_{j=1}^n a_{mj} y_j) \\
				& =\lambda T(x_1,\cdots,x_n)+ T(y_1,\cdots,y_n).
				\end{align*}
			\end{proof}
			\end{obs}
			
			\begin{ejemplo}[Transformaciones de $\R^2$ en $\R^2$] Las rotaciones y reflexiones en $\R^2$ son transformaciones lineales. Sea $\theta \in \R$ tal que  $0 \le \theta \le 2\pi$,  definimos
				\begin{equation*}
					\begin{array}{llll}
					R_\theta:&\R^2 &\to &\R^2 \\
					&(x,y) &\mapsto & (x \cos\theta - y \sin\theta, y\cos\theta+ x\sin\theta)
					\end{array}
				\end{equation*}
				Observemos que si escribimos el vector $(x,y)$  en coordenadas polares,  es decir  si 
				$$
				(x,y)= r(\cos\alpha,\sin\alpha),\quad r> 0, \; 0 \le \alpha < 2\pi, 
				$$
				entonces
				\begin{align*}
					R_\theta(x,y) &= R_\theta(r\cos\alpha,r\sin\alpha) \\
					&= (r\cos\alpha \cos\theta - r\sin\alpha \sin\theta, r\sin\alpha\cos\theta+ r\cos\alpha\sin\theta) \\
					&= (r\cos(\alpha+\theta) , r\sin(\alpha+\theta)) \\
					&= r(\cos(\alpha+\theta) , \sin(\alpha+\theta)).
				\end{align*}
				Por lo tanto $R_\theta(x,y)$ es el vector $(x,y)$ rotado $\theta$ grados en sentido antihorario y en consecuencia  $R_\theta$ es denominada la \textit{rotación antihoraria en $\theta$ radianes}. No es difícil verificar que $R_\theta$ es una transformación lineal. 
				
				
					\begin{figure}[h]	
					\begin{tikzpicture}[scale=5]
					\draw[->] (-0.1,0)  -- (1.1,0) node(xline)[right]	{$x$};
					\draw[->] (0,-0.1) -- (0,1.1) node(yline)[above] {$y$};
					\draw[->] (0,0) -- (0.5,0.866) node(yline)[above] {$R_{\theta}(v)$};
					\draw[->] (0.9238,0.3826) arc[start angle=22.5, end angle=60, radius=1 ];
					\draw[->] (0,0) -- (0.9238,0.3826) node(yline)[right] {$v$};
					\draw[->] (0.7,0) arc[start angle=0, end angle=22.5, radius=0.7 ];
					\draw[->] (0.9238/2,0.3826/2) arc[start angle=22.5, end angle=60, radius=0.5 ];
					\node [right] at (0.3,0.3) {$\theta$};
					\node [right] at (0.59,0.12) {$\alpha$};
					\draw (1,1pt) -- (1,-1pt);
					\node [right] at (0.95,-0.07) {$r$};
					% Lines
					\end{tikzpicture}
					\caption{Rotación  $\theta$ grados. }
					\label{rotacion-theta}
				\end{figure}
				
				
				
				Otras transformaciones lineales importantes de $\R^2$ en $\R^2$ son 
				$$
				S_h(x,y) = (x,-y) \quad \text{ y } \quad S_v(x,y) = (-x,y).
				$$
				La primera es la reflexión en el eje $x$ y la segunda la reflexión en el eje $y$. Las siguientes afirmaciones se comprueban algebraicamente en forma sencilla, pero nos podemos convencer de ellas por su interpretación geométrica:
				\begin{equation*}
				\begin{array}{ll}
				R_\theta \circ R_\varphi = R_{\theta +\varphi}, \quad &\text{(rotar $\varphi$ y $\theta$ $=$  rotar $\theta+\varphi$)} \\
				R_{\pi/2} \circ S_h \circ R_{-\pi/2} = S_v&
				\end{array}
				\end{equation*}
				
			\end{ejemplo}
	
		
			\begin{ejemplo}
			Sea $V = \R[x]$ el espacio vectorial de los polinomios con coeficientes reales. Definimos $D:V \to V$, por
			$$
			D(P)(x) = P'(x),\quad x \in \R. 
			$$
			Observemos primero que la derivada de un polinomio es un polinomio, pues 
			$$
			(a_nx^n+ a_{n-1}x^{n-1}+\cdots + a_1 x + a_0)' = na_nx^{n-1}+ (n-1)a_{n-1}x^{n-2}+\cdots + a_1.
			$$
			Además  $D$  es lineal, pues $(f+g)' = f' + g'$ y $(\lambda f)' = \lambda f'$, para$f,g$ funciones derivables y $\lambda \in \R$.
			\end{ejemplo} 
		
		\begin{obs}	Sean $V$ y $W$ dos espacios vectoriales sobre el cuerpo $\K$ y 	 $T:V \to W$			un transformación lineal. Entonces $T(0) =0$
		\end{obs}
		\begin{proof} $T(0) = T(0+0) = T(0) + T(0)$, por lo tanto 
			\begin{align*}
				-T(0) + T(0) = -T(0) + T(0)+T(0) \;\Rightarrow\; 0 = 0 +T(0) \;\Rightarrow\; 0= T(0).
			\end{align*}
		\end{proof}
		
		\begin{obs}
			Las transformaciones lineales preservan  combinaciones lineales, es decir si $T:V \to W$ es una transformación lineal, $ v_1,\ldots,v_k \in V$ y $\lambda_1, \ldots+ \lambda_k \in \K$,  entonces
			$$
			T(\lambda_1 v_1 + \cdots+ \lambda_k v_k) = \lambda_1 T(v_1) + \cdots+ \lambda_k T(v_k).
			$$
			Observar que el caso $k=2$ se demuestra de la siguiente manera
			$$
			T(\lambda_1 v_1 +  \lambda_2 v_2) =T(\lambda_1 v_1) + T(\lambda_2 v_2) =\lambda_1 T(v_1) + \lambda_2T( v_2).
			$$
			El caso general se demuestra por inducción. 
		
		\end{obs}
		
		
		\begin{teorema}\label{th-tl-definida-en-base}
			Sean $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y $\{v_1,\ldots,v_n\}$  una base ordenada de $V$. Sean $W$ un espacio vectorial sobre el mismo cuerpo y $\{w_1,\ldots,w_n\}$, vectores cualesquiera de $W$. Entonces existe una única transformación  lineal $T$ de $V$ en $W$ tal que
			\begin{equation*}
			T(v_j) = w_j, \quad j=1,\ldots,n.
			\end{equation*}
		\end{teorema}
			\begin{proof}
				Recordemos que si $v \in V$,  existen únicos $a_1,\ldots,a_n \in \K$ (las coordenadas de $v$) tal que $$v = a_1v_1 + \cdots+a_n v_n.$$  Luego para este vector $v$  definimos
				\begin{equation*}
					T(v) = a_1w_1 + \cdots+a_n w_n.
				\end{equation*}
				Entonces, $T$ es una correspondencia bien definida que asocia a cada vector $v$ 
				de $V$ un vector $T(v)$ de $W$. De la definición queda claro que $T(v_j) = w_j$ para cada $j$. Para ver que $T$ es lineal, sea
				\begin{equation*}
					w = b_1v_1 + \cdots+b_n v_n,
				\end{equation*}
				y sea  $\lambda \in \K$. Ahora
				\begin{align*}
					\lambda v+w &= \lambda(a_1v_1 + \cdots+a_n v_n) + b_1v_1 + \cdots+b_n v_n \\
					&= (\lambda a_1+b_1)v_1 + \cdots+(\lambda a_n+b_n)v_n
				\end{align*}
				con lo que, por definición
				\begin{equation*}
					T(\lambda v+w) =(\lambda a_1+b_1)w_1 + \cdots+(\lambda a_n+b_n)w_n. 
				\end{equation*}
				Por otra parte
				\begin{align*}
				\lambda  T(v) + T(w) &= \lambda (a_1w_1 + \cdots+a_n w_n)+b_1w_1 + \cdots+b_n w_n	 \\
				 &=(\lambda a_1+b_1)w_1 + \cdots+(\lambda a_n+b_n)w_n ,			
				\end{align*}
				y así
				\begin{equation*}
					T(\lambda v+w) = 	\lambda  T(v) + T(w).
				\end{equation*}
				
				Finalmente,  debemos probar la unicidad de $T$. Sea $S: V \to W$ transformación lineal tal que $S(v_j) = w_j$ para $1 \le j \le n$. Entonces,  si $v \in V$ un vector arbitrario, $v = \sum_i a_i v_i$ y
				\begin{equation*}
					S(v) = S(\sum_i a_i v_i)\sum_i a_i S( v_i) = \sum_i a_iw_i = 
					 \sum_i a_i T( v_i) =  T(\sum_i a_i v_i) = T(v)
				\end{equation*}
			\end{proof}
		
		El teorema \ref{th-tl-definida-en-base} es muy elemental, pero por su importancia ha sido presentado
		detalladamente. 
		
			\begin{ejemplo} Usando el teorema \ref{th-tl-definida-en-base}, podemos demostrar la observación \ref{obs-tl-1.5} de la siguiente manera: sea  $\mathcal C_n = \{e_1,\ldots,e_n\}$ es la base canónica de $\K^n$ y  sea  $T: \K^n \to \K^m$ la única transformación lineal tal que 
			\begin{equation*}
			T(e_j)  = (a_{1j},\, \ldots\,,a_{mj} ), \quad j=1,\ldots,n
			\end{equation*}	
			Entonces, 
			\begin{equation*}
			T(x_1,\ldots,x_n) = (a_{11}x_1+\cdots + a_{1n}x_n,\, \ldots\,,a_{m1}x_1+\cdots + a_{mn}x_n ).
			\end{equation*}
			es la transformación lineal resultante.
		\end{ejemplo}
		
		\begin{ejemplo}
			Los vectores
			\begin{align*}
				v_1 &= (1,2)\\
				v_2 &= (3,4)
			\end{align*}
			son linealmente independientes y, por tanto, forman una base de $\R^2$. De acuerdo con el teorema \ref{th-tl-definida-en-base}, existe una única transformación lineal de $\R^2$ en $\R^2$ tal que
			\begin{align*}
				T(v_1) &= (3,2, 1) \\
				T(v_2) &= (6, 5,4).
			\end{align*}
			Para poder describir $T$ respecto a las coordenadas canónicas debemos calcular $T(e_1)$ y $T(e_2)$,  ahora bien,
			\begin{align*}
			(1,0) &= c_{1}(1,2) + c_{2}(3,4)\\
			(0,1) &= c_{3}(1,2) + c_{4}(3,4)
			\end{align*}
			y resolviendo este sistema de cuatro ecuaciones con cuatro incógnitas obtenemos
			\begin{equation*}
			\begin{array}{rcrcr}
			(1,0) &=& -2(1,2) &+ &(3,4)\\
			(0,1) &=& \displaystyle\frac32(1,2) &- &\displaystyle\frac12(3,4)
			\end{array}
			\end{equation*}
			Luego, 
				\begin{align*}
			T(1,0) &= -2T(1,2)+ T(3,4) = -2(3,2, 1)+(6, 5,4) = (0,1,2)\\
			T(0,1) &= \displaystyle\frac32T(1,2) - \displaystyle\frac12T(3,4) = \displaystyle\frac32(3,2,1) - \displaystyle\frac12(6, 5,4)= (\displaystyle\frac32,\displaystyle\frac12,-\displaystyle\frac12)
			\end{align*}
			Entonces
				\begin{equation*}
			T(x_1,x_2) = x_1(0,1,2) + x_2		 (\displaystyle\frac32,\displaystyle\frac12,-\displaystyle\frac12)
			= (\displaystyle\frac32x_2,x_1+\displaystyle\frac12x_2,2x_1-\displaystyle\frac12x_2)
			\end{equation*}
		\end{ejemplo}
		
		\end{section}
	
	
		\begin{section}{Núcleo e imagen de una transformación lineal}
		
		\begin{definicion}
			Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal.  Definimos
			\begin{align*}
				\img(T) &:= \{w \in W:\text{existe $v \in V$, tal que } T(v)=w\} = \{T(v): v \in V \}, \\
				\nuc(T) &:= \{v \in V: T(v)=0 \}. 
			\end{align*}
			A $\img(T)$ lo llamamos la \textit{imagen}\index{imagen de una trasnformación lineal} de $T$ y a $ \nuc(T)$ el \textit{núcleo}\index{núcleo  de una transformación lineal} de $T$. 
		\end{definicion}
		
		\begin{teorema}
			Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal; entonces $\img(T) \subset W$ y $\nuc(T) \subset V$ son subespacios vectoriales.
		\end{teorema}
		\begin{proof}
			$\img(T) \ne \emptyset$, pues $0 = T(0) \in \img(T)$. 
			
			Si $T(v_1),T(v_2) \in \img(T)$ y $\lambda \in \K$,  entonces $T(v_1) + T(v_2) = T(v_1+v_2) \in \img(T)$ y $\lambda T(v_1) = T(\lambda v_1) \in \img(T)$.
			
			
			$\nuc(T) \ne \emptyset$ pues $T(0) =0$ y por lo tanto $0 \in \nuc(T)$.
			
			Si $v,w \in V$ tales que $T(v) =0$ y $T(w)=0$,  entonces, $T(v+w)= T(v)+T(w) =0$. por lo tanto $v+w \in \nuc(T)$. Si  $\lambda \in \K$,  entonces $T(\lambda v) = \lambda T(v) = \lambda.0 =0$, luego  $\lambda v \in \nuc(T)$.
		\end{proof}

	
		\begin{definicion}
			Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal. Supongamos que $V$ es de dimensión finita.
			\begin{enumerate}
\item El \textit{rango}\index{rango de una transformación lineal} de $T$ es la dimensión de la imagen de $T$.
\item La \textit{nulidad}\index{nulidad de una transformación lineal} de $T$ es la dimensión del núcleo  de $T$.
			\end{enumerate}
			
		\end{definicion}
				
		\begin{ejemplo}
			Sea $T: \R^3 \to \R$, definida
			$$
			T(x,y,z) = x +2y +3z.
			$$
			Encontrar una base del núcleo y de la imagen.
			\begin{proof}[Solución]
				Es claro que como $T$ no es 0, la imagen es todo $\R$ (y por lo tanto cualquier $r\in \R, r \ne 0$ es base de la imagen). 
				
				Con respecto al núcleo,  debemos encontrar una base del subespacio
				$$
				\nuc(T) = \{(x,y,z): x+2y+3z =0\}.
				$$
				Como $x+2y+3z =0 \Leftrightarrow x = -2y-3z$, luego, 
				\begin{equation}\label{forma-parametrica-2}
					\nuc(T) =  \{(-2s-3t,s,t): s,t \in \R\}.
				\end{equation}
				Ahora bien, $(-2s-3t,s,t) = s(-2,1,0)+t(-3,0,1)$, por lo tanto 
				\begin{equation*}
				\nuc(T) =  <(-2,1,0),(-3,0,1)>,
				\end{equation*}
				y  como $(-2,1,0),(-3,0,1)$ son LI, tenemos que forman una base del núcleo. 
			\end{proof}
		
		La expresión (\ref{forma-parametrica-2}),  que depende de dos parámetros ($s$ y $t$) que son independientes entre ellos, es llamada la \textit{descripción paramétrica} del núcleo	
		\end{ejemplo}
		
		\begin{ejemplo}
		Sea $T: \R^3 \to \R^4$, definida
		$$
		T(x,y,z) = (x +y ,\,x +2y +z,\,3y +3z,\,2x +4y +2z).
		$$
		\begin{enumerate}
			\item Describir $\nuc(T)$  en forma paramétrica y dar una base.
			\item Describir $\img(T)$  en forma paramétrica y  dar una base. 
		\end{enumerate}
		\begin{proof}[Solución] 
		Observemos que 
		\begin{align*}
		\nuc(T) &= \{(x,y,z)\in \R^3: (x +y ,\,x +2y +z,\,3y +3z,\,2x +4y +2z) =(0,0,0,0)\}.
		\end{align*}
		Por  lo tanto, describir el núcleo de $T$ se reduce a encontrar las soluciones de una sistema de ecuaciones lineales homogéneo, y la matriz asociada a este sistema es.   
		\begin{equation*}
		A = \begin{bmatrix}
		1&1&0\\1&2&1\\0&3&3\\2&4&2
		\end{bmatrix}
		\end{equation*}
		
		Por  otro lado,
		\begin{align*}
		\img(T) &= \{(y_1,y_2,y_3,y_4)\in \R^4: (x +y ,\,x +2y +z,\,3y +3z,\,2x +4y +2z) =(y_1,y_2,y_3,y_4)\}\\
		&=  \{(y_1,y_2,y_3,y_4)\in \R^4: A\begin{bmatrix}x\\y\\z	\end{bmatrix}  =
		\begin{bmatrix}y_1\\y_2\\y_3\\y_4\end{bmatrix}\}.
		\end{align*}
		
		
		Por lo tanto, y haciendo abuso de notación,  
		\begin{align*}
		\nuc(T) &= \{v=(x,y,z):   A.{v}=0\}\\
		\img(T) &= \{y= (y_1,y_2,y_3,y_4): \text{ tal que } \exists v \in \R^3, A.{v} = {y}  \}
		\end{align*}
		En  ambos casos, la solución depende de resolver el sistema de ecuaciones cuya matriz asociada es $A$:
		\begin{equation*}
		\left[\begin{array}{rrr|r}1&1&0&y_1\\1&2&1&y_2\\0&3&3&y_3\\2&4&2&y_4 \end{array}\right]
		\underset{F_4-2F_1}{\stackrel{F_2 -F_1}{\longrightarrow}} 
		\left[\begin{array}{rrr|r}1&1&0&y_1\\0&1&1&-y_1+y_2\\0&3&3&y_3\\0&2&2&-2y_1 +y_4 \end{array}\right]
		\underset{F_4-2F_2}{\underset{F_3-3F_2}{\stackrel{F_1 -F_2}{\longrightarrow}} }
		\left[\begin{array}{rrr|r}1&0&-1&2y_1-y_2\\0&1&1&-y_1+y_2\\0&0&0&3y_1-3y_2+y_3\\0&0&0&-2y_2 +y_4 \end{array}\right],
		\end{equation*}
		es decir
		\begin{equation}\label{eq-gen}
		T(x,y,z) = (y_1,y_2,y_3,y_4) \quad\Leftrightarrow \quad
		\begin{array}{rl}
		x -z &= 2y_1-y_2\\ 
		y +z &= -y_1+y_2\\
		0&=3y_1-3y_2+y_3 \\
		0&= -2y_2 +y_4
		\end{array}
		\end{equation}
		Si hacemos $y_1 = y_2 = y_3 = y_4 = 0$, entonces las soluciones del sistema describen el núcleo de $T$, es decir
		$$
		\nuc(T) = \{(x,y,z):x-z=0, y+z =0 \} = \{(s,-s,s):s \in \R \},
		$$
		que es la forma paramétrica. Una base del núcleo de $T$  es $(1,-1,1)$. 
		
		
		Con respecto a la imagen de $T$ tenemos
		\begin{equation*}
		\text{$(y_1,y_2,y_3,y_4) \in \img(T)$} \; \Leftrightarrow \;
		\begin{array}{l}
		\text{$\exists\, x,y,z \in \R$ tal que\, $2y_1-y_2 = x-z$\, y \,$-y_1+y_2 = y+z$; y} \\
		\text{$0=3y_1-3y_2+y_3$\; y\; $0= -2y_2 +y_4$.}
		\end{array}
		\end{equation*}
		Como variando los valores de $x, y,z$ es posible obtener, para cualesquiera valores de  $y_1,y_2,y_3,y_4$,\; $2y_1-y_2 = x-z$ \;y \;$-y_1+y_2 = y+z$, tenemos
		\begin{equation}\label{eq-gen-2}
		\text{$(y_1,y_2,y_3,y_4) \in \img(T)$} \quad \Leftrightarrow \quad \text{$0=3y_1-3y_2+y_3$\; y\; $0= -2y_2 +y_4$.}
		\end{equation}
		Luego, 
		$$
		\img(T) =  \{(y_1,y_2,y_3,y_4): \text{ tal que $0=3y_1-3y_2+y_3$ y $0= -2y_2 +y_4$} \}.
		$$
		Ahora bien, $$0= -2y_2 +y_4 \Leftrightarrow 2y_2 = y_4 \Leftrightarrow\; y_2 = \frac12 y_4.$$
		Por  otro lado 
		$$0=3y_1-3y_2+y_3 \Leftrightarrow 3y_1 =3y_2-y_3 \Leftrightarrow y_1 =y_2-\frac13y_3 \Leftrightarrow\; y_1 =\frac12y_4-\frac13y_3$$ 
		
		Es decir, cambiando $y-3$ por $s$ e $y_4$ por $t$:
		$$
		\img(T) =  \{(-\frac13 s + \frac12 t, \frac 12 t, s,t): s,t \in \R \}.
		$$
		Como $(-\frac13 s + \frac12 t, \frac 12 t, s,t) = s(-\frac13,0,1,0) + t(\frac12,\frac12,0,1)$ el conjunto  $\{(-\frac13,0,1,0),(\frac12,\frac12,0,1) \}$ es una base de $\img(T)$.
		
		\end{proof}

		\end{ejemplo}

		\vskip .5cm
		
		He aquí uno de los resultados más importantes del álgebra lineal.
		
		\begin{teorema}\label{rang+nul=dimV}
			 	Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal. Supóngase que $V$ es de dimensión finita. Entonces 
			 	$$
			 	rango (T) + nulidad (T) = \dim V.
			 	$$
		\end{teorema}
		\begin{proof} Sean 
			\begin{align*}
				n &= \dim V \\
				k &= \dim(\nuc T) = nulidad(T).  
			\end{align*}
			Entonces debemos probar  que 
			$$
			n-k = \dim(\img T) = rango(T).
			$$
		 	Sea $\{v_1,\ldots,v_k \}$ una base de $\nuc T$. Existen vectores $\{v_{k+1},\ldots,v_n \}$, en $V$ tales que $\{v_1,\ldots,v_n \}$ es una base de $V$. Para probar el teorema, demostraremos que 
		 	$\{Tv_{k+1},\ldots,Tv_n \}$ es una base para la imagen de $T$. 
		 	
		 	 \textit{(1) $\{Tv_{k+1},\ldots,Tv_n \}$ genera la imagen de $T$.} 
		 	 
		 	 Si $w \in \img(T)$,  entonces existe $v \in V$ tal que $T(v)=w$, como $\{v_{1},\ldots,v_n \}$ es base de $V$,  existen $\lambda_1,\ldots,\lambda_{n} \in \K$,  tal que $v=  \lambda_1 v_1+ \cdots + \lambda_n v_n$, por lo tanto 
		 	\begin{align*}
		 		w &= T(v) \\ 
		 		&=  \lambda_1T( v_1)+ \cdots + \lambda_kT( v_k)+ \lambda_{k+1}T( v_{k+1}) +\cdots+ \lambda_nT( v_n) \\
		 		&=  0+ \cdots + 0+ \lambda_{k+1}T( v_{k+1}) +\cdots+ \lambda_nT( v_n) \\
		 		&=  \lambda_{k+1}T( v_{k+1}) +\cdots+ \lambda_nT( v_n).
		 	\end{align*}
		 	Por  lo tanto, $\{Tv_{k+1},\ldots,Tv_n \}$ genera la imagen de $T$. 
		 	
		 	\textit{(2) $\{Tv_{k+1},\ldots,Tv_n \}$ es un conjunto linealmente independiente.}
		 	
		 	Para ver que  $\{Tv_{k+1},\ldots,Tv_n \}$ es linealmente independiente, supóngase que se tienen escalares $\mu_i$ tales que
		 	$$
		 	\sum_{i=k+1}^n \mu_i Tv_i = 0,
		 	$$
		 	luego
		 	$$
		 	0 = \sum_{i=k+1}^n \mu_i Tv_i =   T(\sum_{i=k+1}^n\mu_iv_i).  
		 	$$
		 	Por lo tanto $v =\sum_{i=k+1}^n \mu_iv_i\in \nuc(T)$. Como  $\{v_1,\ldots,v_k \}$ es una base de $\nuc T$,  existen escalares $\lambda_i$ tales que
		 	$$
		 	v =  \sum_{i=1}^k \lambda_i v_i,
		 	$$
		 	es decir
		 	$$
		 	\sum_{j=k+1}^n \mu_jv_j =  \sum_{i=1}^k \lambda_i v_i.
		 	$$
		 	Luego
		 	\begin{align*}
		 			0 &= \sum_{i=1}^k \lambda_i v_i - (\sum_{j=k+1}^n \mu_jv_j) \\
		 			&= \lambda_1 v_1 + \cdots +\lambda_k v_k - \mu_{k+1}v_{k+1} -\cdots-\mu_nv_n.
		 	\end{align*}
		 	Como $\{v_1,\ldots,v_n \}$ es una base, y por lo tanto un conjunto LI,  tenemos que  $0=\lambda_1=\cdots=\lambda_k=\mu_{k+1}=\cdots=\mu_n$, y  en particular $0=\mu_{k+1}=\cdots=\mu_n$. Por lo tanto $\{Tv_{k+1},\ldots,Tv_n \}$ es un conjunto linealmente independiente.		  
		\end{proof}
		
	
		
		Sea $A$ una matriz $m \times n$ con coeficientes  en $\K$. El  \textit{rango fila}\index{rango  fila} de $A$ es la dimensión del subespacio de $\K^n$ generado por las filas de $A$, es decir la dimensión del espacio fila de $A$. El \textit{rango columna}\index{rango  columna} de $A$  es es la dimensión del subespacio de $\K^m$ generado por las columna de $A$. Un  consecuencia importante del teorema \ref{rang+nul=dimV} es le siguiente resultado. 
		
		\begin{teorema}
			Si $A$ es una matriz $m \times n$ con coeficientes  en $\K$, entonces
			$$
			\text{\textit{rango fila }} (A) = \text{\textit{rango  columna }} (A).
			$$
		\end{teorema}
		\begin{proof}
			Sea $T$ la transformación lineal
			\begin{equation*}
				\begin{array}{lllll}
				&T: &\K^{n \times 1} &\to &\K^{m \times 1} \\
				&&X &\mapsto &AX.
				\end{array}
			\end{equation*}
			Observar que
			$$
			\nuc(T) = \{X \in \K^{n \times 1}: AX=0 \}.
			$$
			Es decir $\nuc(T)$  es el subespacio de soluciones del sistema homogéneo $AX=0$. Ahora bien, si $k = \text{\textit{rango fila }} (A)$, ya hemos dicho (capítulo \ref{chap-esp-vect}, sección \ref{sec-dimensiones-de-subespacios}) que la dimensión del subespacio de soluciones del sistema homogéneo $AX=0$ es $n-k$. Luego
			\begin{equation}\label{rangofila=n-nulT}
				\text{\textit{rango fila }} (A) = \dim V - \text{\textit{nulidad}}(T). 
			\end{equation}
			
			Por otro lado 
			$$
			\img(T) = \{AX: X \in \K^{n \times 1} \}.
			$$
			Ahora bien, 
			$$
			AX = 
			\begin{bmatrix} a_{11} x_1 +\cdots a_{1n} x_n \\ \vdots \\ a_{m1} x_1 +\cdots a_{mn} x_n  \end{bmatrix}
			= 
			x_1\begin{bmatrix} a_{11} \\ \vdots \\ a_{m1} \end{bmatrix} + \cdots +
			x_n\begin{bmatrix}  a_{1n}  \\ \vdots \\ a_{mn}  \end{bmatrix}
			$$
			Es decir, que la imagen de $T$ es el espacio generado por las columnas de $A$. Por tanto,
			$$
			\text{\textit{ rango}}(T) =\text{\textit{ rango  columna }}(A).
			$$
			Por  el  teorema \ref{rang+nul=dimV}
			$$
			\text{\textit{ rango}}(T) = \dim V - \text{\textit{nulidad}}(T),
			$$
			y por lo tanto
			\begin{equation}\label{rangocolumna=n-nulT}
			\text{\textit{rango columna }} (A) = \dim V - \text{\textit{nulidad}}(T). 
			\end{equation}
			
			Obviamente, las igualdades (\ref{rangofila=n-nulT}) y (\ref{rangocolumna=n-nulT}) implican 
			$$
		\text{\textit{rango fila }} (A) = \text{\textit{rango  columna }} (A).
		$$
		\end{proof}
	
		\begin{definicion}
				Si $A$ es una matriz $m \times n$ con coeficientes  en $\K$,  entonces el \textit{rango} de $A$ es el rango fila de $A$ (que es igual al rango columna).
		\end{definicion}
		
		\end{section}
	
		\begin{section}{Isomorfismos de espacios vectoriales}
		
		\begin{definicion}
			Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\K$ y sea $T:V \to W$ una transformación lineal.
			\begin{enumerate}
				\item $T$  es \textit{epimorfismo}\index{epimorfismo} si $T$ es suryectiva, es decir si $\img(T) = W$.
				\item $T$ es \textit{monomorfismo}\index{monomorfismo} si $T$ es inyectiva (o 1-1),  es decir si dados $v_1,v_2 \in V$ tales que $T(v_1) = T(v_2)$,  entonces $v_1 = v_2$.
			\end{enumerate}  
		\end{definicion}
	
		\begin{obs}
			$T$  es epimorfismo si y sólo si 
			$$
			\text{$T$ es lineal y }\forall\, w \in W, \; \exists v \in V \text{ tal que }T(v)=w.
			$$
			Esto se deduce inmediatamente de la definiciones de función suryectiva y de $\img(T)$
			
			$T$ es monomorfismo si y sólo si 
			$$
				\text{$T$ es lineal y }\forall\, v_1,v_2  \in V: \; v_1 \ne v_2 \Rightarrow T(v_1) \not= T(v_2).
			$$ 
			Esto se obtiene aplicando el contrarrecíproco a la definición de función inyectiva.
		\end{obs}	
		
		
			
		\begin{proposicion}\label{inyectiva-sii-nuT=0}
			Sea $T:V \to W$ una transformación lineal. Entonces $T$ es monomorfismo si y sólo si $\nuc(T) =0$.
		\end{proposicion}	
		\begin{proof} 
			
			\
			
			($\Rightarrow$) Debemos ver que  $\nuc(T)=0$,  es decir que si $T(v)=0$,  entonces  $v=0$. Ahora bien,  si $T(v) = 0$, como  $T(0)=0$, tenemos que $T(v)  = T(0)$, y como $T$ es inyectiva, implica que $v =0$.
			
			($\Leftarrow$) Sean  $v_1,v_2 \in V$ tal que $T(v_1)=T(v_2)$. Entonces 
			$$
			0 = T(v_1)- T(v_2) = T(v_1 -v_2).
			$$
			Por  lo tanto, $v_1 -v_2 \in \nuc(T)$. Por hipótesis, tenemos que $v_1 -v_2 =0$,  es decir $v_1 = v_2$.
		\end{proof}
		
		
		\begin{obs} Sea $T: V \to W$ transformación lineal, 
			\begin{enumerate}
				\item $T$  es {epimorfismo} si y sólo  si $\img(T) = W$ si  y solo si $rango(T) = \dim W$.
				\item $T$ es {monomorfismo} si y sólo  si $\nuc(T) = 0$ si y sólo si $nulidad(T) =0$.
			\end{enumerate}  
		\end{obs}		
		
		\begin{proposicion}\label{prop-T-mono-sii-li-2-li} Sea $T:V \to W$ transformación lineal. Entonces,
			\begin{enumerate}
				\item $T$ es monomorfismo si y sólo si $T$ de un conjunto LI  es  LI.
				\item $T$ es epimorfismo si y sólo si $T$ de un conjunto de generadores de $V$ es un conjunto de generadores de $W$.
			\end{enumerate}
		\end{proposicion}
		\begin{proof}
			
			\
			
			
			(1) ($\Rightarrow$) Sea $\{v_1,\ldots,v_n \}$ un conjunto LI en $V$ y sean  $\lambda_1,\ldots,\lambda_n \in \K$ tales que
			$$
			\lambda_1T(v_1) +\cdots+ \lambda_{n}T(v_n) =0,
			$$
			entonces
			$$
			0 = T(\lambda_1v_1+\cdots + \lambda_{n}v_n).
			$$
			Como $T$  es inyectiva, por proposición \ref{inyectiva-sii-nuT=0}, 
			$$
			\lambda_1v_1+\cdots + \lambda_{n}v_n =0,
			$$
			lo cual implica que $\lambda_1,\ldots,\lambda_n$ son todos nulos. Por lo tanto,  $T(v_1),\ldots,T(v_n)$ son LI.
			
			
			(1) ($\Leftarrow$) Sea $v \in V$ tal que $T(v)=0$. Veremos que eso implica que $v =0$.  Ahora bien, sea  $\{v_1,\ldots,v_n \}$ una base de $V$,  entonces existen $\lambda_1,\ldots,\lambda_n \in \K$ tales que
			$$
			v = \lambda_1v_1+\cdots + \lambda_{n}v_n,
			$$
			por lo tanto
			$$
			0 = T(v) = T(\lambda_1v_1+\cdots + \lambda_{n}v_n) = \lambda_1T(v_1) +\cdots+ \lambda_{n}T(v_n).
			$$
			Como $\{v_1,\ldots,v_n \}$ es LI, por hipótesis, $\{T(v_1),\ldots,T(v_n)\}$ es LI y, por lo tanto, $\lambda_1,\ldots,\lambda_n$ son todos nulos. Luego $v=0$. Es decir probamos  que el núcleo de $T$ es 0, luego por proposición \ref{inyectiva-sii-nuT=0}, $T$ es monomorfismo. 

		(1) ($\Leftarrow$/\textit{alternativa}) Sea $v \in V$ tal que $T(v)=0$.	Si $v\neq 0$, entonces $\{v\}$ es un conjunto LI en $V$. Luego, $\{T(v)\}$ es un conjunto LI en $W$ y por lo tanto $T(v)\neq 0$. Así, si $T(v)=0$ entonces $v=0$ y por lo tanto $T$ es un monomorfismo.
			  
			(2) ($\Rightarrow$) Sea   $\{v_1,\ldots,v_n \}$ un conjunto de generadores de $V$ y sea  $w \in W$. Como $T$  es epimorfismo, existe $v \in V$ tal que $T(v)=w$. Ahora bien, 
			$$
			v = \lambda_1v_1+\cdots + \lambda_{n}v_n,\, \text{ para algún $\lambda_1,\ldots,\lambda_n \in \K$,}
			$$
			por lo tanto,
			$$
			w =T(v) = T(\lambda_1v_1+\cdots + \lambda_{n}v_n) = \lambda_1T(v_1) +\cdots+ \lambda_{n}T(v_n).
			$$ 
			Es decir,  cualquier $w \in W$ se puede escribir como combinación lineal de los  $T(v_1),\ldots,T(v_n)$ y, por lo tanto,  generan $W$.
			
			(2) ($\Leftarrow$) Sea $\{v_1,\ldots,v_n \}$ una base de $V$, por hipótesis $T(v_1),\ldots,T(v_n)$ generan $W$,  es decir dado cualquier $w \in W$,   existen $\lambda_1,\ldots,\lambda_n \in \K$ tales que
			$$
			w = \lambda_1T(v_1)+\cdots + \lambda_{n}T(v_n),
			$$
			y por lo tanto $w = T(v)$,  con 
			$$
			v = \lambda_1v_1+\cdots + \lambda_{n}v_n.
			$$
		\end{proof}
		
			

		\begin{definicion}
			Si $V$ y $W$ son espacios vectoriales sobre el cuerpo $\K$, toda transformación	lineal $T:V \to W$ suryectiva e inyectiva, se dice \textit{isomorfismo}\index{isomorfismo} de $V$ sobre $W$. Si existe un isomorfismo de $V$ sobre $W$, se dice que $V$ es \textit{isomorfo} a $W$ y se denota $V \cong W$.
		\end{definicion}
		
		
		
		Obsérvese que $V$ es trivialmente isomorfo a $V$, ya que el operador identidad es un isomorfismo de $V$ sobre $V$. 
			
		Recordemos que si una función $f: X \to Y$ es suryectiva e inyectiva, es decir biyectiva, existe su inversa, la cual también es biyectiva. La inversa se denota $f^{-1}: Y \to X$ y viene definida por
		$$
		f^{-1}(y) = x \Leftrightarrow f(x) =y.
		$$ 
		
		\begin{teorema}
			Sea $T:V \to W$ un isomorfismo. Entonces $T^{-1}: W \to V$ es lineal y, por lo tanto, también es un isomorfismo.
		\end{teorema}
		\begin{proof}
			\
			
			(1) Sean $w_1, w_2 \in W$ y sean $v_1 = T^{-1}(w_1) $, $v_2 = T^{-1}(w_2)$. Por lo tanto $T(v_1) = w_1$ y $T(v_2) = w_2$. Ahora bien,
			\begin{multline*}
				T^{-1}(w_1+w_2) = 	T^{-1}(T(v_1)+T(v_2))  = 	T^{-1}(T(v_1+v_2)) = \\ =(T^{-1}\circ T)(v_1+v_2) = v_1+v_2 = T^{-1}(w_1)+ T^{-1}(w_2). 
			\end{multline*}    
			
			(2) Sean $w \in W$ y $\lambda  \in \K$. Sea $v = T^{-1}(w)$, entonces
			\begin{equation*}
			T^{-1}(\lambda w) = 	T^{-1}(\lambda T(v))  = 	T^{-1}(T(\lambda v)) = \\ =(T^{-1}\circ T)(\lambda v) = \lambda v = \lambda  T^{-1}(w). 
			\end{equation*}
				\end{proof}
		
		
		\begin{ejercicio} 
			Sean $V$, $W$ y $Z$ espacios vectoriales sobre el cuerpo $\K$ y sean $T:V \to W$, $S:W \to Z$ isomorfismos. Entonces, 
			\begin{enumerate}
				\item $S\circ T:V \to Z$  también es un isomorfismo y
				\item 	$(S\circ T)^{-1} = T^{-1}\circ S^{-1}$.
			\end{enumerate}
		\end{ejercicio}
		
		
		Como ya se ha dicho, $V$ es isomorfo a $V$ vía la identidad. Por  el teorema anterior, si $V$ es isomorfo a $W$,  entonces $W$ es isomorfo a $V$. Por  el ejercicio anterior, si $V$ es isomorfo a $W$ y $W$ es isomorfo a $Z$, entonces $V$ es isomorfo a $Z$. En resumen, el isomorfismo es una relación de equivalencia sobre la clase de espacios vectoriales. Si existe un isomorfismo de $V$ sobre $W$, se dirá a veces que \textit{$V$ y $W$ son isomorfos}, en vez de que $V$ es isomorfo a $W$. Ello no será motivo de confusión porque $V$ es isomorfo a $W$, si, y solo si, $W$ es isomorfo a $V$.	
			
		
			
			
		\begin{teorema}
			Sean $V,W$ espacios vectoriales de dimensión  finita sobre $\K$ tal que $\dim V = \dim W$. Sea $T: V \to W$ transformación lineal. Entonces,  son equivalentes:
			\begin{enumerate}[label=(\alph*),ref=\alph*]
				\item\label{a-dimV=dimW} $T$ es un  isomorfismo.
				\item\label{b-dimV=dimW} $T$ es monomorfismo.
				\item\label{c-dimV=dimW} $T$ es epimorfismo.
				\item\label{d-dimV=dimW} Si $\{v_1,\ldots,v_n \}$ es una base de $V$,  entonces $\{T(v_1),\ldots,T(v_n) \}$ es una base de $W$.
			\end{enumerate}
		\end{teorema}
		\begin{proof}[Demostración (*)] Sea $n = \dim V = \dim W$.
			
			
			(\ref{a-dimV=dimW}) $\Rightarrow$ (\ref{b-dimV=dimW}). Como $T$ es isomorfismo,  es biyectiva y por lo tanto inyectiva.
			
			(\ref{b-dimV=dimW}) $\Rightarrow$ (\ref{c-dimV=dimW}). $T$ monomorfismo,  entonces $nulidad(T) = 0$ (proposición \ref{inyectiva-sii-nuT=0}). Luego, como  $rango(T) +nulidad(T) = \dim V$,  tenemos que $rango(T) = \dim V$. Como $\dim V = \dim W$, tenemos que $\dim \img(T) = \dim W$ y por lo tanto $\img(T) = \dim W$. En  consecuencia, $T$ es suryectiva.
			
			(\ref{c-dimV=dimW}) $\Rightarrow$ (\ref{a-dimV=dimW}). $T$ es suryectiva, entonces $rango(T) = n$, luego  $nulidad(T) = 0$, por lo tanto $\nuc(T)=0$ y en consecuencia $T$ es inyectiva. Como $T$ es suryectiva e inyectiva  es un isomorfismo. 
			
			\vskip .3cm
			
			Hasta aquí probamos que (\ref{a-dimV=dimW}), (\ref{b-dimV=dimW}) y (\ref{c-dimV=dimW}) son equivalentes, luego si probamos que (\ref{a-dimV=dimW}), (\ref{b-dimV=dimW}) o (\ref{c-dimV=dimW}) $\Rightarrow$ (\ref{d-dimV=dimW}) y que (\ref{d-dimV=dimW}) $\Rightarrow$ (\ref{a-dimV=dimW}), (\ref{b-dimV=dimW}) o (\ref{c-dimV=dimW}), estaría probado el teorema. 
			
			\vskip .3cm			
			
			(\ref{a-dimV=dimW}) $\Rightarrow$ (\ref{d-dimV=dimW}). Sea $\{v_1,\ldots,v_n \}$  una base de $V$,  entonces $\{v_1,\ldots,v_n \}$ es LI y  genera $V$. Por proposición \ref{prop-T-mono-sii-li-2-li}, tenemos que $\{T(v_1),\ldots,T(v_n) \}$ es LI y  genera $W$, por lo tanto $\{T(v_1),\ldots,T(v_n) \}$ es una base de $W$.
			
			(\ref{d-dimV=dimW}) $\Rightarrow$ (\ref{a-dimV=dimW}). Como $T$ de una base es una base,  entonces $T$  de un conjunto LI es un conjunto LI y $T$ de un conjunto de generadores de $V$  es un conjunto de generadores de $W$. Por lo tanto, por proposición \ref{prop-T-mono-sii-li-2-li}, $T$ es monomorfismo y epimorfismo, luego $T$ es un isomorfismo. 	
		\end{proof}
	
		\begin{corolario}
			Sean $V,W$ espacios vectoriales de dimensión  finita sobre $\K$ tal que $\dim V = \dim W$. Entonces $V$ y $W$ son  isomorfos. 
		\end{corolario}
		\begin{proof}
			Sea $\{v_1,\ldots,v_n \}$ es una base de $V$ y $\{w_1,\ldots,w_n \}$ es una base de $W$. Poe teorema \ref{th-tl-definida-en-base} existe una única transformación lineal $T:V \to W$ tal que
			\begin{equation*}
			T(v_i) = w_i, \qquad i=1,\ldots, n.
			\end{equation*}
			Por  el teorema anterior, $T$  es un isomorfismo.
		\end{proof}
	
	
	\begin{ejemplo} $\K_n[x] = \{a_0+a_1x+\cdots+a_{n-1}x^{n-1}: a_0,a_1, \ldots,a_{n-1} \in \K \}$  es isomorfo a $\K^n$, esto es  consecuencia inmediata del corolario anterior, pues ambos tienen dimensión $n$. Explícitamente, $1,x,\ldots,x^{n-1}$  es base de $\K_n[x]$ y sea $e_1,\ldots,e_n$ la base canónica  de $\K^n$,  entonces un isomorfismo de $\K_n[x]$ a $\K^n$ viene dado por la única transformación lineal $T:\K_n[x] \to\K^n$ tal que
		$$
		T(x^i) = e_{i+1},\qquad i=0,\ldots, n-1.
		$$   
		
	\end{ejemplo}

	\begin{ejemplo} $M_{m \times n}(\K)$  es isomorfo a $\K^{mn}$. El isomorfismo viene dado por $T: M_{m \times n}(\K) \to \K^{mn}$ tal que
		$$
		T(E_{ij}) = e_{(i-1)n+j}, \qquad i=1,\ldots, m,\; j=1,\ldots, n.
		$$
	Por  ejemplo, en el caso  $2 \times 2$,
	\begin{equation*}
	\begin{array}{llll}
	\begin{bmatrix} 1&0\\0&0\end{bmatrix} &\mapsto (1,0,0,0) \qquad&
	\begin{bmatrix} 0&1\\0&0\end{bmatrix} &\mapsto (0,1,0,0) \\
	&&&\\
	\begin{bmatrix} 0&0\\1&0\end{bmatrix} &\mapsto (0,0,1,0) &
	\begin{bmatrix} 0&0\\0&0\end{bmatrix} &\mapsto (0,0,0,1).
	\end{array}
	\end{equation*}	 
		
	\end{ejemplo}
	
		\end{section}
	
		\begin{section}{Álgebra de las transformaciones lineales}
			En el estudio de las transformaciones lineales de $V$ en $W$ es de fundamental importancia que el conjunto de estas transformaciones hereda una estructura natural de espacio vectorial. El conjunto de las transformaciones lineales de un espacio $V$ en sí mismo tiene incluso una estructura algebraica mayor, pues la composición ordinaria de funciones da una ``multiplicación'' de tales transformaciones. 
			
			
			Observemos primero que si $X$ conjunto y $W$ espacio vectorial sobre el cuerpo $\K$,  entonces
			$$
			F(X,W) := \{f:X\to W\},
			$$ 
			es decir el conjunto de funciones de $X$ en $W$ es un espacio vectorial sobre $\K$ con la suma y el producto por escalares definido:
			\begin{equation*}
			\begin{array}{rlcl}
			(f+g)(x) &= f(x)+ g(x),\qquad &f,g& \in F(X,W),\; x \in X\\
			(\lambda f)(x) &= \lambda f(x), & f& \in F(X,W),\; x \in X, \; \lambda \in \K.
			\end{array}
			\end{equation*}
			La demostración de esto es sencilla y se basa en el hecho que $W$  es un espacio vectorial. 
			
			\begin{teorema}
				Sean $V$ y $W$ espacios vectoriales sobre el cuerpo $\K$ Sean $T,S : V \to W$ transformaciones y $\mu \in \K$. Entonces, $T + S$ y $\mu T$ son transformaciones lineales de $V$ en $W$.
			\end{teorema}
			\begin{proof}
				Sean $v,v' \in V$ y $\lambda \in \K$, entonces
				\begin{equation*}
				\begin{array}{rlll}
					(T + S)(\lambda v + v') &= T(\lambda v + v') + S(\lambda v + v')&\qquad&\text{(definición de $T+S$)} \\
					&= \lambda T(v) + T(v') + \lambda S(v) + S(v')& &\text{($T$ y $S$ lineales)}\\
					&= \lambda (T(v) +S(v)) + T(v') + S(v')&&\text{}\\
					&= \lambda ((T+S)(v)) + (T + S) (v')& &\text{(definición de $T+S$)}\\
					&= \lambda(T + S)(v) +(T + S) (v')&&\text{(definición de $\lambda(T + S)$)}.
				\end{array}
				\end{equation*}
				que dice que $T + U$ es una transformación lineal. En forma análoga, si $\mu \in \K$, 
				\begin{equation*}
				\begin{array}{rlll}
				(\mu T)(\lambda v + v') &= \mu T(\lambda v + v')&\qquad&\text{(definición de $\mu T$)} \\
				&= \mu \lambda T(v) + \mu T(v') &\qquad&\text{($T$ lineal)}\\
				&=  \lambda \mu T(v) + \mu T(v')&\qquad&\text{}\\
				&= \lambda (\mu T)(v) + (\mu T)(v') &\qquad&\text{(definición de $\mu T$)}.
				\end{array}
				\end{equation*}
				que dice que $\mu T$ es una transformación lineal.
			\end{proof}
			
		\begin{corolario}
			Sean $V$ y $W$ espacios vectoriales sobre el cuerpo $\K$. Entonces, el conjunto de transformaciones lineales de $V$ en $W$ es un subespacio vectorial  de $F(V,W)$. 
		\end{corolario} 	
			
		Se denotará  $L(V,W)$ al espacio vectorial de las transformaciones lineales de $V$ en $W$.
		
		\begin{teorema}\label{th-6-hoffman}
			Sean $V$, $W$ y $Z$ espacios vectoriales sobre el cuerpo $\K$.  Sean $T: V \to W$ y $U: W \to Z$ 
			transformaciones lineales. Entonces la función compuesta $U\circ T$ definida por $(U\circ T)(v) = U(T(v))$ es una transformación lineal de $V$ en $Z$.
		\end{teorema} 
		\begin{proof} Sean $v, v' \in V$ y $\lambda \in \K$, entonces
			\begin{equation*}
			\begin{array}{rlll}
				(U\circ T)(\lambda v + v') &= U(T(\lambda v + v'))&\qquad&\text{(definición de composición)} \\
				 &= U(\lambda T( v) + T(v'))&\qquad&\text{($T$ lineal)} \\
				 &= \lambda U(T( v)) + U(T(v'))&\qquad&\text{($U$ lineal)} \\
				 &= \lambda (U\circ T)( v) + (U\circ T)(v')&\qquad&\text{(definición de composición)}. \\
			\end{array}
			\end{equation*}
			
		\end{proof}
		
		Para simplificar, a veces  denotaremos la composición por yuxtaposición,  es decir $$U\circ T = U T.$$
		
		
		En lo que sigue debemos interesarnos principalmente en transformaciones lineales de un espacio vectorial en sí mismo. Como se tendrá a menudo que 	escribir ``$T$ es una transformación lineal de $V$ en $V$'', se dirá más bien: ``$T$ es un operador lineal sobre $V$''. 
		
		\begin{definicion}
			Si $V$ es un espacio vectorial sobre el cuerpo $\K$, un \textit{operador lineal}\index{operador lineal} sobre $V$ es una transformación lineal de $V$ en $V$.
		\end{definicion}
		
		Cuando  en el teorema \ref{th-6-hoffman}, consideramos $V = W = Z$, tenemos que $U$ y $T$ son opera-
		dores lineales en el espacio $V$, y por lo tanto la composición $UT$ es también un operador lineal sobre $V$. Así, el espacio $L(V, V)$ tiene una ``multiplicación'' definida por composición. En este caso el operador $TU$ también está definido, y debe observarse que en general $UT \not= TU$, es decir, $UT - TU \not= 0$. Se ha de advertir de manera especial que si $T$ es un operador lineal sobre $V$, entonces se puede componer $T$ con $T$. Se usará para ello la notación $T^2 = TT$, y en general $T^n = T \cdots T$ ($n$ veces) para $n = 1, 2, 3, \ldots$\,  Si $T \ne 0$, se define $T^0 = I_V$, el operador identidad.
			
		\begin{lema}
			Sea $V$ un espacio vectorial sobre el cuerpo $\K$; sean $U$, $T$ y $S$ operadores lineales sobre $V$ y sea $\lambda$ un elemento de $\K$. Denotemos $I_V$ el operador identidad. Entonces
			\begin{enumerate}
				\item $U = I_VU = UI_V$,
				\item $U(T+S) = UT + US$, $(T+S)U = TU + SU$,
				\item $\lambda (UT) = (\lambda U)T = U (\lambda T)$.
			\end{enumerate}¡
		\end{lema}
		\begin{proof}
			(1) es trivial. 
			
			Demostraremos $U(T+S) = UT + US$ de (2) y todo lo demás se dejará como ejercicio.
			Sea $ v \in V$, entonces
			\begin{equation*}
			\begin{array}{rlll}
			U(T+S)(v) &= U((T+S)(v))&\qquad&\text{(definición de composición)} \\
			&= U(T(v)+S(v))&\qquad&\text{(definicion  de $T+S$)} \\
			&= U(T(v))+U(S(v))&\qquad&\text{($U$ lineal)} \\
			&= UT(v)+US(v)&\qquad&\text{(definición de composición)}.
			\end{array}
			\end{equation*}  
		\end{proof}	
	
	El contenido de este lema, y algunos otros resultados  sobre composición de funciones de un conjunto en si mismo (como ser la asociatividad), dicen que el espacio vectorial $L(V, V)$, junto con la operación de composición, es lo que se conoce tomo una  álgebra asociativa sobre $\K$,  con identidad (ver \href{https://es.wikipedia.org/wiki/Álgebra\_asociativa}{https://es.wikipedia.org/wiki/Álgebra\_asociativa}). 
	
	

		\end{section}
	
		\begin{section}{Matriz de una transformación lineal}
			Sea $V$ un espacio vectorial de dimensión $n$ sobre el cuerpo $\K$, y sea $W$ un espacio vectorial de dimensión $m$ sobre $\K$. Sea $\mathcal B = \{v_1,\ldots,v_n\}$ una base ordenada de $V$, y $\mathcal B' = \{w_1,\ldots,w_n\}$ una base ordenada de $W$. Si $T$ es cualquier 		transformación lineal de $V$ en $W$, entonces $T$ está determinada por su efecto sobre los vectores $v_j$, puesto que todo vector de $V$ es combinación lineal de ellos. Cada uno de los $n$ vectores $Tv_j$ se expresa de manera única 	como combinación lineal
			\begin{equation}\label{matriz-de-T-1}
				Tv_j = \sum_{i=1}^{m} a_{ij} w_i
			\end{equation}
			de los $w_i$. Los escalares $a_{1j},\ldots,a_{mj}$ son las coordenadas de $Tv_j$ en la base or	denada $\mathcal B'$. Por consiguiente, la transformación $T$ está determinada por los
			$mn$ escalares $a_{ij}$ mediante la expresión (\ref{matriz-de-T-1}). 
			
			\begin{definicion}
				La matriz $m \times n$, $A$, definida por $[A]_{ij} = a_{ij}$, donde $a_{ij}$  es como en  (\ref{matriz-de-T-1}), se llama \textit{matriz de $T$ respecto a las bases ordenadas $\mathcal B$ y $\mathcal B'$;}\index{matriz!de una transformación lineal} y se denota 
				$$
				[T]_{\mathcal B \mathcal B'} = A .
				$$.
			\end{definicion}
			
			\begin{ejemplo}
				Sea $T: \R^3 \to \R^4$ definida
				$$
				T(x,y,z) = (2x+y, 3y, x+4z,z).
				$$
				Sean  $B = \{e_1,e_2,e_3\}$ la base canónica de $\R^3$ y  $B' = \{e_1,e_2,e_3,e_4\}$ la base canónica de $\R^4$. Entonces
				\begin{equation*}
					\begin{array}{ccccrcrcrcr}
					T(e_1) & = & (2,0,1,0) & = &  2e_1& + & 0.e_2& + &   e_3& + & 0.e_4  \\
					T(e_2) & = & (1,3,0,0) & = &   e_1& + &  3e_2& + & 0.e_3& + & 0.e_4  \\
					T(e_3) & = & (0,0,4,1) & = & 0.e_1& + & 0.e_2& + &  4e_3& + &   e_4
					\end{array}
				\end{equation*}
				Por  lo tanto
				\begin{equation*}
						[T]_{\mathcal B \mathcal B'} =\begin{bmatrix} 2&1&0 \\0&3&0 \\1&0&4 \\0&0&1
					\end{bmatrix}.
				\end{equation*}
				Observar que si escribimos los vectores en coordenadas con respecto  a las bases canónicas,  tenemos que 
				\begin{equation*}
				\begin{bmatrix} 2&1&0 \\0&3&0 \\1&0&4 \\0&0&1
				\end{bmatrix}
				\begin{bmatrix} x \\ y \\z\end{bmatrix} = 
				\begin{bmatrix} 2x+y\\ 3y\\ x+4z\\z\end{bmatrix}
				\end{equation*}
				o más formalmente
				\begin{equation*}
					[T]_{\mathcal B \mathcal B'} [v]_{\mathcal B} = [T(v)]_{\mathcal B'}.
				\end{equation*}
			\end{ejemplo}
		
		
			\begin{definicion}
				Sa $A$ matriz $m \times n$,  el \textit{operador lineal asociado a $A$} es $A: \R^n \to \R^m$  definido $A(X) = AX$. Es decir
				\begin{equation}
					A(x_1,\ldots,x_n):= \begin{bmatrix} a_{11} &\cdots &a_{1n} \\ \vdots &\ddots& \vdots \\a_{m1} &\cdots &a_{mn}
					\end{bmatrix}\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}.
				\end{equation}
			\end{definicion}	
		
			\begin{ejemplo}
					Sa $A$ matriz $m \times n$, entonces  el operador lineal asociado a $A$ en las bases canónicas es $A$. Es decir  si $\mathcal B$ y $\mathcal B'$ son las bases canónicas de $\R^n$ y $\R^m$,  respectivamente, entonces 
					\begin{equation*}
						[A]_{\mathcal B\mathcal B'} =A.
					\end{equation*}
			\end{ejemplo}	
			
			\begin{proposicion} 	Sea $V$  y $W$ un espacios vectoriales de dimensión $n$ y $m$ respectivamente y sea $T: V \to W$ una transformación lineal. Sea $\mathcal B = \{v_1,\ldots,v_n\}$ una base ordenada de $V$, y $\mathcal B' = \{w_1,\ldots,w_n\}$ una base ordenada de $W$. Entonces
			\begin{equation}\label{matriz-de-tl-2}
			[T]_{\mathcal B \mathcal B'} [v]_{\mathcal B} = [T(v)]_{\mathcal B'}, \quad \forall \,v \in V.
			\end{equation}	
			\end{proposicion}
		\begin{proof}
			Si 
			\begin{equation*}
			Tv_j = \sum_{i=1}^{m} a_{ij} w_i
			\end{equation*}
			entonces $[T]_{ij} = a_{ij}$. Sea $v \in $,  entonces $v = x_1v_1+\cdots+x_n v_n$ con $x_i \in \K$, por lo tanto 
			$$
			[v]_{\mathcal B} = \begin{bmatrix} x_1\\ \vdots\\x_n\end{bmatrix}.
			$$
			Ahora bien,
			\begin{multline*}
				T(v) = T(\sum_{j=1}^{n}x_jv_j) = \sum_{j=1}^{n}x_jT(v_j) =  \sum_{j=1}^{n}\sum_{i=1}^{m} a_{ij} w_i = \sum_{i=1}^{m} (\sum_{j=1}^{n}a_{ij}) w_i =\\
				= ( \sum_{j=1}^{n}a_{1j})w_1+(\sum_{j=1}^{n}a_{2j})w_2+\cdots+(\sum_{j=1}^{n}a_{mj})w_m
			\end{multline*}
			y, por lo tanto,
			\begin{equation}\label{eq-45}
			[T(v)]_{\mathcal B'} = \begin{bmatrix}  \sum_{j=1}^{n}a_{1j}\\\sum_{j=1}^{n}a_{2j}\\ \vdots\\\sum_{j=1}^{n}a_{mj}\end{bmatrix}.
			\end{equation}
			Por otro lado, 
			\begin{equation}\label{eq-46}
			[T]_{\mathcal B \mathcal B'} [v]_{\mathcal B} = 
			\begin{bmatrix} a_{11} &a_{12}&\cdots & a_{1n}
			\\a_{21} &a_{22}&\cdots & a_{2n}\\ \vdots &\vdots&& \vdots \\ a_{m1} &a_{m2}& \cdots & a_{mn}\end{bmatrix}
			\begin{bmatrix} x_1\\x_2\\ \vdots\\x_n\end{bmatrix}
			=
			\begin{bmatrix}  \sum_{j=1}^{n}a_{1j}\\\sum_{j=1}^{n}a_{2j}\\ \vdots\\\sum_{j=1}^{n}a_{mj}\end{bmatrix}.
			\end{equation}
			De las ecuaciones (\ref{eq-45}) y (\ref{eq-46}) se deduce la formula (\ref{matriz-de-tl-2}).
		\end{proof}
		
		\begin{teorema}
			Sea $V$  y $W$ un espacios vectoriales de dimensión $n$ y $m$ respectivamente y $\mathcal B = \{v_1,\ldots,v_n\}$ y $\mathcal B' = \{w_1,\ldots,w_m\}$ dos bases  ordenadas de $V$ y $W$ respectivamente. Entonces 
			$$
			\kappa: L(V,W) \to M_{m \times n}(\K)
			$$
			definida
			$$
			T \mapsto [T]_{\mathcal B \mathcal B'},
			$$
			es un isomorfismos de espacios vectoriales. 
		\end{teorema}
		\begin{proof}[Demostracion ($*$)]
			Primero probaremos que $\kappa$  es lineal y luego que tiene inversa.
			
			Sean $T,T' \in L(V,W)$ y $\lambda \in \K$, veamos que $\kappa(\lambda T+ T') = \lambda \kappa(T)+ \kappa(T')$,  es decir
			\begin{equation}\label{kappa-lineal}
				[\lambda T+ T']_{\mathcal B \mathcal B'} = \lambda[T]_{\mathcal B \mathcal B'}+ [T']_{\mathcal B \mathcal B'}.
			\end{equation}
			Para $1 \le j \le n$, sean 
			\begin{equation*}
				T(v_j) = \sum_{i=1}^m a_{ij} w_i\qquad \text{ y } \qquad T'(v_j) = \sum_{i=1}^m a'_{ij} w_i,
			\end{equation*}
			es  decir
			\begin{equation*}
			[T]_{\mathcal B \mathcal B'} =	[a_{ij}] \qquad\text{ y } \qquad [T']_{\mathcal B \mathcal B'} =	[a'_{ij}],
			\end{equation*}
			entonces
			\begin{align*}
				(\lambda T+ T')(v_j) &= \lambda T(v_j)+ T'(v_j) \\
				&=\lambda \sum_{i=1}^m a_{ij} w_i+ \sum_{i=1}^m a'_{ij} w_i \\
				&= \sum_{i=1}^m (\lambda a_{ij}+ a'_{ij}) w_i,
			\end{align*}
			por lo tanto
			\begin{equation*}
				[\lambda T+ T']_{\mathcal B \mathcal B'} = [\lambda a_{ij}+ a'_{ij}] =
				\lambda[T]_{\mathcal B \mathcal B'}+ [T']_{\mathcal B \mathcal B'}
			\end{equation*}
			y hemos probado (\ref{kappa-lineal}) y, en consecuencia, $\kappa$  es lineal.
			
			\vskip .3cm
			
			Definamos ahora la inversa de $\kappa$: sea $A = [a_{ij}]$ matriz $m \times n$ y sea $T: V \to W$ la única transformación lineal que satisface, para $1 \le j \le n$, que
			$$
			T(v_j) = \sum_{i=1}^m a_{ij} w_i.
			$$
			Es claro que esta aplicación tiene dominio en $M_{m \times n}(\K)$ y su imagen está contenida en  $L(V,W)$. Más aún,  es muy sencillo comprobar que es la aplicación inversa a $\kappa$.
			
		\end{proof}
	
		
		
		\begin{teorema}\label{th-5.5}
			Sean $V$, $W$ y $Z$ espacios vectoriales de dimensión finita sobre el cuerpo $\K$; sean $T: V \to W$ y $U: W \to Z$  transformaciones lineales.  Si $\mathcal B$, $\mathcal B'$ y $\mathcal B''$ son bases ordenadas de los espacios $V$, $W$ y $Z$, respectivamente, entonces
			\begin{equation}
				[UT]_{\mathcal B\mathcal B''} = 	[U]_{\mathcal B'\mathcal B''} 	[T]_{\mathcal B\mathcal B'}.  
			\end{equation} 
		\end{teorema}
		\begin{proof}
			Sean
			\begin{equation*}
				 \mathcal B = \{v_1,\ldots,v_n\},\quad\mathcal B' = \{w_1,\ldots,w_m\},\quad  \mathcal B''= \{z_1,\ldots,z_l\}
			\end{equation*}
			y
			\begin{equation*}
			T(v_j) = \sum_{i=1}^m a_{ij} w_i, \; 1 \le j \le n; \qquad U(w_i) = \sum_{k=1}^l b_{ki} z_k, \; 1 \le i \le m.
			\end{equation*}
			Es decir 
			$$
			[T]_{\mathcal B\mathcal B'} = [a_{ij}]\qquad \text{y} \qquad [U]_{\mathcal B'\mathcal B''} = [b_{ij}]. 
			$$
			Entonces
			\begin{align*}
				(UT)(v_j) &= U(\sum_{i=1}^m a_{ij} w_i) \\
				&=  \sum_{i=1}^m a_{ij} U(w_i) \\
				&=  \sum_{i=1}^m a_{ij} \sum_{k=1}^l b_{ki} z_k \\
				&= \sum_{k=1}^l(\sum_{i=1}^m  b_{ki}a_{ij})  z_k.
			\end{align*}
			Luego el coeficiente $kj$ de la matriz $[UT]_{\mathcal B\mathcal B''}$ es $\sum_{i=1}^m  b_{ki}a_{ij}$ que es igual a la fila $k$ de $[U]_{\mathcal B'\mathcal B''}$ por la columna $j$ de  $[T]_{\mathcal B\mathcal B'}$,  en símbolos,  si $A= 	[T]_{\mathcal B\mathcal B'}$, $B = 	[U]_{\mathcal B'\mathcal B''}$ y $C = 	[UT]_{\mathcal B\mathcal B''}$, entonces
			\begin{equation*}
				[C]_{kj} = \sum_{i=1}^m  b_{ki}a_{ij} = F_k(B)C_j(A) = [BA]_{kj}.
			\end{equation*}
		\end{proof}	
			
		\begin{corolario}\label{cor-5.6} Sean $V$ espacio vectorial de dimensión finita, $\mathcal B = \{v_1,\ldots,v_n\}$ base ordenada de $V$ y $T,U: V \to V$ operadores lineales. Entonces
			\begin{enumerate}
				\item\label{itm-cor-cambio-1} $[UT]_{\mathcal B} = [U]_{\mathcal B} [T]_{\mathcal B}$.
				\item\label{itm-cor-cambio-2} Si $I: V \to V$  es el operador identidad, entonces $[I]_{\mathcal B} =Id$,  donde $Id$  es la matriz identidad $n \times n$.
				\item\label{itm-cor-cambio-3} Si $T$  es invertible,  entonces $[T]_{\mathcal B}$  es una matriz invertible y  $$[T^{-1}]_{\mathcal B} = [T]_{\mathcal B}^{-1}.$$
			\end{enumerate}
		\end{corolario}
		\begin{proof}
			(\ref{itm-cor-cambio-1}). Es inmediato del teorema anterior tomado $\mathcal B' =\mathcal B'' = \mathcal B$. 
			
			(\ref{itm-cor-cambio-2}). $I(v_i) = v_i$ y por lo tanto
			\begin{equation*}
				[I]_{\mathcal B} = \begin{bmatrix} 1&0&\cdots&0 \\0&1&\cdots&0\\\vdots&&&\vdots\\0&0&\cdots&1 	\end{bmatrix} = Id.
			\end{equation*}
			
			
			(\ref{itm-cor-cambio-3}). $I = T T^{-1}$, luego
			\begin{equation*}
				Id = [I]_{\mathcal B} =  [T T^{-1}]_{\mathcal B} =  [T]_{\mathcal B} [T^{-1}]_{\mathcal B}.
			\end{equation*}
			 Análogamente, $I = T^{-1} T$, luego
			\begin{equation*}
			Id = [I]_{\mathcal B} =  [T^{-1} T]_{\mathcal B} =  [T^{-1}]_{\mathcal B} [T]_{\mathcal B}. 
			\end{equation*}
			Por  lo tanto $[T]_{\mathcal B}^{-1} = [T^{-1}]_{\mathcal B}$.
		\end{proof}	
	
	
			
		\begin{teorema}[Matriz de cambio de base]
			Sea $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y sean
			$$
			\mathcal B = \{v_1,\ldots,v_n \}, \qquad \mathcal B' = \{v'_1,\ldots,v'_n \}
			$$
			bases ordenadas de $V$. Sea $T$ es un operador lineal sobre V. Entonces existe $P$ una  matriz invertible  $n \times n$ tal que 
			\begin{equation*}
			[T]_{\mathcal B'} = P^{-1}[T]_{\mathcal B}  P.
			\end{equation*}
			Más aún, $P = [I]_{\mathcal B' \mathcal B}$,  donde $I$ al  operador  identidad de $V$.
		\end{teorema}
		\begin{proof} Tenemos que $T = I T$ y $T =T  I$, luego 
			\begin{align*}
				[T]_{\mathcal B'\mathcal B'} &=  [I T]_{\mathcal B'\mathcal B'}& \qquad& \\
				&= [I]_{\mathcal B \mathcal B'} [T]_{\mathcal B' \mathcal B}& &(\text{teorema \ref{th-5.5}})  \\
				&= [I]_{\mathcal B \mathcal B'} [T I]_{\mathcal B' \mathcal B}& &\\
				&= [I]_{\mathcal B \mathcal B'} [T]_{\mathcal B\mathcal B} [I]_{\mathcal B' \mathcal B}& &(\text{teorema \ref{th-5.5}}).
			\end{align*} 
			Es decir
			\begin{equation}\label{eq-cambio de base}
				[T]_{\mathcal B'} = [I]_{\mathcal B \mathcal B'} [T]_{\mathcal B} [I]_{\mathcal B' \mathcal B}.
			\end{equation}
			Falta probar que $[I]_{\mathcal B \mathcal B'}$  es el inverso de  $[I]_{\mathcal B' \mathcal B}$. Ahora bien,
			\begin{align*}
				[I]_{\mathcal B \mathcal B'} [I]_{\mathcal B' \mathcal B}&= [I]_{\mathcal B \mathcal B} = Id,\\
				[I]_{\mathcal B' \mathcal B} [I]_{\mathcal B \mathcal B'}&= [I]_{\mathcal B' \mathcal B'} = Id.
			\end{align*}
		\end{proof}
	
	
		La matriz $P$ del teorema anterior es llamada la \textit{matriz de cambio de base}. \index{matriz!de cambio de base}La fórmula (\ref{eq-cambio de base}) es importante por si misma y debemos recordarla.
	\vskip .3cm
	
		El teorema anterior nos permite definir el determinante de un operador lineal.	Sea $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y $T$ un operador lineal sobre $V$. Sean  $\mathcal B$, $\mathcal B'$ 
		bases ordenadas de $V$, entonces $[T]_{\mathcal B'} = P^{-1}[T]_{\mathcal B}P$, para $P$ una matriz invertible. Por lo tanto, 
		\begin{equation*}
			\det([T]_{\mathcal B'}) = \det(P^{-1}[T]_{\mathcal B}  P) =  \det([T]_{\mathcal B}  PP^{-1}) =  \det([T]_{\mathcal B}). 
		\end{equation*}
		Es decir,  el determinante de la matriz de $T$ en cualquier base siempre es igual.
		
		\begin{definicion} 
				Sea $V$ un espacio vectorial de dimensión finita sobre el cuerpo $\K$ y $T$ un operador lineal sobre $V$. El \textit{determinante de $T$}\index{determinante de una transformación lineal} es el determinante de la matriz de $T$ en alguna base de $V$.  
		\end{definicion}
		\end{section}
	
	
	
	
	
	
	
		\begin{section}{Autovalores y autovectores}
		Sea $V$ espacio vectorial de dimensión finita. Un operador lineal en $V$ es \textit{diagonalizable}\index{matriz!diagonalizable}  si existe una base ordenada $\mathcal B= \{v_1,\ldots,v_n\}$ de $V$ y $\lambda_1,\ldots,\lambda_n \in \K$ tal que 
		\begin{equation}\label{eq-auto-49}
			T(v_i) = \lambda_i v_i,\qquad 1\le i \le n. 
		\end{equation}
		En  general, los operadores diagonalizables permiten hacer cálculos sobre ellos en forma sencilla, por ejemplo el núcleo del  operador definido por (\ref{eq-auto-49}) es $\nuc(T)=\langle v_i: \lambda_i =0 \rangle$ y  su imagen es $\img(T)=\langle v_i: \lambda_i \not=0 \rangle$ (vermos la demostración de estos resultado más adelante). 
		Otra propiedad importante de los operadores diagonalizables es que la matriz de la transformación lineal en una base adecuada es diagonal (de allí viene el nombre de diagonalizable). En  el caso del  operador definido por (\ref{eq-auto-49}) tenemos que
		$$
		[T]_{\mathcal B} = 
		\begin{bmatrix}
		\lambda_1&0&0&\cdots&0 \\
		0&\lambda_2&0&\cdots&0\\
		0&0&\lambda_3&\cdots&0\\
		\vdots&\vdots&\vdots&&\vdots\\
		0&0&0&\cdots&\lambda_n
		\end{bmatrix}.
		$$
		
		
		No todo operador lineal es diagonalizable y no es inmediato, ni sencillo, de la definición de un operador lineal decidir si es diagonalizable o no. En esta sección veremos herramientas para estudiar un operador lineal $T$ y su posible diagonalización. La ecuación (\ref{eq-auto-49}) sugiere se estudien los vectores que son transformados por $T$ en múltiplos de sí mismos.
		
		\begin{definicion}
			Sea $V$ un espacio vectorial sobre el cuerpo $\K$ y sea $T$ un operador lineal sobre $V$. Un \textit{valor propio}\index{valor propio} o \textit{autovalor}\index{autovalor} de $T$ es un escalar $\lambda$ de $\K$ tal que existe un vector no nulo $v \in V$ con $T(v) = \lambda v$. Si $\lambda$ es un autovalor de $T$, entonces
			\begin{enumerate}
				\item  cualquier  $v \in V$ tal que $T(v) = \lambda v$  se llama un \textit{vector propio}\index{vector propio} o  \textit{autovector}\index{autovector} de $T$ asociado al valor propio $\lambda$;
				\item la colección de todos los $v \in V$ tal que $T(v) = \lambda v$  se llama \textit{espacio propio}\index{espacio propio} o \textit{autoespacio}\index{autoespacio} 	asociado a $\lambda$.
			\end{enumerate}
			
			Los valores propios se llaman también a menudo raíces características, eigenvalores. valores característicos o valores espectrales. Nosotros usaremos, preferentemente, ``autovalores''.
			
			Sea ahora $\lambda \in \K$, definimos
			$$
			V_\lambda := \{v \in V: Tv = \lambda v \}.
			$$
			Observar que $V_\lambda \ne 0$ si y sólo si $\lambda$ es autovalor. 
		\end{definicion}
		
		
		\begin{teorema}
			Sea $V$ un espacio vectorial y sea $T:V \to V$ una aplicación lineal. Sea $\lambda \in \K$ entonces, $V_\lambda$  es subespacio de $V$.
		\end{teorema}
		\begin{proof}
			Sean $v_1,v_2 \in V$ tales que $Tv_1 = \lambda v_1$ y $Tv_2 = \lambda v_2$. Entonces
			$$
			T(v_1+v_2) = T(v_1)+ T(v_2) = \lambda v_1 + \lambda v_2 = \lambda (v_1 + v_2),
			$$
			es decir si  $v_1,v_2 \in V_\lambda$, probamos que $v_1+v_2 \in V_\lambda$. 
			
			Sea ahora $c \in F$, entonces $T(cv_i) = cT(v_1) = c\lambda v_1 = \lambda (cv_1)$. Por lo tanto, si  $v_1\in V_\lambda$ y $c \in F$, probamos que $cv_1 \in V_\lambda$.
			
			Esto termina de probar el teorema.
		\end{proof}

		
		
		\begin{teorema}
			Sea $V$ espacio vectorial y sea $T: V \to V$ una aplicación lineal.  Sean $v_1,\ldots,v_m$ autovectores de $T$, con autovalores $\lambda_1,\ldots,\lambda_m$ respectivamente. Suponga que estos  autovalores son distintos entre si, esto es, $\lambda_i \ne \lambda_j$ si $i \ne j$. Entonces $v_1,\ldots,v_m$ son linealmente independientes.
		\end{teorema}
		\begin{proof}
			Hagamos la demostración por inducción sobre $m$.
			
			\textit{Caso base.} Si $m=1$, no hay nada que demostrar puesto que un vector no nulo el LI.
			
			\medskip
			
			\textit{Paso inductivo.} Supongamos que el enunciado es verdadero para el caso $m-1$ con $m>1$, (hipótesis inductiva o HI), y probemos entonces que esto implica que es cierto para $m$. Debemos ver  que si 
			\begin{equation}
			c_1v_1+	c_2v_2+ \cdots c_mv_m = 0 \tag{$*$}
			\end{equation}
			entonces $c_1 = \cdots c_m = 0$.
			Multipliquemos $(*)$ por $\lambda_1$, obtenemos:
			\begin{equation}
			c_1\lambda_1v_1+ c_2\lambda_1v_2+\cdots c_m\lambda_1v_m = 0. \tag{$**$}
			\end{equation}
			También apliquemos $T$ a $(*)$ y obtenemos
			\begin{equation}
			c_1\lambda_1v_1+ c_2\lambda_2v_2+\cdots c_m\lambda_mv_m = 0. \tag{$***$}
			\end{equation}
			Ahora a $(**)$ le restamos $(***)$ y obtenemos:
			\begin{equation}
			c_2(\lambda_1 -\lambda_2)v_2+\cdots c_m(\lambda_1 -\lambda_m)v_m = 0. 	 
			\end{equation}
			Como, por hipótesis inductiva, $v_2,\ldots,v_m$ son LI, tenemos que $c_i(\lambda_1 -\lambda_i)=0$ para $i\ge 2$. Como $\lambda_1 -\lambda_i \ne 0$ para $i\ge 2$, obtenemos que $c_i = 0$ para $i\ge 2$. Por $(*)$ eso implica que $c_1=0$ y por lo tanto $c_i=0$ para todo $i$.
		\end{proof}
		
		\begin{corolario}\label{cor-aut-li}
			Sea $V$ espacio vectorial de dimensión $n$ y sea $T: V \to V$ una aplicación lineal que tiene $n$
			autovectores $v_1,\ldots, v_n$ cuyos autovalores $\lambda_1,\ldots,\lambda_n$ son distintos entre
			si. Entonces $\{v_1,\ldots, v_n\}$ es una base de $V$.
		\end{corolario}
		
		Recordemos que si $T$  es una transformación lineal, el determinante de $T$  se define como el determinante de la matriz de la transformación lineal en una base dada y que este determinante no depende de la base.    
		
		\begin{definicion}
			Sea $A \in M_n(\K)$   el \textit{polinomio característico}\index{polinomio característico} de $A$ es $\chi_A(x) = \det(A-xI)$, donde $x$ es una indeterminada. 
			
			Sea $V$ espacio vectorial de dimensión finita y sea $T: V \to V$ lineal, el  \textit{polinomio característico de $T$} es $\chi_T(x) = \det(x Id-T)$.
		\end{definicion}
	
			
		En general,  si $A = [a_{ij}]$ matriz $n \times n$, tenemos que
		\begin{equation}
		\chi_A(x) = \det(A- x\,Id) = \det
		\begin{bmatrix}
		x -a_{11}&-a_{12}&\cdots&-a_{1n}\\
		-a_{21}&x -a_{22}&\cdots&-a_{2n}\\
		\vdots&\vdots&\ddots&\vdots\\
		-a_{n1}&-a_{n2}&\cdots&x -a_{nn}\\
		\end{bmatrix}
		\end{equation} 
		y el polinomio característico de $A$ es un polinomio  de grado $n$,  más precisamente  
		$$
		\chi_A(x) =x^n + a_{n-1}x^{n-1}+ \cdots + a_1x + a_0.
		$$ 
		Esto se puede demostrar fácilmente por inducción. 
		
		\begin{ejemplo}
			Sea $T: \R^2 \to \R^2$ y su matriz en la base canónica es
			\begin{equation*}
				A = \begin{bmatrix}
					a&b\\c&d
				\end{bmatrix},
			\end{equation*}
		entonces
		\begin{equation*}
				 \det \begin{bmatrix}
				x-a & -b \\ -c &x-d
				\end{bmatrix} = 
				(x-a)(x-d) - bc = x^2 -(a+d)x + (ad -bc).
		\end{equation*}
		Es decir,
		$$
		\chi_T(x) = x^2 -(a+d)x + (ad -bc).
		$$ 
		\end{ejemplo}

	
	
		
		\begin{ejemplo} \label{ej-autovectores}
			Sea
			$$ 
			A=\begin{bmatrix}10&-10&6\\8& -8& 6\\-5& 5& -3\end{bmatrix},
			$$
			entonces el  polinomio característico de $A$ es
			$$
			\det \begin{bmatrix}10-x&-10&6\\8& -8-x& 6\\-5& 5& -3-x\end{bmatrix} = x^3  + x^2 - 6 x .
			$$
			Es posible factorizar esta expresión y obtenemos
			$$
			\chi_A(x) = x (x-2)(x+3).
			$$
		\end{ejemplo}
		
	\begin{comment}
		\begin{ejemplo}
		Sea
		$$ 
		A=\begin{bmatrix}1&2&1\\ 6&-1&0\\ -1&-2&-1\end{bmatrix},
		$$
		entonces el  polinomio característico de $A$ es
		$$
		\det \begin{bmatrix}1-x&2&1\\ 6&-1-x&0\\ -1&-2&-1-x\end{bmatrix} = -x^3 - x^2 + 12 x.
		$$
		Es posible factorizar esta expresión y obtenemos
		$$
		\chi_A(x) = -x(x-3)(x+4).
		$$
		\end{ejemplo}
	\end{comment}	
		
	

		
		\begin{proposicion}\label{autovalores}
			Sea $V$ espacio vectorial de dimensión $n$ y sea $T: V \to V$ lineal. Entonces $\lambda\in \K$ es autovalor si y sólo si $\lambda$ es raíz del polinomio característico.  
		\end{proposicion}
		\begin{proof}${}^{}$
			
			($\Rightarrow$) Si $\lambda$ es autovalor, entonces existe $v \in V$, no nulo, tal que $Tv = \lambda v$, luego 
			$$
			0 = \lambda v-Tv  =  \lambda Id \, v - Tv =  (\lambda Id-T)v.
			$$
			Por lo tanto, $\lambda Id-T$ no es invertible, lo cual implica que $0 = \det(\lambda Id-T) = \chi_T(\lambda)$. Es decir, $\lambda$ es raíz del polinomio característico. 
			
			($\Leftarrow$) Si $\lambda$ es raíz del polinomio característico, es decir si $0 = \chi_T(\lambda) = \det(\lambda Id-T)$, entonces $\lambda Id-T$ no es una matriz invertible, por lo tanto  su núcleo es no trivial. Es decir existe $v \in V$ tal que $(\lambda Id-T)v =0$, luego $Tv =\lambda v$, por lo tanto $v$ es autovector con autovalor $\lambda$.   
		\end{proof}
		
		Repetimos ahora algunos conceptos ya expresados al comienzo de la sección. 
		
		\begin{definicion}
			Sea $V$ espacio vectorial de dimensión finita y sea $T: V \to V$ lineal. Diremos que \textit{$T$ es diagonalizable} si existe una base de $V$ de autovectores de $T$. 
		\end{definicion}	
		
		En el caso que $T$ sea una transformación lineal diagonalizable y $\mathcal{B} = \{v_1,\ldots,v_n \}$ sea una base de autovectores con autovalores $\lambda_1,\ldots,\lambda_n$, entonces
		$$
		T(v_i) = \lambda_i v_i, \qquad 1 \le i \le n,
		$$ 
		y, por lo tanto, la matriz de $T$ en  la base $\mathcal{B}$ es diagonal, más precisamente
		$$
		[T]_\mathcal{B} = \begin{bmatrix}
		\lambda_1 &0 & \cdots & 0 \\
		0 & \lambda_2 & \cdots &0 \\
		\vdots & &\ddots & \vdots \\
		0 & 0 & \ldots &\lambda_n
		\end{bmatrix}
		$$
		
		\begin{ejemplo} Consideremos la transformación lineal de $\R^3$ en $\R^3$  definida por la matriz $A$ del ejemplo \ref{ej-autovectores},  es decir (con abuso de notación incluido)
			\begin{equation*}
				\begin{bmatrix}10&-10&6\\8& -8& 6\\-5& 5& -3\end{bmatrix}
				\begin{bmatrix} x\\y\\z \end{bmatrix} =
				\begin{bmatrix} 10x-10y+6z\\8x -8y +6z \\-5x+5y-3z\end{bmatrix}.
			\end{equation*}
		Ya vimos que  el  polinomio característico de esta aplicación es 
		$$
		\chi_A(x) = x (x-2)(x+3).
		$$
		Luego, por 	proposición \ref{autovalores}, los autovalores de $A$ son $0$, $2$ y $-3$. Debido al corolario \ref{cor-aut-li} existe una base de autovectores de $A$. Veamos cuales son. Si $\lambda$ autovalor de $A$, para encontrar los autovectores con autovalor $\lambda$  debemos resolver la ecuación $Av -\lambda v=0 $,  en este caso sería
		\begin{equation*}
		\begin{bmatrix}10- \lambda &-10&6\\8& -8- \lambda & 6\\-5& 5& -3- \lambda \end{bmatrix}
		\begin{bmatrix} x\\y\\z \end{bmatrix} =
		\begin{bmatrix} 0\\0 \\0\end{bmatrix},
		\end{equation*}  
		para $\lambda =0, 2,-3$. Resolviendo estos tres sistemas, obtenemos que 
		\begin{equation*}
			V_0 = \{(y,y,0): y \in \R \},\quad V_2 = \{(-2z,-z,z): z \in \R \},\quad V_{-3} = \{(-2z,-2z,z): z \in \R \}. 
		\end{equation*}
		Por lo tanto, $\{(1,1,0), (-2,-1,1), (-2,-2,1)\}$ es una base de autovectores de la transformación lineal. 
		\end{ejemplo}
		
		\begin{proposicion} Sea $V$ espacio vectorial de dimensión $n$ y sea $T: V \to V$ lineal tal que tiene una base de autovectores $\mathcal{B} = \{v_1,\ldots,v_n \}$  con autovalores $\lambda_1,\ldots,\lambda_n$. Entonces $\nuc(T)=\langle v_i: \lambda_i =0 \rangle$ e  $\img(T)=\langle v_i: \lambda_i \not=0 \rangle$.			
		\end{proposicion}
		\begin{proof} Reordenemos la base de tal forma que  $\lambda_i =0$ para $1 \le i \le k$ y $\lambda_i \ne 0$ para $k < i \le n$. 
		Todo $v \in V$ se escribe en términos de la base como 
		$$
		v = x_1v_1 + \cdots+ x_k v_k+ x_{k+1} v_{k+1}+\cdots+ x_n v_n,\quad (x_i \in \K),
		$$
		y entonces
		\begin{equation}\label{eq-av-01}
			T(v) =  \lambda_{k+1}x_{k+1} v_{k+1}+\cdots+ \lambda_nx_n v_n.
		\end{equation}
		Luego, $T(v) =0$ si y sólo si $x_{k+1} = \cdots = x_n=0$, y esto se cumple si y solo si $v =  x_1v_1 + \cdots+ x_k v_k$,  es decir $v \in \langle v_i: \lambda_i =0 \rangle$. 
		También es claro por la ecuación (\ref{eq-av-01}) que 
		\begin{align*}
			\img(T) &= \{\lambda_{k+1}x_{k+1} v_{k+1}+\cdots+ \lambda_nx_n v_n: x_i \in \K \} \\
			&=\{\mu_{k+1} v_{k+1}+\cdots+ \mu_n v_n: \mu_i \in \K \}\\
			&= \langle v_i: \lambda_i \not=0 \rangle.
		\end{align*}
		\end{proof}
		
		\begin{ejemplo}
			Sea $T:\R^2 \longrightarrow \R^2$ el operador definido por $T(x,y)=(y,x)$. Probar que $T$ es diagonalizable y encontrar una base de autovectores. 
		\end{ejemplo}
		\begin{proof}
			Por la proposición \ref{autovalores}, los autovalores de $T$  son las raíces del polinomio característico,  es decir las raíces de 
			$$
			\det\left[\begin{matrix}
			-x& 1 \\ 1 & -x
			\end{matrix} \right]= x^2 -1  =(x -1)(x +1). 
			$$
			Luego los autovalores son 1 y $-1$. Para hallar un autovector con autovalor 1 debemos resolver la ecuación $T(x,y) = (x,y)$. Ahora bien, 
			$$
			(x,y) = T(x,y) = (y,x),
			$$
			luego $x =y$ y claramente $(1,1)$ es autovector con autovalor 1.
			
			Por otro lado $T(x,y) = -(x,y)$, implica  que $(y,x) = -(x,y)$, es decir $y = -x$ y claramente podemos elegir $(1,-1)$ como autovector con autovalor $-1$.
			
			Luego $\mathcal{B} = \{(1,1),(1,-1) \}$  es una base de $\R^2$ de autovectores de  $T$.
		\end{proof}
		
		\vskip .3cm
		
		No todas la matrices son diagonalizables,  como veremos en el ejemplo a continuación. 
		
		
		\begin{ejemplo}
			Sea $T:\R^2 \longrightarrow \R^2$ el operador definido por $T(x,y)=(2x-y,x+4y)$. Probar que $T$ tiene un \'unico autovalor $\lambda$ cuyo autoespacio $V_\lambda=\{v\in \R^2: Tv=\lambda v\}$ es de dimensi\'on 1.
		\end{ejemplo}
		\begin{proof} La matriz de  $T$ en la base canónica es 
			$$
			A = \begin{bmatrix}
			2 & -1 \\ 1 & 4
			\end{bmatrix}. 
			$$
			Por la proposición \ref{autovalores}, los autovalores de $T$  son las raíces del polinomio característico,  es decir las raíces de 
			$$
			\det\left[\begin{matrix}
			x-2 & 1 \\ -1 & x-4
			\end{matrix} \right] = (2-x)(4-x)+1 = x^2 -6x + 9 = 
			(x -3)^2. 
			$$
			Es decir el  único autovalor posible es 3.
			
			Debemos ver para que valores  $(x,y)\in \R^2$ se satisface la ecuación
			$$
			T(x,y) = 3(x,y).
			$$
			tiene solución. Esta ecuación es equivalente a 
			\begin{equation*}
				\begin{array}{rcll}
				(2x-y,x+4y)&=&(3 x, 3 y)\quad &\Rightarrow \\
				2x-y = 3 x&,& x+4y = 3y \quad &\Rightarrow \\
				-y =  x&,& x = -y \quad &\Rightarrow \\
				y &=&  -x \quad &
				\end{array}
			\end{equation*}
			Luego $V_3 = \{(x,-x): x \in \R \}$ que es de dimensión 1.  	
			
		\end{proof}
	
		\begin{proposicion}
			 Sea $T$ un operador lineal diagonalizable sobre un espacio vectorial $V$ de dimensión finita. Sean $\lambda_1,\ldots,\lambda_k$ los autovalores distintos de $T$. Entonces,  el  polinomio característico de $T$ es
			 	$$
			 	\chi_T(x) = x -\lambda_1)^{d_1}\ldots(x -\lambda_k)^{d_k}
			 	$$
			 	con
			 	$$
			 	d_i =  \dim V_{\lambda_i},
			 	$$
			 	para  $i=1, \ldots, k$.
		\end{proposicion}
		\begin{proof}[Demostración ($*$)]
			$T$ es un operador lineal diagonalizable y $\lambda_1,\ldots,\lambda_k$ los valores propios distintos de $T$. Entonces existe una base ordenada $\mathcal B$ con respecto a la cual $T$ está representado por una matriz diagonal; es decir, los elementos de la diagonal son los escalares $\lambda_j$ cada uno de los cuales se repite un cierto número de veces. Más específicamente, si
			 $v_{j1},\ldots,v_{jd_j}$ son los vectores en $\mathcal{B}$ con autovalor $\lambda_j$ ($1 \le j \le k$),  reordenamos la base de tal forma que primero estén los autovectores con autovalor $\lambda_1$, a continuación los de autovalor $\lambda_2$, etc.: 
			\begin{equation*}
				\mathcal{B} = \{v_{11},\ldots,v_{1d_1},\ldots,v_{k1},\ldots,v_{kd_k}\}. 
			\end{equation*}
			Ahora bien,  si $v \in V$,  entonces 
			\begin{align*}
			v &=x_1v_{11}+\cdots+x_{d_1}v_{1d_1}+\cdots+x_nv_{n1}+\cdots+x_{d_n}v_{nd_n} \\
			&= v_1 + v_2 +\cdots + v_k
			\end{align*}
			con $v_i = x_iv_{i1}+\cdots+x_{d_i}v_{id_i} \in V_{\lambda_i}$. Luego
			\begin{equation}\label{eq-desc-vectores-propios}
				T(v)  =\lambda_1v_1+\lambda_2v_2\cdots+\lambda_kv_k
			\end{equation} 
			
			Veamos que $V_{\lambda_i} = <v_{i1},\ldots,v_{id_i}>$ para $1 \le i \le k$. Es claro que  $<v_{i1},\ldots,v_{id_i}> \subset V_{\lambda_i}$. Probemos  ahora que,  $V_{\lambda_i} \subset <v_{i1},\ldots,v_{id_i}>$: si  $v \in V_{\lambda_i}$,  entonces $T(v)$ es como en (\ref{eq-desc-vectores-propios}) y, por lo tanto, si $v_j \ne 0$ para $j\not=i$ entonces $T(v) \ne \lambda_j v$, lo que contradice la hipótesis. Es decir $v = v_i \in <v_{i1},\ldots,v_{id_i}>$. Hemos probado que    $V_{\lambda_i} = <v_{i1},\ldots,v_{id_i}>$ y como $v_{i1},\ldots,v_{id_i}$ son LI, entonces $\dim V_{\lambda_i} = d_i$.
			
			Por otro lado, la matriz de $T$ en la base $\mathcal{B}$ tiene la forma
			\begin{equation*}
				\begin{bmatrix}
				\lambda_1 I_1 &0&\cdots&0 \\0&\lambda_2 I_2&\cdots&0 \\\vdots&\vdots&&\vdots \\0&0&\cdots&\lambda_n I_n 
				\end{bmatrix}
			\end{equation*}
			donde $I_j$ es la matriz identidad $d_j \times d_j$. Luego,  el polinomio característico de $T$ es el producto
			\begin{align*}
			(x -\lambda_1)^{d_1}\ldots(x -\lambda_k)^{d_k}.
			\end{align*}
		\end{proof}
	
		\begin{ejemplo}
			Sea $T$ un operador lineal sobre $\R^3$  representado en la base ordenada canónica por la matriz
			\begin{equation*}
				A = 
				\begin{bmatrix}
				5 &-6 &-6\\ -1& 4& 2\\3 &-6& -4
				\end{bmatrix}.
			\end{equation*}
			El  polinomio característico de $A$ es
			\begin{equation*}
				\chi_A(x) = \det \begin{bmatrix}
				x-5 &6 &6\\ 1& x-4& -2\\-3 &6& x+4
				\end{bmatrix} 
				= x^3 - 5 x^2 + 8 x - 4 = (x-2)^2(x-1).
			\end{equation*}
			¿Cuáles son las dimensiones de los espacios de los vectores propios asociados
			con los dos valores propios? Se deben resolver las ecuaciones asociadas a las matrices
			\begin{equation*}
			A - 2I = 
			\begin{bmatrix}
			3 &-6 &-6\\ -1& 2& 2\\3 &-6& -6
			\end{bmatrix} 
			\end{equation*}
			y 
			\begin{equation*}
			A -I= 
			\begin{bmatrix}
			4 &-6 &-6\\ -1& 3& 2\\3 &-6& -5
			\end{bmatrix}.
			\end{equation*}
			Las soluciones de estos sistemas son los autoespacios de autovalor 2 y 1 respectivamente. En  el primer caso, 
			\begin{equation*}
			\begin{bmatrix} 3 &-6 &-6\\ -1& 2& 2\\3 &-6& -6 \end{bmatrix}
			\underset{F_3+3F_2}{\stackrel{F_1+3 F_2}{\longrightarrow}} 
			\begin{bmatrix} 0 &0 &0\\ -1& 2& 2\\0 &0& 0 \end{bmatrix}.
			\end{equation*}
			Luego,  la solución del sistema asociado a $A-2I$ es 
			$$
			V_2 = \{(2y+2z,y,z): y,z \in \R\} = <(2,1,0),(2,0,1)>
			$$
			cuya dimensión es 2. 
			
			Por otro lado, 
			\begin{equation*}
			\begin{bmatrix}4 &-6 &-6\\ -1& 3& 2\\3 &-6& -5 	\end{bmatrix}
			\underset{F_3+3F_2}{\stackrel{F_1+4 F_2}{\longrightarrow}} 
			\begin{bmatrix}0 &6 &2\\ -1& 3& 2\\0 &3& 1 	\end{bmatrix}
			\underset{F_2-F_3}{\stackrel{F_1-2 F_3}{\longrightarrow}}
			\begin{bmatrix}0 &0 &0\\ -1& 0& 1\\0 &3& 1 	\end{bmatrix}.
			\end{equation*}
			Luego,  la solución del sistema asociado a  $A-I$ es 
			$$
			V_1 = \{(z,-\frac13z,z): z \in \R\} = <(1,-\frac13,1)>.
			$$
			
			Entonces, una base de autovectores de $T$ podría ser
			$$
			\mathcal{B} = \{(2,1,0),(2,0,1),(1,-\frac13,1) \}
			$$
			y en esa base la matriz de la transformación lineal es
			\begin{equation*}
			[T]_{\mathcal{B}} = \begin{bmatrix}2 &0 &0\\ 0& 2& 0\\0 &0& 1 	\end{bmatrix}.
			\end{equation*}
		\end{ejemplo}
	
		
		\end{section}
	
	
		
	
		
	\end{chapter}

	\begin{chapter}{Espacio con producto interno}\label{chap-esp-prod-int}
	

	Las propiedades algebraicas de $\R^n$ no son suficientes para hacer frente a ciertas nociones geométricas  como  ángulos, perpendicularidad y longitud. Hemos visto en el  capítulo \ref{chap-vectores} que con la introducción del producto escalar pudimos definir y  trabajar  los conceptos previamente mencionados. En  este capítulo daremos algunas propiedades adicionales del producto escalar, veremos que las matrices simétricas pueden ser interpretadas a partir del producto escalar y, finalmente, probaremos que las matrices simétricas son diagonalizables.
	
	
	\begin{section}{Producto interno}
		
		
		\medskip 
		
		Recordemos que en
		 el  capítulo \ref{chap-vectores} hemos visto que el producto escalar entre dos vectores $x,y \in \R^n$ se define como
		\begin{equation*}
			\la x,y\ra = x_1y_1 + x_2y_2+\cdots+x_ny_n = \sum_{i=1}^{n} x_iy_i.
		\end{equation*}
		También recordemos que si  $x \in \R^n$,  entonces la norma de $x$ es $||x|| = \sqrt{\la x,x\ra} = \sqrt{ \sum_{i=1}^{n} x_i^2}$.
		
		Como hemos visto en  el capítulo \ref{chap-vectores} el producto escalar  cumple cuatro propiedades básicas,  que hemos llamado \ref{prop-P1} (simetría), \ref{prop-P2} y \ref{prop-P3} (bilinealidad o linealidad en cada variable), y \ref{prop-P4} (positividad). Estas son las únicas propiedades que usaremos, y no la definición explícita de producto escalar,  para deducir los resultados de esta sección. 
		
		\begin{definicion} Sea $V$  un espacio  vectorial y  una función
			\begin{equation*}
			\la \,,\,\ra: V \times V \to \R .
			\end{equation*}
			Diremos que $\la \,,\,\ra$ es un \textit{producto interno}\index{producto interno} si para todo $v, w, u \in \R^n$, se satisface:
			
			\begin{enumerate}[label=\textbf{P\arabic*.},ref=P\arabic*]
				\item\label{prop-P1-2}
				\begin{equation*}
					\langle v , w \rangle = \langle w , v \rangle.
				\end{equation*} 	
				\item\label{prop-P2-2} 
				\begin{equation*}
				\langle v , w + u \rangle =\langle v , w \rangle + \langle v , u \rangle = \langle w +u , v \rangle.
				\end{equation*}
				\item\label{prop-P3-2}  Si $\lambda \in \R$, entonces 
				\begin{equation*}
				\langle \lambda v , w \rangle = \lambda \langle v , w \rangle \quad \text{ y } \quad  \langle v , \lambda w \rangle = \lambda \langle v , w \rangle.
				\end{equation*}
				\item\label{prop-P4-2} Si $v=0$ es el vector cero, entonces $\langle v , v \rangle =0$,  de lo contrario
				\begin{equation*}
				\langle v , v \rangle >0
				\end{equation*}
			\end{enumerate}
			Es decir  $\la \,,\,\ra$ es una forma bilineal (\ref{prop-P2-2} y \ref{prop-P3-2}),  simétrica (\ref{prop-P1-2}) y positiva (\ref{prop-P4-2})
		\end{definicion}
	
		
		Obviamente el producto escalar en $\R^n$  es un producto interno,  que llamaremos  el  \textit{producto interno canónico} de $\R^n$. Los resultados de esta sección valen en general para un producto interno en un espacio vectorial de dimensión finita, pero tendremos siempre en mente el producto escalar en $\R^n$. 

			\begin{ejemplo} El producto escalar es uno entre  muchos de los productos internos que podemos tener en $\R^n$, por ejemplo,  en $\R^3$,  la función definida:
				\begin{equation*}
					\langle (x_1,x_2,x_3) , (y_1,y_2,y_3)\rangle =2x_1y_1-x_1y_2-x_2y_1+2x_2y_2-x_2y_3-x_3y_2+2x_3y_3
				\end{equation*}
				Es un producto interno (ejercicio).
			\end{ejemplo}
		
		\begin{ejemplo} También  se puede definir un producto interno en un espacio de dimensión infinita, como veremos a continuación.
			
			Sea $E = C^0([a,b])$ el espacio vectorial cuyos elementos son las funciones continuas $f: [a, b] \to\R$. Se puede definir un producto interno en $E$ de la siguiente manera: sean $f,g \in  C^0([a,b])$,  entonces
			\begin{equation*}
			\la f, g \ra = \int_a^b f(x)g(x) dx.
			\end{equation*}
			Usando las propiedades de la integral es sencillo ver que $\la , \ra $ es una 2-forma, bilineal y simétrica. Por propiedades de las funciones continuas se demuestra que además la 2-forma es positiva. 
			
			Este producto interno se utiliza en el estudio de series de Fourier.
		\end{ejemplo}
			
			
		
		
		\begin{proposicion} Sean   $x,y \in \R^n$. Entonces, 
			\begin{enumerate}
				\item\label{prop-pi-2} Si $c \in \R$, tenemos $||cx|| = |c|||x||$.
				\item\label{prop-pi-3} $||x+y||^2 = ||x||^2 + ||y||^2 + 2\la x,y\ra$. 
			\end{enumerate}
		\end{proposicion}
		\begin{proof}
			
			${}^{}$
			
			\textit{Demostración de \ref{prop-pi-2}.}  Es exactamente, proposición \ref{prop-lambda-norma} (que se demuestra usando \ref{prop-P3-2}).
			
			\textit{Demostración de \ref{prop-pi-3}.}
			\begin{align*}
				||u+v||^2 &= \la u+v,u+v\ra\\
				&= \la u,u+v\ra+ \la v,u+v\ra  \\
				&\overset{(\ref{prop-P2-2})}{=} \la u,u\ra+ \la u,v\ra+\la v,u\ra+ \la v,v\ra  \\
				&\overset{(\ref{prop-P2-2})}{=} \la u,u\ra+ 2\la u,v\ra+ \la v,v\ra  \\
				&\overset{(\ref{prop-P1-1})}{=} ||u||^2 + ||v||^2 + 2\la u,v\ra.
			\end{align*}
			
			
		\end{proof}
		
		\medskip
		
		Recordemos que dos vectores $x,y$  de $\R^n$ son perpendiculares u ortogonales si $\la x,y\ra =0$, lo cual era denotado $x \perp y$. 
		
		\begin{definicion} Sea $X \subset \R^n$, diremos que $X$ es un \emph{conjunto ortogonal} si $v\perp w$ para $v,w \in X$, $v\not= w$. Diremos que $X$ es un \emph{conjunto ortonormal} si $X$ es ortogonal y todos los vectores de $X$ son  \textit{unitarios} (es decir $||v|| =1$ para $v \in X$).
		\end{definicion}
	
		
		\begin{proposicion}\label{ortogonal->ortonormal}
			Sea  $$X = \{v_1,\ldots,v_r \} \subset \R^n$$ un conjunto ortogonal. Sea 
			\begin{equation*}
				X' =  \left\{\frac{v_1}{||v_1||},\ldots,\frac{v_r}{||v_r||} \right\}.
			\end{equation*}
			Entonces $X'$ es  un conjunto ortonormal. 
		\end{proposicion}
		\begin{proof}
			Para demostrar esto debemos ver que dos vectores distintos de $X'$ son ortogonales y que cada vector de $X'$ es de norma 1.
			
			Sea $i \ne j$, entonces
			\begin{equation*}
			\la \frac{v_i}{||v_i||} | \frac{v_j}{||v_j||}\ra = \frac{1}{||v_i||||v_j||} \la v_i | v_j\ra = 0.
			\end{equation*}
			Por otro lado,
			\begin{equation*}
			\la \frac{v_i}{||v_i||} | \frac{v_i}{||v_i||}\ra = \frac{1}{||v_i||^2} \la v_i | v_i\ra=  \frac{1}{||v_i||^2}||v_i||^2 =1.
			\end{equation*}
		\end{proof}	
			
			
			
		
		
		\medskip
		
		\begin{teorema}\label{th-ortogonal-implica-li} Sea $X \subset \R^n$ un conjunto  ortogonal. Entonces $X$ es LI. 
		\end{teorema}
		\begin{proof} Sea $X =\{v_1,\ldots,v_r \}$ y sea $a_1,\ldots,a_r$ en $F$ tales que  $\sum_{i=1}^r a_iv_i =0$. Entonces,  dado $j$ con $1 \le j \le r$,  tenemos 
			$$
			0=\la\sum_{i=1}^r a_iv_i ,v_j \ra = \sum_{i=1}^r a_i\la v_i ,v_j \ra = a_j\la v_j ,v_j \ra = a_j||v_j||^2.
			$$
			Como $X$  es un conjunto ortogonal,  $||v_j|| >0$, luego $a_j =0$ para cualquier $j$. Es decir hemos probado que todos los coeficientes de la suma son cero y por lo tanto $X$  es LI.
		\end{proof}
		
		
		
		\begin{proposicion}
			${}^{}$
			\begin{enumerate}
				\item Teorema de Pitágoras: si $u\perp v$, entonces $||u+v||^2 = ||u||^2 + ||v||^2$.
				\item Ley del Paralelogramo: $||u+v||^2+ ||u-v||^2 = 2||u||^2 + 2||v||^2$.
			\end{enumerate}
		\end{proposicion}
		\begin{proof} Ambas demostraciones se hacen desarrollando las fórmulas y usando las propiedades del producto escalar.
			
			\textit{Demostración de 1.}
			\begin{equation*}
			||u+v||^2 = \la u+v, u+v\ra = \la u, u+v\ra+\la v, u+v\ra = \la u, u\ra+\la u, v\ra+\la v, u\ra+\la v, v\ra.
			\end{equation*} 
			Las igualdades de arriba se deben a la bilinealidad del producto interno. 
			Ahora bien,  como $u\perp v$,  tenemos que $0 = \la u, v\ra=\la v, u\ra$, luego
			\begin{equation*}
			||u+v||^2 = \la u, u\ra+ \la v, v\ra =  ||u||^2 + ||v||^2.
			\end{equation*} 
			
			\textit{Demostración de 2.}
			\begin{align*}
			||u+v||^2+ ||u-v||^2 &= \la u+v, u+v\ra + \la u-v, u-v\ra \\
			&= \la u, u\ra+2\la u, v\ra+\la v, v\ra + \la u, u\ra-2\la u, v\ra+\la v, v\ra \\
			&=  2||u||^2 + 2||v||^2.
			\end{align*}
		\end{proof}
		
		\begin{definicion} Si $X \subset \R^n$ es ortogonal (ortonormal) y es base, diremos que $X$ es una \emph{base ortogonal} (resp. \emph{base ortonormal}) o diremos que $X$ es \textit{BO} (resp. \textit{BON}).
		\end{definicion}
		
		\begin{ejemplo} ${}^{}$
			\begin{enumerate}
				\item La base canónica de $\R^n$ es ortonormal. 
				\item Si $u=(1,1)$, $v=(1,-1)$, entonces $u,v$ es una base ortogonal.  
			\end{enumerate}
		\end{ejemplo}
		
		\begin{proposicion} Sea $ X = \{v_1,\ldots,v_n\}$ una base ortogonal, entonces 
			\begin{equation*}
				X' = \left\{\frac{v_1}{||v_1||},\ldots,\frac{v_n}{||v_n||} \right\}
			\end{equation*}
			es una base ortonormal.
		\end{proposicion}
		\begin{proof}
			Hemos probado en la proposición \ref{ortogonal->ortonormal} que $X'$ es un conjunto ortonormal.  Por  teorema \ref{th-ortogonal-implica-li}. $X'$ es un conjunto LI.   Veamos ahora que es $X'$ genera a $V$. 
			
			Sea $v \in V$, como $X$  es base de  $V$, en particular genera a $V$, luego existen $a_i \in R$, tal que  $v = \sum_i a_i v_i$. Luego
			$$
			v = \sum_i a_i v_i = \sum_i a_i \frac{||v_i||}{||v_i||} v_i = \sum_i (a_i ||v_i||) \frac{v_i}{||v_i||}.
			$$
			Luego $X'$ es un conjunto de generadores de $V$. 
		\end{proof}
		\begin{ejemplo} 
			\begin{enumerate}
				\item Si $u=(1,1)$, $v=(1,-1)$, entonces $||u||=||v|| = \sqrt{2}$ y $(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}),(\frac{1}{\sqrt{2}},\frac{-1}{\sqrt{2}})$ es una base ortonormal.  
			\end{enumerate}
		\end{ejemplo}
		
		\medskip
		
		\begin{observacion}\label{obs-proyeccion-ort}
			No es difícil ver en un dibujo que 
			\begin{equation*}
				\operatorname{pr}_u(v) := \frac{\la u|v\ra}{\la u|u\ra}u
			\end{equation*}
			 es la proyección de $v$ en $u$ y que   $(v - \operatorname{pr}(v)) \perp u$. Es decir,   los vectores
			\begin{equation*}
					u, \quad   v - \frac{\la u|v\ra}{\la u|u\ra}u,
			\end{equation*}
			son ortogonales.
			\begin{figure}[h]
				falta
				\caption{Proyección de $v$ en $u$ cuando $||v||=1$.}\label{fig-proyeccion-ort}
			\end{figure}
		
		
			Esto,  además de la interpretación geométrica, lo podemos demostrar algebraicamente:
			\begin{equation*}
			\la v - \frac{\la u,v\ra}{\la u,u\ra} u , u \ra =
			\la v , u \ra  -\frac{\la u,v\ra}{\la u,u\ra}\la u , u \ra =  \la v , u \ra -  \la u , v \ra =0.
			\end{equation*}
			
		\end{observacion}
		
		\begin{proposicion}[Desigualdad de Schwarz] Sean $u, v\in \R^n$. Entonces 
			$$|\la u, v \ra| \le ||u||||v||.$$
		\end{proposicion}
		\begin{proof} Primero daremos una demostración basada en los resultados geométricos vistos en el capítulo \ref{chap-vectores}, luego,  el lector interesado, podrá leer una demostración algebraica del resultado. 
		
		\textit{Demostración 1.} Esta demostración es válida para el producto escalar en $\R^n$. La fórmula (\ref{eq-cos-theta}) nos dice que 
		\begin{equation*}
			\la u, v \ra = ||u||||v||\cos(\theta)
		\end{equation*}
		donde $\theta$ es el ángulo comprendido entre $u$ y $v$. Como $|\cos(\theta)| \le 1$,  tenemos
		\begin{equation*}
			|\la u, v \ra| = ||u||\,||v||\,|\cos(\theta)| \le  ||u||\,||v||.
		\end{equation*}
		
			
		\textit{Demostración 2 ($*$).}
			Sea $c = \displaystyle\frac{\la u,v\ra}{\la u,u\ra} = \frac{\la u,v\ra}{||u||^2}$, entonces, por la observación \ref{obs-proyeccion-ort}, tenemos que  $ v-cu$ es ortogonal a $u$. Ahora bien, 
			\begin{equation*}
				v = (v - cu) + cu   
			\end{equation*}
			y  $(v - cu) \perp cu$. Por Pitágoras
			\begin{align*}
			||v||^2 &= ||v-cu||^2 +||cu||^2  \\
			&=||v-cu||^2 +|c|^2||u||^2.
			\end{align*}
			Como $||v-cu||^2 \ge 0$, tenemos que $|c|^2||u||^2 \le 	||v||^2$ y sacando raíces cuadradas obtenemos
			$$
			|c|||u|| \le ||v|| \Rightarrow \frac{|\la u,v\ra|}{||u||^2}||u|| \le ||v|| \Rightarrow \frac{|\la u,v\ra|}{||u||}\le ||v|| \Rightarrow |\la u,v\ra|\le ||v||||u||.
			$$
		\end{proof}
		
		\begin{teorema}[Desigualdad triangular] Sean $u, v\in \R^n$, entonces 
			$$||u + v|| \le ||u|| + ||v||	$$
		\end{teorema}
		\begin{proof}
			Probar 
			$$
			||u + v||^2 \le (||u|| + ||v||)^2
			$$
			desarrollando  el lado izquierdo de la desigualdad  como $\la u+v, u+v \ra$ y el lado derecho por el cálculo del binomio al cuadrado. Luego usar la desigualdad de Schwarz.
		\end{proof}
		
		
		\medskip
		
		Ahora vamos a demostrar el proceso de ortonormalización de Gram-Schmidt, que consta de  un algoritmo que permite pasar de una base cualquiera $\{v_1,\ldots, v_n\}$ de $\R^n$ a una base ortonormal $\{u_1,\ldots, u_n\}$, con la importante propiedad de que, para $m$ con  $1 \le m \le n$, el subespacio generado por los vectores $\{u_1,\ldots, u_m\}$ es el mismo que el subespacio generado por los vectores $\{v_1,\ldots, v_m\}$.
		
	
		
		\medskip 
		
		La idea del proceso es sencillo para dos vectores: sean  $v_1,v_2 \in \R^n$ no  nulos y no proporcionales, vimos en la observación \ref{obs-proyeccion-ort} que  los vectores
		\begin{equation*}
			w_1 = v_1, \quad w_2 = v_2 - \operatorname{pr}_{v_1}(v_2) = v_2 - \frac{\la v_1,v_2\ra}{\la v_1,v_1\ra}v_1
		\end{equation*}
		son ortogonales. Ahora bien, $v_1 = w_1$ y $v_2  = \frac{\la v_1,v_2\ra}{\la v_1,v_1\ra}w_1 + w_2$, luego  $w_1,w_2$ generan el mismo subespacio que $v_1,v_2$. Concluyendo, dados  $v_1,v_2$ dos vectores LI, $w_1,w_2$ son dos vectores ortogonales que generan el mismo subespacio. Para $n >2$ la idea es similar. 
		
		\medskip 
		
		
		
		\begin{proposicion}[Proceso de ortogonalización de Gram-Schmidt]
			Sea $\{v_1,\ldots,v_n\}$ una base de $\R^n$. Entonces existe una base ortogonal $\{w_1,\ldots, w_n\}$ tal que el subespacio generado por los vectores $\{w_1,\ldots, w_m\}$ es el mismo que el subespacio generado por $\{v_1,\ldots, v_m\}$ ($1\le m \le n$). Explícitamente, la base es
			\begin{align}
			w_1 &= v_1, \tag{1} \\
			w_2 &= v_2 - \frac{\la v_2,w_1\ra}{\la w_1,w_1\ra}w_1, \tag{2}\\
			w_3 &= v_3 - \frac{\la v_3,w_1\ra}{\la w_1,w_1\ra}w_1- \frac{\la v_3,w_2\ra}{\la w_2,w_2\ra}w_2,\tag{3} \\
			\vdots &\quad \vdots \notag \\
			w_n &= v_n - \frac{\la v_n,w_1\ra}{\la w_1,w_1\ra}w_1- \frac{\la v_n,w_2\ra}{\la w_2,w_2\ra}w_2 - \cdots - \frac{\la v_n,w_{n-1}\ra}{\la w_{n-1},w_{n-1}\ra}w_{n-1}.\tag{$n$}	
			\end{align}
			En forma más breve, para $1 \le i \le n$, 
			\begin{equation}
			w_i = v_i - \sum_{j = 1}^{i-1} \frac{\la v_i,w_{j}\ra}{\la w_{j},w_{j}\ra}w_{j} \tag{$i$}
			\end{equation}
		\end{proposicion} 
		\begin{proof}[Demostración ($*$)]
			
			
			Haremos la demostración por inducción sobre $n$. 
			
			Para $n= 1$ el resultado es trivial.
			
			Supongamos que el resultado valga para $k-1>0$, es decir  $\{w_1,\ldots, w_{k-1}\}$ es ortogonal y 
			$\operatorname{span}(w_1,\ldots, w_{k-1}) = \operatorname{span}(v_1,\ldots, v_{k-1})$. Probemos el resultado para $k$.  Si  $i < k$, 
			\begin{align*}
			\la w_k, w_i \ra &= \la  v_k - \sum_{j = 1}^{k-1} \frac{\la v_k,w_{j}\ra}{\la w_{j},w_{j}\ra}w_{j} , w_i \ra 
			= \la v_k, w_i\ra -  \sum_{j = 1}^{k-1} \frac{\la v_k,w_{j}\ra}{\la w_{j},w_{j}\ra}\la w_{j} , w_i \ra \\
			&=  \la v_k, w_i\ra -  \la v_k, w_i\ra = 0.
			\end{align*}
			Es decir $	\la w_k, w_i \ra =0$ para todo $i < k$. Por consiguiente,  $\{w_1,\ldots, w_{k}\}$ es ortogonal.
			
			Demostremos ahora que $\operatorname{span}\{w_1,\ldots, w_{m}\} = \operatorname{span}\{v_1,\ldots, v_{m}\}$ para $1 \le m \le n$. 
			
			$\operatorname{span}\{w_1,\ldots, w_{m}\} \subset \operatorname{span}\{v_1,\ldots, v_{m}\}$: por la fórmula ($i$) es claro que $w_m$  es combinación lineal de $v_m$ y $w_1,\ldots, w_{m-1}$. Por hipótesis inductiva, los $w_1,\ldots, w_{m-1}$ son combinación lineal de  los $v_1,\ldots, v_{m-1}$,  luego los $w_1,\ldots, w_{m}$ son combinación lineal de los  $v_1,\ldots, v_{m}$.
			
			$\operatorname{span}\{v_1,\ldots, v_{m}\} \subset \operatorname{span}\{w_1,\ldots, w_{m}\}$: Como
			$$
			v_k = w_k + \sum_{j = 1}^{k-1} \frac{\la v_k,w_{j}\ra}{\la w_{j},w_{j}\ra}w_{j},
			$$
			tenemos que 	$\operatorname{span}\{v_1,\ldots, v_{m}\} \subset \operatorname{span}\{w_1,\ldots, w_{m}\}$.
		\end{proof}
		
		
		\medskip
		
		\begin{obs} Sea $W$ subespacio de $\R^n$, entonces existe una base ortogonal de $W$. Esto se deduce del proceso de ortogonalización de Gram-Schmidt: sea $v_1,\ldots,v_k$ una base de $W$ y completamos a $v_{1},\ldots,v_n$ una base de $\R^n$. Por Gram-Schmidt obtenemos una BO $w_{1},\ldots,w_n$ tal que el subespacio generado  por $w_{1},\ldots,w_i$ es igual al subespacio generado por $v_{1},\ldots,v_i$ para $1 \le i \le n$. En  particular $W = \la v_{1},\ldots,v_k \ra =  \la w_{1},\ldots,w_k \ra$ y por lo tanto  $w_{1},\ldots,w_k$ es una BON de $W$.
			
		En la práctica, dada una base $v_1,\ldots,v_k$ de $W$,  con los primeros $k$ pasos del proceso de ortogonalización de Gram-Schmidt obtenemos $ w_{1},\ldots,w_k $ una base ortogonal de $W$.
		\end{obs}
		
		\begin{ejemplo}
			Encontrar una base ortogonal del subespacio de $\R^3$ generado por los vectores $(1,2,-1)$ y $(-2,-1,0)$
			\begin{proof}[Solución] Por Gram-Schmidt:
				\begin{align*}
				w_1 &= (1,2,-1),  \\
				w_2 &= (-2,-1,0) - \frac{\la (-2,-1,0),w_1\ra}{\la w_1,w_1\ra}w_1,
				\end{align*}
				es una base ortogonal de $W$. Calculemos 
				\begin{align*}
					w_2  &= (-2,-1,0) - \frac{\la (-2,-1,0),(1,2,-1)\ra}{\la(1,2,-1),(1,2,-1)\ra}(1,2,-1)\\
					&= (-2,-1,0) - \frac{-4}{6}(1,2,-1) \\
					&= (-2,-1,0) - (\frac{-2}{3},\frac{-4}{3},\frac{2}{3}) \\
					&= (\frac{-4}{3},\frac{1}{3},\frac{-2}{3}).
				\end{align*}
				Para simplificar, multiplicamos a $w_2$ por $3$ y obtenemos que
				\begin{equation*}
					(1,2,-1), (-4,1,-2)
				\end{equation*} 
				es una BO de $W$. 
				
			\end{proof}  
		\end{ejemplo}
		
		\medskip
		
		\begin{definicion}  Sean $U, W$ subespacios de $\R^n$. Diremos que \textit{$U$ es ortogonal a $W$} y denotaremos $U \perp W$ si  para todo $u \in U$ y para todo $w \in W$ tenemos que $ \la u,w\ra =0$. 
			
			Si $X$ es subconjunto de $\R^n$,  definimos 
			$$
			X^\perp := \{u \in \R^n: \la u,x\ra = 0, \forall\, x \in X\} = \{u \in \R^n: \la u,X\ra = 0\}.
			$$ 
		\end{definicion}
		
		\begin{proposicion}
			Sea  $X \subset \R^n$, entonces $X^\perp$  es un subespacio de $\R^n$.
		\end{proposicion}
		\begin{proof}
			Debemos probar que si $u,v \in X^\perp$ y $c \in \R$,  entonces $cu+v \in X^\perp$,  es decir que para todo $x \in X$,  se cumple que  $\la cu+v,x\ra=0$. Ahora bien,
			$$
			\la cu+v,x\ra= c\la u,x\ra +\la v,x\ra= 0.
			$$
		\end{proof}
		
		\begin{definicion}
			Sea $\R^n$ espacio vectorial con producto interno $\la \;,\;\ra$ y sea $X$ subconjunto de $\R^n$. Diremos que $X^\perp$ es el  \textit{subespacio  ortogonal a $X$ en $\R^n$}.   
		\end{definicion}
		
		\vskip .5cm 
	
	\end{section}



\begin{section}{Diagonalización de matrices simétricas}
	

	
	
	Observar que en $\R^n$ el producto escalar es 
	\begin{equation*}
		\la (x_1,\ldots,x_n),(y_1,\ldots,y_n)  \ra = \sum_i x_iy_i = \begin{bmatrix} x_1& \cdots &x_n\end{bmatrix}\begin{bmatrix} y_1 \\ \vdots \\ y_n \end{bmatrix}.
	\end{equation*}
	Es decir si usamos la convención que un vector en $\R^n$  se escribe como una matriz columna (de $n$ filas y una columna),  tenemos que dados $x,y \in \R^n$, 
	\begin{equation*}
		\la x,y \ra = x^\t y.
	\end{equation*}
	
	Recordemos que una matriz es simétrica si $A^\t =A$. 
	
\begin{proposicion}
	Si $A$ es una matriz $n \times n$ simétrica, entonces 
	\begin{enumerate}[label=\emph{\alph*})]
		\item 
		\begin{equation*}
		\la Ax,y \ra =  \la x,Ay \ra .
		\end{equation*}
		\item Si $W$ un subespacio de $\R^n$ invariante por $A$, entonces $W^\perp$ es invariante por $A$.
	\end{enumerate}
\end{proposicion}
\begin{proof}
	Notar que
	\begin{equation*}
	\la Ax,y \ra = (Ax)^\t y = x^\t A^\t y  = x^\t A y = \la x,Ay \ra 
	\end{equation*}
\end{proof}
	
	
	
	\begin{proposicion}\label{wperpinv}
		Sea $A$ matriz simétrica $n \times n$ y $W$ un subespacio de $\R^n$ invariante por $A$, entonces $W^\perp$ es invariante por $A$.
	\end{proposicion}
	\begin{proof}
		Sea $u \in W^\perp$,  debemos ver que $Au \in W^\perp$,  es decir que $\la Au,w\ra =0$ para todo $w \in W$. Ahora bien,
		\begin{equation*}
			\la Au,w\ra =(Au)^\t w = u^\t A^\t w =u^\t A w = u^\t (A w) = \la u, Aw\ra=0.  
		\end{equation*}
		La útima igualdad es válida pues $u \in W^\perp$ y  como $w \in W$, por hipótesis $Aw \in W$.
	\end{proof}
	
	
	
 	\begin{comment}
 		\begin{proposicion} Sean $A$ y $B$ dos matrices simétricas. Entonces, $AB$  es simétrica si y sólo  si $A$ y $B$ conmutan. 
 		\end{proposicion}
 		\begin{proof}
 		($\Rightarrow$) Como $AB$ es simétrica, tenemos que $AB = (AB)^\t$. Por proposición \ref{prop-matriz-transpuesta} tenemos que  $(AB)^\t = B^\t A^\t$, y  como $A,B$ son simétricas, $B^\t A^\t = BA$. 
 		Reconstruyendo las igualdades tenemos
 		$$
 		AB = (AB)^\t = B^\t A^\t = BA,
 		$$
 		es decir, $A$ y $B$ conmutan.
 		
 		\vskip .3cm
 		
 		($\Leftarrow$)  
 		$$
 		(AB)^\t = B^\t A^\t = BA=AB.
 		$$
 		\end{proof}
 		
 		
 		\begin{ejemplo}
 		Sean 
 		\begin{equation*}
 		A = \begin{bmatrix} 1&0\\0&2 \end{bmatrix} \quad \text{ y } \quad B = \begin{bmatrix}0&1\\1&0\end{bmatrix}.
 		\end{equation*}
 		Tanto $A$ como $B$ son matrices simétricas. Sin embargo, 
 		\begin{align*}
 		AB = \begin{bmatrix}0&1\\2&0\end{bmatrix} 
 		\end{align*}
 		no es simétrica. Esto ocurre, pues $A$ y $B$ no conmutan. 
 		\end{ejemplo}
 		
 	\end{comment}
	
	
	
	\vskip 0.5 cm
	Veremos ahora que una matriz simétrica es diagonalizable, es decir que hay una base de autovectores del operador asociado a la matriz o, equivalentemente, existe matriz $P$ invertible tal que $P^{-1}AP$ es diagonal. 
	
	\vskip .3cm
	
	Usaremos el siguiente resultado sin demostración.
	
	\begin{teorema}[Teorema fundamental del álgebra]
		Todo polinomio no constante con coeficientes complejos tiene al menos una raíz compleja. Es decir si 
		\begin{equation*}
			\text{$p(x) = a_nx^n+ a_{n-1}x^{n-1} +\cdots+a_0$, con $a_i \in \mathbb{C}$,  $a_n\ne 0$ y $n\ge 1$,}
		\end{equation*}
		entonces existe $\alpha \in \mathbb{C}$ tal que $p(\alpha)=0$.
	\end{teorema}

	Pese a llamarse ``Teorema fundamental del álgebra'', este resultado  no suele demostrarse en los cursos de álgebra, pues su demostración requiere del uso de análisis matemático.
	
	Si $\alpha$ es raíz de $p$, un polinomio de grado $n$, por  el teorema del resto,  $p(x) = (x-\alpha)p_1(x)$, con  $p_1$ un polinomio de grado $n-1$.  Aplicando inductivamente este procedimiento, podemos deducir:
	
	\begin{corolario} Si $p$ es un polinomio de de grado $n\ge 1$ con coeficientes en $\C$,  entonces
		\begin{equation*}
			p(x)= c(x-\alpha_1)(x-\alpha_2)\ldots(x-\alpha_n),
		\end{equation*}
		con $c,\alpha_i \in \C$.
	\end{corolario}
	
	
	
	
	\begin{obs}\label{5.9} 	Recordemos que si $a+bi \in \mathbb{C}$, $a$ es la \textit{parte real} y $b$ es la \textit{parte imaginaria}. El conjugado $a+b_i$ es $\overline{a +bi} = a-bi$. La conjugación cumple que $\overline{\overline{z}} = z$, $\overline{z +w} = \overline{z}+\overline{w}$ y $\overline{z w} = \overline{z}\,\overline{w}$ ($z,w \in \mathbb{C}$). Recordemos también que $z\overline{z} = |z|^2$.
		
		Si $x \in \mathbb{C}^n$,  entonces cada coordenada de $x$ es un número complejo,  es decir $x_i = a_i + ib_i$, con $a_i,b_i \in \R$. Luego si $v = (a_1,\ldots,a_n)$ y $w = (b_1,\ldots,b_n)$, tenemos que  $x = v + wi$  con  $v,w \in \R^n$.  En  este caso, diremos que $v$  es la parte real de $x$ y $w$ la parte imaginaria. También podemos extender la conjugación a $\mathbb{C}^n$ y  $\mathbb{C}^{n \times m}$ coordenada a coordenada y entonces no es difícil verificar que si $A, B \in \mathbb{C}^{n \times m}$
		\begin{equation*}\overline{\overline{A}} = A,\qquad \overline{A+B} = \overline{A}+\overline{B},
		\end{equation*}
		y que si $A\in \mathbb{C}^{n \times m}$, $ B \in \mathbb{C}^{m \times k}$, $\alpha \in \mathbb{C}$, entonces $\overline{\alpha A} =\overline{\alpha}\, \overline{A}$ y además
		\begin{equation*}\overline{\alpha A B} =\overline{\alpha}\, \overline{A}\,\overline{B} = \overline{A}\,\overline{\alpha B}.
		\end{equation*} 
		Notar también que si  $z = (z_1,\ldots,z_n)$, 
		\begin{equation*}z^\t \, \overline{z} = [ z_1 \, \ldots \, z_n ]
		{\begin{bmatrix} \overline{z_1} \\\vdots \\\overline{z_n} \end{bmatrix}} = |z_1|^2+\cdots+|z_n|^2,
		\end{equation*}   
		que es $>0$ si el vector no es nulo. Denotaremos la expresión de arriba como $||z||^2$.
		
	\end{obs}
	
	\begin{teorema}\label{th-existeauto}
		Sea $A$ matriz simétrica $n \times n$, con coeficientes reales. Entonces existe $\lambda \in \R$ autovalor de $A$.   
	\end{teorema}
	\begin{proof} Extendamos $A$  a una transformación lineal de $A: \mathbb{C}^n \to \mathbb{C}^n$ de manera natural (con el producto de matrices). Sea $\chi_A$ el polinomio característico de $A$. Por el teorema fundamental  del álgebra, existe $\lambda \in \mathbb{C}$ tal que $\chi_A(\lambda)=0$. Luego existe $x \in \mathbb{C}^n$, no nulo,  tal que $Ax = \lambda x$.  Veremos que $\lambda$ es un número real. Por un lado, como $A$ tiene coeficientes reales, tenemos que $A = \overline{A}$ y entonces:
		\begin{equation*}
		x^\t A\overline{x} = x^\t \overline{A}\overline{x} =x^\t\overline{Ax} =x^\t\overline{\lambda x} = \overline{\lambda} x^\t\overline{x} = \overline{\lambda} ||x||^2 .
		\end{equation*}
	Por otro lado,  como $A$  es simétrica,
		\begin{equation*}
		x^\t A\overline{x} = x^\t A^t\overline{x}= (A x)^\t\overline{x} = (\lambda x)^\t\overline{x} = \lambda x^\t\overline{x} = \lambda ||x||^2. 
		\end{equation*}
	Por lo tanto, $\lambda = \overline{\lambda}$, lo cual nos dice que ${\lambda} \in \R$.  Es decir, tenemos un vector $x \in \C^n$ y $\lambda \in \R$, tal que $Ax=\lambda x$. Si  $x = v +iw$ con $v,w \in \R^n$, entonces 
		\begin{equation*}
		\lambda v + i \lambda w = \lambda x = Ax = Av + i Aw.
		\end{equation*}
	Como $A$ es una matriz real $Av,Aw \in \R^n$ y como $\lambda \in \R$, tenemos que $Av = \lambda v$ y $Aw = \lambda w$, luego hay al menos un autovector en $\R^n$ con autovalor $\lambda \in \R$. 
	\end{proof}
	
	\vskip .3cm
	
	El siguiente resultado, el \textit{teorema espectral}, requiere para su demostración una generalización del resultado anterior para espacios de producto interno  de dimensión finita y matrices (transformaciones lineales)  simétricas respecto a este producto interno. Todos estos conceptos y resultados son generalizaciones sencillas, pero llevan algún tiempo desarrollarlas y no las veremos ahora. La demostración que daremos del teorema espectral se hará por inducción en la dimensión del espacio y utiliza en el paso inductivo la generalización del teorema \ref{th-existeauto}.   
		
	\begin{teorema}[Teorema espectral]\index{Teorema espectral} Sea $A$ matriz simétrica $n \times n$. Entonces existe $\mathcal{U} = \{u_1,\ldots,u_n\}$ una BON de $\R^n$ de autovectores de $A$.
	\end{teorema}
	\begin{proof}		
		Se hará por inducción en $n= \dim(V)$.
		
		Si $n=1$ es trivial. 
		
		Supongamos que vale  para $n-1$ con $n >1$, y  probaremos el resultado para $n$. Por le teorema  \ref{th-existeauto} existe $\lambda \in \R$ y $x \in \R^n$ tal que $Ax = \lambda x$. Si $u_n = x/||x||$, $u_n$ tiene norma 1 y  cumple también que $Au_n = \lambda u_n$. Sea $W = span\{u_n\}$. Entonces por  corolario \ref{wperpinv}, $W^\perp$ es invariante por $A$. Podemos considerar entonces a  $A$ como una transformación lineal de $W^\perp$ a $W^\perp$. No es difícil verificar que el producto interno canónico es un producto interno en $W^\perp$ y que $A$ es una matriz simétrica con este producto interno restringido.  
		
		Como $\dim(W^\perp) = n-1$, por hipótesis inductiva existe   $\{u_1,\ldots,u_{n-1}\}$ una BON de $W^\perp$ de autovectores de $A: W^\perp \to W^\perp$. Es claro entonces que $\mathcal{U} = \{u_1,\ldots,u_n\}$  una BON de $\R^n$ de autovectores de $A$. 
	\end{proof}

	\begin{corolario}
		 Sea $A$ matriz simétrica $n \times n$, entonces $A$ es diagonalizable.
	\end{corolario}


	\begin{ejemplo} Encontremos autovalores y autovectores de  la matriz
		\begin{equation*}
			A = \begin{bmatrix} 2&-1&0\\-1&2&-1\\0&-1&2 \end{bmatrix}.
		\end{equation*}
		Como es una matriz simétrica sabemos que es diagonalizable, es decir tiene una base de autovectores. 
		El polinomio característicos es
		\begin{equation*}
			\chi_A(x) = \det  \begin{bmatrix} 2-x&-1&0\\-1&2-x&-1\\0&-1&2-x \end{bmatrix} =	-x^3 + 6 x^2 - 10 x + 4.
		\end{equation*}
		Ahora bien, las raices de $-x^3 + 6 x^2 - 10 x + 4 $  son 
		\begin{align*}
			\lambda_1 &= 2 + \sqrt 2 \\
			\lambda_2 &= 2 \\
			\lambda_3 &= 2 - \sqrt 2
		\end{align*}
		Para averiguar los autovectores debemos plantear las ecuaciones $Ax = \lambda_ix$,  que resultan en los siguiente sistema de ecuaciones
		\begin{align*}
		\begin{split}
		2x_1 - x_2  &= \lambda_i x_1 \\
		-x_1 + 2x_2 -x_3 &=\lambda_ix_2 \\
		-x_2 + 2x_3 &= \lambda_ix_3,  
		\end{split}
		\end{align*}
		($i=1,2,3$),  o equivalentemente,
		\begin{align*}
		\begin{split}
		(2 -\lambda_i)x_1+ x_2  &= 0 \\
		-x_1 +(2-\lambda_i)x_2  -x_3 &=0 \\
		-x_2 + (2-\lambda_i)x_3 &= 0,  
		\end{split}
		\end{align*}
		($i=1,2,3$).
		En el caso de $\lambda_1= 2 + \sqrt 2$,  resulta
		\begin{align*}
		\begin{split}
		-\sqrt 2x_1+ x_2  &= 0 \\
		-x_1 -\sqrt 2x_2  -x_3 &=0 \\
		-x_2 -\sqrt 2x_3 &= 0,  
		\end{split}
		\end{align*}
		cuya solución es $\lambda(1, -\sqrt2, 1)$. Si continuamos resolviendo los sistemas de ecuaciones, podemos encontrar la siguiente base de autovectores:
		\begin{align*}
		v_1 &= (1, -\sqrt2, 1) \\		
		v_2 &= (-1, 0, 1) \\		
		v_3 &= (1, \sqrt2, 1).
		\end{align*}
		
	\end{ejemplo}
	
	\end{section}

\end{chapter}



\appendix
\setcounter{chapter}{0}
\renewcommand{\thechapter}{\Alph{chapter}}

\begin{chapter}{Números complejos}\label{chap-num-compl}
	
	\begin{section}{Cuerpos}
		
		En el cuatrimestre pasado se ha visto el concepto de cuerpo, del cual haremos un repaso (ver también  \href{https://es.wikipedia.org/wiki/Cuerpo\_(matemáticas)}{https://es.wikipedia.org/wiki/Cuerpo\_(matemáticas)}).
		
		
		\begin{definicion}
			Un  conjunto $\K$ es un \textit{cuerpo}\index{cuerpo} si es un anillo de división conmutativo, es decir, un anillo conmutativo con unidad en el que todo elemento distinto de cero es invertible respecto del producto. Por tanto,  un cuerpo es un conjunto $\K$ en el que se han definido dos operaciones, '$+$' y '$\cdot$', llamadas \textit{adición} y \textit{multiplicación} respectivamente, que cumplen las siguientes propiedades. En la siguiente lista de axiomas $a$, $b$, $c$ denotan elementos arbitrarios de $\K$, y $0$ y $1$ denotan elementos especiales de $\K$ que cumplen las propiedades especificadas más abajo.
			\begin{enumerate}
				\item[{\bf I1.}] $a+b$ y $a\cdot b$ pertenecen a ${\K}$.
				\item[{\bf I2.}] {\em Conmutatividad.}\, $a+b = b+a$; $ab=ba$. 
				\item[{\bf I3.}] {\em Asociatividad.}\, $(a+b)+c = a+(b+c)$;\; $(a\cdot b)\cdot c = a\cdot (b\cdot c)$. 
				\item[{\bf I4.}] {\em Existencia de elemento neutro.}\, Existen números $0$, $1 \in \K$ con $0\not=1$ tal que $a+0=a$; $a\cdot 1=a$. 
				\item[{\bf I5.}] {\em Distributividad.}\, $a\cdot (b+c)=a\cdot b+a\cdot c$. 
				\item[{\bf I6.}] {\em Existencia del inverso aditivo.}\, Por cada $a$ en ${\K}$ existe un único  $-a$ en ${\K}$ tal que $a+(-a)=0$. 
				\item[{\bf I7.}] {\em Existencia de inverso multiplicativo.}\, Si $a$ es distinto de 0, existe un único elemento $a^{-1} \in \K$  tal que $a\cdot a^{-1}=1$. 
			\end{enumerate}
		\end{definicion}
		
		Muchas veces denotaremos el producto yuxtaponiendo los elementos,  es decir $ab := a\cdot b$, para $a,b \in \K$. Debido a la ley de asociatividad para la suma (axioma {\bf I3}) $(a+b)+c$ es igual a $a+(b+c)$ y por lo tanto podemos eliminar los paréntesis sin ambigüedad. Es decir, denotamos
		$$
		a+b+c := (a+b)+c = a+(b+c).
		$$
		De forma análoga, usaremos la notación
		$$
		abc = (ab)c = a(bc).
		$$
		Debido a la ley de conmutatividad (axioma {\bf I2}), es claro que del axioma {\bf I4} se deduce que  $0+a=a+0=a$ y $1a = a1=a$. Análogamante,  por  {\bf I2} e  {\bf I6} obtenemos que  $-a+a =   
		a+(-a)=0$, y por {\bf I6} que  $a a^{-1} = a^{-1}a=1$.
		
		
		
		Todos los axiomas corresponden a propiedades familiares de los cuerpos que ya conocemos,  como ser el cuerpo de los números reales, denotado $\R$ y el cuerpo de los números racionales (fracciones),  denotado $\Q$. De ellas pueden deducirse la mayoría de las reglas comunes a los cuerpos. Por ejemplo, podemos {\it definir} la operación de sustracción diciendo que $a-b$ es lo mismo que $a+(-b)$; y deducir las reglas elementales por ejemplo,
		\begin{equation*}
		a-(-b) = a+b, \qquad -(-a) = a.
		\end{equation*}	
		También podemos deducir
		\begin{equation*}
		(ab)^{-1} = a^{-1}b^{-1}
		\end{equation*}
		con tal que $a$ y $b$ sean diferentes de cero. Otras reglas útiles incluyen	
		\begin{equation*}
		-a = (-1)a 
		\end{equation*}					
		y más generalmente
		\begin{equation*}
		- (ab) = (-a) b = a  (-b),
		\end{equation*}	
		y  también 
		\begin{equation*}
		ab = (-a) (-b),
		\end{equation*}
		así como
		\begin{equation*}
		a\cdot 0 = 0,
		\end{equation*}
		todas reglas familiares de la aritmética elemental.
	\end{section}
	
	
	\begin{subsection}{Un cuerpo finito}
		A modo de ejemplo, y para entrenar la intuición de que un cuerpo no necesariamente tiene un número infinito de elementos, consideremos el conjunto con dos elementos $\F_2=\{0,1\}$. Definimos la suma $+\colon\F_2\times \F_2\to \F_2$ mediante la regla
		\begin{align*}
		0+0&=0, & 0+1&=1, & 1+0&=1, & 1+1&=0
		\end{align*}
		y el producto $\cdot \colon\F_2\times \F_2\to \F_2$ como 
		\begin{align*}
		0\cdot 0&=0, & 0\cdot 1&=0, & 1\cdot 0&=0, & 1\cdot 1&=1.
		\end{align*}
		Dejamos como ejercicio para el lector comprobar que estas operaciones así definidas satisfacen los axiomas {\bf I1.} a {\bf I7.} y por lo tanto $\F_2$ es un cuerpo, con dos elementos.
		
		\begin{obs}
			El lector suspicaz reconocerá en estas operaciones a la suma y el producto definidos en el conjunto $\Z_2=\{\overline{0},\overline{1}\}$ de congruencias módulo 2  definido en Álgebra 1/Matemática Discreta I. En efecto, resultados desarrollados en ese curso permiten demostrar que los conjuntos $\Z_p$, con $p$ primo, son ejemplos de cuerpos, en este caso con $p$ elementos.
		\end{obs}
		
	\end{subsection}
	
	
	\begin{section}{Números complejos}
		La ecuación polinómica $x^2 + 1 =0$ (¿cuál es el número que elevado  al cuadrado y adicionado 1 da 0?) no tiene solución dentro del cuerpo de los números reales,  pues todos sabemos que  $x^2 \ge 0$ para todo $x \in \R$ y por lo tanto $x^2 + 1 >0$ $\forall\; x \in \R$. Podemos extender $\R$ a otro cuerpo,  de tal forma que \textit{toda} ecuación polinómica con coeficentes en $\R$ tenga solución. 
		
		\begin{definicion} 
			Los \textit{números complejos}\index{números complejos} es el conjunto $\C$  de los pares ordendados $(a,b)$,  denotados $a+ib$, con $a, b$  en $\R$, con las operaciones '$+$' y '$\cdot$', definidas
			\begin{align}
			(a+ib)+ (c+id) &:= (a+c) + i(c+d), \label{sumacompleja} \\
			(a+ib) \cdot (c+id) &:= (ac -bd) + i(ad+bc). \label{productocomplejo}
			\end{align}
			Al número complejo $i = 0 + i\cdot 1$ lo llamamos el \textit{imaginario puro}.  Si $z= a + ib$  es un número complejo,  diremos que $a$ es la \textit{parte real} de $z$ y  la denotamos $a = \operatorname{Re} z$. Por otro lado,  $b$ es la \textit{parte  imaginaria} de $z$ que es denotada $b = \operatorname{Im} z$.
		\end{definicion}
		
		Es claro  que $z=a+ib$ es igual a $w = c+id$ si coinciden su parte real e imaginaria, es decir
		\begin{equation*}
		a+ bi = c+ di\quad \Leftrightarrow\quad a=c \;\wedge\; b = d.
		\end{equation*}
		
		Podemos  ver a $\R$ contenido en $\C$,  con la correspondencia $a \to a + i \cdot 0$ y  observamos que si  nos restringimos a $\R$, tenemos las reglas de adición y  multiplicación usuales.  
		
		La definición de la suma de dos números complejos no debería sorprendernos, pues es la suma ``coordenada a coordenada''. La definición del producto se basa en que deseamos que $i^2 = -1$,  es decir que $i$  sea la solución de la ecuación polinómica $x^2 + 1 =0$,   y que el producto sea distributivo. 	
		
		Primero, comprobemos que  $i^2 = -1$. Esto es debido a que
		\begin{equation*}
		i^2 = (0 + i\cdot 1)(0 + i\cdot 1) = (0\cdot 0 - 1 \cdot 1) + i(0\cdot 1 + 1 \cdot 0) = -1,
		\end{equation*} 
		y por lo tanto $i^2 + 1 = -1+1 = 0$.  
		
		
		Sean $0 = 0 + i\cdot 0, 1 = 1 + i\cdot 0 \in \C$,  es fácil comprobar que son los elementos neutros de la suma y el producto,  respectivamente. Por otro lado, si $z = a + ib$,  entonces $-z = -a -ib$ es el opuesto aditivo de $z$. 
		El inverso multiplicativo es un poco más complicado. Primero observemos que dado $a+ib \in \C$,
		\begin{equation*}
		(a+ ib)(a-ib) = aa -b(-b) = a^2 + b^2 \in \R. 
		\end{equation*}
		Supongamos que $a+ib\ne0$,  encontremos  a partir  de las reglas de adición y multiplicación la inversa de $z$. Sea $c+id$ tal que $(a+ib)(c+id)=1$, luego
		\begin{equation*}
		c + id = \frac{1}{a+ib} = \frac{1}{a+ib}\,\frac{a-ib}{a-ib} = \frac{a-ib}{(a+ib)(a-ib)} = 
		\frac{a-ib}{ a^2 + b^2} = \frac{a}{ a^2 + b^2} - i\frac{b}{ a^2 + b^2}
		\end{equation*}  
		(observar que como $a+ib\ne0$,  entonces $a^2 + b^2 >0$.)
		
		Usando lo anterior,  y un poco más de trabajo, obtenemos
		
		\begin{proposicion}
			Sean $0 = 0 + i\cdot 0, 1 = 1 + i\cdot 0\in \C$. Entonces, $\C$ con las operaciones '$+$' y '$\cdot$', definidas en (\ref{sumacompleja}) y (\ref{productocomplejo}),  respectivamente, es un cuerpo con elementos neutros $0$ y $1$, y
			\begin{align*}
			-(a+ib) &= -a -ib \\
			(a+ib)^{-1} &= 	\frac{a-ib}{ a^2 + b^2}, \qquad \text{para $a+ib \ne 0$}.
			\end{align*}
		\end{proposicion}
		\begin{proof}
			Ejercicio.
		\end{proof}
		
		\vskip .3cm
		
		Hemos definido los números complejos como pares ordenados y como tales es posible representarlos en el plano $\R \times \R$:
		
		
		
		\begin{figure}[h]
			\begin{tikzpicture}
			\draw[->] (-4.0,0) -- (4.0,0) node[right] {}; % eje x
			\draw[->] (0,-3) -- (0,3) node[above] {}; % eje y
			\draw[fill] (2.5,1.5) circle [radius=0.05];
			\node [right] at (2.5,1.5) {$a+ ib$};
			\node [below] at (2.5,-3pt) {$a$};
			\node [left] at (-3pt,1.5) {$b$};
			\draw (2.5,-3pt) -- (2.5,3pt);
			\draw (-3pt, 1.5) -- (3pt, 1.5);
			\draw [dashed] (0,1.5) -- (2.5,1.5);
			\draw [dashed] (2.5,0) -- (2.5,1.5);
			\end{tikzpicture}
			
			
			\caption{Representación gráfica de los números complejos. }
		\end{figure}
		
		
		\begin{figure}[h]
			\begin{tikzpicture}
			\draw[->] (-4.0,0) -- (4.0,0) node[right] {}; % eje x
			\draw[->] (0,-3) -- (0,3) node[above] {}; % eje y
			\foreach \x in {-4,...,-1}
			\draw (\x,3pt) -- (\x,-3pt)
			node[anchor=north] {\x};
			\foreach \x in {1,...,4}
			\draw (\x,3pt) -- (\x,-3pt)
			node[anchor=north] {\x};
			\foreach \y in {-3,...,-1}
			\draw (3pt,\y) -- (-3pt,\y) 
			node[anchor=east] {\y}; 
			\foreach \y in {1,...,3}
			\draw (3pt,\y) -- (-3pt,\y) 
			node[anchor=east] {\y}; 
			\draw[fill] (2,1) circle [radius=0.05];
			\node [right] at (2,1) {$2+ i$};
			\draw [dashed] (0,1) -- (2,1);
			\draw [dashed] (2,0) -- (2,1);
			\draw[fill] (-1,2.5) circle [radius=0.05];
			\node [left] at (-1,2.5) {$-1+i \,2.5$};
			\draw [dashed] (0,2.5) -- (-1,2.5);
			\draw [dashed] (-1,0) -- (-1,2.5);
			\draw[fill] (-2.5,-2.5) circle [radius=0.05];
			\node [below] at (-2.5,-2.5) {$-2.5-i\,2.5$};
			\draw [dashed] (0,-2.5) -- (-2.5,-2.5);
			\draw [dashed] (-2.5,0) -- (-2.5,-2.5);
			\end{tikzpicture}
			
			
			\caption{Ejemplos de la representación gráfica de los números complejos. }
		\end{figure}
		
		\vskip .3cm
		
		Por el teorema de Pitágoras, la distancia del  número complejo $a+ib$ al $0$ es $\sqrt{a^2+b^2}$.
		
		\begin{definicion} Sea $z = a + ib \in \C$. El \textit{módulo} de $z$ es
			\begin{equation*}
			|z| = \sqrt{a^2+b^2}.
			\end{equation*}
			El  \textit{conjugado} de $z$ es
			\begin{equation*}
			\bar z = a-ib.
			\end{equation*}
		\end{definicion} 
		
		\vskip .5cm 
		
		
		\begin{ejemplo}
			$|4+3i| = \sqrt{4^2+3^2} = \sqrt{25} =5$, $\overline{4+3i} = 4-3i$.
		\end{ejemplo}
		
		\vskip .5cm 
		
		\begin{proposicion} Sean $z$ y $w$ números complejos.
			\begin{enumerate}
				\item $z\bar{z} = |z|^2$.
				\item Si $z \ne 0$, $z^{-1} = \displaystyle\frac{\overline{z}}{|z|^2}$.
				\item  $\overline{z+w} = \overline{z} + \overline{w}$.
				\item  $\overline{zw} = \overline{z}\;  \overline{w}$.
			\end{enumerate}
			\begin{proof}
				Son comprobaciones rutinarias. Para ejemplificar, hagamos la demostración  de (4).
				
				Si $z = a + bi$ y $w = c +di$, entonces $(a+bi) (c+di) = (ac -bd) + (ad+bc)i$. Por lo tanto,
				\begin{equation*}
				\overline{zw} = (ac -bd) - (ad+bc)i.
				\end{equation*} 
				Como $\overline{z} = a - bi$ y $\overline{w} = c -di$,
				\begin{equation*}
				\overline{z}\;  \overline{w} = (ac -(-b)(-d)) + (a(-d)+b(-c) )i = (ac -bd) - (ad+bc)i.
				\end{equation*} 
				Por lo tanto $	\overline{zw} = \overline{z}\;  \overline{w}$.
			\end{proof}	
		\end{proposicion}
		
		
		\begin{ejercicio}
			Determinar el número complejo $2- 3i + \displaystyle\frac{i}{1-i}$.
		\end{ejercicio}
		\begin{proof}[Solución] 
			El  ejercicio nos pide que escribamos el número en el formato $a + bi$. En general, para eliminar un cociente donde el divisor tiene parte imaginaria no nula, multiplicamos arriba y abajo por el conjugado del divisor, como $z\overline{z} \in \R$, obtenemos un divisor real. En  el ejemplo: 
			\begin{align*}
			2+ 3i + \frac{i}{1-i} &= 2+3i +\frac{i}{1-i}\times\frac{1+i}{1+i}  = 2+3i +\frac{i(1+i)}{(1-i)(1+i)}\\ 
			&=  2+3i +\frac{i-1}{2} = 2+3i +\frac{i}{2}-\frac{1}{2} =\frac{3}{2}+i\frac{7}{2}
			\end{align*}
		\end{proof}
		
		\vskip .3cm
		\textbf{Un poco de trigonometría}. Recordemos que dado un punto $p=(x,y)$ en el plano, la recta que une el origen con $p$ determina un ángulo $\theta$ con el eje $x$ y entonces 
		\begin{equation*}
		x = r\sin(\theta) , \qquad y =  r\cos(\theta)  
		\end{equation*}  
		donde $r$ es la longitud del segmento determinado por $(0,0)$ y $(x,y)$. En  el  lenguaje  de los números complejos, si $z = a +bi$ y $\theta$  el ángulo determinado por $z$ y  el eje horizontal, entonces 
		\begin{equation*}
		a = |z|\sin(\theta) , \qquad b =   |z|\cos(\theta), 
		\end{equation*}  
		es decir
		\begin{equation}\label{forma-polar}
		z = |z|(\cos(\theta)+i\sin(\theta)).
		\end{equation}
		Si $z \in \C$, la fórmula (\ref{forma-polar}) e llamada la \textit{forma polar}\index{forma polar} de $z$ y $\theta$ es llamado  el  \textit{argumento de $z$}. 
		
		\vskip .3cm 
		
		
		\textbf{Notación exponencial.} Otra notación para representar a los números complejos es la \textit{notación exponencial}, \index{notación exponencial}en la cual se denota
		\begin{equation*}
		e^{i\theta} := \cos(\theta) + i\sin(\theta).
		\end{equation*}
		Por lo tanto si $z \in \C$ y $\theta$  es el argumento de  $z$,
		\begin{equation*}
		z = r e^{i\theta} 
		\end{equation*} 
		donde  $r = |z|$. No  perder de vista,  que la notación exponencial no es más que una notación (por ahora). 
		
		\begin{proposicion}
			Sean $z_1 = r_1 e^{i\theta_1}$, $z_2 = r_2 e^{i\theta_2}$,  entonces
			$$
			z_1 z_2 =  r_1r_2 \,e^{i(\theta_1+ \theta_2)}.
			$$
		\end{proposicion}
		\begin{proof}
			$z_1 = r_1(\cos(\theta_1)+i\sin(\theta_1))$, $z_2 = r_2(\cos(\theta_2)+i\sin(\theta_2))$, luego
			\begin{align*}
			z_1z_2 &= r_1r_2(\cos(\theta_1)+i\sin(\theta_1))(\cos(\theta_2)+i\sin(\theta_2)) \\
			&= r_1r_2(\cos(\theta_1)\cos(\theta_2)+i\cos(\theta_1)\sin(\theta_2)+i\sin(\theta_1)\cos(\theta_2)+i^2\sin(\theta_1)\sin(\theta_2)) \\
			&= r_1r_2((\cos(\theta_1)\cos(\theta_2)-\sin(\theta_1)\sin(\theta_2))+i(\sin(\theta_1)\cos(\theta_2) +\cos(\theta_1)\sin(\theta_2))) \\
			&\overset{(*)}= r_1r_2(\cos(\theta_1+\theta_2) + i\sin(\theta_1+\theta_2)) =  r_1r_2e^{i(\theta_1+ \theta_2)}.
			\end{align*}
			La igualdad ($*$) se debe a las tradicionales fórmulas trigonométrica del coseno y  seno de la suma de ángulos.
		\end{proof}
		
		
		
		
	\end{section}	
	\end{chapter}

		
	\begin{chapter}{Funciones polinómicas} 
	
	En este apéndice se definirán las funciones polinómicas y se mostrarán algunas de sus propiedades fundamentales. Trabajaremos sobre $\K$  cuerpo con $\K=\R$ o $\K=\C$. 
	
	\begin{section}{Definición de funciones polinómicas}
		
	
	\begin{definicion}
			Una función $f: \K \to \K$ es \textit{polinomial} o \textit{polinómica} o directamente decimos que $f$  es  un \textit{polinomio}, si existen $a_0,a_1,\ldots,a_n \in \K$ tal que
			\begin{equation}\label{eq-funcion-polinomica}
				f(x) = a_nx^n + a_{n-1}x^{n-1}+\cdots + a_1x +a_0 
			\end{equation}
			para todo $x \in \K$. En  este caso  diremos que  \textit{$f$ tiene grado $\le n$.}
	\end{definicion}

	Estaríamos tentados en decir que  una función polinómica como en (\ref{eq-funcion-polinomica}) con $a_n\ne0$ tiene grado $n$, pero debemos ser cuidadosos pues todavía no sabemos si la escritura de una función polinómica es única. Es  decir,  existe la posibilidad de $f$  se escriba de otra forma y por lo tanto el coeficiente más significativo sea diferente. Veremos más adelante que esto no puede ocurrir. 
	
	\vskip .3cm
	
	Sea $f$ un polinomio. Si $c$ es un número tal que $f (c) = 0$, entonces llamamos a \textit{$c$ una raíz de $f$}. Veremos en un momento que un polinomio distinto de cero puede tener solo un número finito de raíces, y daremos un límite para la cantidad de estas raíces.
	
	\begin{ejemplo}
		Sea $f (x) = x^2 - 3x + 2$. Entonces $f(1)=0$ y por lo tanto, 1 es una raíz de $f$.
		Además, $f (2) = 0$. Por lo tanto, 2 es también una raíz de $f$.
	\end{ejemplo}

	\begin{ejemplo}
		Sean $a,b,c \in \R$ y  $f(x) = ax^2 + bx + c$, un polinomio en $\R$.  Si $b^2 - 4 ac = 0$, entonces el polinomio tiene una raíz real, que es
		\begin{equation*}
			-\frac{b}{2a}.
		\end{equation*}
		Si $b^2 - 4 ac > 0$, entonces el polinomio tiene dos raíces reales distintas que son
		\begin{equation*}
			\frac{-b + \sqrt{b^2 - 4 ac}}{2a}\quad \text{ y }\quad \frac{-b - \sqrt{b^2 - 4 ac}}{2a}. 
		\end{equation*}
		En el caso que $b^2 - 4 ac < 0$ el polinomio no tiene raíces reales. 
	\end{ejemplo}
	
	\begin{teorema}\label{th-fact-raiz}
		 Sea $f$ un polinomio de grado $\le n$ y sea $c$ una raíz. Entonces existe un polinomio $g$ de grado $\le n - 1$ tal que para todo $x$ se cumple
		 \begin{equation*}
		 	f (x) = (x - c) g (x).
		 \end{equation*}
	\end{teorema}
	\begin{proof} Escribamos $f(x)$ en función de las potencias de  $x$:
	\begin{equation*}
		f(x) = a_nx^n + a_{n-1}x^{n-1}+\cdots + a_1x +a_0.
	\end{equation*}
	 Veremos a continuación que $f$ puede también escribirse en potencias de $x-c$: escribamos 
	 \begin{equation*}
	 	x = (x-c)+ c,
	 \end{equation*}
	 luego 
	 \begin{equation*}
	 f(x) = a_n((x-c)+ c)^n + a_{n-1}((x-c)+ c)^{n-1}+\cdots + a_1((x-c)+ c) +a_0.
	 \end{equation*}
	 Expandiendo las potencias de los binomios $((x-c)+ c)^k$ ($1 \le k \le n$),  obtenemos
	 \begin{equation*}
	 f(x) = b_n(x-c)^n + b_{n-1}(x-c)^{n-1}+\cdots + b_1(x-c) +b_0,
	 \end{equation*}
	 para ciertos $b_0, b_1,\ldots ,b_n \in \K$. Como $f(c) = 0$,  entonces $0=f(c)=b_0$,  luego 
	 \begin{align*}
	 	f(x) &= b_n(x-c)^n + b_{n-1}(x-c)^{n-1}+\cdots + b_1(x-c) \\
	 	&= (x-c)(b_n(x-c)^{n-1} + b_{n-1}(x-c)^{n-2}+\cdots + b_1) \\
	 	&=(x-c)g(x),
	 \end{align*}
	 con $g(x) =b_n(x-c)^{n-1} + b_{n-1}(x-c)^{n-2}+\cdots + b_1$,  que es una función polinómica de grado $\le n-1$, y vemos que nuestro teorema está probado.
	\end{proof}
		
	El polinomio $f$ es el \textit{polinomio nulo} si $f(x)=0$ para toda $x \in \K$. Si $f$ es el polinomio nulo,  denotamos $f =0$. 
		
	\begin{teorema}\label{th-pol-raiz}
		Sea $f$ un polinomio de grado $\le n$ tal que
		\begin{equation*}
		f(x) = a_nx^n + a_{n-1}x^{n-1}+\cdots + a_1x +a_0,
		\end{equation*}
		y  $a_n \ne 0$. Entonces $f$ tiene a lo  más  $n$ raíces. 
	\end{teorema}
	\begin{proof} 
		Lo probaremos haciendo inducción sobre $n$. 
		
		Si $n=0$, $a_0 \ne 0$, es decir  $f(x)=a_0\ne 0$, que es lo que teníamos que probar ($f$ no tiene raíces). 
		
		
		Sea $n>0$. Sea $c$ raíz de $f$. Por  el teorema \ref{th-fact-raiz},  
		\begin{equation*}
		f(x) = (x-c)g(x),
		\end{equation*}
		con 
		\begin{equation*}
		g(x) = b_{n-1}x^{n-1} + \cdots + b_1x +b_0.
		\end{equation*}
		Es claro que $b_{n-1} = a_n \ne 0$ y por lo tanto, por hipótesis inductiva, $g(x)$ tiene a lo más $n-1$ raíces. Ahora bien 
		\begin{equation*}
			0 =f(x) = (x-c)g(x) \quad \Leftrightarrow \quad x-c=0 \text{ o } g(x) =0.
		\end{equation*} 
		Es decir $x$ es raíz de $f$ si y solo si $x=c$ o $x$ es raíz de $g$. Como $g$ tiene a lo más $n-1$ raíces,  $f$ tiene a lo más $n$ raíces.
		
		
		
		
		\begin{comment}
			Sea 
			\begin{equation*}
			f(x) = a_nx^n + a_{n-1}x^{n-1}+\cdots + a_1x +a_0.
			\end{equation*}
			Probaremos, haciendo inducción en $n$, que
			\begin{equation}\label{eq-desc-rai}
			f(x) = (x-c_1)(x-c_2)\cdots(x-c_k)h_k,
			\end{equation} 
			donde $k \le n$ y $h_k$ no tiene raíces. Observar que  esto implica  que si $f(c)=0$, entonces
			\begin{equation*}
			0 = (c-c_1)(c-c_2)\cdots(c-c_k)h_k(c).
			\end{equation*}
			Por lo tanto algunos de los factores es igual a 0. Como $h_k(c) \ne 0$ ($h_k$ no tiene raíces),  entonces $c=c_i$ para algún $i$. Luego, las raíces de $f$ son $c_1,\ldots,c_k$ con $k \le n$, lo cual implica el enunciado del teorema. 
			
			Probemos  entonces (\ref{eq-desc-rai}).
			
			Si $n=0$, como $f$ no es nulo, $f(x)=a_0\ne 0$, que es lo que teníamos que probar. 
			
			Sea $n >0$. Supongamos que el resultado es cierto para los polinomios de grado  $\le n-1$ (hipótesis inductiva). Si $f$ no tiene raíces, está probado nuestro enunciado. En  caso contrario existe $c_1 \in \K$ tal que $f(c_1) =0$. Por el teorema \ref{th-fact-raiz}, 
			\begin{equation*}
			f(x) = (x-c_1)h_1(x)
			\end{equation*}
			con $h_1$ un polinomio de grado $\le n-1$. Luego, por hipótesis inductiva
			\begin{equation*}
			h_1(x) = (x-d_1)(x-d_2)\cdots(x-d_r)g_r,
			\end{equation*} 
			con $r \le n-1$ y $g_r$ que no tiene raíces. Entonces,
			\begin{equation*}
			f(x) = (x-c_1)(x-d_1)(x-d_2)\cdots(x-d_r)g_r,
			\end{equation*}
			con $r \le n-1$ y $g_r$ que no tiene raíces. Esto es equivalente a (\ref{eq-desc-rai}).   
		\end{comment}
		
	\end{proof}
						
	\begin{corolario}
		Sea $f$ un polinomio y 
		\begin{equation*}
			f(x) = a_nx^n + a_{n-1}x^{n-1}+\cdots + a_1x +a_0,
		\end{equation*}
		con $a_n \ne 0$.  Entonces $n$ y $a_0,\ldots,a_n$ están determinados de forma única.
	\end{corolario}
	\begin{proof}
		Debemos ver que si 
		\begin{equation*}
		f(x) = b_mx^m + b_{m-1}x^{m-1}+\cdots + b_1x +b_0,
		\end{equation*}
		con $b_m \ne 0$, entonces $m=n$ y $a_i=b_i$ para $1 \le i \le n$. 
		
		Supongamos que $m \le n$ (el caso $n \le m$ es similar), entonces definiendo $b_k=0$ para $m < k \le n$, tenemos que
		\begin{equation*}
		f(x) = b_nx^n + b_{n-1}x^{n-1}+\cdots + b_1x +b_0.
		\end{equation*}
		Por lo tanto si 
		\begin{equation}\label{eq-h-igual-f-g}
			h(x) = (a_n- b_n)x^n + (a_{n-1}-b_{n-1})x^{n-1}+\cdots + (a_{1}-b_{1})x +(a_{0}-b_{0}), 
		\end{equation}
		tenemos  que $h(x)= f(x)-f(x)=0$,  es un polinomio que admite infinitas raíces. 
		Supongamos  que algún coeficiente de la expresión (\ref{eq-h-igual-f-g}) de $h$ sea no nulo. Sea $k$  el máximo coeficiente de la expresión (\ref{eq-h-igual-f-g}) no nulo. Por el teorema   \ref{th-pol-raiz}, $h$ no  puede tener más de $k$ raíces, absurdo pues hemos dicho que tenía infinitas. El absurdo vino de suponer que algún $a_i-b_i$ era no nulo. Por lo tanto $a_i -b_i=0$ para $0 \le i \le n$,  es decir $a_i=b_i$ para $1 \le i \le n$. 
		
		
	\end{proof}

	Este corolario nos dice que la escritura de un polinomio $f$ como 
	\begin{equation*}
	f(x) = a_nx^n + a_{n-1}x^{n-1}+\cdots + a_1x +a_0,
	\end{equation*}
	con $a_n \ne 0$, es única. Diremos entonces que $n$ es el  \textit{grado} de $f$ y lo denotaremos $\operatorname{gr}(f)=n$. En  el caso del polinomio 0, el grado no está definido y se usa la convención $\operatorname{gr}(0)=-\infty$. 
	
	Diremos también que  $a_0,\ldots,a_n$ son los \textit{coeficientes} de $f$, $a_0$ es el \textit{término constante} de $f$ y $a_n$  el \textit{coeficiente principal.} 

	Observemos que si $f$ y $g$ son polinomios con   
	\begin{equation*}
	f(x) = a_nx^n + \cdots + a_1x +a_0 \quad\text{ y } \quad g(x) = b_nx^n +\cdots + b_1x +b_0,
	\end{equation*}
	entonces como $ax^i + b x^i = (a+b)x^i$, tenemos que $f+g$ es un polinomio definido por 
	\begin{equation*}
	(f + g)(x) = (a_n+b_n)x^n + \cdots + (a_1+b_1)x +(a_0+b_0).
	\end{equation*}
	Por otro  lado,  debido  a que $(ax^i)(bx^j) = abx^{i+j}$, el producto de dos polinomios también es un polinomio. Más precisamente,
	
	\begin{proposicion}
		Sean $f$ y $g$ polinomios de grado $n$ y $m$,  respectivamente. Entonces $fg$ es un  polinomio de grado $n+m$
	\end{proposicion}
	\begin{proof}
		Sean 
		\begin{equation*}
		f(x) = a_nx^n + \cdots + a_1x +a_0 \quad\text{ y } \quad g(x) = b_mx^m +\cdots + b_1x +b_0,
		\end{equation*}
		con $a_n, b_m \ne 0$. Entonces, debido a que $x^ix^j = x^{i+j}$, 
		\begin{equation}
			(fg)(x) = a_nb_m x^{n+m} + h(x),
 		\end{equation}
		con $h(x)$ un polinomio de grado menor a $n+m$. Por lo tanto, el coeficiente principal de $fg$ es $a_nb_m \ne 0$ y,  entonces, $fg$ tiene grado $n+m$.
	\end{proof}

	
		
	\begin{ejemplo} Sean $f(x) = 4x^3 - 3x^2 + x + 2$ y $g(x) = x^2 + 1$. Entonces,
		\begin{align*}
			(f+g)(x) &= (4+0)x^3 +(-3 +1)x^2 + (1+0)x + (2+1)\\
			&= 4x^3 - 2x^2 + x + 3,
		\end{align*}
		y
		\begin{align*}
		(fg)(x) &= (4x^3 - 3x^2 + x + 2)(x^2 + 1)\\
		&= (4x^3 - 3x^2 + x + 2)x^2 + (4x^3 - 3x^2 + x + 2)1\\
		&= 4x^5 - 3x^4 + x^3 + 2x^2 + 4x^3 - 3x^2 + x + 2 \\
		&= 4x^5 - 3x^4 + 5x^3 - x^2 + x + 2
		\end{align*}
	\end{ejemplo}
	

	\end{section}

	\begin{section}{División de polinomios} Si $f$ y $g$ son polinomios,  entonces no necesariamente la función $f/g$ está bien definida en todo punto y puede que tampoco sea un polinomio. Cuando trabajamos con enteros, en cursos anteriores,  probamos la existencia del algoritmo de división, más precisamente.
	
	\vskip .1cm	
		
		\textit{Sean $n$, $d$ enteros positivos. Entonces existe un entero $r$ tal que
			$0 \le  r <d$  un entero $q \ge 0$ tal que
			} 
		\begin{equation*}
			n = qd + r.
		\end{equation*}
		
	\vskip .3cm	
	
	Ahora describiremos un procedimiento análogo para polinomios.
	\vskip .3cm	
	\textit{\textbf{Algoritmo de División.} Sean $f$ y $g$ polinomios distintos de cero. Entonces existen polinomios $q$, $r$ tales que $\operatorname{gr}(r) < \operatorname{gr}(g)$ y tales que}
	\begin{equation*}
		f (x) = q (x) g (x) + r (x).
	\end{equation*}
	A $q(x)$ lo llamamos el \textit{cociente} de la \textit{división polinomial} y  a $r(x)$ lo llamamos el \textit{resto}  de la división polinomial. 
	
	No veremos aquí la demostración del algoritmo de división, basta decir que es muy similar a  la demostración del algoritmo de división para números enteros. En los siguientes ejemplos se verá como se calculan el cociente y resto de la división polinomial. 
	 
	\vskip .3cm	
	
	
	\begin{ejemplo} Sean $f(x) = 4x^3 - 3x^2 + x + 2$ y $g(x) = x^2 + 1$. Para encontrar la división polinomial, debemos multiplicar por un monomio $ax^k$ a $g(x)$ de tal forma que el coeficiente principal de  $ax^kg(x)$ sea igual al coeficiente principal de $f(x)$. En este caso, multiplicamos a $g(x)$ por $4x$ y nos queda 
	\begin{equation*}
		f(x) = 4xg(x) + r_1(x) = (4x^3 +4x)+(-3x^2-3x +2)
	\end{equation*}
	Ahora,  con $r_1(x)=-3x^2-3x +2$ hacemos el mismo procedimiento,  es decir multiplicamos por $-3$  a $g(x)$ y vemos que es lo que "falta":
	\begin{equation*}
		r_1(x) = (-3)g(x) +r(x) = (-3x^2 -3) + (-3x+5).
	\end{equation*}
	Como $r(x) = -3x+5$ tiene grado menor que 2, tenemos que
	\begin{align*}
	f(x) &= 4xg(x) + r_1(x)\\ &=4xg(x) + (-3)g(x) +r(x)\\& = (4x-3)g(x)+r(x).
	\end{align*}
	Es decir, 
	\begin{equation*}
	f(x) = q(x)g(x)+r(x),
	\end{equation*}
	con $q(x) =4x-3$ y $r(x) = -3x+5$. 
	
	Observemos que se puede hacer un esquema parecido a  la división de números enteros, el cual nos facilita el cálculo:
	\begin{equation*}
		\polylongdiv{4x^3 - 3x^2 + x + 2}{ x^2 + 1}
	\end{equation*}
\end{ejemplo}

\begin{ejemplo}
	Sean
	\begin{equation*}
		f (x) = 2x^4 - 3x^2 + 1 \quad \text{ y } \quad g (x) = x^2 - x + 3.
	\end{equation*}
	Deseamos encontrar $q (x)$ y $r (x)$ como en el algoritmo de Euclides. Haciendo la división como en el ejercicio anterior:
	\begin{equation*}
	\polylongdiv{2x^4 +0x^3- 3x^2 +0x+ 1}{x^2 - x + 3}
	\end{equation*}
	Es decir $q(x) = 2x^2+2x-7$ y $r(x)= -13x+22$.
\end{ejemplo}

Observemos que el algoritmo de división nos dice que si dividimos un polinomio por uno de grado 1,  entonces el resto es una constante (que puede ser 0). Más aún:

\begin{teorema}[Teorema del  resto] Sea $f$ polinomio y $c \in \K$. Entonces,  el resto de dividir $f$ por $x-c$ es $f(c)$. 
\end{teorema}
\begin{proof} Por  el algoritmo de Euclides
	\begin{equation*}
		f(x) = q(x)(x-c) + r,
	\end{equation*}
	con $r$ de grado $<1$,  es decir $r \in \K$.  Ahora bien
	\begin{equation*}
	f(c) = q(c)(c-c) + r = r,
	\end{equation*}
	luego $f(c)$  es el resto de dividir $f$ por $x-c$. 
\end{proof}

Observar que esto  nos da otra prueba del teorema \ref{th-pol-raiz}: $f(c)=0$, luego por teorema del resto $ f(x)=q(x)(x-c)$. 
		
	\end{section}	
	\end{chapter}	

	\begin{chapter}{Determinante}\label{apend.Determinante}
		En  el apéndice se harán las demostraciones de los resultados correspondientes a la sección de determinantes (sección  \ref{seccion-determinate}).
		
		\begin{section}{Determinantes}
			
		Lo primero que veremos será la demostración del  teorema \ref{det-prop-fundamentales}. Los tres resultados de ese teorema los demostraremos en forma separada: serán los teoremas \ref{det-prop-mult-esc}, \ref{det-prop-perm-filas} y \ref{det-prop-fila-mas-cfila}.
		
		
		\begin{teorema} \label{det-prop-mult-esc}
			Sea $A  \in M_n(\K)$ y sea $c \in \K$ y $B$ la matriz que se obtiene de $A$ multiplicando la fila $r$ por $c$, es decir $A  \stackrel{cF_r}{\longrightarrow} B$, entonces $\det B = c \det A$.
		\end{teorema}
		\begin{proof}
			Si multiplicamos la fila $r$ por $c$ obtenemos
			\begin{equation*}
			B = \begin{bmatrix}
			a_{11}&a_{12}&\cdots&a_{1n}\\
			\vdots&&\ddots&\vdots \\
			ca_{r1}&ca_{r2}&\cdots&ca_{rn} \\
			\vdots&&\ddots&\vdots \\
			a_{n1}&&\cdots&a_{nn}
			\end{bmatrix}.
			\end{equation*}
			Observemos que al hacer el desarrollo por la primera columna obtenemos
			\begin{equation*}
			|B| = \sum_{i=1}^{r-1}  a_{i1}C^B_{1i} + ca_{r1}C^B_{r1} + \sum_{i=r+1}^{n}  a_{i1}C^B_{1i}.
			\end{equation*}
			Ahora bien, si $i \ne r$, la matriz $B(i|1)$ es la matriz   $A(i|1)$ con una fila multiplicada por $c$, luego $|B(i|1)| = c|A(i|1)|$ y, en consecuencia $C^B_{i1} = c\,C^A_{i1}$. Además, $B(r|1) = A(r|1)$, luego  $C^B_{r1} = C^A_{r1}$. Por lo tanto, reemplazando en la ecuación anterior $C^B_{i1}$ por $c\,C^A_{i1}$ si $i\ne r$ y $C^B_{r1}$ por $C^A_{r1}$, obtenemos
			\begin{equation*}
			|B| = \sum_{i=1}^{r-1}  a_{i1}c\,C^A_{1i} + ca_{r1}C^A_{r1} + \sum_{i=r+1}^{n}  a_{i1}cC^A_{1i} = c|A|.
			\end{equation*}
		\end{proof}
		

		
		
		\begin{teorema} \label{det-prop-perm-filas}
			Sea $A  \in M_n(\K)$ y sean $1 \le r,s \le n$.
			Sea $B$ la matriz que se obtiene de $A$ permutando la fila $r$ con la fila $s$, es decir  $A  \stackrel{F_r \leftrightarrow F_s}{\longrightarrow} B$, entonces $\det B = -\det A$.
		\end{teorema}
		\begin{proof}
			Primero probaremos el teorema bajo el supuesto de que la fila $1$ es permutada con la fila $k$, para $k > 1$. Esto será suficiente para probar el teorema, puesto que intercambiar las filas $k$ y $k_ 0$ es equivalente a realizar tres permutaciones de filas: primero intercambiamos las filas $1$ y $k$, luego las filas $1$ y $k_{0}$, y finalmente intercambiando las filas $1$ y $k$. Cada permutación cambia el signo del determinante y al ser tres permutaciones,  el intercambio de la fila $k$ con la fila $k_0$ cambia el signo.
			
			
			La prueba es por inducción en $n$. El caso base $n = 1$ es completamente trivial.
			(O, si lo prefiere, puede tomar $n = 2$ como el caso base, y el teorema es
			fácilmente probado usando la fórmula para el determinante de una matriz $2 \times 2$).
			Las definiciones de los determinantes de $A$ y $B$ son:
			\begin{equation*}
			\det(A) = \sum_{i=1}^{n}  a_{i1}C^A_{i1} \quad \text{ y } \quad \det(B) = \sum_{i=1}^{n}  b_{i1}C^B_{i1}.
			\end{equation*}
			
			
			Supongamos primero que $i \ne 1, k$. En este caso, está claro que $A(i|1)$ y $B(i|1)$ son iguales, excepto que dos filas se intercambian. Por lo tanto, por hipótesis inductiva $C^A_{i1} = -C^B_{i1}$. Ya que también $a_{i1} = b_{i1}$, tenemos entonces que
			\begin{equation}\label{det-perm-01}
			a_{i1}C^A_{i1} = - b_{i1}C^B_{i1}, \quad \text{ para } i\ne 1,k.
			\end{equation}
			
			Queda por considerar los términos $i = 1$ y $i = k$. Nosotros afirmamos que
			\begin{equation}\label{det-perm-02}
			-	a_{k1}C^A_{k1} =  b_{11}C^B_{11}  \quad \text{ y } \quad - a_{11}C^A_{11} = b_{k1}C^B_{k1}. 
			\end{equation}
			Si probamos esto, entonces 
			\begin{align*}
			\det(A) &= \sum_{i=1}^{n}  a_{i1}C^A_{i1} \\
			&= a_{11}C^A_{11} +  \sum_{i=2}^{k-1}  a_{i1}C^A_{i1} + a_{k1}C^A_{k1} + \sum_{i=k+1}^{n}  a_{i1}C^A_{i1}\qquad \text{por (\ref{det-perm-01}) y (\ref{det-perm-02})} \\
			&= -b_{k1}C^B_{k1} - \sum_{i=2}^{k-1}  b_{i1}C^B_{i1} - b_{11}C^B_{11} - \sum_{i=k+1}^{n}  b_{i1}C^B_{i1}  \\
			&= - \sum_{i=1}^{n}  b_{i1}C^B_{i1} =- \det(B).
			\end{align*}
			Luego el teorema está probado. Por lo tanto debemos probar (\ref{det-perm-02}). Por simetría, basta probar la primera identidad de (\ref{det-perm-02}),  es decir  que $	a_{k1}C^A_{k1} = - b_{11}C^B_{11}$. 
			
			Para esto, primero debemos observar que $a_{k1} = b_{11}$, por lo tanto sólo hace falta probar que $-C^A_{k1} = C^B_{11}$. En segundo lugar, debemos tener  en cuenta que $B(1|1)$ se obtiene de $A(k|1)$ reordenando las filas $1,2,\ldots, k -1$  de $A(k|1)$ en el orden $2,3, \ldots, k-1,1$. Este reordenamiento puede hacerse permutando la fila $1$ con la fila $2$, luego permutando esa fila con la fila $3$, etc., terminando con una permutación con la fila $k-1$. Esto es un total de $k - 2$  permutaciones de fila. Asi que, por hipótesis inductiva,
			\begin{equation*}
			\det(B(1|1)) = (-1)^{k-2}\det(A(k|1)) = (-1)^{k}\det(A(k|1)) = - (-1)^{k+1}\det(A(k|1)), 
			\end{equation*}
			es decir $C^B_{11} = -C^A_{k1}$. Esto completa la demostración del teorema.
		\end{proof}
		
	\begin{observacion}
		Recordar que del resultado anterior se deduce fácilmente que si una matriz tiene dos filas iguales entonces su determinante es 0. Esto se debía a que, intercambiando las dos filas iguales obtenemos la misma matriz, pero calculando el determinante con el teorema anterior vemos que cambia de signo y el único número en $\K$ que es igual a su opuesto es el 0. 
	\end{observacion}
		
		\begin{lema} Sean $A,B,C$ matrices $n \times n$ tal que
			\begin{equation*}
			A= \begin{bmatrix}
			a_{11}&a_{12}&\cdots&a_{1n}\\
			\vdots&\vdots&&\vdots \\
			a_{r1}&a_{r2}&\cdots&a_{rn} \\
			\vdots&\vdots&&\vdots \\
			a_{n1}&a_{n2}&\cdots&a_{nn}
			\end{bmatrix}, 
			\quad B=  \begin{bmatrix}
			a_{11}&a_{12}&\cdots&a_{1n}\\
			\vdots&\vdots&&\vdots \\
			b_{r1}&b_{r2}&\cdots&b_{rn} \\
			\vdots&\vdots&&\vdots \\
			a_{n1}&a_{n2}&\cdots&a_{nn}
			\end{bmatrix}
			\end{equation*}	
			y
			\begin{equation*}
			C= \begin{bmatrix}
			a_{11}&a_{12}&\cdots&a_{1n}\\
			\vdots&\vdots&&\vdots \\
			a_{r1}+b_{r1}&a_{r2}+b_{r2}&\cdots&a_{rn}+b_{rn} \\
			\vdots&\vdots&&\vdots \\
			a_{n1}&a_{n2}&\cdots&a_{nn}
			\end{bmatrix}.
			\end{equation*}
			Es decir $B$ es igual a $A$ pero con la fila $r$ cambiada y $C$ es como $A$ y $B$ excepto en la fila $r$ donde cada coeficiente el la suma del de $A$ y $B$ correspondiente. Entonces $\det(C) = \det(A)+ \det(B)$.
		\end{lema}
		\begin{proof} Se hará por inducción en $n$. Para $n=1$, del resultado se reduce a probar que $\det[a+b] = \det[a]+ \det[b]$, lo cual es trivial, pues el determinante en matrices $1 \times 1$  es la identidad. 
			
			Primero consideremos el caso $r = 1$. En  este caso tenemos que  $A(1|1) =B(1|1) = C(1|1)$, pues en la única fila que difieren las matrices es en la primera. Además, si $i>1$,  $A(i|1)$,  $B(i|1)$ y $C(i|1)$ son iguales, excepto que difieren en la primera fila donde los coeficientes de $C(i|1)$ son la suma de los de  $A(i|1)$ y  $B(i|1)$,  entonces, por hipótesis inductiva, 
			$\det C(i|1) = \det A(i|1) + \det B(i|1)$. Concluyendo, tenemos que
			\begin{align*}
			\det A(1|1) &=\det B(1|1) =\det  C(1|1),&& \\ \det C(i|1) &= \det A(i|1) + \det B(i|1), \qquad i>1&&,
			\end{align*}
			lo cual implica que 
			\begin{align*}
			C^C_{11}&= C^A_{11} = C^B_{11} ,&& \\ C^C_{i1} &= C^A_{i1} + C^B_{i1}, \qquad i>1&&.
			\end{align*}
			Luego
			\begin{align*}
			\det C &= (a_{11}+b_{11}) C^C_{11} + \sum_{i=2}^{n} a _{i1}C^C_{i1} \\
			&= a_{11} C^C_{11} + b_{11}  C^C_{11}+ \sum_{i=2}^{n} a _{i1}( C^A_{i1} + C^B_{i1}) \\
			&= a_{11} C^A_{11} + b_{11}  C^B_{11}+ \sum_{i=2}^{n} a _{i1}( C^A_{i1} + C^B_{i1}) \\
			&= a_{11} C^A_{11} +\sum_{i=2}^{n} a _{i1}C^A_{i1} + b_{11}  C^B_{11}+ \sum_{i=2}^{n} a _{i1} C^B_{i1}\\
			&= \det A + \det B.
			\end{align*}
			
			El caso $r >1$ se demuestra de manera similar o,  si se prefiere, puede usarse el teorema \ref{det-prop-perm-filas}, observando que la permutación entre la fila $1$ y la fila $r$ cambia el signo del determinante.  
		\end{proof}
	
		\begin{teorema}\label{det-prop-fila-mas-cfila}
			Sea $A  \in M_n(\K)$. Sea $c \in \K$ y $B$ la matriz que se obtiene de $A$ sumando a la fila $r$ la fila $s$ multiplicada por $c$, es decir  $A  \stackrel{F_r + cF_s}{\longrightarrow} B$, entonces $\det B = \det A$.
		\end{teorema}
		\begin{proof}
			$A$ y $B$ difieren solo en la fila $r$, donde los coeficientes de $B$ son los los de $A$ más $c$ por los de la fila $s$. Luego si 
			\begin{equation*}
			A = \begin{bmatrix} F_1 \\ \vdots \\ F_s \\\vdots \\ F_r \\ \vdots \\ F_n \end{bmatrix}, \qquad 
			B = \begin{bmatrix} F_1 \\ \vdots \\ F_s \\\vdots \\ F_r + cF_s \\ \vdots \\ F_n \end{bmatrix},\qquad 
			A'= \begin{bmatrix} F_1 \\ \vdots \\ F_s \\\vdots \\ cF_s \\ \vdots \\ F_n \end{bmatrix},
			\end{equation*}
			el lema anterior nos dice que 
			\begin{equation}
			\det B = \det A + \det A'.
			\end{equation}
			Ahora bien, por teorema \ref{det-prop-mult-esc}, 
			$$
			\det A' = c \left| \begin{matrix} F_1 \\ \vdots \\ F_s \\\vdots \\ F_s \\ \vdots \\ F_n \end{matrix} \right|,
			$$
			y este último determinante es cero, debido a que la matriz tiene dos filas iguales. Luego,  $\det B = \det A$. 
		\end{proof}
		
		 \vskip .3cm
		 
		 A continuación veremos que el determinante del producto de matrices es el producto de los determinantes de las matrices. 
		 
		 \begin{teorema}\label{det-elem-por-mtrx} Sea $A  \in M_n(\K)$ y $E$ una matriz elemental $n \times n$. Entonces
		 	\begin{equation}\label{det-prod-elem-matr}
		 	\det (EA) = \det E \det A.
		 	\end{equation}  
		 \end{teorema}
		 \begin{proof}
		 	(1) Si $c \not=0$, y $E$ es la matriz elemental que se obtiene de multiplicar por  $c$ la fila $r$ de $I_n$, sabemos, por corolario \ref{det-mtrx-elem}, que tiene determinante igual a $c$. Por otro lado, el teorema \ref{th-mrtx-elem} nos dice que $EA$ es la matriz que se obtiene a partir de $A$ multiplicando la fila $r$ por $c$, y entonces, por teorema \ref{det-prop-fundamentales}, $\det(EA) = c \det A = \det E \det A$.
		 	
		 	(2) Si $E$ es la matriz elemental que se obtiene de sumar a la fila $r$ de $I_n$  la fila $s$ multiplicada por $c$, entonces $\det E =1$. Por  otro lado  $\det(EA) = \det(A)$, por lo tanto  $\det(EA) = \det(E)\det(A)$.
		 	
		 	(3) Finalmente, si $E$ es la matriz elemental que se obtiene de intercambiar la fila $r$ por la fila $s$ de $I_n$, entonces $\det E=-1$. Por  otro lado  $\det(EA) = -\det(A)$, por lo tanto  $\det(EA) = \det(E)\det(A)$.
		 \end{proof}
		 
		 \begin{corolario}\label{coro-det-prod-elem-mtrx}
		 	Sea $A  \in M_n(\K)$ y $E_1,\ldots,E_k$ matrices elementales $n \times n$. Entonces
		 	\begin{equation*}
		 	\det (E_kE_{k-1}\ldots E_1A) = \det (E_k) \det (E_{k-1})\ldots \det (E_1) \det (A).
		 	\end{equation*}  
		 \end{corolario}
		 \begin{proof}
		 	Por  la aplicación reiterada del teorema \ref{det-elem-por-mtrx} tenemos, 
		 	\begin{equation*}
		 	\begin{matrix*}[l]
		 	\det (E_kE_{k-1}\ldots E_1A) &= \det (E_k) \det(E_{k-1}\ldots E_1 A) \\
		 	&= \det( E_k) \det(E_{k-1})\det(E_{k-2}\ldots E_1A) \\
		 	&\qquad\quad \vdots \\
		 	& = \det( E_k) \det(E_{k-1})\det(E_{k-2})\ldots \det(E_1) \det(A).
		 	\end{matrix*}		
		 	\end{equation*}
		 \end{proof}
		 
		 
		 \begin{obs}\label{mtrx-elem-por-merf}
		 	Sea $A  \in M_n(\K)$,  entonces $A$ es equivalente por filas a una matriz $R$ que es  MERF, por lo tanto $R$ es equivalente por filas a $A$. Es decir, existen $E_1,\ldots,E_k$ matrices elementales, tal que
		 	\begin{equation}
		 	A = E_kE_{k-1}\ldots E_1R.
		 	\end{equation}
		 	Ya vimos,  en el  teorema \ref{mtrx-inv-equiv} que si $A$ invertible $R= I_n$ y por lo tanto  $A$ es equivalente por filas a un producto de  matrices elementales. Si $A$ no es invertible, entonces $R$ no es la identidad y por lo tanto tiene filas nulas. 
		 \end{obs}
		
		 
		 
		 \begin{teorema}\label{th-dem-detAB}  Sean $A,B \in M_n(\K)$,  entonces
		 	\begin{enumerate}
		 		\item $\det (A B) = \det(A)\det(B)$. 
		 		\item $A$ invertible si y solo si  $\det(A)\ne0$.
		 	\end{enumerate}
		 \end{teorema}
		 \begin{proof} 	
		 		Sea $A = E_kE_{k-1}\ldots E_1R$ con $E_i$  elemental y $R$ una MERF.
		 	
		 	(1) Como $AB = E_kE_{k-1}\ldots E_1RB$, por corolario \ref{coro-det-prod-elem-mtrx}  tenemos que  $\det(AB) = \det( E_k) \ldots \det(E_1)\det(RB)$.
		 	
		 	
		 	Separaremos la demostración en dos casos: (a) $A$ es invertible y  (b) $A$ no es invertible. 
		 	
		 	(a) Si $A$  es invertible,  entonces $R=I_n$ y  $A = E_k\ldots E_1$, por lo tanto 
		 	\begin{equation*}
		 	\det(AB) = \det(E_k) \ldots \det(E_1)\det(B) = \det(A)\det(B).
		 	\end{equation*}
		 	
		 	(b) Si $A$ no es invertible, entonces $R$ no es la identidad, por lo tanto tiene la última fila nula y por lo tanto $\det(R)=0$, y en consecuencia $\det(A)=0$. Como $R$ tiene la última fila nula, no es difícil ver que entonces $RB$ tiene la última fila nula y por lo tanto $\det(RB)=0$. Luego 
		 	$$
		 	\det(AB) = \det( E_k) \ldots \det(E_1)\det(RB) =0.
		 	$$
		 	Como  $\det(A)=0$, tenemos también 
		 	$$
		 	\det(A)\det(B) =0.
		 	$$ 
		 	
		 	\vskip .3cm
		 	
		 	(2) ($\Rightarrow$) Como $A$ invertible, $R = I_n$ y $\det A = \det( E_k) 	 \ldots \det(E_1)$ es no nulo, pues las matrices elementales tiene determinante no nulo. 
		 	
		 	(2)($\Leftarrow$)  Si $\det(A) \ne 0$, entonces $\det(R) \ne 0$, luego $R = I_n$ y $A$ es producto de matrices elementales, por lo tanto invertible. 
		 \end{proof}
		 
		 Haremos ahora la demostración del teorema \ref{det-a-trans}. 
		 
		 \begin{teorema}
		 	Sea $E$ matriz elemental, entonces $E^\t$ es matriz elemental del mismo tipo y $\det(E) = \det(E^\t)$. 
		 \end{teorema}
		 \begin{proof}
		 	Si $c \not=0$ y $E$ es la matriz elemental que se obtiene de multiplicar por  $c$ la fila $r$ de $I_n$, es claro que $E^t = E$ y por lo tanto 	$\det(E) = \det(E^\t)$.
		 	
		 	Si $E$ es la matriz elemental que se obtiene de sumar a la fila $r$ de $I_n$ la fila $s$ multiplicada por $c \in \K$,  entonces  $E^\t$  es la matriz elemental que se obtiene de sumar a la fila $s$ de $I_n$ la fila $r$ multiplicada por $c$. Luego,   $\det(E) = \det(E^\t) = 1$.
		 	
		 	Finalmente, si $E$ es la  matriz elemental que se obtiene de intercambiar la fila $r$ por la fila $s$ de $I_n$,entonces $E^\t = E$ y por lo tanto $\det(E) = \det(E^\t)$.
		 \end{proof}
		 

		 
		 \begin{teorema}\label{th-det-a-trans-app} 
		 	Sea $A \in M_n(\K)$,  entonces 
		 	$\det(A) = \det(A^\t)$
		 \end{teorema}
		 \begin{proof}
		 	Si $A$ es invertible, entonces  $A = E_kE_{k-1}\ldots E_1$ con $E_i$ elemental, por lo tanto $\det(A) = \det( E_k)\det(E_{k-1})\ldots \det(E_1)$. Luego,
		 	\begin{equation*}
		 	\det(A^\t) = \det(E_1^\t \ldots E_k^\t) = \det(E_1^\t) \ldots \det(E_k^\t) = 
		 	\det(E_1) \ldots \det(E_k) = \det(A).
		 	\end{equation*}
		 	Si $A$ no es invertible, entonces $A^\t$ no es invertible y en ese caso $\det(A) = \det(A^\t)=0$.
		 \end{proof}
		 
		 \vskip .3cm
		 
		 Finalmente,  demostremos el teorema \ref{th-expancion-cofactores}.
	
		 \begin{teorema}\label{th-dessarrollo-por-columnas} El determinante de una matriz $A$ de orden $n \times n$ puede ser calculado por la expansión de los cofactores en  cualquier columna o cualquier fila. Más específicamente, 
		 	\begin{enumerate}
		 		\item si usamos la expansión por la $j$-ésima columna, $1 \le j \le n$, tenemos
		 		\begin{align*}
		 		\det A &= \sum_{i=1}^{n} a_{ij} C_{ij} \\
		 		& = a_{1j}C_{1j}+a_{2j}C_{2j}+\cdots+a_{nj}C_{nj}.
		 		\end{align*} 
		 		\item si usamos la expansión por la $i$-ésima fila, $1 \le i \le n$, tenemos
		 		\begin{align*}
		 		\det A &= \sum_{j=1}^{n} a_{ij} C_{ij} \\
		 		& = a_{i1}C_{i1}+a_{i2}C_{i2}+\cdots+a_{in}C_{in};
		 		\end{align*} 
		 	\end{enumerate}
		 \end{teorema}
		 \begin{proof} (1) Primero hagamos la demostración para $j=2$,  es decir para el desarrollo por la segunda columna.  Escribamos $A$ en función de sus columnas,  es decir 
		 	$$
		 	A = \begin{bmatrix}C_1& C_2&C_3 &\cdots& C_n\end{bmatrix},
		 	$$
		 	donde $C_k$  es la columna $k$ de $A$.
		 	Sea $B=[b_{ij}]$ la matriz definida por
		 	$$
		 	B = \begin{bmatrix} C_2 &C_1 &C_3 &\cdots &C_n\end{bmatrix}.
		 	$$
		 	Entonces, $\det(B) = -\det(A)$. Por otro lado, por la definición de determinante, 
		 	\begin{align*}
		 	\det(B) &=   \sum_{i=1}^{n} b_{i1} C^B_{i1} \\
		 	&= \sum_{i=1}^{n} b_{i1} (-1)^{i+1}B(i|1) \\
		 	& = \sum_{i=1}^{n} a_{i2} (-1)^{i+1}B(i|1).
		 	\end{align*} 
		 	Ahora bien, es claro que $B(i|1) = A(i|2)$, por lo tanto 
		 	$$
		 	\det(B)  = \sum_{i=1}^{n} a_{i2} (-1)^{i+1}A(i|2) = - \sum_{i=1}^{n} a_{i2} C_{i2}.
		 	$$
		 	Es decir, $\det(A) = -\det(B)= \sum_{i=1}^{n} a_{i2} C_{i2}$.
		 	
		 	El caso $j>2$  se demuestra de forma similar: si $B$ es la matriz
		 	$$
		 	B = \begin{bmatrix} C_j &C_1 &C_2 &\cdots& C_{j-1}&C_{j+1}&\cdots &C_n\end{bmatrix}.
		 	$$        
		 	entonces $\det(B)=(-1)^{j-1}\det(A)$, pues son necesarios $j-1$ permutaciones para recuperar la matriz $A$ (es decir, llevar la columna $j$ a su lugar).
		 	%que se obtiene de $A$ permutando la columna $1$ por la columna $j$,  entonces $\det(B) = -\det(A)$. 
		 	Como $B(i|1) = A(i|j)$,  desarrollando por la primera columna el determinante de $B$ obtenemos el resultado.  
		 	
		 	
		 	(2) Observemos primero que $A^\t(j|i) = A(i|j)^\t$, por lo tanto, si calculamos $\det(A^\t)$ por desarrollo por columna $i$, obtenemos 
		 	\begin{align*}
		 	\det A = \det(A^\t)&=\sum_{j=1}^{n} [A^\t]_{ji} (-1)^{i+j}\det (A^\t(j|i)) \\
		 	&=  \sum_{j=1}^{n} a_{ij} (-1)^{i+j}\det (A(i|j)^\t) \\
		 	&= \sum_{j=1}^{n} a_{ij} (-1)^{i+j}\det (A(i|j)).
		 	\end{align*}
		 	
		 	
		 \end{proof}
		 
		 
		\end{section}
	
		 
		 
		 \begin{section}{Regla de Cramer} Veremos ahora que la inversa de una matriz invertible se puede escribir en términos de determinantes de algunas matrices relacionadas y esto, junto a otros resultados, nos permitirá resolver ecuaciones lineales con $n$-variables y $n$-incógnitas cuya matriz asociada es invertible.  
		 	
		 	\begin{teorema} \label{th-cof-01}
		 		Sea $A$ matriz $n \times n$, entonces
		 		\begin{align*}
		 		\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix} A &= 
		 		\begin{bmatrix} 0 & \cdots& 0 & \det A & 0 &\cdots  & 0 \end{bmatrix}. \\
		 		&\hspace{3cm} \uparrow i
		 		\end{align*}
		 		Es decir, la matriz fila formada por los cofactores correspondientes a la columna  $i$ multiplicada por la matriz $A$ es igual a la matriz fila con valor $\det A$ en la posición $i$ y 0 en las otras posiciones.	 
		 	\end{teorema}
		 	\begin{proof}
		 		Si $C_j$ denota la matriz formada por la columna $j$ de $A$ debemos probar que
		 		$$
		 		\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix} C_j = \sum_{k=1}^{n} a_{kj}C_{ki} = \left\{ \begin{matrix*}[l] \det(A) &\text{si $j = i$}  \\ 0 &\text{si $j \ne i$.}\end{matrix*}\right.
		 		$$ 
		 		Ahora bien,
		 		\begin{align*}
		 		\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix} C_i = \sum_{j=1}^{n} C_{ji}a_{ji},
		 		\end{align*}
		 		y esto último no es más que el cálculo del determinante por desarrollo de la  columna $i$,  es decir,  es igual a $\det(A)$.
		 		
		 		Para ver el caso $i \ne j$, primero observemos que si 
		 		\begin{align*}
		 		B &= \begin{bmatrix} C_1 & C_2&\cdots &C_j& \cdots &C_j & \cdots &C_{n-1}& C_{n}\end{bmatrix}, \\
		 		&\hspace{3.2cm} \uparrow i \hspace{1.2cm} \uparrow j
		 		\end{align*}
		 		es decir, $B$ es la matriz $A$ donde reemplazamos la columna $i$ por la columna $j$, entonces como $B$ tiene dos columnas iguales, $\det(B) =0$. Por lo tanto, si calculamos el determinante de $B$ por el desarrollo en la columna $i$, obtenemos
		 		\begin{equation} \label{eq-cof}
		 		0 = \det(B) = \sum_{k=1}^{n} a_{kj}C_{ki}.
		 		\end{equation}
		 		Por otro lado, 
		 		\begin{align*}
		 		\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix} C_j = \sum_{k=1}^{n} C_{ki}a_{kj},
		 		\end{align*}
		 		luego, por la ecuación (\ref{eq-cof}) tenemos que 
		 		$$
		 		\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix} C_j =0
		 		$$
		 		si $i \ne j$.
		 		
		 	\end{proof}
		 	
		 	\begin{definicion} Sea $A$ matriz $n \times n$, la \textit{matriz de cofactores}\index{matriz!de cofactores} es la matriz cuyo coeficiente $ij$ vale $C_{ij}$. La matriz de cofactores de $A$  se denota $\operatorname{cof}(A)$. La \textit{matriz adjunta de A} es $\operatorname{adj}(A) = \operatorname{cof}(A)^\t$.
		 	\end{definicion}
		 	
		 	\begin{teorema} Sea $A$ matriz $n \times n$,  entonces
		 		\begin{equation*}
		 		\operatorname{adj}(A).A = \det(A)I_n.
		 		\end{equation*}
		 	\end{teorema}
		 	\begin{proof}
		 		Observar que la fila $i$ de $\operatorname{adj}(A)$ es $\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix}$. Por lo tanto, la fila $i$ de $\operatorname{adj}(A).A$ es
		 		$$
		 		\begin{bmatrix} C_{1i} & C_{2i} & \cdots & C_{ni}\end{bmatrix} A,
		 		$$ 
		 		que por el teorema \ref{th-cof-01} es una matriz fila con el valor $\det A$ en la posición $i$ y todos los demás coeficientes iguales a 0. Luego
		 		$$
		 		\operatorname{adj}(A).A = 
		 		\begin{bmatrix} C_{11} & C_{21} & \cdots & C_{n1}\\
		 		C_{12} & C_{22} & \cdots & C_{n2} \\
		 		\vdots & \vdots & \ddots & \vdots \\
		 		C_{1n} & C_{2n} & \cdots & C_{nn}\end{bmatrix}. A = 
		 		\begin{bmatrix}\det A & 0 & \cdots & 0\\
		 		0 & \det A & \cdots & 0 \\
		 		\vdots & \vdots & \ddots & \vdots \\
		 		0 & 0 & \cdots & \det A\end{bmatrix} = \det(A) I_n
		 		$$
		 	\end{proof}
		 	
		 	\begin{corolario}\label{cor-inv-x-det}
		 		Si $A$  es invertible,  entonces 
		 		\begin{equation*}
		 		A^{-1} = \frac{1}{\det A} \operatorname{adj}A.
		 		\end{equation*}
		 	\end{corolario}
		 	\begin{proof}
		 		\begin{equation*}
		 		\frac{1}{\det A} \operatorname{adj}A.A = \frac{1}{\det A}\det A I_n = I_n.
		 		\end{equation*}
		 	\end{proof}
		 	
		 	
		 	
		 	\begin{teorema}[Regla de Cramer]\index{regla de Cramer} Sea $AX=Y$ un sistema de ecuaciones tal que $A \in M_n(\K)$ es invertible. Entonces, el sistema tiene una única solución $(x_1,\ldots,,x_n)$ con 
		 		\begin{equation*}
		 		x_j = \frac{\det A_j}{\det A},\qquad j=1,\ldots,n,
		 		\end{equation*}
		 		donde $A_j$ es la matriz $n \times n$ que se obtiene de $A$ remplazando la columna $j$ de
		 		$A$ por $Y$.
		 	\end{teorema}
		 	\begin{proof}
		 		Haremos la demostración para matrices $3 \times 3$. La demostración en el caso general es completamente análoga. 
		 		
		 		Como $A$  es invertible, existe $A^{-1}$ y multiplicamos la ecuación a izquierda por $A^{-1}$ y obtenemos que $A^{-1}A X = A^{-1}Y$,  es decir $X = A^{-1}Y$ y esta es la única solución. Luego
		 		\begin{align*}
		 		A^{-1}Y & = \frac{1}{\det A}
		 		\begin{bmatrix} C_{11} & C_{21} & C_{31}\\
		 		C_{12} & C_{22} &  C_{32} \\
		 		C_{13} & C_{23} & C_{33}\end{bmatrix}
		 		\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix} \\
		 		&= \frac{1}{\det A}\begin{bmatrix} y_1C_{11}+  y_2C_{21} + y_3C_{31}\\
		 		y_1C_{12}+  y_2C_{22}+   y_3C_{32} \\
		 		y_1C_{13}+  y_2C_{23}+  y_3C_{33}\end{bmatrix} \tag{$*$}
		 		\end{align*}
		 		Ahora bien,  $y_1C_{11}+  y_2C_{21}+   y_3C_{31}$ es el cálculo de determinante por desarrollo de la primera columna de la matriz 
		 		$$
		 		\begin{bmatrix}
		 		y_1 & a_{12} & a_{13} \\y_2 & a_{22} & a_{23} \\y_1 & a_{32} & a_{33} 
		 		\end{bmatrix},
		 		$$
		 		y,  de forma análoga, el segundo y tercer coeficiente de la matriz ($*$) son el determinante de las matrices $3 \times 3$ que se obtienen de $A$ remplazando la columna $2$ y $3$, respectivamente, de
		 		$A$ por $Y$. Es decir
		 		\begin{align*}
		 		\begin{bmatrix} x_1\\
		 		x_2 \\
		 		x_3\end{bmatrix} = A^{-1}Y & = \frac{1}{\det A}\begin{bmatrix} \det A_1\\
		 		\det A_2 \\
		 		\det A_3 \end{bmatrix} = \begin{bmatrix} \frac{\det A_1}{\det A}\\
		 		\frac{\det A_2}{\det A} \\
		 		\frac{\det A_3}{\det A}\end{bmatrix},
		 		\end{align*}
		 		luego $x_j = \displaystyle\frac{\det A_j}{\det A}$ para $j =1,2,3$.
		 	\end{proof}
		 	
		 	\begin{ejemplo}
		 		Resolvamos usando la regla de Cramer el siguiente sistema:
		 		\begin{align*}
		 		x_1 + x_2 - x_3 &= 6 \\
		 		3x_1 -2 x_2 + x_3 &= -5 \\ 
		 		x_1 +3 x_2 - 2x_3 &= 14.
		 		\end{align*}
		 		La matriz asociada al sistema es 
		 		$$
		 		A= \begin{bmatrix}
		 		1 &1  &-1 \\
		 		3 &-2  &1 \\ 
		 		1 &3 &-2
		 		\end{bmatrix}.
		 		$$
		 		Luego 
		 		$$
		 		A_1 = \begin{bmatrix}
		 		6 &1  &-1 \\
		 		-5 &-2  &1 \\ 
		 		14 &3 &-2
		 		\end{bmatrix}, \qquad
		 		A_2 = \begin{bmatrix}
		 		1 &6  &-1 \\
		 		3 &-5  &1 \\ 
		 		1 &14 &-2
		 		\end{bmatrix}, \qquad
		 		A_2 = \begin{bmatrix}
		 		1 &1  &6 \\
		 		3 &-2  &-5 \\ 
		 		1 &3 &14
		 		\end{bmatrix}, \qquad
		 		$$
		 		y
		 		$$
		 		\det A = -3,\qquad\det A_1 = -3,\qquad \det A_2 = -9,\qquad \det A_3=6.
		 		$$
		 		Por lo tanto,
		 		\begin{align*}
		 		x_1 &= \frac{\det A_1}{\det A} = \frac{-3}{-3} = 1 \\
		 		x_2 &= \frac{\det A_2}{\det A} = \frac{-9}{-3} = 3\\
		 		x_3 &= \frac{\det A_3}{\det A} = \frac{6}{-3} = -2.
		 		\end{align*} 
		 	\end{ejemplo}
		 	
		 	\vskip .5cm
		 	
		 	\begin{observacion}
		 		En  general, la regla de Cramer no es utilizada para resolver sistemas  de ecuaciones. El  método de Gauss, además de ser mucho más rápido, tiene el beneficio de ser un método que abarca más casos y nos da más información que la regla de Cramer. Por ejemplo,  el método de Gauss también funciona para matrices no invertibles, mientras que la regla de Cramer no. Más aún, el método de Gauss se puede utilizar para resolver sistemas con matrices no cuadradas (es decir, sistemas de ecuaciones con números no iguales de variables y ecuaciones), mientras que la regla de Cramer no.
		 		
		 		Con respecto a la velocidad  de resolución, para matrices $n \times n$ el método de Gauss requiere ``alrededor de $n^3$ operaciones'', mientras que la regla de Cramer requiere ``alrededor de $n^4$ operaciones'', haciendo el primer método $n$ veces más rápido que el segundo. En  lenguaje técnico, el método de Gauss tiene complejidad $O(n^3)$ y la regla de Cramer complejidad $O(n^4)$ (ver
		 		\href{ https://es.wikipedia.org/wiki/Eficiencia\_Algorítmica}{ https://es.wikipedia.org/wiki/Eficiencia\_Algorítmica}) 
		 	\end{observacion}
		 	
		 \end{section}
	\end{chapter}
	

\printindex




\end{document}
%configuration={"latex_command":"latexmk -pdf -f -g -bibtex -synctex=1 -interaction=nonstopmode 'teoricoAlgII.tex'"}}